{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm, tqdm_notebook, tnrange\n",
    "from S3_read_write import load_df_s3, save_df_s3\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas('Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_name = 'amazon-reviews-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off using only the title (`summary`) and body (`reviewText`) of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 2.01 s, total: 4.17 s\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = load_df_s3(bucket_name, filepath='amazon_reviews/data_clean_v3', filetype='feather')\n",
    "\n",
    "# df = load_df_s3(bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                 int64\n",
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "cat1                 object\n",
       "cat2                 object\n",
       "cat3                 object\n",
       "cat4                 object\n",
       "cat5                 object\n",
       "cat6                 object\n",
       "cat7                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[:, ['asin', 'reviewText', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217530, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I started taking this after both my parents di...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I really liked this product because it stayed ...</td>\n",
       "      <td>I can't find this product any longer, and I wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Resveratrol is a polar compound, very insolubl...</td>\n",
       "      <td>Just the Resveratrol product we need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>I bought several of these bracelets for my YMC...</td>\n",
       "      <td>The kids love these bracelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>I bought a few the other week just to see what...</td>\n",
       "      <td>Pleasant Surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                         reviewText  \\\n",
       "0  0978559088  I started taking this after both my parents di...   \n",
       "1  0978559088  I really liked this product because it stayed ...   \n",
       "2  0978559088  Resveratrol is a polar compound, very insolubl...   \n",
       "3  1427600228  I bought several of these bracelets for my YMC...   \n",
       "4  1427600228  I bought a few the other week just to see what...   \n",
       "\n",
       "                                             summary  \n",
       "0                         Bioavailability is the key  \n",
       "1  I can't find this product any longer, and I wi...  \n",
       "2               Just the Resveratrol product we need  \n",
       "3                      The kids love these bracelets  \n",
       "4                                  Pleasant Surprise  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each review, concatenate the review title and body\n",
    "df.reviewText = df.summary + '. ' + df.reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...</td>\n",
       "      <td>Just the Resveratrol product we need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...</td>\n",
       "      <td>The kids love these bracelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...</td>\n",
       "      <td>Pleasant Surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0978559088   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  1427600228   \n",
       "4  1427600228   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \\\n",
       "0  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...   \n",
       "1  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...   \n",
       "2  Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...   \n",
       "3  The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...   \n",
       "4  Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...   \n",
       "\n",
       "                                                     summary  \n",
       "0                                 Bioavailability is the key  \n",
       "1  I can't find this product any longer, and I wish I could.  \n",
       "2                       Just the Resveratrol product we need  \n",
       "3                              The kids love these bracelets  \n",
       "4                                          Pleasant Surprise  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `summary` column now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0978559088   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  1427600228   \n",
       "4  1427600228   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \n",
       "0  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...  \n",
       "1  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...  \n",
       "2  Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...  \n",
       "3  The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...  \n",
       "4  Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.isnull().sum()    # 73 reviews have neither a review body text, nor a review title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few actual review texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great product. A friend told me about this and I love it. It works just like it says. Highly recommend this product.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes! It works fast and lowers your BP withi a month.. It works...it works...lowered my blood pressure by 30 points. I take 2 pills in the AM and 2 in the afternoon. My BP was high and now it is borderline.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love it!. I am very sensitive to most things. I can't drink coffee, I can't drink soda, I can't drink most green teas. However with this, I can have just one teaspoon of the Matcha Green Tea with water and I feel a gradual alertness that stays for quite some time with no crashing. I also find it curbs my appetite. Love it!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217530 entries, 0 to 217529\n",
      "Data columns (total 2 columns):\n",
      "asin          217530 non-null object\n",
      "reviewText    217530 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = list(df.reviewText.values)    # make an iterable to store only the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent for sent in text if len(sent) == 0]   # there are no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspiring. Doing some research on the Internet, it is indicated that taking resveratrol in lozenge form is preferable as it is broken down by stomach acids.  The ez-melt formula recommended in another review is OK, but it is dissolved in the mouth much more quickly than this lozenge formula, while dissolving more slowly is preferable according to my research.This product has the greatest side effect - since taking it, I haven't had colds or sore throats.  Soon after starting to take it every day, I was starting to come down with a cold, with all my usual symptoms, and was anticipating being very sick the next day, as is my usual pattern.  But I never did get as sick as anticipated - taking this product is the only reason I can come up with.  Since then, I've had no colds or sore throats - it has been great.  I recommend this product to everyone I know, and have given it as gifts to my family. \n",
      "\n",
      "I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of resveratrol products and this was one of my favorites.  I can't find it anymore and wish I could still find it.When I contacted the company, they said there was no date for re-introduction of this product, and they implied that it would no longer be produced.  I also found someplace online where the product manufacturer was trying to find someone to market it for them.  I hope this happens.  I really enjoyed taking these little pills and felt they were doing some good.  The claims on their website made sense, and I believe that resveratrol is almost a wonder drug/suppliment.  (The 60 Minutes Episode convinced me of that.) \n",
      "\n",
      "Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in the mouth and be slowly dissolved.  The lozenge would have to contain enough Resveratrol since most of the Resveratrol will be swallowed with the saliva.  A better known and competing product contains only about a milligram of Resveratrol.  These lozenges contain 20 milligrams.  Nutrihill Resveratrol lozenges dissolve very slowly and contain, IMO, just the right amount of Resveratrol to optimize absorption.  Most of the product will be swallowed but that's OK as just fractions of a milligram need to be absorbed though the mouth's mucosa to produce sustained high levels of Resveratrol in the blood and be absorbed into the cells to do some real good.  I can see where many producers of Resveratrol products like, for example Revgenetics, would not want this product to catch on as their selling point is how well they try to increase absorption of Resveratrol in the digestive system.  Taking Quercetin helps protect Resveratrol when swallowed from first pass liver metabolism, but at an additional financial cost which makes suppliers happy because they can sell more supplements. An additional cost of taking Quercetin with Resveratrol is joint inflammation.I hope this product catches on so it's available from many sellers.Nutrihill Resveratrol Lozenges \n",
      "\n",
      "The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds confidence in kids/teens. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at a few sample reviews\n",
    "for rev in text[:4]:\n",
    "    print(rev, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below are from:\n",
    "\n",
    "http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `gensim`'s `Phrases` class to detect natural combinations of words (like 'vanilla ice cream'), we need to format our text into a list of sentences, with each sentence being a list of words.  This process takes a large amount of processing time (for reference, the times shown under the cells are for running the tasks on a c5.18xlarge EC2 instance (equivalent spot fleet)), so `text` has been split into 3 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Unigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split text into 9 parts\n",
    "text_first  = text[:50000]\n",
    "text_second = text[50000:100000]\n",
    "text_third  = text[100000:150000]\n",
    "text_fourth = text[150000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:56, 120.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  50000\n",
      "current sent_num:  289565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rev_num = 0    # review tracker\n",
    "sent_num = 0   # sentence tracker\n",
    "unigram_sents_pos = [] # to store lists of lemmatized tokens for each sentence\n",
    "\n",
    "for parsed_review in tqdm(nlp.pipe(text_first, batch_size=10000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289158"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_sents_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, [('bioavailability', 'PROPN'), ('be', 'VERB'), ('the', 'DET'), ('key', 'NOUN')]]\n",
      "[1, 2, [('-PRON-', 'PRON'), ('start', 'VERB'), ('take', 'VERB'), ('this', 'DET'), ('after', 'ADP'), ('both', 'CCONJ'), ('-PRON-', 'ADJ'), ('parent', 'NOUN'), ('die', 'VERB'), ('of', 'ADP'), ('cancer', 'NOUN'), ('as', 'ADP'), ('-PRON-', 'PRON'), ('suppose', 'VERB'), ('to', 'PART'), ('enhance', 'VERB'), ('-PRON-', 'ADJ'), ('immune', 'ADJ'), ('system', 'NOUN'), ('the', 'DET'), ('story', 'NOUN'), ('on', 'ADP'), ('60', 'NUM'), ('minutes', 'PROPN'), ('on', 'ADP'), ('resveratrol', 'NOUN'), ('be', 'VERB'), ('incredibly', 'ADV'), ('inspiring', 'ADJ')]]\n",
      "[1, 3, [('do', 'VERB'), ('some', 'DET'), ('research', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('internet', 'NOUN'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('indicate', 'VERB'), ('that', 'ADP'), ('take', 'VERB'), ('resveratrol', 'NOUN'), ('in', 'ADP'), ('lozenge', 'NOUN'), ('form', 'NOUN'), ('be', 'VERB'), ('preferable', 'ADJ'), ('as', 'ADP'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('break', 'VERB'), ('down', 'PART'), ('by', 'ADP'), ('stomach', 'NOUN'), ('acid', 'NOUN')]]\n",
      "[1, 4, [('the', 'DET'), ('ez', 'ADP'), ('melt', 'NOUN'), ('formula', 'NOUN'), ('recommend', 'VERB'), ('in', 'ADP'), ('another', 'DET'), ('review', 'NOUN'), ('be', 'VERB'), ('ok', 'ADJ'), ('but', 'CCONJ'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('dissolve', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('mouth', 'NOUN'), ('much', 'ADV'), ('more', 'ADV'), ('quickly', 'ADV'), ('than', 'ADP'), ('this', 'DET'), ('lozenge', 'NOUN'), ('formula', 'NOUN'), ('while', 'ADP'), ('dissolve', 'VERB'), ('more', 'ADV'), ('slowly', 'ADV'), ('be', 'VERB'), ('preferable', 'ADJ'), ('accord', 'VERB'), ('to', 'ADP'), ('-PRON-', 'ADJ'), ('research', 'NOUN')]]\n",
      "[1, 5, [('this', 'DET'), ('product', 'NOUN'), ('have', 'VERB'), ('the', 'DET'), ('great', 'ADJ'), ('side', 'NOUN'), ('effect', 'NOUN'), ('since', 'ADP'), ('take', 'VERB'), ('-PRON-', 'PRON'), ('-PRON-', 'PRON'), ('have', 'VERB'), ('not', 'ADV'), ('have', 'VERB'), ('cold', 'NOUN'), ('or', 'CCONJ'), ('sore', 'ADJ'), ('throat', 'NOUN')]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(unigram_sents_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if there are any blank sentences\n",
    "for sent in unigram_sents_pos:\n",
    "    if len(sent[2]) == 0:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:55, 120.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  100000\n",
      "current sent_num:  576892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_second, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576034\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_sents_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:27, 129.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  150000\n",
      "current sent_num:  855567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_third, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67530it [09:13, 122.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  217530\n",
      "current sent_num:  1243596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fourth, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DON'T LOAD THIS FILE - there's a _v1 version further down!\n",
    "# del unigram_sentences_savedf\n",
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number          unigram_pos    unigram_sentences\n",
       "0              1                1  PROPN+-+||+-+VER...  bioavailability+...\n",
       "1              1                2  PRON+-+||+-+VERB...  -PRON-+-+||+-+st...\n",
       "2              1                3  VERB+-+||+-+DET+...  do+-+||+-+some+-...\n",
       "3              1                4  DET+-+||+-+ADP+-...  the+-+||+-+ez+-+...\n",
       "4              1                5  DET+-+||+-+NOUN+...  this+-+||+-+prod..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_number, sentence_number, unigram_pos, unigram_sentences]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].head()  # no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up(sentence, sentence_pos, sep):\n",
    "    \"\"\"Expects a sentence as a single string as input 1, and its corresponding part-of-speech tags as input 2 (also single string).\n",
    "    sep is the string pattern used to separate words in each sentence string\n",
    "    Cleans it up and returns a single string.\n",
    "    Also updates corresponding part-of-speech string.\n",
    "    \"\"\"\n",
    "    # get rid of webpage links\n",
    "    cond = ['http' in sentence, 'www' in sentence]\n",
    "    if any(cond):\n",
    "        words = sentence.split(sep)\n",
    "        words_pos = sentence_pos.split(sep)\n",
    "        to_remove = []\n",
    "        for i in range(len(words)):\n",
    "            cond_word = ['http' in words[i], 'www' in words[i]]\n",
    "            if any(cond_word):\n",
    "                to_remove.append(i)\n",
    "        # remove words that are links\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del words[j]\n",
    "            del words_pos[j]\n",
    "        # reconstruct sentence after deleting links\n",
    "        sentence = sep.join(words)\n",
    "        sentence_pos = sep.join(words_pos)\n",
    "\n",
    "    # replace underscores with blanks to avoid mix-up with paired words later\n",
    "    # cannot replace with spaces because the strings are split on spaces later \n",
    "    # and this would create new words with no corresponding pos tags\n",
    "    if '_' in sentence:\n",
    "        sentence = sentence.replace('_', '')\n",
    "    return sentence, sentence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!',\n",
       " 'this is a normal sentence',\n",
       " '__ what is this ____ http',\n",
       " '_',\n",
       " 'http']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = ['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!', 'this is a normal sentence', \n",
    "              '__ what is this ____ http', '_', 'http']\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clean_pos = ['X X X X X X X X X X X X', 'X X X X X', 'X X X X X X', 'X', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy and also BAM! underscoretime!',\n",
       " 'this is a normal sentence',\n",
       " ' what is this ']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if clean_up works as expected\n",
    "to_remove = []\n",
    "for i in range(len(test_clean)):\n",
    "    sentence = test_clean[i]\n",
    "    sentence_pos = test_clean_pos[i]\n",
    "    test_clean[i], test_clean_pos[i] = clean_up(sentence, sentence_pos, sep=' ')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if test_clean[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del test_clean[j]\n",
    "    del test_clean_pos[j]\n",
    "\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X X X X X X X X X X X', 'X X X X X', 'X X X X X']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_joined_all = unigram_sentences_savedf.unigram_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241850"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if '_' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'http' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'www' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>607</td>\n",
       "      <td>3386</td>\n",
       "      <td>X</td>\n",
       "      <td>http://www.amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8051</th>\n",
       "      <td>1454</td>\n",
       "      <td>8071</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>no+-+||+-+jet_la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12199</th>\n",
       "      <td>2166</td>\n",
       "      <td>12224</td>\n",
       "      <td>ADJ+-+||+-+PART+...</td>\n",
       "      <td>easy+-+||+-+to+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12201</th>\n",
       "      <td>2166</td>\n",
       "      <td>12226</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18188</th>\n",
       "      <td>3106</td>\n",
       "      <td>18214</td>\n",
       "      <td>ADV+-+||+-+ADJ+-...</td>\n",
       "      <td>overall+-+||+-+-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_number  sentence_number          unigram_pos  \\\n",
       "3375             607             3386                    X   \n",
       "8051            1454             8071  DET+-+||+-+NOUN+...   \n",
       "12199           2166            12224  ADJ+-+||+-+PART+...   \n",
       "12201           2166            12226  PRON+-+||+-+VERB...   \n",
       "18188           3106            18214  ADV+-+||+-+ADJ+-...   \n",
       "\n",
       "         unigram_sentences  \n",
       "3375   http://www.amazo...  \n",
       "8051   no+-+||+-+jet_la...  \n",
       "12199  easy+-+||+-+to+-...  \n",
       "12201  -PRON-+-+||+-+ha...  \n",
       "18188  overall+-+||+-+-...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences.str.contains('_')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.amazon.com/gp/product/b0000533z8/ref=cm_cr_rev_prod_title',\n",
       " 'no+-+||+-+jet_lag+-+||+-+pill',\n",
       " 'easy+-+||+-+to+-+||+-+use_work+-+||+-+well',\n",
       " '-PRON-+-+||+-+have+-+||+-+have+-+||+-+pedometer+-+||+-+in+-+||+-+the+-+||+-+past_all+-+||+-+difficult+-+||+-+and+-+||+-+confusing+-+||+-+to+-+||+-+use+-+||+-+to+-+||+-+the+-+||+-+point+-+||+-+-PRON-+-+||+-+simply+-+||+-+give+-+||+-+up+-+||+-+on+-+||+-+-PRON-',\n",
       " 'overall+-+||+-+-PRON-+-+||+-+mother+-+||+-+be+-+||+-+very+-+||+-+satisfied+-+||+-+with+-+||+-+this+-+||+-+product!-d_lionz',\n",
       " 'this+-+||+-+inexpensive+-+||+-+strap+-+||+-+with+-+||+-+a+-+||+-+metal+-+||+-+clip+-+||+-+http://www.amazon.com/gp/product/b000bitymg/ref=oh_details_o00_s00_i00?ie=utf8&psc;=1+-+||+-+be+-+||+-+a+-+||+-+good+-+||+-+replacement+-+||+-+for+-+||+-+the+-+||+-+flimsy+-+||+-+omron+-+||+-+plastic+-+||+-+clip+-+||+-+but+-+||+-+-PRON-+-+||+-+have+-+||+-+not+-+||+-+be+-+||+-+use+-+||+-+-PRON-+-+||+-+long',\n",
       " 'hj_112+-+||+-+digital+-+||+-+pemium+-+||+-+pedometer+-+||+-+update',\n",
       " 'at+-+||+-+the+-+||+-+end+-+||+-+of+-+||+-+the+-+||+-+2+-+||+-+week+-+||+-+i+-+||+-+get+-+||+-+retest+-+||+-+-PRON-+-+||+-+heamoglobin+-+||+-+be+-+||+-+a+-+||+-+14.2+-+||+-+and+-+||+-+hematocrit+-+||+-+be+-+||+-+a+-+||+-+wopp+-+||+-+44+-+||+-+o_o.',\n",
       " 'and+-+||+-+-PRON-+-+||+-+easy+-+||+-+to+-+||+-+cover+-+||+-+-PRON-+-+||+-+up+-+||+-+with+-+||+-+a+-+||+-+little+-+||+-+orange_pineapple+-+||+-+juice',\n",
       " 'this+-+||+-+work+-+||+-+excellent+-+||+-+for+-+||+-+calm+-+||+-+the+-+||+-+nerve+-+||+-+and+-+||+-+to+-+||+-+ease+-+||+-+stress+-+||+-+^_^']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence for sentence in words_joined_all if '_' in sentence][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up all unigrams\n",
    "to_remove = []\n",
    "for i in range(len(words_joined_all)):\n",
    "    sentence = words_joined_all[i]\n",
    "    sentence_pos = pos_joined_all[i]\n",
    "    words_joined_all[i], pos_joined_all[i] = clean_up(sentence, sentence_pos, sep='+-+||+-+')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if words_joined_all[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del words_joined_all[j]\n",
    "    del pos_joined_all[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows from unigram_sentences_savedf corresponding to the row numbers (indices) of sentences\n",
    "# that will be blank after the transformation above\n",
    "unigram_sentences_savedf.drop(unigram_sentences_savedf.index[to_remove], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf.drop(['unigram_sentences'], axis=1, inplace=True)\n",
    "unigram_sentences_savedf.drop(['unigram_pos'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf['unigram_sentences'] = words_joined_all\n",
    "unigram_sentences_savedf['unigram_pos'] = pos_joined_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...\n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...\n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...\n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...\n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updated, cleaned up version of unigram_sentences.feather\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = [sentence.split('+-+||+-+') for sentence in words_joined_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bioavailability', 'be', 'the', 'key'], ['-PRON-', 'start', 'take', 'this', 'after', 'both', '-PRON-', 'parent', 'die', 'of', 'cancer', 'as', '-PRON-', 'suppose', 'to', 'enhance', '-PRON-', 'immune', 'system', 'the', 'story', 'on', '60', 'minutes', 'on', 'resveratrol', 'be', 'incredibly', 'inspiring'], ['do', 'some', 'research', 'on', 'the', 'internet', '-PRON-', 'be', 'indicate', 'that', 'take', 'resveratrol', 'in', 'lozenge', 'form', 'be', 'preferable', 'as', '-PRON-', 'be', 'break', 'down', 'by', 'stomach', 'acid'], ['the', 'ez', 'melt', 'formula', 'recommend', 'in', 'another', 'review', 'be', 'ok', 'but', '-PRON-', 'be', 'dissolve', 'in', 'the', 'mouth', 'much', 'more', 'quickly', 'than', 'this', 'lozenge', 'formula', 'while', 'dissolve', 'more', 'slowly', 'be', 'preferable', 'accord', 'to', '-PRON-', 'research']]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sentences[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241826"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 1.12 s, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The common_terms parameter add a way to give special treatment to common terms \n",
    "# (aka stop words) such that their presence between two words wont prevent bigram detection. \n",
    "# It allows to detect expressions like bank of america\n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\"]\n",
    "\n",
    "# Train a first-order phrase detector\n",
    "bigram_model = Phrases(unigram_sentences, threshold=0.6, scoring='npmi', common_terms=common_terms)\n",
    "\n",
    "# Transform unigram sentences into bigram sentences\n",
    "# Paired words are connected by an underscore, e.g. ice_cream\n",
    "bigram_sentences = []\n",
    "for sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 1.29 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a second-order phrase detector\n",
    "# trigram_model = Phrases(bigram_sentences, min_count=5)\n",
    "trigram_model = Phrases(bigram_sentences, threshold=0.5, scoring='npmi')\n",
    "\n",
    "# Transform bigram sentences into trigram sentences\n",
    "trigram_sentences = []\n",
    "for sentence in bigram_sentences:\n",
    "    trigram_sentences.append(trigram_model[sentence])\n",
    "\n",
    "# remove any remaining stopwords\n",
    "# trigram_sentences = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in trigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the trigrams will be saved in a dataframe with a single column.\n",
    "# each row is one sentence from any review\n",
    "# each sentence is a single string separated by a single space.\n",
    "trigram_sentences_savedf = pd.DataFrame([u'+-+||+-+'.join(sentence) for sentence in trigram_sentences], columns=['preprocessed_review'])\n",
    "save_df_s3(trigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/preprocessed_reviews.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/preprocessed_reviews.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this_product+-+|...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preprocessed_review\n",
       "0  bioavailability+...\n",
       "1  -PRON-+-+||+-+st...\n",
       "2  do+-+||+-+some+-...\n",
       "3  the+-+||+-+ez+-+...\n",
       "4  this_product+-+|..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del unigram_sentences_savedf\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...\n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...\n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...\n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...\n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df[unigram_sents_pos_df.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = pd.merge(unigram_sents_pos_df, trigram_sentences_savedf, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this_product+-+|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>soon+-+||+-+afte...</td>\n",
       "      <td>ADV+-+||+-+ADP+-...</td>\n",
       "      <td>soon+-+||+-+afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "      <td>CCONJ+-+||+-+PRO...</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>since+-+||+-+the...</td>\n",
       "      <td>ADP+-+||+-+ADV+-...</td>\n",
       "      <td>since+-+||+-+the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-PRON-+-+||+-+re...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...   \n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...   \n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...   \n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...   \n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+...   \n",
       "5              1                6  soon+-+||+-+afte...  ADV+-+||+-+ADP+-...   \n",
       "6              1                7  but+-+||+-+-PRON...  CCONJ+-+||+-+PRO...   \n",
       "7              1                8  since+-+||+-+the...  ADP+-+||+-+ADV+-...   \n",
       "8              1                9  -PRON-+-+||+-+re...  PRON+-+||+-+VERB...   \n",
       "9              2               10  -PRON-+-+||+-+ca...  PRON+-+||+-+VERB...   \n",
       "\n",
       "   preprocessed_review  \n",
       "0  bioavailability+...  \n",
       "1  -PRON-+-+||+-+st...  \n",
       "2  do+-+||+-+some+-...  \n",
       "3  the+-+||+-+ez+-+...  \n",
       "4  this_product+-+|...  \n",
       "5  soon+-+||+-+afte...  \n",
       "6  but+-+||+-+-PRON...  \n",
       "7  since+-+||+-+the...  \n",
       "8  -PRON-+-+||+-+re...  \n",
       "9  -PRON-+-+||+-+ca...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(unigram_sents_pos_df, bucket_name, 'amazon_reviews/kk/preprocessed_reviews_v1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/kk/preprocessed_reviews_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>73</td>\n",
       "      <td>401</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>73</td>\n",
       "      <td>402</td>\n",
       "      <td>so+-+||+-+just+-...</td>\n",
       "      <td>ADV+-+||+-+ADV+-...</td>\n",
       "      <td>so+-+||+-+just+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>73</td>\n",
       "      <td>403</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>73</td>\n",
       "      <td>404</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>73</td>\n",
       "      <td>405</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "      <td>CCONJ+-+||+-+PRO...</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>74</td>\n",
       "      <td>406</td>\n",
       "      <td>excellent+-+||+-...</td>\n",
       "      <td>INTJ+-+||+-+PRON...</td>\n",
       "      <td>excellent+-+||+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>74</td>\n",
       "      <td>407</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>74</td>\n",
       "      <td>408</td>\n",
       "      <td>-PRON-+-+||+-+ac...</td>\n",
       "      <td>PRON+-+||+-+ADV+...</td>\n",
       "      <td>-PRON-+-+||+-+ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>74</td>\n",
       "      <td>409</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>74</td>\n",
       "      <td>410</td>\n",
       "      <td>one+-+||+-+pill+...</td>\n",
       "      <td>NUM+-+||+-+NOUN+...</td>\n",
       "      <td>one+-+||+-+pill+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             73              401  -PRON-+-+||+-+ha...  PRON+-+||+-+VERB...   \n",
       "401             73              402  so+-+||+-+just+-...  ADV+-+||+-+ADV+-...   \n",
       "402             73              403  -PRON-+-+||+-+ca...  PRON+-+||+-+VERB...   \n",
       "403             73              404  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "404             73              405  but+-+||+-+-PRON...  CCONJ+-+||+-+PRO...   \n",
       "405             74              406  excellent+-+||+-...  INTJ+-+||+-+PRON...   \n",
       "406             74              407  -PRON-+-+||+-+ha...  PRON+-+||+-+VERB...   \n",
       "407             74              408  -PRON-+-+||+-+ac...  PRON+-+||+-+ADV+...   \n",
       "408             74              409  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...   \n",
       "409             74              410  one+-+||+-+pill+...  NUM+-+||+-+NOUN+...   \n",
       "\n",
       "     preprocessed_review  \n",
       "400  -PRON-+-+||+-+ha...  \n",
       "401  so+-+||+-+just+-...  \n",
       "402  -PRON-+-+||+-+ca...  \n",
       "403  -PRON-+-+||+-+do...  \n",
       "404  but+-+||+-+-PRON...  \n",
       "405  excellent+-+||+-...  \n",
       "406  -PRON-+-+||+-+ha...  \n",
       "407  -PRON-+-+||+-+ac...  \n",
       "408  -PRON-+-+||+-+be...  \n",
       "409  one+-+||+-+pill+...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df['has_paired_words'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df.loc[unigram_sents_pos_df.preprocessed_review.str.contains('_'), ['has_paired_words']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532356"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.has_paired_words.sum()  # number of sentences with paired words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this_product+-+|...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...   \n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...   \n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...   \n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...   \n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  bioavailability+...                 0  \n",
       "1  -PRON-+-+||+-+st...                 1  \n",
       "2  do+-+||+-+some+-...                 1  \n",
       "3  the+-+||+-+ez+-+...                 0  \n",
       "4  this_product+-+|...                 1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 2.31 s, total: 15.8 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigram_sents_pos_df.unigram_pos = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_pos.tolist()]\n",
    "unigram_sents_pos_df.unigram_sentences = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_sentences.tolist()]\n",
    "unigram_sents_pos_df.preprocessed_review = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.preprocessed_review.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>73</td>\n",
       "      <td>401</td>\n",
       "      <td>[-PRON-, have, u...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, have, u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>73</td>\n",
       "      <td>402</td>\n",
       "      <td>[so, just, take,...</td>\n",
       "      <td>[ADV, ADV, VERB,...</td>\n",
       "      <td>[so, just, take,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>73</td>\n",
       "      <td>403</td>\n",
       "      <td>[-PRON-, can, no...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, can_not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>73</td>\n",
       "      <td>404</td>\n",
       "      <td>[-PRON-, do, not...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, do_not,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>73</td>\n",
       "      <td>405</td>\n",
       "      <td>[but, -PRON-, be...</td>\n",
       "      <td>[CCONJ, PRON, VE...</td>\n",
       "      <td>[but, -PRON-, be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>74</td>\n",
       "      <td>406</td>\n",
       "      <td>[excellent, prod...</td>\n",
       "      <td>[INTJ, PRON, ADJ...</td>\n",
       "      <td>[excellent, prod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>74</td>\n",
       "      <td>407</td>\n",
       "      <td>[-PRON-, have, t...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, have, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>74</td>\n",
       "      <td>408</td>\n",
       "      <td>[-PRON-, actuall...</td>\n",
       "      <td>[PRON, ADV, VERB...</td>\n",
       "      <td>[-PRON-, actuall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>74</td>\n",
       "      <td>409</td>\n",
       "      <td>[-PRON-, be, con...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, be, con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>74</td>\n",
       "      <td>410</td>\n",
       "      <td>[one, pill, seem...</td>\n",
       "      <td>[NUM, NOUN, VERB...</td>\n",
       "      <td>[one, pill, seem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             73              401  [-PRON-, have, u...  [PRON, VERB, VER...   \n",
       "401             73              402  [so, just, take,...  [ADV, ADV, VERB,...   \n",
       "402             73              403  [-PRON-, can, no...  [PRON, VERB, ADV...   \n",
       "403             73              404  [-PRON-, do, not...  [PRON, VERB, ADV...   \n",
       "404             73              405  [but, -PRON-, be...  [CCONJ, PRON, VE...   \n",
       "405             74              406  [excellent, prod...  [INTJ, PRON, ADJ...   \n",
       "406             74              407  [-PRON-, have, t...  [PRON, VERB, VER...   \n",
       "407             74              408  [-PRON-, actuall...  [PRON, ADV, VERB...   \n",
       "408             74              409  [-PRON-, be, con...  [PRON, VERB, ADJ...   \n",
       "409             74              410  [one, pill, seem...  [NUM, NOUN, VERB...   \n",
       "\n",
       "     preprocessed_review  has_paired_words  \n",
       "400  [-PRON-, have, u...                 1  \n",
       "401  [so, just, take,...                 0  \n",
       "402  [-PRON-, can_not...                 1  \n",
       "403  [-PRON-, do_not,...                 1  \n",
       "404  [but, -PRON-, be...                 0  \n",
       "405  [excellent, prod...                 0  \n",
       "406  [-PRON-, have, t...                 1  \n",
       "407  [-PRON-, actuall...                 1  \n",
       "408  [-PRON-, be, con...                 0  \n",
       "409  [one, pill, seem...                 1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "has_paired_words       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an arbitrary sentence and it's transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'buy', 'nu', 'skin', 'product', 'about', '5', 'year', 'ago', 'from', 'a', 'beauty', 'salon', 'and', '-PRON-', 'love', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_sentences.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'VERB', 'PROPN', 'PROPN', 'NOUN', 'ADV', 'NUM', 'NOUN', 'ADV', 'ADP', 'DET', 'NOUN', 'NOUN', 'CCONJ', 'PRON', 'VERB', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_pos.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'buy', 'nu', 'skin', 'product', 'about', '5', 'year_ago', 'from', 'a', 'beauty', 'salon', 'and', '-PRON-', 'love', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.preprocessed_review.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramlist = [word for sent in trigram_sentences for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this_product', 69494),\n",
       " ('do_not', 59653),\n",
       " ('seem_to', 14671),\n",
       " ('can_not', 14216),\n",
       " ('fish_oil', 12919),\n",
       " ('highly_recommend', 12415),\n",
       " ('as_well', 10388),\n",
       " ('lot_of', 9337),\n",
       " ('so_far', 6983),\n",
       " ('would_recommend', 6173),\n",
       " ('will_continue', 5383),\n",
       " ('every_day', 5290),\n",
       " ('side_effect', 5008),\n",
       " ('along_with', 4968),\n",
       " ('vitamin_d', 4601),\n",
       " ('at_least', 4305),\n",
       " ('per_day', 4262),\n",
       " ('high_quality', 4237),\n",
       " ('year_ago', 4147),\n",
       " ('at_night', 3881),\n",
       " ('very_happy', 3827),\n",
       " ('vitamin_c', 3800),\n",
       " ('suffer_from', 3715),\n",
       " ('immune_system', 3494),\n",
       " ('five_star', 3471),\n",
       " ('omega_3', 3426),\n",
       " ('run_out', 3420),\n",
       " ('year_old', 3134),\n",
       " ('long_time', 3126),\n",
       " ('no_longer', 3036),\n",
       " ('krill_oil', 3001),\n",
       " ('make_sure', 2684),\n",
       " ('out_there', 2672),\n",
       " ('very_pleased', 2620),\n",
       " ('wake_up', 2582),\n",
       " ('blood_pressure', 2492),\n",
       " ('energy_level', 2438),\n",
       " ('no_side_effect', 2353),\n",
       " ('get_sick', 2260),\n",
       " ('anyone_who', 2208),\n",
       " ('every_morning', 2101),\n",
       " ('go_away', 2018),\n",
       " ('joint_pain', 1996),\n",
       " ('weight_loss', 1992),\n",
       " ('vitamin_d3', 1952),\n",
       " ('exactly_what', 1941),\n",
       " ('come_back', 1892),\n",
       " ('6_month', 1823),\n",
       " ('hot_flash', 1800),\n",
       " ('better_than', 1785),\n",
       " ('month_ago', 1685),\n",
       " ('by_far', 1668),\n",
       " ('get_rid', 1638),\n",
       " ('lose_weight', 1638),\n",
       " ('even_though', 1605),\n",
       " ('health_benefit', 1584),\n",
       " ('clear_up', 1508),\n",
       " ('blood_sugar', 1495),\n",
       " ('before_bed', 1494),\n",
       " ('digestive_system', 1416),\n",
       " ('no_fishy', 1322),\n",
       " ('very_satisfied', 1290),\n",
       " ('5_star', 1287),\n",
       " ('500_mg', 1263),\n",
       " ('those_who', 1253),\n",
       " ('vitamin_e', 1235),\n",
       " ('right_away', 1227),\n",
       " ('people_who', 1227),\n",
       " ('customer_service', 1210),\n",
       " ('end_up', 1209),\n",
       " ('look_forward', 1182),\n",
       " ('as_describe', 1168),\n",
       " ('love_it!.', 1164),\n",
       " ('acid_reflux', 1149),\n",
       " ('coconut_oil', 1130),\n",
       " ('health_food_store', 1110),\n",
       " ('long_term', 1088),\n",
       " ('an_empty_stomach', 1082),\n",
       " ('b_complex', 1078),\n",
       " ('1000_mg', 1056),\n",
       " ('fast_shipping', 1054),\n",
       " ('little_bit', 1042),\n",
       " ('swear_by', 1035),\n",
       " ('milk_thistle', 1030),\n",
       " ('blood_test', 1029),\n",
       " ('week_ago', 1024),\n",
       " ('rather_than', 1004),\n",
       " ('green_tea', 996),\n",
       " ('as_advertise', 994),\n",
       " ('reasonable_price', 989),\n",
       " ('big_difference', 972),\n",
       " ('huge_difference', 962),\n",
       " ('come_across', 957),\n",
       " ('nature_make', 954),\n",
       " ('worry_about', 942),\n",
       " ('cod_liver_oil', 937),\n",
       " ('through_amazon', 913),\n",
       " ('second_bottle', 909),\n",
       " ('less_expensive', 907),\n",
       " ('multi_vitamin', 901)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq = Counter(gramlist)\n",
    "paired_words_frq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wishful_thinking!ftc_disclosure', 1),\n",
       " ('none_after!.', 1),\n",
       " ('34;healthy_fat&#34', 1),\n",
       " ('atrail_fibrillationso', 1),\n",
       " ('superior_product!paula', 1),\n",
       " ('ever!!!highly_recommended', 1),\n",
       " ('george_flansbaum_whom', 1),\n",
       " ('side_effects).this', 1),\n",
       " ('side_effects!so', 1),\n",
       " ('onelife_pharma_sound', 1),\n",
       " ('coco_mak_seriously', 1),\n",
       " ('34;off_days&#34', 1),\n",
       " ('i&#8217;m_assuming', 1),\n",
       " ('yeast_infection_every2', 1),\n",
       " ('plant_derived', 1),\n",
       " ('protease_enzym', 1),\n",
       " ('virtually_untreatable', 1),\n",
       " ('con_su_rodilla', 1),\n",
       " ('mi_mama', 1),\n",
       " ('ayuda_mucho', 1),\n",
       " ('productmuy_buen_producto', 1),\n",
       " ('sodium_free.&#34_wow', 1),\n",
       " ('1000_mg_serving!.', 1),\n",
       " ('countless_cortizone', 1),\n",
       " ('tons_of_benefits!.', 1),\n",
       " ('34;clear_out&#34', 1),\n",
       " ('bri_nutrition&#8217;s_unconditional_guarantee', 1),\n",
       " ('dieting_pilling', 1),\n",
       " ('nutrition!._bri_nutrition', 1),\n",
       " ('garage_and_repaint', 1),\n",
       " ('bri_nutrition_triphalia', 1),\n",
       " ('unhygienic_colon', 1),\n",
       " ('slouchy_and_drain', 1),\n",
       " ('8220;bowel_issues&#8221_lately', 1),\n",
       " ('feeling_better!!!!!.', 1),\n",
       " ('34;the_master_cleanse&#34', 1),\n",
       " ('34;all_natural&#34;i', 1),\n",
       " ('satisfaction_gaurantee', 1),\n",
       " ('crucial_andnot_user_friendly', 1),\n",
       " ('defy_reccomemd', 1),\n",
       " ('racetam_or_sulbutiamine', 1),\n",
       " ('greg_bastin', 1),\n",
       " ('favorite_navaho_teas', 1),\n",
       " ('braniac_allow', 1),\n",
       " ('unlike_cleartea', 1),\n",
       " ('tea!!._braniac', 1),\n",
       " ('34;truth_of_reality&#34;.', 1),\n",
       " ('definitley_34;up&#34', 1),\n",
       " ('renew_interest!.', 1),\n",
       " ('customer_ssupport', 1),\n",
       " ('semi_pornographic', 1),\n",
       " ('enzyte_commercial', 1),\n",
       " ('hott_natural!!.', 1),\n",
       " ('comment_frombf', 1),\n",
       " ('slam_bam', 1),\n",
       " ('chinese_herbalists', 1),\n",
       " ('fst_arrival', 1),\n",
       " ('pleasantely_surprised!.', 1),\n",
       " ('own_viagra-', 1),\n",
       " ('sexual_stimulant!!.', 1),\n",
       " ('excessive_stimulation.it', 1),\n",
       " ('34;caffeine_blues&#34', 1),\n",
       " ('raw&#8230;.they_cook', 1),\n",
       " ('voracious_sex', 1),\n",
       " ('wrist_clown', 1),\n",
       " ('meek_limp', 1),\n",
       " ('puppy_contract_parvovirus', 1),\n",
       " ('parvovirus_puppy', 1),\n",
       " ('ordinary_slowness', 1),\n",
       " ('internal_34;cleansing&#34', 1),\n",
       " ('eric_meghan', 1),\n",
       " ('mebe_nutrition', 1),\n",
       " ('complementary_ebook', 1),\n",
       " ('overcome_plateaus', 1),\n",
       " ('franklin_forskolin', 1),\n",
       " ('behavioral_functioning', 1),\n",
       " ('triple_amazeball', 1),\n",
       " ('athletic_clientele', 1),\n",
       " ('senior_citizens', 1),\n",
       " ('34;asparagus_like&#34_urine', 1),\n",
       " ('mind_foreskin', 1),\n",
       " ('sun_wrinkel', 1),\n",
       " ('probably_qualitatively', 1),\n",
       " ('ceo_respond', 1),\n",
       " ('contact_aurora', 1),\n",
       " ('secondly_aurora', 1),\n",
       " ('highly_reccommend!!!!!!.', 1),\n",
       " ('lava_lamp', 1),\n",
       " ('milk_thistle_estract', 1),\n",
       " ('health/_regeneration', 1),\n",
       " ('a++quality_milk_thistle', 1),\n",
       " ('milk_thistle_extract!!i', 1),\n",
       " ('thisle_extract!!.', 1),\n",
       " ('acupuncture_eft', 1),\n",
       " ('rigorous_hiking', 1),\n",
       " ('natures_ayurved', 1),\n",
       " ('co@_prevent', 1),\n",
       " ('critical_co2', 1),\n",
       " ('epa_vesisorb_and_sup', 1),\n",
       " ('34;joint_health&#34', 1)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 100 most infrequent paired words\n",
    "paired_words_frq.most_common()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83377"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq)  # number of paired terms  (this drops down to 46,785 after further processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[this_product, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [this_product, h...                 1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove):\n",
    "    # split up paired words failing our format requirements\n",
    "    to_remove.extend([i])\n",
    "    sent_paired.extend(sent[i + skip: i + skip + num_paired])\n",
    "\n",
    "\n",
    "def filter_pairs(sent, sent_paired, sent_pos):\n",
    "    \"\"\"modify sent_paired in place\"\"\"\n",
    "    paired_sent_len = len(sent_paired)\n",
    "    skip = 0\n",
    "    to_remove = []\n",
    "    \n",
    "    for i in range(paired_sent_len):\n",
    "        word = sent_paired[i]\n",
    "        if '_' in word:\n",
    "            num_paired = word.count('_') + 1\n",
    "            \n",
    "            # more than 3 words paired - ignore pairing\n",
    "            if num_paired > 3:\n",
    "                handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                skip += num_paired - 1\n",
    "                continue\n",
    "            \n",
    "            # bigrams: noun/adj, noun\n",
    "            elif num_paired == 2:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_2 == 'NOUN')\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "            \n",
    "            # trigrams: noun/adj, all types, noun/adj\n",
    "            elif num_paired == 3:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                pos_word_3 = sent_pos[i + skip + 2]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_3 in ('NOUN', 'ADJ'))\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "        \n",
    "            # num. of words to skip indexing over sent and sent_pos in the next iter\n",
    "            skip += num_paired - 1\n",
    "        \n",
    "    # remove rejected pairs that are already split and added back individually\n",
    "    if len(to_remove) > 0:\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del sent_paired[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the filtering function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent = ['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "# Expected output:\n",
    "print(['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "sent = ['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'a', 'lot', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[this_product, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [this_product, h...                 1  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_reviews = unigram_sents_pos_df.preprocessed_review.tolist()\n",
    "unigram_sentences = unigram_sents_pos_df.unigram_sentences.tolist()\n",
    "unigram_pos = unigram_sents_pos_df.unigram_pos.tolist()\n",
    "has_paired_words = unigram_sents_pos_df.has_paired_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1241826/1241826 [00:03<00:00, 393305.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# get rid of paired words from the corpus which\n",
    "# (1) have more than 3 words joined\n",
    "# (2) bigrams not in the format: noun/adj, noun\n",
    "# (3) trigrams not in the format: noun/adj, all types, noun/adj\n",
    "for i in tqdm(range(len(preprocessed_reviews))):\n",
    "    if has_paired_words[i] == 1:\n",
    "        filter_pairs(sent=unigram_sentences[i], sent_paired=preprocessed_reviews[i], sent_pos=unigram_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[have, the, grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [have, the, grea...                 1  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_review_updated = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241826"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_review_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bioavailability', 'be', 'the', 'key'],\n",
       " ['-PRON-',\n",
       "  'start',\n",
       "  'take',\n",
       "  'this',\n",
       "  'after',\n",
       "  'both',\n",
       "  '-PRON-',\n",
       "  'parent',\n",
       "  'die',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'as',\n",
       "  '-PRON-',\n",
       "  'suppose',\n",
       "  'to',\n",
       "  'enhance',\n",
       "  '-PRON-',\n",
       "  'immune_system',\n",
       "  'the',\n",
       "  'story',\n",
       "  'on',\n",
       "  'on',\n",
       "  'resveratrol',\n",
       "  'be',\n",
       "  '60',\n",
       "  'minutes',\n",
       "  'incredibly',\n",
       "  'inspiring'],\n",
       " ['do',\n",
       "  'some',\n",
       "  'research',\n",
       "  'on',\n",
       "  'the',\n",
       "  'internet',\n",
       "  '-PRON-',\n",
       "  'be',\n",
       "  'indicate',\n",
       "  'that',\n",
       "  'take',\n",
       "  'resveratrol',\n",
       "  'in',\n",
       "  'lozenge',\n",
       "  'form',\n",
       "  'be',\n",
       "  'preferable',\n",
       "  'as',\n",
       "  '-PRON-',\n",
       "  'be',\n",
       "  'by',\n",
       "  'stomach',\n",
       "  'acid',\n",
       "  'break',\n",
       "  'down']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_review_updated[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramlist_updated = [word for sent in preprocessed_review_updated for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fish_oil', 10809),\n",
       " ('side_effect', 4969),\n",
       " ('high_quality', 3908),\n",
       " ('vitamin_d', 3852),\n",
       " ('immune_system', 3374),\n",
       " ('long_time', 3112),\n",
       " ('vitamin_c', 2660),\n",
       " ('blood_pressure', 2457),\n",
       " ('energy_level', 2408),\n",
       " ('anyone_who', 2207),\n",
       " ('joint_pain', 1932),\n",
       " ('weight_loss', 1891),\n",
       " ('hot_flash', 1758),\n",
       " ('health_benefit', 1577),\n",
       " ('blood_sugar', 1465),\n",
       " ('digestive_system', 1354),\n",
       " ('krill_oil', 1312),\n",
       " ('people_who', 1227),\n",
       " ('vitamin_e', 1203),\n",
       " ('customer_service', 1112),\n",
       " ('long_term', 1076),\n",
       " ('acid_reflux', 1044),\n",
       " ('health_food_store', 1038),\n",
       " ('little_bit', 1038),\n",
       " ('fast_shipping', 1035),\n",
       " ('blood_test', 956),\n",
       " ('big_difference', 951),\n",
       " ('huge_difference', 950),\n",
       " ('reasonable_price', 950),\n",
       " ('coconut_oil', 928),\n",
       " ('second_bottle', 908),\n",
       " ('small_amount', 867),\n",
       " ('multi_vitamin', 808),\n",
       " ('green_tea', 807),\n",
       " ('daily_basis', 788),\n",
       " ('sore_throat', 785),\n",
       " ('local_health', 784),\n",
       " ('digestive_issue', 781),\n",
       " ('food_store', 763),\n",
       " ('soft_gel', 753),\n",
       " ('love_it!.', 725),\n",
       " ('leg_cramp', 721),\n",
       " ('hair_growth', 706),\n",
       " ('yeast_infection', 701),\n",
       " ('regular_basis', 690),\n",
       " ('orange_juice', 677),\n",
       " ('vitamin_d3', 673),\n",
       " ('dry_eye', 668),\n",
       " ('local_store', 667),\n",
       " ('bowel_movement', 657),\n",
       " ('sinus_infection', 657),\n",
       " ('fishy_aftertaste', 648),\n",
       " ('fish_burps', 637),\n",
       " ('hair_loss', 625),\n",
       " ('b_vitamin', 623),\n",
       " ('family_member', 621),\n",
       " ('high_blood_pressure', 619),\n",
       " ('fishy_taste', 614),\n",
       " ('gel_cap', 598),\n",
       " ('pre_workout', 596),\n",
       " ('digestive_enzyme', 593),\n",
       " ('digestive_problem', 590),\n",
       " ('cod_liver_oil', 590),\n",
       " ('upset_stomach', 588),\n",
       " ('life_saver', 576),\n",
       " ('prescription_drug', 567),\n",
       " ('fast_delivery', 542),\n",
       " ('flu_season', 536),\n",
       " ('protein_powder', 532),\n",
       " ('someone_who', 518),\n",
       " ('liquid_form', 514),\n",
       " ('prescription_medication', 513),\n",
       " ('free_shipping', 511),\n",
       " ('normal_range', 508),\n",
       " ('olive_oil', 508),\n",
       " ('drug_store', 500),\n",
       " ('real_deal', 499),\n",
       " ('whole_food', 498),\n",
       " ('natural_remedy', 494),\n",
       " ('whole_family', 485),\n",
       " ('fishy_burps', 484),\n",
       " ('milk_thistle', 471),\n",
       " ('expiration_date', 454),\n",
       " ('negative_side_effect', 447),\n",
       " ('worth_every_penny', 443),\n",
       " ('essential_oil', 423),\n",
       " ('first_sign', 410),\n",
       " ('colloidal_silver', 409),\n",
       " ('placebo_effect', 393),\n",
       " ('noticeable_difference', 393),\n",
       " ('digestive_tract', 387),\n",
       " ('mood_swing', 373),\n",
       " ('positive_effect', 371),\n",
       " ('tea_tree_oil', 369),\n",
       " ('amino_acid', 366),\n",
       " ('lemon_flavor', 365),\n",
       " ('kidney_stone', 362),\n",
       " ('peppermint_oil', 362),\n",
       " ('aerobic_step', 360),\n",
       " ('prescription_med', 355)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq_updated = Counter(gramlist_updated)\n",
    "paired_words_frq_updated.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27137"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq_updated)   # final number of cleaned-up paired words in the specified phrase format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Clean-up: Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[have, the, grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [have, the, grea...                 1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 6)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_review_final = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in preprocessed_review_updated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>has_paired_words</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bioavailability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>[research, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ez, melt, formu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, side_eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   has_paired_words  preprocessed_review  \n",
       "0                 0  [bioavailability...  \n",
       "1                 1  [-PRON-, start, ...  \n",
       "2                 1  [research, inter...  \n",
       "3                 0  [ez, melt, formu...  \n",
       "4                 1  [great, side_eff...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.drop(['preprocessed_review'], axis=1, inplace=True)\n",
    "unigram_sents_pos_df['preprocessed_review'] = preprocessed_review_final\n",
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>has_paired_words</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>0</td>\n",
       "      <td>[dpe, job]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>[b, flax, d, regular, -PRON-, house]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-PRON-, -PRON-, job, simply, good, result]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, reasonable, long, time, able, obtain,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[fast, shipping, good, communication]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>1</td>\n",
       "      <td>[fast_shipping, good, communication]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>[study, show, that, resveratrol, be, poorly, a...</td>\n",
       "      <td>[NOUN, VERB, ADP, PROPN, VERB, ADV, VERB, ADV,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[study, resveratrol, poorly, absorb, pill, loz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[hardly, any, company, be, sell, lozenge]</td>\n",
       "      <td>[ADV, DET, NOUN, VERB, VERB, NOUN]</td>\n",
       "      <td>0</td>\n",
       "      <td>[hardly, company, sell, lozenge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>[this, company, promise, 99, purity, and, have...</td>\n",
       "      <td>[DET, NOUN, VERB, NUM, NOUN, CCONJ, VERB, ADJ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[company, promise, 99, purity, fast_shipping, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[-PRON-, can, not, comment, on, the, quality, ...</td>\n",
       "      <td>[PRON, VERB, ADV, VERB, ADP, DET, NOUN, ADP, N...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, comment, quality, product, -PRON-, ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "5              2                6   \n",
       "6              2                7   \n",
       "7              2                8   \n",
       "8              2                9   \n",
       "9              2               10   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "5              [fast, shipping, good, communication]   \n",
       "6  [study, show, that, resveratrol, be, poorly, a...   \n",
       "7          [hardly, any, company, be, sell, lozenge]   \n",
       "8  [this, company, promise, 99, purity, and, have...   \n",
       "9  [-PRON-, can, not, comment, on, the, quality, ...   \n",
       "\n",
       "                                         unigram_pos  has_paired_words  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]                 0   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...                 0   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...                 0   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...                 1   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]                 0   \n",
       "5                             [ADJ, NOUN, ADJ, NOUN]                 1   \n",
       "6  [NOUN, VERB, ADP, PROPN, VERB, ADV, VERB, ADV,...                 0   \n",
       "7                 [ADV, DET, NOUN, VERB, VERB, NOUN]                 0   \n",
       "8  [DET, NOUN, VERB, NUM, NOUN, CCONJ, VERB, ADJ,...                 1   \n",
       "9  [PRON, VERB, ADV, VERB, ADP, DET, NOUN, ADP, N...                 1   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0                                         [dpe, job]  \n",
       "1               [b, flax, d, regular, -PRON-, house]  \n",
       "2        [-PRON-, -PRON-, job, simply, good, result]  \n",
       "3  [-PRON-, reasonable, long, time, able, obtain,...  \n",
       "4         [good, product, good, price, good, result]  \n",
       "5               [fast_shipping, good, communication]  \n",
       "6  [study, resveratrol, poorly, absorb, pill, loz...  \n",
       "7                   [hardly, company, sell, lozenge]  \n",
       "8  [company, promise, 99, purity, fast_shipping, ...  \n",
       "9  [-PRON-, comment, quality, product, -PRON-, ch...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_num = unigram_sents_pos_df.review_number.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dpe', 'job'], ['b', 'flax', 'd', 'regular', '-PRON-', 'house'], ['-PRON-', '-PRON-', 'job', 'simply', 'good', 'result']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = unigram_sents_pos_df.groupby(('review_number'))['preprocessed_review'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = tokenized_reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584828"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dpe', 'job', 'b', 'flax', 'd', 'regular', '-PRON-', 'house', '-PRON-', '-PRON-', 'job', 'simply', 'good', 'result', '-PRON-', 'reasonable', 'long', 'time', 'able', 'obtain', 'free_shipping', '-PRON-', 'hunt', 'good', 'product', 'good', 'price', 'good', 'result']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(tokenized_reviews, bucket_name, filepath='amazon_reviews/kk/tokenized_reviews_v1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(17822 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #20000 to Dictionary(30475 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #30000 to Dictionary(39820 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #40000 to Dictionary(47546 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #50000 to Dictionary(54542 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #60000 to Dictionary(62429 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #70000 to Dictionary(68571 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #80000 to Dictionary(75729 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #90000 to Dictionary(82207 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #100000 to Dictionary(88258 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #110000 to Dictionary(94123 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #120000 to Dictionary(99442 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #130000 to Dictionary(105399 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #140000 to Dictionary(111842 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #150000 to Dictionary(117328 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #160000 to Dictionary(122511 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #170000 to Dictionary(127831 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #180000 to Dictionary(133931 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #190000 to Dictionary(139341 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #200000 to Dictionary(143897 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #210000 to Dictionary(148398 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #220000 to Dictionary(153003 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #230000 to Dictionary(158132 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #240000 to Dictionary(162755 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #250000 to Dictionary(166951 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #260000 to Dictionary(170942 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #270000 to Dictionary(175472 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #280000 to Dictionary(179880 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #290000 to Dictionary(184280 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #300000 to Dictionary(188258 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #310000 to Dictionary(192446 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #320000 to Dictionary(196779 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #330000 to Dictionary(200974 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #340000 to Dictionary(205234 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #350000 to Dictionary(209685 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #360000 to Dictionary(213721 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #370000 to Dictionary(218292 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #380000 to Dictionary(222248 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #390000 to Dictionary(226133 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #400000 to Dictionary(230561 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #410000 to Dictionary(234296 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #420000 to Dictionary(238561 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #430000 to Dictionary(242171 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #440000 to Dictionary(245615 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #450000 to Dictionary(249392 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #460000 to Dictionary(253058 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #470000 to Dictionary(256894 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #480000 to Dictionary(259840 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #490000 to Dictionary(262641 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #500000 to Dictionary(265804 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #510000 to Dictionary(269024 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #520000 to Dictionary(272581 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #530000 to Dictionary(275483 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #540000 to Dictionary(278864 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #550000 to Dictionary(282254 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #560000 to Dictionary(285639 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #570000 to Dictionary(288797 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : adding document #580000 to Dictionary(291675 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n",
      "INFO : built Dictionary(292938 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...) from 584828 documents (total 22445865 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 768 ms, total: 23.8 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to learn the full vocabulary of the corpus to be modeled\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "vocab_dictionary = Dictionary(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 291004 tokens: [('dpe', 1), ('-PRON-', 2487546), ('hunt', 239), ('obtain', 980), ('communication', 262), ('lozenge', 785), ('poorly', 616), ('99', 534), ('chemist', 209), ('legitimate', 307)]...\n",
      "INFO : keeping 1934 tokens which were in no less than 1000 and no more than 2159571 (=60.0%) documents\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n",
      "INFO : resulting dictionary: Dictionary(1934 unique tokens: ['job', 'b', 'd', 'flax', 'house']...)\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n"
     ]
    }
   ],
   "source": [
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "# vocab_dictionary.filter_extremes(no_below=1000, no_above=0.6)\n",
    "# vocab_dictionary.compactify()   # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(292938 unique tokens: ['-PRON-', 'able', 'b', 'd', 'dpe']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(vocab_dictionary, bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_v1.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_dictionary = Dictionary.load('../vocab_dictionary.dict')  # load the finished dictionary from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag-of-words representation of the corpus/ doc-term matrix\n",
    "bow_corpus = [vocab_dictionary.doc2bow(review) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.1\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 10 topics, 3 passes over the supplied corpus of 584828 documents, updating every 350000 documents, evaluating every ~350000 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 35 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "DEBUG : processing chunk #7 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9788/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9728/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 9738/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9786/10000 documents converged within 400 iterations\n",
      "DEBUG : 9750/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9772/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9781/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9710/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9802/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9725/10000 documents converged within 400 iterations\n",
      "DEBUG : 9770/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : processing chunk #45 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9743/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9746/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 9731/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : processing chunk #48 of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9724/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9766/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9786/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9757/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9792/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : 9759/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "DEBUG : 9773/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9801/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9778/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9630/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 9769/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : 9796/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9754/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9746/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9692/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9797/10000 documents converged within 400 iterations\n",
      "DEBUG : 9789/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9739/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9769/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9769/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9712/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "INFO : topic #8 (0.100): 0.209*\"-PRON-\" + 0.023*\"product\" + 0.016*\"good\" + 0.013*\"use\" + 0.011*\"like\" + 0.008*\"day\" + 0.008*\"great\" + 0.007*\"help\" + 0.007*\"work\" + 0.006*\"recommend\"\n",
      "INFO : topic #5 (0.100): 0.168*\"-PRON-\" + 0.021*\"good\" + 0.016*\"product\" + 0.013*\"taste\" + 0.009*\"like\" + 0.009*\"help\" + 0.008*\"work\" + 0.008*\"use\" + 0.008*\"great\" + 0.007*\"feel\"\n",
      "INFO : topic #9 (0.100): 0.121*\"-PRON-\" + 0.035*\"good\" + 0.025*\"product\" + 0.010*\"vitamin\" + 0.008*\"price\" + 0.008*\"use\" + 0.008*\"love\" + 0.008*\"great\" + 0.007*\"help\" + 0.006*\"day\"\n",
      "INFO : topic #1 (0.100): 0.108*\"-PRON-\" + 0.028*\"product\" + 0.016*\"good\" + 0.015*\"help\" + 0.009*\"use\" + 0.006*\"taste\" + 0.006*\"excellent\" + 0.005*\"work\" + 0.005*\"day\" + 0.005*\"vitamin\"\n",
      "INFO : topic #6 (0.100): 0.132*\"-PRON-\" + 0.015*\"great\" + 0.014*\"good\" + 0.014*\"product\" + 0.010*\"day\" + 0.010*\"use\" + 0.008*\"body\" + 0.008*\"energy\" + 0.007*\"year\" + 0.007*\"help\"\n",
      "INFO : topic diff=9.321262, rho=1.000000\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : 9780/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9757/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9747/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9739/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9772/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4737/4828 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9775/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9781/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9774/10000 documents converged within 400 iterations\n",
      "DEBUG : 9798/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9748/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9735/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -6.582 per-word bound, 95.8 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "DEBUG : 9748/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 9771/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9798/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9779/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9787/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9770/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9779/10000 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9782/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9746/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9758/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9808/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9720/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n",
      "INFO : topic #6 (0.100): 0.134*\"-PRON-\" + 0.015*\"great\" + 0.014*\"product\" + 0.014*\"good\" + 0.010*\"day\" + 0.010*\"use\" + 0.009*\"energy\" + 0.008*\"body\" + 0.007*\"help\" + 0.007*\"3\"\n",
      "INFO : topic #9 (0.100): 0.122*\"-PRON-\" + 0.035*\"good\" + 0.026*\"product\" + 0.010*\"vitamin\" + 0.008*\"love\" + 0.008*\"price\" + 0.008*\"use\" + 0.007*\"great\" + 0.007*\"help\" + 0.006*\"day\"\n",
      "INFO : topic #2 (0.100): 0.097*\"-PRON-\" + 0.040*\"product\" + 0.024*\"great\" + 0.016*\"use\" + 0.015*\"good\" + 0.010*\"time\" + 0.008*\"need\" + 0.007*\"vitamin\" + 0.006*\"day\" + 0.006*\"work\"\n",
      "INFO : topic #1 (0.100): 0.111*\"-PRON-\" + 0.029*\"product\" + 0.016*\"help\" + 0.015*\"good\" + 0.009*\"use\" + 0.006*\"taste\" + 0.006*\"excellent\" + 0.006*\"work\" + 0.006*\"day\" + 0.005*\"vitamin\"\n",
      "INFO : topic #5 (0.100): 0.168*\"-PRON-\" + 0.021*\"good\" + 0.017*\"product\" + 0.012*\"taste\" + 0.009*\"like\" + 0.009*\"help\" + 0.009*\"work\" + 0.008*\"use\" + 0.008*\"feel\" + 0.008*\"great\"\n",
      "INFO : topic diff=3.030081, rho=0.166667\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.349 per-word bound, 81.5 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n",
      "DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51\n",
      "DEBUG : processing chunk #7 of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9939/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9953/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : 9951/10000 documents converged within 400 iterations\n",
      "DEBUG : 9952/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : 9951/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9951/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9934/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : 9947/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9940/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #45 of 10000 documents\n",
      "DEBUG : 9947/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 9958/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 10000 documents\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9945/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9941/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : 9944/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 9942/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9958/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9945/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #9 (0.100): 0.125*\"-PRON-\" + 0.037*\"good\" + 0.026*\"product\" + 0.012*\"vitamin\" + 0.010*\"price\" + 0.009*\"love\" + 0.008*\"great\" + 0.008*\"use\" + 0.007*\"help\" + 0.006*\"day\"\n",
      "INFO : topic #1 (0.100): 0.109*\"-PRON-\" + 0.029*\"product\" + 0.016*\"help\" + 0.015*\"good\" + 0.009*\"use\" + 0.007*\"excellent\" + 0.006*\"work\" + 0.006*\"taste\" + 0.005*\"day\" + 0.004*\"recommend\"\n",
      "INFO : topic #7 (0.100): 0.053*\"-PRON-\" + 0.038*\"product\" + 0.017*\"great\" + 0.016*\"mg\" + 0.014*\"good\" + 0.008*\"coq10\" + 0.007*\"$\" + 0.007*\"capsule\" + 0.007*\"recommend\" + 0.006*\"excellent\"\n",
      "INFO : topic #4 (0.100): 0.062*\"-PRON-\" + 0.041*\"product\" + 0.019*\"great\" + 0.019*\"good\" + 0.016*\"use\" + 0.011*\"work\" + 0.010*\"recommend\" + 0.008*\"vitamin\" + 0.007*\"price\" + 0.007*\"taste\"\n",
      "INFO : topic #8 (0.100): 0.211*\"-PRON-\" + 0.024*\"product\" + 0.016*\"good\" + 0.013*\"use\" + 0.012*\"like\" + 0.008*\"great\" + 0.008*\"day\" + 0.007*\"help\" + 0.007*\"work\" + 0.007*\"recommend\"\n",
      "INFO : topic diff=0.319842, rho=0.128583\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9953/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4801/4828 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9945/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9938/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n",
      "INFO : topic #2 (0.100): 0.097*\"-PRON-\" + 0.043*\"product\" + 0.029*\"great\" + 0.017*\"use\" + 0.016*\"good\" + 0.010*\"time\" + 0.008*\"need\" + 0.007*\"price\" + 0.007*\"vitamin\" + 0.006*\"easy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #8 (0.100): 0.211*\"-PRON-\" + 0.025*\"product\" + 0.016*\"good\" + 0.013*\"use\" + 0.012*\"like\" + 0.008*\"great\" + 0.008*\"day\" + 0.007*\"help\" + 0.007*\"work\" + 0.007*\"recommend\"\n",
      "INFO : topic #6 (0.100): 0.128*\"-PRON-\" + 0.015*\"great\" + 0.014*\"product\" + 0.013*\"good\" + 0.010*\"day\" + 0.010*\"use\" + 0.010*\"energy\" + 0.009*\"body\" + 0.008*\"supplement\" + 0.007*\"help\"\n",
      "INFO : topic #1 (0.100): 0.106*\"-PRON-\" + 0.030*\"product\" + 0.017*\"help\" + 0.015*\"good\" + 0.009*\"use\" + 0.007*\"excellent\" + 0.006*\"work\" + 0.005*\"day\" + 0.005*\"taste\" + 0.005*\"recommend\"\n",
      "INFO : topic #4 (0.100): 0.061*\"-PRON-\" + 0.043*\"product\" + 0.019*\"great\" + 0.018*\"good\" + 0.015*\"use\" + 0.012*\"work\" + 0.010*\"recommend\" + 0.008*\"vitamin\" + 0.008*\"star\" + 0.007*\"price\"\n",
      "INFO : topic diff=0.267372, rho=0.128583\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.303 per-word bound, 78.9 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n",
      "DEBUG : processing chunk #7 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9974/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9965/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9953/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9957/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9965/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 9954/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9943/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9973/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9965/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 9957/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : processing chunk #48 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9979/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9954/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9981/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "INFO : topic #1 (0.100): 0.105*\"-PRON-\" + 0.030*\"product\" + 0.018*\"help\" + 0.014*\"good\" + 0.010*\"use\" + 0.007*\"excellent\" + 0.006*\"work\" + 0.005*\"day\" + 0.005*\"recommend\" + 0.004*\"taste\"\n",
      "INFO : topic #3 (0.100): 0.093*\"-PRON-\" + 0.019*\"good\" + 0.016*\"taste\" + 0.013*\"oil\" + 0.010*\"omega-3\" + 0.009*\"vitamin\" + 0.009*\"'s\" + 0.008*\"like\" + 0.008*\"use\" + 0.008*\"fish\"\n",
      "INFO : topic #5 (0.100): 0.171*\"-PRON-\" + 0.022*\"good\" + 0.017*\"product\" + 0.015*\"taste\" + 0.010*\"like\" + 0.009*\"help\" + 0.009*\"work\" + 0.008*\"use\" + 0.008*\"great\" + 0.007*\"feel\"\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "INFO : topic #8 (0.100): 0.213*\"-PRON-\" + 0.025*\"product\" + 0.016*\"good\" + 0.013*\"use\" + 0.012*\"like\" + 0.008*\"great\" + 0.008*\"day\" + 0.007*\"recommend\" + 0.006*\"help\" + 0.006*\"work\"\n",
      "INFO : topic #2 (0.100): 0.100*\"-PRON-\" + 0.044*\"product\" + 0.032*\"great\" + 0.018*\"use\" + 0.017*\"good\" + 0.011*\"time\" + 0.009*\"price\" + 0.008*\"need\" + 0.007*\"easy\" + 0.006*\"recommend\"\n",
      "INFO : topic diff=0.271474, rho=0.127533\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9974/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9988/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9984/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4813/4828 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9974/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9973/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9973/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n",
      "INFO : topic #7 (0.100): 0.047*\"-PRON-\" + 0.037*\"product\" + 0.028*\"mg\" + 0.014*\"great\" + 0.012*\"good\" + 0.011*\"$\" + 0.011*\"coq10\" + 0.009*\"100\" + 0.009*\"capsule\" + 0.007*\"great_product\"\n",
      "INFO : topic #4 (0.100): 0.061*\"-PRON-\" + 0.046*\"product\" + 0.020*\"great\" + 0.018*\"good\" + 0.015*\"use\" + 0.013*\"work\" + 0.011*\"recommend\" + 0.009*\"star\" + 0.008*\"price\" + 0.007*\"vitamin\"\n",
      "INFO : topic #0 (0.100): 0.243*\"-PRON-\" + 0.015*\"product\" + 0.013*\"work\" + 0.011*\"good\" + 0.010*\"use\" + 0.009*\"day\" + 0.008*\"try\" + 0.007*\"feel\" + 0.007*\"time\" + 0.006*\"help\"\n",
      "INFO : topic #8 (0.100): 0.214*\"-PRON-\" + 0.025*\"product\" + 0.016*\"good\" + 0.013*\"use\" + 0.012*\"like\" + 0.008*\"great\" + 0.008*\"day\" + 0.007*\"recommend\" + 0.006*\"help\" + 0.006*\"work\"\n",
      "INFO : topic #2 (0.100): 0.101*\"-PRON-\" + 0.046*\"product\" + 0.034*\"great\" + 0.018*\"use\" + 0.017*\"good\" + 0.011*\"time\" + 0.009*\"price\" + 0.008*\"need\" + 0.007*\"easy\" + 0.007*\"recommend\"\n",
      "INFO : topic diff=0.252363, rho=0.127533\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.269 per-word bound, 77.1 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=292938, num_topics=10, decay=0.5, chunksize=10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "DEBUG : performing inference on a chunk of 584828 documents\n",
      "DEBUG : 583225/584828 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 32s, sys: 34.4 s, total: 9min 6s\n",
      "Wall time: 9min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "# num_topics_list = np.arange(3, 21, 1)\n",
    "num_topics = 10\n",
    "chunksize = 10000    # number of docs processed at a time\n",
    "passes = 3\n",
    "iterations = 400\n",
    "eval_every = 1\n",
    "\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "# We set alpha = 'auto' and eta = 'auto'. Thus, we are automatically learning \n",
    "# 2 parameters in the model that we usually would have to specify explicitly.\n",
    "# for num_topics in tqdm(num_topics_list):\n",
    "# training the model\n",
    "lda = LdaMulticore(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta='auto', eval_every = eval_every, iterations=iterations)\n",
    "# performance metric\n",
    "cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "coherenceList_umass.append(cm.get_coherence())\n",
    "\n",
    "# visualization\n",
    "vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "pyLDAvis.save_html(vis, 'pyLDAvis_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherenceList_umass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.06666666666666667\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 15 topics, 3 passes over the supplied corpus of 584828 documents, updating every 350000 documents, evaluating every ~350000 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 35 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "DEBUG : processing chunk #7 of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9709/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9705/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9673/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9712/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9711/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : 9643/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9704/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9658/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9712/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9661/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9723/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9634/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9655/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9727/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9646/10000 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9700/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9705/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9699/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9639/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9736/10000 documents converged within 400 iterations\n",
      "DEBUG : 9740/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : 9713/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9717/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "DEBUG : 9769/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9746/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9606/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9705/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : 9715/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9697/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9719/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9737/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9529/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9731/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9706/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9712/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 9676/10000 documents converged within 400 iterations\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "INFO : topic #5 (0.067): 0.054*\"-PRON-\" + 0.019*\"product\" + 0.014*\"good\" + 0.012*\"great\" + 0.012*\"use\" + 0.011*\"star\" + 0.009*\"recommend\" + 0.008*\"great_product\" + 0.008*\"taste\" + 0.007*\"work\"\n",
      "INFO : topic #8 (0.067): 0.146*\"-PRON-\" + 0.027*\"good\" + 0.013*\"product\" + 0.011*\"work\" + 0.010*\"recommend\" + 0.010*\"day\" + 0.009*\"use\" + 0.007*\"help\" + 0.006*\"great\" + 0.006*\"vitamin\"\n",
      "INFO : topic #3 (0.067): 0.202*\"-PRON-\" + 0.025*\"good\" + 0.022*\"product\" + 0.013*\"use\" + 0.012*\"great\" + 0.010*\"work\" + 0.008*\"feel\" + 0.007*\"vitamin\" + 0.007*\"find\" + 0.007*\"try\"\n",
      "INFO : topic #1 (0.067): 0.142*\"-PRON-\" + 0.053*\"product\" + 0.031*\"use\" + 0.018*\"good\" + 0.011*\"taste\" + 0.010*\"great\" + 0.009*\"year\" + 0.009*\"help\" + 0.009*\"work\" + 0.007*\"easy\"\n",
      "INFO : topic #13 (0.067): 0.165*\"-PRON-\" + 0.041*\"product\" + 0.012*\"good\" + 0.009*\"supplement\" + 0.009*\"recommend\" + 0.008*\"great\" + 0.008*\"use\" + 0.008*\"mg\" + 0.007*\"buy\" + 0.007*\"help\"\n",
      "INFO : topic diff=13.919875, rho=1.000000\n",
      "DEBUG : 9714/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : 4696/4828 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9726/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9730/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9720/10000 documents converged within 400 iterations\n",
      "DEBUG : 9714/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9746/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9744/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9652/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9700/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9727/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9671/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -6.650 per-word bound, 100.4 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "DEBUG : 9740/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9765/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9696/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9725/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9709/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : 9752/10000 documents converged within 400 iterations\n",
      "DEBUG : 9677/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9728/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9748/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9681/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9728/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n",
      "INFO : topic #8 (0.067): 0.147*\"-PRON-\" + 0.026*\"good\" + 0.014*\"product\" + 0.011*\"work\" + 0.010*\"recommend\" + 0.010*\"day\" + 0.009*\"use\" + 0.007*\"help\" + 0.006*\"great\" + 0.006*\"vitamin\"\n",
      "INFO : topic #6 (0.067): 0.183*\"-PRON-\" + 0.017*\"good\" + 0.013*\"product\" + 0.011*\"use\" + 0.010*\"work\" + 0.010*\"day\" + 0.008*\"mg\" + 0.008*\"find\" + 0.008*\"try\" + 0.006*\"year\"\n",
      "INFO : topic #4 (0.067): 0.112*\"-PRON-\" + 0.021*\"product\" + 0.015*\"good\" + 0.010*\"use\" + 0.009*\"try\" + 0.008*\"order\" + 0.007*\"'s\" + 0.007*\"taste\" + 0.007*\"work\" + 0.006*\"like\"\n",
      "INFO : topic #13 (0.067): 0.167*\"-PRON-\" + 0.042*\"product\" + 0.012*\"good\" + 0.009*\"supplement\" + 0.009*\"recommend\" + 0.008*\"great\" + 0.008*\"use\" + 0.007*\"mg\" + 0.007*\"buy\" + 0.007*\"help\"\n",
      "INFO : topic #11 (0.067): 0.179*\"-PRON-\" + 0.018*\"use\" + 0.017*\"day\" + 0.016*\"product\" + 0.015*\"work\" + 0.010*\"great\" + 0.007*\"supplement\" + 0.006*\"week\" + 0.005*\"recommend\" + 0.005*\"start\"\n",
      "INFO : topic diff=3.283267, rho=0.166667\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.358 per-word bound, 82.0 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40\n",
      "DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #7 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : 9942/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9953/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9952/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9947/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9945/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9953/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9952/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : 9947/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9942/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #45 of 10000 documents\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9933/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9954/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9951/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9969/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 9944/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "INFO : topic #3 (0.067): 0.203*\"-PRON-\" + 0.025*\"good\" + 0.023*\"product\" + 0.013*\"use\" + 0.012*\"great\" + 0.010*\"work\" + 0.009*\"feel\" + 0.007*\"try\" + 0.007*\"find\" + 0.007*\"vitamin\"\n",
      "INFO : topic #4 (0.067): 0.106*\"-PRON-\" + 0.020*\"product\" + 0.016*\"good\" + 0.009*\"use\" + 0.008*\"order\" + 0.008*\"try\" + 0.008*\"'s\" + 0.007*\"taste\" + 0.006*\"work\" + 0.006*\"like\"\n",
      "INFO : topic #11 (0.067): 0.179*\"-PRON-\" + 0.019*\"use\" + 0.017*\"day\" + 0.016*\"work\" + 0.016*\"product\" + 0.010*\"great\" + 0.007*\"supplement\" + 0.006*\"week\" + 0.005*\"recommend\" + 0.005*\"start\"\n",
      "INFO : topic #6 (0.067): 0.182*\"-PRON-\" + 0.018*\"good\" + 0.013*\"product\" + 0.011*\"use\" + 0.010*\"work\" + 0.010*\"day\" + 0.009*\"mg\" + 0.008*\"find\" + 0.007*\"try\" + 0.006*\"buy\"\n",
      "INFO : topic #5 (0.067): 0.053*\"-PRON-\" + 0.020*\"product\" + 0.018*\"star\" + 0.016*\"great\" + 0.016*\"good\" + 0.012*\"great_product\" + 0.011*\"use\" + 0.010*\"recommend\" + 0.009*\"price\" + 0.008*\"excellent\"\n",
      "INFO : topic diff=0.339086, rho=0.128583\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9943/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9956/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9964/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9946/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9960/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : 4801/4828 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9951/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9952/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9947/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9950/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9955/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9931/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #11 (0.067): 0.179*\"-PRON-\" + 0.019*\"use\" + 0.017*\"work\" + 0.017*\"day\" + 0.016*\"product\" + 0.010*\"great\" + 0.007*\"supplement\" + 0.006*\"week\" + 0.005*\"recommend\" + 0.005*\"start\"\n",
      "INFO : topic #3 (0.067): 0.203*\"-PRON-\" + 0.025*\"good\" + 0.024*\"product\" + 0.013*\"use\" + 0.012*\"great\" + 0.011*\"work\" + 0.009*\"feel\" + 0.007*\"try\" + 0.007*\"find\" + 0.006*\"help\"\n",
      "INFO : topic #1 (0.067): 0.143*\"-PRON-\" + 0.061*\"product\" + 0.032*\"use\" + 0.019*\"good\" + 0.012*\"great\" + 0.010*\"taste\" + 0.009*\"work\" + 0.009*\"help\" + 0.008*\"year\" + 0.007*\"easy\"\n",
      "INFO : topic #7 (0.067): 0.087*\"-PRON-\" + 0.025*\"use\" + 0.023*\"good\" + 0.022*\"product\" + 0.012*\"price\" + 0.009*\"work\" + 0.008*\"great\" + 0.006*\"month\" + 0.005*\"try\" + 0.005*\"brand\"\n",
      "INFO : topic #10 (0.067): 0.227*\"-PRON-\" + 0.016*\"product\" + 0.012*\"help\" + 0.012*\"use\" + 0.008*\"love\" + 0.008*\"day\" + 0.007*\"great\" + 0.006*\"work\" + 0.006*\"like\" + 0.006*\"time\"\n",
      "INFO : topic diff=0.275763, rho=0.128583\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.298 per-word bound, 78.7 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #10000/584828, outstanding queue size 1\n",
      "DEBUG : processing chunk #0 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #20000/584828, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #30000/584828, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #40000/584828, outstanding queue size 4\n",
      "DEBUG : processing chunk #1 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #2 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #50000/584828, outstanding queue size 5\n",
      "DEBUG : processing chunk #3 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #60000/584828, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #70000/584828, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #80000/584828, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #90000/584828, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #100000/584828, outstanding queue size 10\n",
      "DEBUG : processing chunk #4 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #110000/584828, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #120000/584828, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #130000/584828, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #140000/584828, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #150000/584828, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #160000/584828, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #170000/584828, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #180000/584828, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #190000/584828, outstanding queue size 19\n",
      "DEBUG : processing chunk #5 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #200000/584828, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #210000/584828, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #220000/584828, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #230000/584828, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #240000/584828, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #250000/584828, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #260000/584828, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #270000/584828, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #280000/584828, outstanding queue size 28\n",
      "DEBUG : processing chunk #6 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #290000/584828, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #300000/584828, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #310000/584828, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #320000/584828, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #330000/584828, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #340000/584828, outstanding queue size 34\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #350000/584828, outstanding queue size 35\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #360000/584828, outstanding queue size 36\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #370000/584828, outstanding queue size 37\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #380000/584828, outstanding queue size 38\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #390000/584828, outstanding queue size 39\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #400000/584828, outstanding queue size 40\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #410000/584828, outstanding queue size 41DEBUG : processing chunk #7 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #420000/584828, outstanding queue size 42\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #430000/584828, outstanding queue size 43\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #440000/584828, outstanding queue size 44\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #450000/584828, outstanding queue size 45\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #460000/584828, outstanding queue size 46\n",
      "DEBUG : processing chunk #8 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #470000/584828, outstanding queue size 47\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #480000/584828, outstanding queue size 48\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #490000/584828, outstanding queue size 49\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #500000/584828, outstanding queue size 50\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #510000/584828, outstanding queue size 51\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #520000/584828, outstanding queue size 52\n",
      "DEBUG : processing chunk #9 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #530000/584828, outstanding queue size 53\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #540000/584828, outstanding queue size 54\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #550000/584828, outstanding queue size 55\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #560000/584828, outstanding queue size 56\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #570000/584828, outstanding queue size 57\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #580000/584828, outstanding queue size 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #584828/584828, outstanding queue size 59\n",
      "DEBUG : processing chunk #10 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #11 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #12 of 10000 documents\n",
      "DEBUG : processing chunk #13 of 10000 documents\n",
      "DEBUG : processing chunk #14 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #15 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #16 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #17 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #18 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9948/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 10000 documents\n",
      "DEBUG : 9945/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9969/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : 9961/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9966/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 10000 documents\n",
      "DEBUG : processing chunk #23 of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9974/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 9973/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 10000 documents\n",
      "DEBUG : processing chunk #26 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9973/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 10000 documents\n",
      "DEBUG : processing chunk #28 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 10000 documents\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : processing chunk #30 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #32 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #33 of 10000 documents\n",
      "DEBUG : 9970/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 10000 documents\n",
      "DEBUG : processing chunk #35 of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9969/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #36 of 10000 documents\n",
      "DEBUG : 9949/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 10000 documents\n",
      "DEBUG : processing chunk #38 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #39 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9965/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9969/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 10000 documents\n",
      "DEBUG : 9977/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 10000 documents\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 10000 documents\n",
      "DEBUG : 9977/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processing chunk #46 of 10000 documents\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 9963/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9978/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9977/10000 documents converged within 400 iterations\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9977/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 10000 documents\n",
      "DEBUG : 9980/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #50 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #51 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #52 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "INFO : merging changes from 350000 documents into a model of 584828 documents\n",
      "DEBUG : processing chunk #53 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9984/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #55 of 10000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9979/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #4 (0.067): 0.098*\"-PRON-\" + 0.018*\"product\" + 0.016*\"good\" + 0.009*\"bar\" + 0.009*\"'s\" + 0.008*\"use\" + 0.008*\"order\" + 0.007*\"try\" + 0.007*\"coq10\" + 0.006*\"taste\"\n",
      "INFO : topic #2 (0.067): 0.241*\"-PRON-\" + 0.014*\"work\" + 0.013*\"product\" + 0.012*\"day\" + 0.008*\"good\" + 0.008*\"try\" + 0.007*\"great\" + 0.007*\"time\" + 0.007*\"lose\" + 0.006*\"pill\"\n",
      "INFO : topic #13 (0.067): 0.160*\"-PRON-\" + 0.044*\"product\" + 0.012*\"good\" + 0.011*\"supplement\" + 0.010*\"recommend\" + 0.009*\"mg\" + 0.008*\"great\" + 0.007*\"use\" + 0.007*\"buy\" + 0.006*\"help\"\n",
      "INFO : topic #5 (0.067): 0.050*\"-PRON-\" + 0.031*\"star\" + 0.025*\"great\" + 0.021*\"product\" + 0.019*\"great_product\" + 0.018*\"good\" + 0.014*\"price\" + 0.011*\"thank\" + 0.011*\"recommend\" + 0.010*\"excellent\"\n",
      "INFO : topic #8 (0.067): 0.140*\"-PRON-\" + 0.029*\"good\" + 0.014*\"product\" + 0.013*\"recommend\" + 0.011*\"work\" + 0.009*\"day\" + 0.008*\"use\" + 0.007*\"great\" + 0.007*\"help\" + 0.006*\"highly\"\n",
      "INFO : topic diff=0.271000, rho=0.127533\n",
      "DEBUG : processing chunk #56 of 10000 documents\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : 9979/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 10000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9971/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 10000 documents\n",
      "DEBUG : processing chunk #58 of 4828 documents\n",
      "DEBUG : performing inference on a chunk of 4828 documents\n",
      "DEBUG : 9976/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 9980/10000 documents converged within 400 iterations\n",
      "DEBUG : 9967/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9981/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9972/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9982/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9979/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9984/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 4817/4828 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 9976/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9975/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9962/10000 documents converged within 400 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9977/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9959/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 9968/10000 documents converged within 400 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 234828 documents into a model of 584828 documents\n",
      "INFO : topic #3 (0.067): 0.203*\"-PRON-\" + 0.025*\"good\" + 0.025*\"product\" + 0.013*\"use\" + 0.012*\"great\" + 0.011*\"work\" + 0.010*\"feel\" + 0.008*\"try\" + 0.007*\"find\" + 0.006*\"help\"\n",
      "INFO : topic #5 (0.067): 0.049*\"-PRON-\" + 0.039*\"star\" + 0.028*\"great\" + 0.022*\"great_product\" + 0.022*\"product\" + 0.019*\"good\" + 0.015*\"price\" + 0.013*\"thank\" + 0.011*\"recommend\" + 0.011*\"excellent\"\n",
      "INFO : topic #4 (0.067): 0.095*\"-PRON-\" + 0.018*\"product\" + 0.015*\"good\" + 0.010*\"bar\" + 0.009*\"'s\" + 0.008*\"order\" + 0.008*\"use\" + 0.008*\"coq10\" + 0.007*\"try\" + 0.006*\"taste\"\n",
      "INFO : topic #12 (0.067): 0.235*\"-PRON-\" + 0.016*\"product\" + 0.013*\"use\" + 0.012*\"good\" + 0.009*\"work\" + 0.009*\"try\" + 0.008*\"feel\" + 0.008*\"help\" + 0.007*\"like\" + 0.007*\"supplement\"\n",
      "INFO : topic #9 (0.067): 0.208*\"-PRON-\" + 0.016*\"like\" + 0.016*\"taste\" + 0.015*\"good\" + 0.012*\"vitamin\" + 0.011*\"product\" + 0.008*\"use\" + 0.007*\"great\" + 0.006*\"day\" + 0.006*\"supplement\"\n",
      "INFO : topic diff=0.256444, rho=0.127533\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -6.255 per-word bound, 76.4 perplexity estimate based on a held-out corpus of 4828 documents with 193875 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=292938, num_topics=15, decay=0.5, chunksize=10000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "DEBUG : performing inference on a chunk of 584828 documents\n",
      "DEBUG : 583632/584828 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 36s, sys: 43.2 s, total: 10min 19s\n",
      "Wall time: 10min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "# num_topics_list = np.arange(3, 21, 1)\n",
    "num_topics = 15\n",
    "chunksize = 10000    # number of docs processed at a time\n",
    "passes = 3\n",
    "iterations = 400\n",
    "eval_every = 1\n",
    "\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "# We set alpha = 'auto' and eta = 'auto'. Thus, we are automatically learning \n",
    "# 2 parameters in the model that we usually would have to specify explicitly.\n",
    "# for num_topics in tqdm(num_topics_list):\n",
    "# training the model\n",
    "lda = LdaMulticore(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta='auto', eval_every = eval_every, iterations=iterations)\n",
    "# performance metric\n",
    "cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "coherenceList_umass.append(cm.get_coherence())\n",
    "\n",
    "# visualization\n",
    "vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "pyLDAvis.save_html(vis, 'pyLDAvis_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0752244412473058]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherenceList_umass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topic coherence plot\n",
    "plotData = pd.DataFrame({'Number of topics':num_topics_list,\n",
    "                         'CoherenceScore':coherenceList_umass})\n",
    "f,ax = plt.subplots(figsize=(10,6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.pointplot(x='Number of topics',y= 'CoherenceScore',data=plotData)\n",
    "plt.axhline(y=-3.9)\n",
    "plt.title('Topic coherence')\n",
    "plt.savefig('Topic coherence plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
