{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# import cPickle as pickle\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm, tqdm_notebook, tnrange\n",
    "from S3_read_write import load_df_s3, save_df_s3\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas('Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'amazon-reviews-project'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_df_s3(bucket_name, 'amazon_reviews/reviews_data_clean', filetype='text', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585444, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape    # 585,444 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>categories_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>B-flax-D is a re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dpes the job well</td>\n",
       "      <td>Contains Organic...</td>\n",
       "      <td>New Generation B...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Studies show tha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fast shipping, g...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I started taking...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bioavailability ...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I tried Nutrihil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Other Resveratro...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I really liked t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I can't find thi...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful           reviewText  overall              summary  \\\n",
       "0  0929619730  [0, 0]  B-flax-D is a re...      5.0    Dpes the job well   \n",
       "1  0978559088  [1, 1]  Studies show tha...      4.0  Fast shipping, g...   \n",
       "2  0978559088  [1, 1]  I started taking...      5.0  Bioavailability ...   \n",
       "3  0978559088  [0, 1]  I tried Nutrihil...      1.0  Other Resveratro...   \n",
       "4  0978559088  [0, 0]  I really liked t...      5.0  I can't find thi...   \n",
       "\n",
       "           description                title     categories_clean  \n",
       "0  Contains Organic...  New Generation B...  Health & Persona...  \n",
       "1  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "2  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "3  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "4  Everyone knows t...  Nutrihill Resver...  Health & Persona...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, Resveratrol',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multivitamins',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Vitamins, Vitamin B, B3 (Niacin)',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Green Tea',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements, Green Coffee Bean Extract',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, CoQ10',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Ginkgo Biloba'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.categories_clean.unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catergories' list indicates that there may be some reviews in the dataset unrelated to health supplements.  Let's get rid of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Liturgy of St. John Chrysostom', 'Origins',\n",
       "       'Sounds of the Earth: Soft Ocean Sounds', 'Bali',\n",
       "       'Tranquil Waters', 'Bach: St. John Passion, BWV 245',\n",
       "       '21st Century Soul', 'Bodies for Strontium', \"John's Bunch\",\n",
       "       'An Evening of Paganini', \"John's Other Bunch\",\n",
       "       'Sus Mas Grandes Exitos', 'Complex Simplicity',\n",
       "       'Kidnapped By Neptune', 'Roman Chant / Easter Vespers', 'Dead 60s',\n",
       "       \"Cilla in the 60's\", 'Chromium', 'Letters From the Vitamin Sea',\n",
       "       'The Stinging Nettles', 'Tendres Annees 60', 'Wehiwehi Hawaii',\n",
       "       'none'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.categories_clean.str.contains('CDs & Vinyl')].title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews[reviews.categories_clean.str.contains('CDs & Vinyl')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product titles shown above are all music albums/songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews[~(reviews.categories_clean.str.contains('CDs & Vinyl'))]   # remove rows with category including 'CDs & Vinyl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, Resveratrol',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multivitamins',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Vitamins, Vitamin B, B3 (Niacin)',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Green Tea',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements, Green Coffee Bean Extract',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, CoQ10',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Ginkgo Biloba'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt.categories_clean.unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>categories_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>B00009QP4Q</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>The company has ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lives up to its ...</td>\n",
       "      <td>Alpha Five's QLi...</td>\n",
       "      <td>none</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>B0002TIEQQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I ordered this f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>waste of money</td>\n",
       "      <td>Self help tutori...</td>\n",
       "      <td>none</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin helpful           reviewText  overall              summary  \\\n",
       "3639   B00009QP4Q  [2, 2]  The company has ...      5.0  lives up to its ...   \n",
       "50015  B0002TIEQQ  [0, 0]  I ordered this f...      1.0       waste of money   \n",
       "\n",
       "               description title     categories_clean  \n",
       "3639   Alpha Five's QLi...  none  Health & Persona...  \n",
       "50015  Self help tutori...  none  Health & Persona...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt[reviews_filt.categories_clean.str.contains('Software')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt[~(reviews_filt.categories_clean.str.contains('Software'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of reviews of pet-related products\n",
    "search_for = [' pet ', ' cat ', ' dog ']\n",
    "pattern = '|'.join(search_for)\n",
    "reviews_filt.title.str.contains(pattern, case=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'Composure Liquid for Dogs and Cat (188 SERVINGS)'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt[reviews_filt.title.str.contains(pattern, case=False)]['title'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all pet products\n",
    "reviews_filt = reviews_filt[~(reviews_filt.title.str.contains(pattern, case=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned dataframe\n",
    "save_df_s3(df=reviews_filt, bucket_name=bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48501"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt.asin.nunique()     # 48,535 unique products and 585,179 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine One Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = reviews_filt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0929619730'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.asin     # Amazon Standard Identification Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Generation B-Flax-D'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.title     # this is the product's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.categories_clean   # previously filtered/curated categories of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contains Organic Cold-Milled Flaxseed\\nValuable source of soluble and insoluble fiber\\nProvides Omega-3 essential fats, and many other nutrients to help achieve and maintain optimal bowel function.\\n\\nContains Vitamin B12\\nB12 helps prevent nerve damage\\nB12 aids in healthy cell formation.\\nB12 helps prevent anemia\\n\\nContains Vitamin D\\nVitamin D assists the body in the absorption of important minerals like calcium.\\n\\nContains Seleno-yeast\\nA source of selenium, a mineral with powerful anti-viral and disease-fighting properties.\\n\\nContains Vitamin K2\\nMenaQ7TM provides vitamin K2 (menaquinone), extracted and concentrated from natto without solvents. Vitamin K2 prevents arterial calcification and promotes strong bones by improving cross-linking of osteocalcin, a protein found in bones. The amount here has been clinically shown not to interfere with blood anti-coagulant medication. \\n\\nServing Size:\\n1/4 Cup (30 Grams)\\n\\nServings Per Container:\\n30 Servings per container\\n\\nNet Wt. 32 oz / 2 lb (908 g)\\n\\nB-Flax-D is a 100% vegetarian product.\\n\\nDoes Not Contain:\\nContains no artificial colors or preservatives.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.description       # product description provided by the seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dpes the job well'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.summary      # review title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt around. Good product, good price, good results.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.reviewText   # review content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the actual review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.overall     # the rating provided by the reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/amazon_review_screenshot.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"images/amazon_review_screenshot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off using only the title (`summary`) and body (`reviewText`) of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 s, sys: 3.88 s, total: 7.18 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = load_df_s3(bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['helpful', 'overall', 'title', 'categories_clean', 'description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>B-flax-D is a regular at our house. It does it...</td>\n",
       "      <td>Dpes the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Studies show that Resveratrol is poorly absorb...</td>\n",
       "      <td>Fast shipping, good communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I started taking this after both my parents di...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I tried Nutrihill, but did not feel any of the...</td>\n",
       "      <td>Other Resveratrol Supplements are Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I really liked this product because it stayed ...</td>\n",
       "      <td>I can't find this product any longer, and I wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                         reviewText  \\\n",
       "0  0929619730  B-flax-D is a regular at our house. It does it...   \n",
       "1  0978559088  Studies show that Resveratrol is poorly absorb...   \n",
       "2  0978559088  I started taking this after both my parents di...   \n",
       "3  0978559088  I tried Nutrihill, but did not feel any of the...   \n",
       "4  0978559088  I really liked this product because it stayed ...   \n",
       "\n",
       "                                             summary  \n",
       "0                                  Dpes the job well  \n",
       "1                  Fast shipping, good communication  \n",
       "2                         Bioavailability is the key  \n",
       "3           Other Resveratrol Supplements are Better  \n",
       "4  I can't find this product any longer, and I wi...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each review, concatenate the review title and body\n",
    "df.reviewText = df.summary + '. ' + df.reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...</td>\n",
       "      <td>Dpes the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...</td>\n",
       "      <td>Fast shipping, good communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...</td>\n",
       "      <td>Other Resveratrol Supplements are Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0929619730   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  0978559088   \n",
       "4  0978559088   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \\\n",
       "0  Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...   \n",
       "1  Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...   \n",
       "2  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...   \n",
       "3  Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...   \n",
       "4  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...   \n",
       "\n",
       "                                                     summary  \n",
       "0                                          Dpes the job well  \n",
       "1                          Fast shipping, good communication  \n",
       "2                                 Bioavailability is the key  \n",
       "3                   Other Resveratrol Supplements are Better  \n",
       "4  I can't find this product any longer, and I wish I could.  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `summary` column now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0929619730   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  0978559088   \n",
       "4  0978559088   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \n",
       "0  Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...  \n",
       "1  Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...  \n",
       "2  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...  \n",
       "3  Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...  \n",
       "4  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.isnull().sum()    # 73 reviews have neither a review body text, nor a review title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop reviews with no text\n",
    "df = df[~(df.reviewText.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.asin.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few actual review texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Had high hopes but very dissapointed.. After listening to Dr. Oz speak highly of Collagen +C benefits on skin, I ordered and tried for two weeks.  Only took half dose, 3 a day instead of 6, very hard to swallow these horse pills.  Can cause terrible pain in esophagus if not enough water is taken along with pills.I must have had a very bad reaction to Super Collagen because canker sores started appearing on tongue and  top of tongue got very sore.  Haven't been able to eat for a few days.  Stopped taking and still waiting for tongue to heal.\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Too Strong!. I just had a bad reaction from it. Very light headed. I'm not blaming the product, for someone else it might work well, but for me it was too strong...\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rotten oil. I take flax seed oil, and when the local supermarket stopped selling it, I was happy to find it on Amazon. But I find this brand of oil gives me bad gas when I take it, which I believe is due to the oil being bad. I bought two bottles which I don't know what I am going to do with, hate to throw them away.\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 584829 entries, 0 to 584901\n",
      "Data columns (total 2 columns):\n",
      "asin          584829 non-null object\n",
      "reviewText    584829 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(df.reviewText.values)    # make an iterable to store only the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584829"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt around. Good product, good price, good results. \n",
      "\n",
      "Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This company promises 99% purity and has fast shipping and good communication. I can't comment on the quality of product because I'm not a chemist but they seem to be legitimate. \n",
      "\n",
      "Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspiring. Doing some research on the Internet, it is indicated that taking resveratrol in lozenge form is preferable as it is broken down by stomach acids.  The ez-melt formula recommended in another review is OK, but it is dissolved in the mouth much more quickly than this lozenge formula, while dissolving more slowly is preferable according to my research.This product has the greatest side effect - since taking it, I haven't had colds or sore throats.  Soon after starting to take it every day, I was starting to come down with a cold, with all my usual symptoms, and was anticipating being very sick the next day, as is my usual pattern.  But I never did get as sick as anticipated - taking this product is the only reason I can come up with.  Since then, I've had no colds or sore throats - it has been great.  I recommend this product to everyone I know, and have given it as gifts to my family. \n",
      "\n",
      "Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the dose is too small in this supplement. I then tried Resveratrol 150 from ezmelts. It's also a lozenge (they call it melt) and it literally melts in your mouth. I have a lot more energy, and haven't been sick at all. I used to get colds and flus all the time before. You can find ezmelts here on amazon, ebay and a bunch of other suppliers. You can find it here:EZ Melts Resveratrol, Natural Grape Flavor, 60 Tablets \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at a few sample reviews\n",
    "for rev in text[:4]:\n",
    "    print(rev, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below are from:\n",
    "\n",
    "http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `gensim`'s `Phrases` class to detect natural combinations of words (like 'vanilla ice cream'), we need to format our text into a list of sentences, with each sentence being a list of words.  This process takes a large amount of processing time (for reference, the times shown under the cells are for running the tasks on a c5.18xlarge EC2 instance (equivalent spot fleet)), so `text` has been split into 3 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Unigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584829"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into 9 parts\n",
    "text_first  = text[:50000]\n",
    "text_second = text[50000:100000]\n",
    "text_third  = text[100000:150000]\n",
    "text_fourth = text[150000:300000]\n",
    "text_fifth  = text[300000:350000]\n",
    "text_sixth  = text[350000:400000]\n",
    "text_seventh= text[400000:450000]\n",
    "text_eighth = text[450000:500000]\n",
    "text_ninth = text[500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:06, 102.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  50000\n",
      "current sent_num:  305895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rev_num = 0    # review tracker\n",
    "sent_num = 0   # sentence tracker\n",
    "unigram_sents_pos = [] # to store lists of lemmatized tokens for each sentence\n",
    "\n",
    "for parsed_review in tqdm(nlp.pipe(text_first, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305895"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_sents_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, [('dpe', 'NOUN'), ('the', 'DET'), ('job', 'NOUN'), ('well', 'ADV')]]\n",
      "[1, 2, [('b', 'NOUN'), ('flax', 'NOUN'), ('d', 'NOUN'), ('be', 'VERB'), ('a', 'DET'), ('regular', 'ADJ'), ('at', 'ADP'), ('-PRON-', 'ADJ'), ('house', 'NOUN')]]\n",
      "[1, 3, [('-PRON-', 'PRON'), ('do', 'VERB'), ('-PRON-', 'ADJ'), ('job', 'NOUN'), ('simply', 'ADV'), ('and', 'CCONJ'), ('with', 'ADP'), ('good', 'ADJ'), ('result', 'NOUN')]]\n",
      "[1, 4, [('-PRON-', 'PRON'), ('be', 'VERB'), ('reasonable', 'ADJ'), ('last', 'VERB'), ('a', 'DET'), ('long', 'ADJ'), ('time', 'NOUN'), ('and', 'CCONJ'), ('be', 'VERB'), ('able', 'ADJ'), ('to', 'PART'), ('be', 'VERB'), ('obtain', 'VERB'), ('with', 'ADP'), ('free', 'ADJ'), ('shipping', 'NOUN'), ('if', 'ADP'), ('-PRON-', 'PRON'), ('hunt', 'VERB'), ('around', 'ADV')]]\n",
      "[1, 5, [('good', 'ADJ'), ('product', 'NOUN'), ('good', 'ADJ'), ('price', 'NOUN'), ('good', 'ADJ'), ('result', 'NOUN')]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(unigram_sents_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "      <td>dpe the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN NOUN NOUN V...</td>\n",
       "      <td>b flax d be a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PRON VERB ADJ NO...</td>\n",
       "      <td>-PRON- do -PRON-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON VERB ADJ VE...</td>\n",
       "      <td>-PRON- be reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ADJ NOUN ADJ NOU...</td>\n",
       "      <td>good product goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number          unigram_pos    unigram_sentences\n",
       "0              1                1    NOUN DET NOUN ADV     dpe the job well\n",
       "1              1                2  NOUN NOUN NOUN V...  b flax d be a re...\n",
       "2              1                3  PRON VERB ADJ NO...  -PRON- do -PRON-...\n",
       "3              1                4  PRON VERB ADJ VE...  -PRON- be reason...\n",
       "4              1                5  ADJ NOUN ADJ NOU...  good product goo..."
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:04, 103.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  100000\n",
      "current sent_num:  616751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_second, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616751\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_sents_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:55, 105.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  150000\n",
      "current sent_num:  923642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_third, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [23:51, 104.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  300000\n",
      "current sent_num:  1843092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fourth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:43, 107.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  350000\n",
      "current sent_num:  2144424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fifth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:46, 107.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  400000\n",
      "current sent_num:  2447985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_sixth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:41, 108.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  450000\n",
      "current sent_num:  2754623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_seventh, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:04, 103.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  500000\n",
      "current sent_num:  3073060\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_eighth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84829it [13:30, 104.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  584829\n",
      "current sent_num:  3605491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_ninth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = ' '.join(word for word in word_list)\n",
    "    pos_joined   = ' '.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T LOAD THIS FILE - there's a _v1 version further down!\n",
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "      <td>dpe the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ADJ NOUN ADJ NOUN ADJ NOUN</td>\n",
       "      <td>good product good price good result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                                  NOUN DET NOUN ADV   \n",
       "1           NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN   \n",
       "2          PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN   \n",
       "3  PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...   \n",
       "4                         ADJ NOUN ADJ NOUN ADJ NOUN   \n",
       "\n",
       "                                   unigram_sentences  \n",
       "0                                   dpe the job well  \n",
       "1              b flax d be a regular at -PRON- house  \n",
       "2   -PRON- do -PRON- job simply and with good result  \n",
       "3  -PRON- be reasonable last a long time and be a...  \n",
       "4                good product good price good result  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(sentence, sentence_pos):\n",
    "    \"\"\"Expects a sentence as a single string as input 1, and its corresponding part-of-speech tags as input 2 (also single string).\n",
    "    Cleans it up and returns a single string.\n",
    "    Also updates corresponding part-of-speech string.\n",
    "    \"\"\"\n",
    "    # get rid of webpage links\n",
    "    cond = ['http' in sentence, 'www' in sentence]\n",
    "    if any(cond):\n",
    "        words = sentence.split(' ')\n",
    "        words_pos = sentence_pos.split(' ')\n",
    "        to_remove = []\n",
    "        for i in range(len(words)):\n",
    "            cond_word = ['http' in words[i], 'www' in words[i]]\n",
    "            if any(cond_word):\n",
    "                to_remove.append(i)\n",
    "        # remove words that are links\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del words[j]\n",
    "            del words_pos[j]\n",
    "        # reconstruct sentence after deleting links\n",
    "        sentence = ' '.join(words)\n",
    "        sentence_pos = ' '.join(words_pos)\n",
    "\n",
    "    # replace underscores with blanks to avoid mix-up with paired words later\n",
    "    # cannot replace with spaces because the strings are split on spaces later \n",
    "    # and this would create new words with no corresponding pos tags\n",
    "    if '_' in sentence:\n",
    "        sentence = sentence.replace('_', '')\n",
    "    return sentence, sentence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!',\n",
       " 'this is a normal sentence',\n",
       " '__ what is this ____ http',\n",
       " '_',\n",
       " 'http']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = ['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!', 'this is a normal sentence', \n",
    "              '__ what is this ____ http', '_', 'http']\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_pos = ['X X X X X X X X X X X X', 'X X X X X', 'X X X X X X', 'X', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy and also BAM! underscoretime!',\n",
       " 'this is a normal sentence',\n",
       " ' what is this ',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if clean_up works as expected\n",
    "for i in range(len(test_clean)):\n",
    "    sentence = test_clean[i]\n",
    "    sentence_pos = test_clean_pos[i]\n",
    "    test_clean[i], test_clean_pos[i] = clean_up(sentence, sentence_pos)\n",
    "\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X X X X X X X X X X X', 'X X X X X', 'X X X X X', 'X', '']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5, 1, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5, 1, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_joined_all = unigram_sentences_savedf.unigram_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605491"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if '_' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'http' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'www' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>1290</td>\n",
       "      <td>7548</td>\n",
       "      <td>X</td>\n",
       "      <td>http://www.amazon.com/gp/product/b0000533z8/re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16179</th>\n",
       "      <td>2775</td>\n",
       "      <td>16180</td>\n",
       "      <td>DET NOUN NOUN</td>\n",
       "      <td>no jet_lag pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>2837</td>\n",
       "      <td>16629</td>\n",
       "      <td>PRON VERB VERB PROPN PART NUM PROPN ADP ADP AD...</td>\n",
       "      <td>-PRON- do recommend women 's one a_day though ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23009</th>\n",
       "      <td>3833</td>\n",
       "      <td>23010</td>\n",
       "      <td>PRON VERB ADJ NOUN CCONJ ADJ NOUN NOUN</td>\n",
       "      <td>-PRON- be less money and good quality https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25117</th>\n",
       "      <td>4169</td>\n",
       "      <td>25118</td>\n",
       "      <td>ADJ PART NOUN ADV</td>\n",
       "      <td>easy to use_work well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_number  sentence_number  \\\n",
       "7547            1290             7548   \n",
       "16179           2775            16180   \n",
       "16628           2837            16629   \n",
       "23009           3833            23010   \n",
       "25117           4169            25118   \n",
       "\n",
       "                                             unigram_pos  \\\n",
       "7547                                                   X   \n",
       "16179                                      DET NOUN NOUN   \n",
       "16628  PRON VERB VERB PROPN PART NUM PROPN ADP ADP AD...   \n",
       "23009             PRON VERB ADJ NOUN CCONJ ADJ NOUN NOUN   \n",
       "25117                                  ADJ PART NOUN ADV   \n",
       "\n",
       "                                       unigram_sentences  \n",
       "7547   http://www.amazon.com/gp/product/b0000533z8/re...  \n",
       "16179                                    no jet_lag pill  \n",
       "16628  -PRON- do recommend women 's one a_day though ...  \n",
       "23009  -PRON- be less money and good quality https://...  \n",
       "25117                              easy to use_work well  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences.str.contains('_')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.amazon.com/gp/product/b0000533z8/ref=cm_cr_rev_prod_title',\n",
       " 'no jet_lag pill',\n",
       " \"-PRON- do recommend women 's one a_day though with extra calcium\",\n",
       " '-PRON- be less money and good quality https://www.amazon.com/review/review-your-purchases/ref=pe_6680_116681230_cm_add_2_star3?_encoding=utf8&asins;=b0000ccw1n%3a3%2cb000sar2dk&channel;=ec_phy&crauthtoken;=ge5g%2bbf%2btr%2f%2fdliytbmmzxn6ajjlfxjdtx902p0aaaadaaaaafnfv%2bbyyxcaaaaa&customerid;=a1pansxlpbgvng#top',\n",
       " 'easy to use_work well',\n",
       " '-PRON- have have pedometer in the past_all difficult and confusing to use to the point -PRON- simply give up on -PRON-',\n",
       " 'overall -PRON- mother be very satisfied with this product!-d_lionz',\n",
       " 'this inexpensive strap with a metal clip http://www.amazon.com/gp/product/b000bitymg/ref=oh_details_o00_s00_i00?ie=utf8&psc;=1 be a good replacement for the flimsy omron plastic clip but -PRON- have not be use -PRON- long',\n",
       " 'hj_112 digital pemium pedometer update',\n",
       " 'accordingly for 100gr serving size of;sesame__flax__chia__amaranth__quinoa__hemp__pistachio__almond__cashew__peanutcalories565__534__490__371__368__567__557__575__553__585sat fat34__18__16__7__4__17__27__19__39__34calcium99__26__63__16__5__0__11__26__4__5magnesium89__98__0__62__49__167__30__67__73__44potassium14__23__5__15__16__167__29__20__19__19zinc48__29__23__19__21__83__15__21__39__22fiber47__109__151__8__11__4 41__49__13__32phytosterols714__0__0__0__0__n a__214__0__0__0freshness fresh mean nutritious and delicious']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence for sentence in words_joined_all if '_' in sentence][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up all unigrams\n",
    "for i in range(len(words_joined_all)):\n",
    "    sentence = words_joined_all[i]\n",
    "    sentence_pos = pos_joined_all[i]\n",
    "    words_joined_all[i], pos_joined_all[i] = clean_up(sentence, sentence_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'http' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if '_' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     dpe the job well\n",
       "1                b flax d be a regular at -PRON- house\n",
       "2     -PRON- do -PRON- job simply and with good result\n",
       "3    -PRON- be reasonable last a long time and be a...\n",
       "4                  good product good price good result\n",
       "Name: unigram_sentences, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.unigram_sentences.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dpe the job well',\n",
       " 'b flax d be a regular at -PRON- house',\n",
       " '-PRON- do -PRON- job simply and with good result',\n",
       " '-PRON- be reasonable last a long time and be able to be obtain with free shipping if -PRON- hunt around',\n",
       " 'good product good price good result']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_joined_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf.drop(['unigram_sentences'], axis=1, inplace=True)\n",
    "unigram_sentences_savedf.drop(['unigram_pos'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf['unigram_sentences'] = words_joined_all\n",
    "unigram_sentences_savedf['unigram_pos'] = pos_joined_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe the job well</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "      <td>NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "      <td>PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "      <td>PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good product good price good result</td>\n",
       "      <td>ADJ NOUN ADJ NOUN ADJ NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                                   dpe the job well   \n",
       "1              b flax d be a regular at -PRON- house   \n",
       "2   -PRON- do -PRON- job simply and with good result   \n",
       "3  -PRON- be reasonable last a long time and be a...   \n",
       "4                good product good price good result   \n",
       "\n",
       "                                         unigram_pos  \n",
       "0                                  NOUN DET NOUN ADV  \n",
       "1           NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN  \n",
       "2          PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN  \n",
       "3  PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...  \n",
       "4                         ADJ NOUN ADJ NOUN ADJ NOUN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated, cleaned up version of unigram_sentences.feather\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences_v1.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = [sentence.split(' ') for sentence in words_joined_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dpe', 'the', 'job', 'well'], ['b', 'flax', 'd', 'be', 'a', 'regular', 'at', '-PRON-', 'house'], ['-PRON-', 'do', '-PRON-', 'job', 'simply', 'and', 'with', 'good', 'result'], ['-PRON-', 'be', 'reasonable', 'last', 'a', 'long', 'time', 'and', 'be', 'able', 'to', 'be', 'obtain', 'with', 'free', 'shipping', 'if', '-PRON-', 'hunt', 'around']]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sentences[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605491"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 1.5 s, total: 3min 55s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The common_terms parameter add a way to give special treatment to common terms \n",
    "# (aka stop words) such that their presence between two words wont prevent bigram detection. \n",
    "# It allows to detect expressions like bank of america\n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\"]\n",
    "\n",
    "# Train a first-order phrase detector\n",
    "bigram_model = Phrases(unigram_sentences, threshold=0.7, scoring='npmi', common_terms=common_terms)\n",
    "\n",
    "# Transform unigram sentences into bigram sentences\n",
    "# Paired words are connected by an underscore, e.g. ice_cream\n",
    "bigram_sentences = []\n",
    "for sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 1.44 s, total: 4min\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a second-order phrase detector\n",
    "# trigram_model = Phrases(bigram_sentences, min_count=5)\n",
    "trigram_model = Phrases(bigram_sentences, threshold=0.7, scoring='npmi')\n",
    "\n",
    "# Transform bigram sentences into trigram sentences\n",
    "trigram_sentences = []\n",
    "for sentence in bigram_sentences:\n",
    "    trigram_sentences.append(trigram_model[sentence])\n",
    "\n",
    "# remove any remaining stopwords\n",
    "# trigram_sentences = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in trigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trigrams will be saved in a dataframe with a single column.\n",
    "# each row is one sentence from any review\n",
    "# each sentence is a single string separated by a single space.\n",
    "trigram_sentences_savedf = pd.DataFrame([u' '.join(sentence) for sentence in trigram_sentences], columns=['preprocessed_review'])\n",
    "# note: v1 is the version with higher threshold (0.7); the file without v1 uses 0.5.\n",
    "save_df_s3(trigram_sentences_savedf, bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dpe the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good product good price good result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 preprocessed_review\n",
       "0                                   dpe the job well\n",
       "1              b flax d be a regular at -PRON- house\n",
       "2   -PRON- do -PRON- job simply and with good result\n",
       "3  -PRON- be reasonable last a long time and be a...\n",
       "4                good product good price good result"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram_sentences = trigram_sentences_savedf.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605491"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(trigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe the job well</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "      <td>NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "      <td>PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "      <td>PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good product good price good result</td>\n",
       "      <td>ADJ NOUN ADJ NOUN ADJ NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                                   dpe the job well   \n",
       "1              b flax d be a regular at -PRON- house   \n",
       "2   -PRON- do -PRON- job simply and with good result   \n",
       "3  -PRON- be reasonable last a long time and be a...   \n",
       "4                good product good price good result   \n",
       "\n",
       "                                         unigram_pos  \n",
       "0                                  NOUN DET NOUN ADV  \n",
       "1           NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN  \n",
       "2          PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN  \n",
       "3  PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...  \n",
       "4                         ADJ NOUN ADJ NOUN ADJ NOUN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3605491, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unigram_sentences_savedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = pd.merge(unigram_sents_pos_df, trigram_sentences_savedf, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe the job well</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "      <td>dpe the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "      <td>NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "      <td>PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "      <td>PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good product good price good result</td>\n",
       "      <td>ADJ NOUN ADJ NOUN ADJ NOUN</td>\n",
       "      <td>good product good price good result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>fast shipping good communication</td>\n",
       "      <td>ADJ NOUN ADJ NOUN</td>\n",
       "      <td>fast shipping good communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>study show that resveratrol be poorly absorb w...</td>\n",
       "      <td>NOUN VERB ADP PROPN VERB ADV VERB ADV VERB ADP...</td>\n",
       "      <td>study show that resveratrol be poorly absorb w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>hardly any company be sell lozenge</td>\n",
       "      <td>ADV DET NOUN VERB VERB NOUN</td>\n",
       "      <td>hardly any company be sell lozenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>this company promise 99 purity and have fast s...</td>\n",
       "      <td>DET NOUN VERB NUM NOUN CCONJ VERB ADJ NOUN CCO...</td>\n",
       "      <td>this company promise 99 purity and have fast s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-PRON- can not comment on the quality of produ...</td>\n",
       "      <td>PRON VERB ADV VERB ADP DET NOUN ADP NOUN ADP P...</td>\n",
       "      <td>-PRON- can not comment on the quality of produ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "5              2                6   \n",
       "6              2                7   \n",
       "7              2                8   \n",
       "8              2                9   \n",
       "9              2               10   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                                   dpe the job well   \n",
       "1              b flax d be a regular at -PRON- house   \n",
       "2   -PRON- do -PRON- job simply and with good result   \n",
       "3  -PRON- be reasonable last a long time and be a...   \n",
       "4                good product good price good result   \n",
       "5                   fast shipping good communication   \n",
       "6  study show that resveratrol be poorly absorb w...   \n",
       "7                 hardly any company be sell lozenge   \n",
       "8  this company promise 99 purity and have fast s...   \n",
       "9  -PRON- can not comment on the quality of produ...   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                                  NOUN DET NOUN ADV   \n",
       "1           NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN   \n",
       "2          PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN   \n",
       "3  PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...   \n",
       "4                         ADJ NOUN ADJ NOUN ADJ NOUN   \n",
       "5                                  ADJ NOUN ADJ NOUN   \n",
       "6  NOUN VERB ADP PROPN VERB ADV VERB ADV VERB ADP...   \n",
       "7                        ADV DET NOUN VERB VERB NOUN   \n",
       "8  DET NOUN VERB NUM NOUN CCONJ VERB ADJ NOUN CCO...   \n",
       "9  PRON VERB ADV VERB ADP DET NOUN ADP NOUN ADP P...   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0                                   dpe the job well  \n",
       "1              b flax d be a regular at -PRON- house  \n",
       "2   -PRON- do -PRON- job simply and with good result  \n",
       "3  -PRON- be reasonable last a long time and be a...  \n",
       "4                good product good price good result  \n",
       "5                   fast shipping good communication  \n",
       "6  study show that resveratrol be poorly absorb w...  \n",
       "7                 hardly any company be sell lozenge  \n",
       "8  this company promise 99 purity and have fast s...  \n",
       "9  -PRON- can not comment on the quality of produ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_s3(unigram_sents_pos_df, bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>70</td>\n",
       "      <td>401</td>\n",
       "      <td>-PRON- do not know buy this product will becom...</td>\n",
       "      <td>PRON VERB ADV VERB VERB DET NOUN VERB VERB DET...</td>\n",
       "      <td>-PRON- do_not know buy this product will becom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>70</td>\n",
       "      <td>402</td>\n",
       "      <td>-PRON- think -PRON- just fraud</td>\n",
       "      <td>PRON VERB ADJ ADJ NOUN</td>\n",
       "      <td>-PRON- think -PRON- just fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>70</td>\n",
       "      <td>403</td>\n",
       "      <td>do not recommend this product</td>\n",
       "      <td>VERB ADV VERB DET NOUN</td>\n",
       "      <td>do_not recommend this product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>71</td>\n",
       "      <td>404</td>\n",
       "      <td>mould motion 5 do not work!!. -PRON- recently ...</td>\n",
       "      <td>VERB PROPN NUM VERB ADV ADJ PRON ADV VERB DET ...</td>\n",
       "      <td>mould_motion 5 do_not work!!. -PRON- recently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>71</td>\n",
       "      <td>405</td>\n",
       "      <td>-PRON- do not sweat or ne thing just burn</td>\n",
       "      <td>PRON VERB ADV VERB CCONJ NOUN NOUN ADV VERB</td>\n",
       "      <td>-PRON- do_not sweat or ne thing just burn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>71</td>\n",
       "      <td>406</td>\n",
       "      <td>and i have -PRON- over -PRON- shirt because -P...</td>\n",
       "      <td>CCONJ PRON VERB PRON ADP ADJ NOUN ADP PRON VER...</td>\n",
       "      <td>and i have -PRON- over -PRON- shirt because -P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>71</td>\n",
       "      <td>407</td>\n",
       "      <td>and besides -PRON- still have to boil -PRON- f...</td>\n",
       "      <td>CCONJ ADP PRON ADV VERB PART VERB PRON ADP NUM...</td>\n",
       "      <td>and besides -PRON- still have to boil -PRON- f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>71</td>\n",
       "      <td>408</td>\n",
       "      <td>just too much to wait til -PRON- put -PRON- on</td>\n",
       "      <td>ADV ADV ADJ PART VERB ADV PRON VERB PRON PART</td>\n",
       "      <td>just too much to wait til -PRON- put -PRON- on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>71</td>\n",
       "      <td>409</td>\n",
       "      <td>do not buy</td>\n",
       "      <td>VERB ADV VERB</td>\n",
       "      <td>do_not buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>72</td>\n",
       "      <td>410</td>\n",
       "      <td>be a gift</td>\n",
       "      <td>VERB DET NOUN</td>\n",
       "      <td>be a gift</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number  \\\n",
       "400             70              401   \n",
       "401             70              402   \n",
       "402             70              403   \n",
       "403             71              404   \n",
       "404             71              405   \n",
       "405             71              406   \n",
       "406             71              407   \n",
       "407             71              408   \n",
       "408             71              409   \n",
       "409             72              410   \n",
       "\n",
       "                                     unigram_sentences  \\\n",
       "400  -PRON- do not know buy this product will becom...   \n",
       "401                     -PRON- think -PRON- just fraud   \n",
       "402                      do not recommend this product   \n",
       "403  mould motion 5 do not work!!. -PRON- recently ...   \n",
       "404          -PRON- do not sweat or ne thing just burn   \n",
       "405  and i have -PRON- over -PRON- shirt because -P...   \n",
       "406  and besides -PRON- still have to boil -PRON- f...   \n",
       "407     just too much to wait til -PRON- put -PRON- on   \n",
       "408                                         do not buy   \n",
       "409                                          be a gift   \n",
       "\n",
       "                                           unigram_pos  \\\n",
       "400  PRON VERB ADV VERB VERB DET NOUN VERB VERB DET...   \n",
       "401                             PRON VERB ADJ ADJ NOUN   \n",
       "402                             VERB ADV VERB DET NOUN   \n",
       "403  VERB PROPN NUM VERB ADV ADJ PRON ADV VERB DET ...   \n",
       "404        PRON VERB ADV VERB CCONJ NOUN NOUN ADV VERB   \n",
       "405  CCONJ PRON VERB PRON ADP ADJ NOUN ADP PRON VER...   \n",
       "406  CCONJ ADP PRON ADV VERB PART VERB PRON ADP NUM...   \n",
       "407      ADV ADV ADJ PART VERB ADV PRON VERB PRON PART   \n",
       "408                                      VERB ADV VERB   \n",
       "409                                      VERB DET NOUN   \n",
       "\n",
       "                                   preprocessed_review  \n",
       "400  -PRON- do_not know buy this product will becom...  \n",
       "401                     -PRON- think -PRON- just fraud  \n",
       "402                      do_not recommend this product  \n",
       "403  mould_motion 5 do_not work!!. -PRON- recently ...  \n",
       "404          -PRON- do_not sweat or ne thing just burn  \n",
       "405  and i have -PRON- over -PRON- shirt because -P...  \n",
       "406  and besides -PRON- still have to boil -PRON- f...  \n",
       "407     just too much to wait til -PRON- put -PRON- on  \n",
       "408                                         do_not buy  \n",
       "409                                          be a gift  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df['has_paired_words'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df.loc[unigram_sents_pos_df.preprocessed_review.str.contains('_'), ['has_paired_words']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650163"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.has_paired_words.sum()  # number of sentences with paired words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe the job well</td>\n",
       "      <td>NOUN DET NOUN ADV</td>\n",
       "      <td>dpe the job well</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "      <td>NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN</td>\n",
       "      <td>b flax d be a regular at -PRON- house</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "      <td>PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN</td>\n",
       "      <td>-PRON- do -PRON- job simply and with good result</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "      <td>PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...</td>\n",
       "      <td>-PRON- be reasonable last a long time and be a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good product good price good result</td>\n",
       "      <td>ADJ NOUN ADJ NOUN ADJ NOUN</td>\n",
       "      <td>good product good price good result</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                                   dpe the job well   \n",
       "1              b flax d be a regular at -PRON- house   \n",
       "2   -PRON- do -PRON- job simply and with good result   \n",
       "3  -PRON- be reasonable last a long time and be a...   \n",
       "4                good product good price good result   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                                  NOUN DET NOUN ADV   \n",
       "1           NOUN NOUN NOUN VERB DET ADJ ADP ADJ NOUN   \n",
       "2          PRON VERB ADJ NOUN ADV CCONJ ADP ADJ NOUN   \n",
       "3  PRON VERB ADJ VERB DET ADJ NOUN CCONJ VERB ADJ...   \n",
       "4                         ADJ NOUN ADJ NOUN ADJ NOUN   \n",
       "\n",
       "                                 preprocessed_review  has_paired_words  \n",
       "0                                   dpe the job well                 0  \n",
       "1              b flax d be a regular at -PRON- house                 0  \n",
       "2   -PRON- do -PRON- job simply and with good result                 0  \n",
       "3  -PRON- be reasonable last a long time and be a...                 0  \n",
       "4                good product good price good result                 0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 2.94 s, total: 28.9 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigram_sents_pos_df.unigram_pos = unigram_sents_pos_df.unigram_pos.str.split(' ')\n",
    "unigram_sents_pos_df.unigram_sentences = unigram_sents_pos_df.unigram_sentences.str.split(' ')\n",
    "unigram_sents_pos_df.preprocessed_review = unigram_sents_pos_df.preprocessed_review.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>70</td>\n",
       "      <td>401</td>\n",
       "      <td>[-PRON-, do, not, know, buy, this, product, wi...</td>\n",
       "      <td>[PRON, VERB, ADV, VERB, VERB, DET, NOUN, VERB,...</td>\n",
       "      <td>[-PRON-, do_not, know, buy, this, product, wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>70</td>\n",
       "      <td>402</td>\n",
       "      <td>[-PRON-, think, -PRON-, just, fraud]</td>\n",
       "      <td>[PRON, VERB, ADJ, ADJ, NOUN]</td>\n",
       "      <td>[-PRON-, think, -PRON-, just, fraud]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>70</td>\n",
       "      <td>403</td>\n",
       "      <td>[do, not, recommend, this, product]</td>\n",
       "      <td>[VERB, ADV, VERB, DET, NOUN]</td>\n",
       "      <td>[do_not, recommend, this, product]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>71</td>\n",
       "      <td>404</td>\n",
       "      <td>[mould, motion, 5, do, not, work!!., -PRON-, r...</td>\n",
       "      <td>[VERB, PROPN, NUM, VERB, ADV, ADJ, PRON, ADV, ...</td>\n",
       "      <td>[mould_motion, 5, do_not, work!!., -PRON-, rec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>71</td>\n",
       "      <td>405</td>\n",
       "      <td>[-PRON-, do, not, sweat, or, ne, thing, just, ...</td>\n",
       "      <td>[PRON, VERB, ADV, VERB, CCONJ, NOUN, NOUN, ADV...</td>\n",
       "      <td>[-PRON-, do_not, sweat, or, ne, thing, just, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>71</td>\n",
       "      <td>406</td>\n",
       "      <td>[and, i, have, -PRON-, over, -PRON-, shirt, be...</td>\n",
       "      <td>[CCONJ, PRON, VERB, PRON, ADP, ADJ, NOUN, ADP,...</td>\n",
       "      <td>[and, i, have, -PRON-, over, -PRON-, shirt, be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>71</td>\n",
       "      <td>407</td>\n",
       "      <td>[and, besides, -PRON-, still, have, to, boil, ...</td>\n",
       "      <td>[CCONJ, ADP, PRON, ADV, VERB, PART, VERB, PRON...</td>\n",
       "      <td>[and, besides, -PRON-, still, have, to, boil, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>71</td>\n",
       "      <td>408</td>\n",
       "      <td>[just, too, much, to, wait, til, -PRON-, put, ...</td>\n",
       "      <td>[ADV, ADV, ADJ, PART, VERB, ADV, PRON, VERB, P...</td>\n",
       "      <td>[just, too, much, to, wait, til, -PRON-, put, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>71</td>\n",
       "      <td>409</td>\n",
       "      <td>[do, not, buy]</td>\n",
       "      <td>[VERB, ADV, VERB]</td>\n",
       "      <td>[do_not, buy]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>72</td>\n",
       "      <td>410</td>\n",
       "      <td>[be, a, gift]</td>\n",
       "      <td>[VERB, DET, NOUN]</td>\n",
       "      <td>[be, a, gift]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number  \\\n",
       "400             70              401   \n",
       "401             70              402   \n",
       "402             70              403   \n",
       "403             71              404   \n",
       "404             71              405   \n",
       "405             71              406   \n",
       "406             71              407   \n",
       "407             71              408   \n",
       "408             71              409   \n",
       "409             72              410   \n",
       "\n",
       "                                     unigram_sentences  \\\n",
       "400  [-PRON-, do, not, know, buy, this, product, wi...   \n",
       "401               [-PRON-, think, -PRON-, just, fraud]   \n",
       "402                [do, not, recommend, this, product]   \n",
       "403  [mould, motion, 5, do, not, work!!., -PRON-, r...   \n",
       "404  [-PRON-, do, not, sweat, or, ne, thing, just, ...   \n",
       "405  [and, i, have, -PRON-, over, -PRON-, shirt, be...   \n",
       "406  [and, besides, -PRON-, still, have, to, boil, ...   \n",
       "407  [just, too, much, to, wait, til, -PRON-, put, ...   \n",
       "408                                     [do, not, buy]   \n",
       "409                                      [be, a, gift]   \n",
       "\n",
       "                                           unigram_pos  \\\n",
       "400  [PRON, VERB, ADV, VERB, VERB, DET, NOUN, VERB,...   \n",
       "401                       [PRON, VERB, ADJ, ADJ, NOUN]   \n",
       "402                       [VERB, ADV, VERB, DET, NOUN]   \n",
       "403  [VERB, PROPN, NUM, VERB, ADV, ADJ, PRON, ADV, ...   \n",
       "404  [PRON, VERB, ADV, VERB, CCONJ, NOUN, NOUN, ADV...   \n",
       "405  [CCONJ, PRON, VERB, PRON, ADP, ADJ, NOUN, ADP,...   \n",
       "406  [CCONJ, ADP, PRON, ADV, VERB, PART, VERB, PRON...   \n",
       "407  [ADV, ADV, ADJ, PART, VERB, ADV, PRON, VERB, P...   \n",
       "408                                  [VERB, ADV, VERB]   \n",
       "409                                  [VERB, DET, NOUN]   \n",
       "\n",
       "                                   preprocessed_review  has_paired_words  \n",
       "400  [-PRON-, do_not, know, buy, this, product, wil...                 1  \n",
       "401               [-PRON-, think, -PRON-, just, fraud]                 0  \n",
       "402                 [do_not, recommend, this, product]                 1  \n",
       "403  [mould_motion, 5, do_not, work!!., -PRON-, rec...                 1  \n",
       "404  [-PRON-, do_not, sweat, or, ne, thing, just, b...                 1  \n",
       "405  [and, i, have, -PRON-, over, -PRON-, shirt, be...                 0  \n",
       "406  [and, besides, -PRON-, still, have, to, boil, ...                 0  \n",
       "407  [just, too, much, to, wait, til, -PRON-, put, ...                 0  \n",
       "408                                      [do_not, buy]                 1  \n",
       "409                                      [be, a, gift]                 0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "has_paired_words       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an arbitrary sentence and it's transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_sentences.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_pos.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des_intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.preprocessed_review.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramlist = [word for sent in trigram_sentences for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do_not', 268437),\n",
       " ('weight_loss', 35842),\n",
       " ('side_effect', 28079),\n",
       " ('fish_oil', 24658),\n",
       " ('highly_recommend', 23157),\n",
       " ('garcinia_cambogia', 11913),\n",
       " ('dr._oz', 7675),\n",
       " ('blood_pressure', 6993),\n",
       " ('five_star', 6845),\n",
       " ('krill_oil', 6513),\n",
       " ('immune_system', 6365),\n",
       " ('customer_service', 6246),\n",
       " ('gel_cap', 6239),\n",
       " ('green_coffee_bean_extract', 5419),\n",
       " ('blood_sugar', 5019),\n",
       " ('raspberry_ketone', 4497),\n",
       " ('look_forward', 4409),\n",
       " ('green_tea', 4303),\n",
       " ('empty_stomach', 4001),\n",
       " ('500_mg', 3867),\n",
       " ('30_minute', 3716),\n",
       " ('appetite_suppressant', 3521),\n",
       " ('raspberry_ketones', 3372),\n",
       " ('hot_flash', 3326),\n",
       " ('green_coffee', 3264),\n",
       " ('1000_mg', 3197),\n",
       " ('fat_burner', 2647),\n",
       " ('dr_oz', 2513),\n",
       " ('new_chapter', 2357),\n",
       " ('fatty_acid', 2238),\n",
       " ('green_coffee_bean', 2222),\n",
       " ('expiration_date', 2139),\n",
       " ('fall_asleep', 2138),\n",
       " ('acid_reflux', 2104),\n",
       " ('pre_workout', 2028),\n",
       " ('b_complex', 1989),\n",
       " ('jarrow_formulas', 1824),\n",
       " ('bowel_movement', 1757),\n",
       " ('anti_inflammatory', 1740),\n",
       " ('milk_thistle', 1731),\n",
       " ('side_affect', 1728),\n",
       " ('cod_liver_oil', 1716),\n",
       " ('soft_gel', 1694),\n",
       " ('almond_milk', 1622),\n",
       " ('amino_acid', 1622),\n",
       " ('folic_acid', 1612),\n",
       " ('garden_of_life', 1608),\n",
       " ('peanut_butter', 1587),\n",
       " ('source_naturals', 1527),\n",
       " ('life_extension', 1467),\n",
       " ('colon_cleanse', 1420),\n",
       " ('sore_throat', 1344),\n",
       " ('rainbow_light', 1331),\n",
       " ('family_member', 1328),\n",
       " ('nordic_naturals', 1322),\n",
       " ('pleasantly_surprised', 1311),\n",
       " ('1000_mgs_omega-3', 1247),\n",
       " ('eating_habit', 1235),\n",
       " ('subscribe_and_save', 1207),\n",
       " ('nature_wise', 1178),\n",
       " ('bottom_line', 1160),\n",
       " ('non_gmo', 1159),\n",
       " ('grocery_store', 1152),\n",
       " ('coffee_bean', 1150),\n",
       " ('aloe_vera', 1127),\n",
       " ('amazonrecommended_serving', 1122),\n",
       " ('chia_seed', 1115),\n",
       " ('magnesium_stearate', 1114),\n",
       " ('certify_kosher', 1108),\n",
       " ('l_carnitine', 1092),\n",
       " ('ice_cream', 1078),\n",
       " ('allergic_reaction', 1066),\n",
       " ('hyaluronic_acid', 1060),\n",
       " ('four_stars', 1033),\n",
       " ('flax_seed', 1029),\n",
       " ('red_yeast_rice', 1010),\n",
       " ('colloidal_silver', 1009),\n",
       " ('magnesium_oxide', 1000),\n",
       " ('ascorbic_acid', 998),\n",
       " ('artificial_sweetener', 969),\n",
       " ('enteric_coat', 958),\n",
       " ('d_mannose', 928),\n",
       " ('bone_density', 899),\n",
       " ('co_worker', 882),\n",
       " ('african_mango', 858),\n",
       " ('flu_season', 855),\n",
       " ('mood_swing', 842),\n",
       " ('timely_manner', 795),\n",
       " ('green_vibrance', 786),\n",
       " ('bee_pollen', 785),\n",
       " ('digestive_tract', 773),\n",
       " ('buyer_beware', 767),\n",
       " ('macular_degeneration', 765),\n",
       " ('fish_oils', 717),\n",
       " ('anti_oxidant', 716),\n",
       " ('heavy_metal', 712),\n",
       " ('alpha_lipoic_acid', 708),\n",
       " ('emergen_c', 707),\n",
       " ('olive_leaf', 696),\n",
       " ('apple_cider_vinegar', 693)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq = Counter(gramlist)\n",
    "paired_words_frq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('veep_university---', 1),\n",
       " ('expereienc_with_veep', 1),\n",
       " ('veep_lookcut', 1),\n",
       " ('8220_recommended&#8221', 1),\n",
       " ('wishful_thinking!ftc', 1),\n",
       " ('atrail_fibrillationso', 1),\n",
       " ('george_flansbaum', 1),\n",
       " ('34;daily_supplements&#34', 1),\n",
       " ('rebecca_peagler', 1),\n",
       " ('channel_uctciyg3wusbfxkgyjfpz8og', 1),\n",
       " ('su_rodilla', 1),\n",
       " ('productmuy_buen_producto', 1),\n",
       " ('bri_nutrition&#8217;s_unconditional', 1),\n",
       " ('8220;bowel_issues&#8221', 1),\n",
       " ('34;all_natural&#34;i', 1),\n",
       " ('occurrence_of_fosmon', 1),\n",
       " ('34;last_diet.&#34', 1),\n",
       " ('34;total_diet&#34', 1),\n",
       " ('defy_reccomemd', 1),\n",
       " ('greg_bastin', 1),\n",
       " ('navaho_teas', 1),\n",
       " ('tea!!._braniac', 1),\n",
       " ('34;truth_of_reality&#34;.', 1),\n",
       " ('hott_natural!!.', 1),\n",
       " ('34;roller_coaster&#34;.', 1),\n",
       " ('ashley_sutherland', 1),\n",
       " ('pleasantely_surprised!.', 1),\n",
       " ('xanthoparmelia_scabrosa', 1),\n",
       " ('34;xanthoparmelia_cautioni', 1),\n",
       " ('34;caffeine_blues&#34', 1),\n",
       " ('3-in1_solution!.', 1),\n",
       " ('slowness_and_harshness', 1),\n",
       " ('alb_flatten', 1),\n",
       " ('fabled_freshman', 1),\n",
       " ('eric_meghan', 1),\n",
       " ('thatis_whatit', 1),\n",
       " ('coleus_forskohli', 1),\n",
       " ('34;asparagus_like&#34', 1),\n",
       " ('fast!lisa_p.', 1),\n",
       " ('8220;must_have&#8221', 1),\n",
       " ('irrigate_or_hothouse_crop', 1),\n",
       " ('34;want_more&#34;.', 1),\n",
       " ('inconsistentencie_and_sensational', 1),\n",
       " ('34&#34_waist.i', 1),\n",
       " ('aurora_nutriscience', 1),\n",
       " ('down.$25_lt;---------research', 1),\n",
       " ('liposome_nanosphere', 1),\n",
       " ('34;review_writer&#34', 1),\n",
       " ('thisle_extract!!.', 1),\n",
       " ('entitled_to!end', 1),\n",
       " ('galaxo_smith_kline', 1),\n",
       " ('hace_sentir_bien_despues', 1),\n",
       " ('muy_buen_limpiador_del', 1),\n",
       " ('progressively_34;more_effective&#34', 1),\n",
       " ('specialised_array', 1),\n",
       " ('easy----until_now!!!with', 1),\n",
       " ('34;famine_food&#34', 1),\n",
       " ('8220;mud_pies&#8221;.', 1),\n",
       " ('caraluma_fimbriata', 1),\n",
       " ('smaller&#34_and_34;what', 1),\n",
       " ('maria_rodriguez', 1),\n",
       " ('oz&#34_lemming', 1),\n",
       " ('alll_of_magnanatural', 1),\n",
       " ('analogous_sn2', 1),\n",
       " ('allylic_carbocation', 1),\n",
       " ('tertiary_carbocation', 1),\n",
       " ('carbocation_mediate_cyclization', 1),\n",
       " ('geranylgeranyl_pyrophosphate_ggpp', 1),\n",
       " ('34;bad_carbs&#34', 1),\n",
       " ('34;light_and_love&#34', 1),\n",
       " ('kim_bishop', 1),\n",
       " ('offersan_advantageous', 1),\n",
       " ('inventory_ilabel_placement', 1),\n",
       " ('crushers_inc', 1),\n",
       " ('34;dynamic_duo&#34', 1),\n",
       " ('34;joint_health&#34', 1),\n",
       " ('market!!._nutrielite', 1),\n",
       " ('thanbefore_jenny', 1),\n",
       " ('&#34;why_not&#34', 1),\n",
       " ('barbara_didomizio', 1),\n",
       " ('34;pharmaceutical_grade&#34;.', 1),\n",
       " ('34;functional_food&#34', 1),\n",
       " ('34;famine_food,&#34', 1),\n",
       " ('cactii_and_succulent', 1),\n",
       " ('8220;first_step&#8221', 1),\n",
       " ('amz_fatuburner', 1),\n",
       " ('8220;belly_blasting_supplements.&#8221', 1),\n",
       " ('revascularization_or_arrythmia', 1),\n",
       " ('munificent_and_superlative', 1),\n",
       " ('melting_melting!.', 1),\n",
       " ('hey&#8230;i_won&#8217;t', 1),\n",
       " ('say&#8230;&#8221;whoa_tonto&#8221', 1),\n",
       " ('me!alphamale_xl', 1),\n",
       " ('housewife!._alphamale_xl', 1),\n",
       " ('chancing_shadow', 1),\n",
       " ('8220;that_area&#8221', 1),\n",
       " ('100mgmuira_puama_20mgarginine', 1),\n",
       " ('20mg**tongkat_ali_100mgsaw', 1),\n",
       " ('250mgmucuna_pruriens_30mgpolypodium_vulgare', 1),\n",
       " ('34;scam_reviews&#34', 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 100 most infrequent paired words\n",
    "paired_words_frq.most_common()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48329"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq)  # number of paired terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]   \n",
       "\n",
       "                                 preprocessed_review  has_paired_words  \n",
       "0                              [dpe, the, job, well]                 0  \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]                 0  \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...                 0  \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...                 0  \n",
       "4         [good, product, good, price, good, result]                 0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove):\n",
    "    to_remove.extend([i])\n",
    "    sent_paired.extend(sent[i + skip: i + skip + num_paired])\n",
    "\n",
    "\n",
    "def filter_pairs(k, sent, sent_paired, sent_pos):\n",
    "    \"\"\"modify sent_paired in place\"\"\"\n",
    "    paired_sent_len = len(sent_paired)\n",
    "    skip = 0\n",
    "    to_remove = []\n",
    "    \n",
    "    if len(sent) != len(sent_pos):\n",
    "        \n",
    "        print('len(sent): ', len(sent))\n",
    "        print('len(sent_pos): ', len(sent_pos))\n",
    "        print('sent: ', sent)\n",
    "        print(' pos: ', sent_pos)\n",
    "        print('k: ', k)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    for i in range(paired_sent_len):\n",
    "        word = sent_paired[i]\n",
    "        if '_' in word:\n",
    "            num_paired = word.count('_') + 1\n",
    "            \n",
    "            # more than 3 words paired - ignore pairing\n",
    "            if num_paired > 3:\n",
    "                handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                skip += num_paired - 1\n",
    "                continue\n",
    "            \n",
    "            # bigrams: noun/adj, noun\n",
    "            elif num_paired == 2:\n",
    "#                 print('sent_paired: ', sent_paired)\n",
    "#                 print('len(sent): ', len(sent))\n",
    "#                 print('len(sent_pos): ', len(sent_pos))\n",
    "#                 print('i + skip + 1: ', i + skip + 1)\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_2 == 'NOUN')\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "            \n",
    "            # trigrams: noun/adj, all types, noun/adj\n",
    "            elif num_paired == 3:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                pos_word_3 = sent_pos[i + skip + 2]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_3 in ('NOUN', 'ADJ'))\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "        \n",
    "            # num. of words to skip indexing over sent and sent_pos in the next iter\n",
    "            skip += num_paired - 1\n",
    "        \n",
    "    # remove rejected pairs that are already split and added back individually\n",
    "    if len(to_remove) > 0:\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del sent_paired[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the filtering function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent = ['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "# Expected output:\n",
    "print(['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "sent = ['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'a', 'lot', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]   \n",
       "\n",
       "                                 preprocessed_review  has_paired_words  \n",
       "0                              [dpe, the, job, well]                 0  \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]                 0  \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...                 0  \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...                 0  \n",
       "4         [good, product, good, price, good, result]                 0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18396, 115645,\n",
       "       list(['the', 'bad', 'news', 'about', 'possible', 'copy', 'of', 'the', 'derry', 'New', 'Hampshire', 'product']),\n",
       "       list(['DET', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'PROPN', 'PROPN', 'NOUN']),\n",
       "       list(['the', 'bad', 'news', 'about', 'possible', 'copy', 'of', 'the', 'derry_New_Hampshire', 'product']),\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[115644].values\n",
    "# 115644    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18396, 115646,\n",
       "       list(['-PRON-', 'first', 'experience', 'with', 'buy', 'this', 'product', 'online', 'be', 'talk', 'to', 'the', 'people', 'at', 'derry', 'n.h.my', '1st', 'buy', 'be', 'from', 'a', 'health', 'products', 'store', 'but', 'derry', 'refer', '-PRON-', 'to', 'buy', '-PRON-', 'online']),\n",
       "       list(['ADJ', 'ADJ', 'NOUN', 'ADP', 'VERB', 'DET', 'NOUN', 'ADV', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADP', 'DET', 'PROPN', 'PROPN', 'NOUN', 'CCONJ', 'PROPN', 'VERB', 'PRON', 'ADP', 'VERB', 'PRON', 'ADV']),\n",
       "       list(['-PRON-', 'first', 'experience', 'with', 'buy', 'this', 'product', 'online', 'be', 'talk', 'to', 'the', 'people', 'at', '1st', 'buy', 'be', 'from', 'a', 'health', 'products', 'store', 'but', 'derry', 'refer', '-PRON-', 'to', 'buy', '-PRON-', 'online', 'derry', 'n.h.my']),\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[115645].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"The bad news about possible copies of the Derry, N.H. product.. My first experience with buying this product online was talking to the people at Derry, N.H.My 1st buy was from a Health Products store, but Derry referred me to buying it online.My biggest hope is that this is the same products because it's fantastic.Your delivery was fine.  I tend to trust Amazon, but check everything; even counted the # of tabs I recv'd : )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Hampshire', 'product', '..', '-PRON-', 'first']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp(doc)][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN', 'NOUN', 'PUNCT', 'ADJ', 'ADJ']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in nlp(doc)][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.H.', 'product', '..', 'My', 'first']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in nlp(doc)][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"The bad news about possible copies of the Derry, N.H. product.. My first experience with buying this product online was talking to the people at Derry, N.H.My 1st buy was from a Health Products store, but Derry referred me to buying it online.My biggest hope is that this is the same products because it's fantastic.Your delivery was fine.  I tend to trust Amazon, but check everything; even counted the # of tabs I recv'd : )\",\n",
       "       \"Jarrow is A++++. Jarrow is my preferred brand.  The price and quality can't be beat. I've been told by my doctors to take my vitamin D3. I do not tolerate sunlight, and I am a homebody.  I was taking vitamin D, but my dermatologist has me taking high doses of D3.  D3 is better than D alone.  He is a vitamin nut, and he has me taking all kinds of supplements.  D3 is very important for bones, cells, immune system.  And, I'm sure more.I just had my vitamin levels checked and I am doing really good. I'm 58, and haven't worked out in 2 years.  I worked out yesterday, and it didn't hurt a bit.  In fact, it loosened me up.I have Osteopenia or Osteoporosis (I will find out soon).  I have heard some bad news about calcium recently.  I'm taking half doses.  I take Boniva, and I take magnesium. I am covered.  Now I just have to walk...I don't want to end up in a wheelchair.  I feel 35!\",\n",
       "       \"ok. I have spent 5-6 years trying to reduce from 245# to a goal of 185#. I got to 215#, and can't lose another ounce. I am 69 and my wife is 50+. We are both enjoying some of Nature's finest dirty tricks. Since we could find no bad news about Svetol, we decided to try it. We are on our 3rd bottle being shared between us. I am now at 202# and slowly dropping. My lucky lady has only reduced about 5-6#. Since my mones have flopped, and her's are still flopping, for us to lose anything is a defiance of nature.\",\n",
       "       'Weigh in on Raspberry Ketones Miracle Fat Burner and 100 % Purest Professional Formula for Natural Weight Loss, 120 Count. If Dr. Oz says it\\'s good...well, at least that\\'s how it seems from all the amazon.com reviews on Raspberry Ketones Miracle Fat Burner.I\\'ve trained my brain to be independent from pop-media brainwashing and, in fact, did not even know that the Barnum of the medical circus arena \"promoted\" this product until I read a few of the reviews. So, rest assured, I went into trying these capsules without any preconceived notions or bias. Now, a number of weeks later, I can honestly say that my appetite, food cravings and mid-night musings for cold spaghetti straight out of the fridge have remained throughout despite this new \"healthy addition,\" (this is, after all, an all-natural product).Therefore, I was really looking forward to bursting the diet pill bandwagon bubble and proclaiming the bad news about how this miracle fat burner is nothing but a placebo. But, in all fairness, here\\'s the latest turn of events...or shall I say, turn back of the scale. I\\'ve shed about four pounds without doing anything I am consciously aware of in my lifestyle. So what\\'s that about?All in all, I can\\'t say that Raspberry Ketones Miracle Fat Burner suppressed my appetite, but it looks like it did help burn some pounds off. With the weigh off, the weigh in on this one measures up in the right direction!Stacy Lytwyn, Marketing Guru/Motivational Speaker/JournalistAuthor: CONSUMMATE CONNECTICUT: DAY TRIPS WITH PANACHE',\n",
       "       \"You can do better!. The bad news about this product is that it smells really, really bad.  I mean, terrible.  The good news is, it does not taste as bad as it smells.  Still, this is a pretty low bar for successful and I feel strongly that, all fiber supplements being more or less equal, you could definitely do better.  It is nice to have probiotics and fiber in gummy form for people who don't like to swallow pills, but there are alternatives from other brands and I guarantee you that you will prefer one of those.\"], dtype=object)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.reviewText.str.contains('bad news about'), ['reviewText']].reviewText.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt around. Good product, good price, good results.'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_reviews = unigram_sents_pos_df.preprocessed_review.tolist()\n",
    "unigram_sentences = unigram_sents_pos_df.unigram_sentences.tolist()\n",
    "unigram_pos = unigram_sents_pos_df.unigram_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 169409/3605491 [00:00<00:06, 564657.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  29\n",
      "len(sent_pos):  28\n",
      "sent:  ['-PRON-', 'North', 'Dakota', 'turn', '-PRON-', 'on', 'to', 'triphala', 'as', 'a', 'way', 'to', 'stop', 'the', 'pain', 'and', 'discomfort', 'associate', 'with', 'these', 'incident', 'and', 'the', 'herbs', 'work', 'within', '20', '30', 'minute']\n",
      " pos:  ['ADJ', 'PROPN', 'VERB', 'PRON', 'PART', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'PART', 'VERB', 'DET', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'NOUN', 'ADP', 'NUM', 'NUM', 'NOUN']\n",
      "k:  105365\n",
      "len(sent):  12\n",
      "len(sent_pos):  11\n",
      "sent:  ['the', 'bad', 'news', 'about', 'possible', 'copy', 'of', 'the', 'derry', 'New', 'Hampshire', 'product']\n",
      " pos:  ['DET', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'PROPN', 'PROPN', 'NOUN']\n",
      "k:  115644\n",
      "len(sent):  9\n",
      "len(sent_pos):  8\n",
      "sent:  ['so', '-PRON-', 'be', 'going', 'to', 'cut', '2', 'da', 'chase']\n",
      " pos:  ['ADV', 'PRON', 'VERB', 'DET', 'NOUN', 'NUM', 'PROPN', 'NOUN']\n",
      "k:  120909\n",
      "len(sent):  10\n",
      "len(sent_pos):  9\n",
      "sent:  ['the', 'label', 'say', 'New', 'Jersey', 'address', 'but', 'not', 'much', 'else']\n",
      " pos:  ['DET', 'NOUN', 'VERB', 'PROPN', 'NOUN', 'CCONJ', 'ADV', 'ADJ', 'ADV']\n",
      "k:  122443\n",
      "len(sent):  35\n",
      "len(sent_pos):  33\n",
      "sent:  ['what', 'the', 'physician', 'and', 'pharmacist', 'must', 'know', 'about', 'vitamins', 'minerals', 'foods', 'and', 'herbs', 'by', 'chris', 'd.', 'meletis', 'North', 'Dakota', 'and', 'thad', 'jacobs', 'North', 'Dakota', 'pages', '60', '61', 'have', 'the', 'drug', 'that', 'may', 'interact', 'with', 'q-10}.']\n",
      " pos:  ['NOUN', 'DET', 'PROPN', 'CCONJ', 'PROPN', 'VERB', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'CCONJ', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'CCONJ', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'NUM', 'NUM', 'VERB', 'DET', 'NOUN', 'ADJ', 'VERB', 'VERB', 'ADP', 'PROPN']\n",
      "k:  180563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 404989/3605491 [00:00<00:05, 578534.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  24\n",
      "len(sent_pos):  23\n",
      "sent:  ['dear', 'fellow', 'readers', '-PRON-', 'first', 'hear', 'about', 'this', 'product', 'from', 'dr.', 'rosenfeld', 'm.d.', 'professor', 'of', 'clinical', 'medicine', 'at', 'wild', 'cornell', 'university', 'in', 'New', 'York']\n",
      " pos:  ['ADJ', 'PROPN', 'PROPN', 'PRON', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'PROPN']\n",
      "k:  327453\n",
      "len(sent):  6\n",
      "len(sent_pos):  5\n",
      "sent:  ['dr.', 'marlborough', 's.', 'nichols', 'North', 'Dakota']\n",
      " pos:  ['PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  349535\n",
      "len(sent):  14\n",
      "len(sent_pos):  13\n",
      "sent:  ['in3weeks', 'i', 'take', 'before', 'and', 'after', 'pic', 'totally', 'amazing', 'non', 'believer', 'fayetteville', 'North', 'Carolina']\n",
      " pos:  ['ADJ', 'PRON', 'VERB', 'ADV', 'CCONJ', 'ADP', 'NOUN', 'ADV', 'ADJ', 'ADJ', 'NOUN', 'PROPN', 'PROPN']\n",
      "k:  355290\n",
      "len(sent):  12\n",
      "len(sent_pos):  11\n",
      "sent:  ['have', 'buy', '-PRON-', 'from', 'North', 'Carolina', 'to', 'ca', 'and', 'everywhere', 'in', 'between']\n",
      " pos:  ['VERB', 'VERB', 'PRON', 'ADP', 'PROPN', 'ADP', 'PROPN', 'CCONJ', 'ADV', 'ADP', 'ADP']\n",
      "k:  430966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 642316/3605491 [00:01<00:05, 583906.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  12\n",
      "len(sent_pos):  11\n",
      "sent:  ['taste', 'like', 'crap', 'but', '-PRON-', 'be', 'going', 'to', 'drink', '-PRON-', 'this', 'once']\n",
      " pos:  ['VERB', 'ADP', 'NOUN', 'CCONJ', 'PRON', 'VERB', 'DET', 'NOUN', 'PRON', 'DET', 'ADV']\n",
      "k:  544200\n",
      "len(sent):  29\n",
      "len(sent_pos):  28\n",
      "sent:  ['going', 'to', 'also', 'say', 'this', '-PRON-', 'have', 'to', 'work', 'twice', 'as', 'hard', 'on', '-PRON-', 'core', 'then', '-PRON-', 'arm', 'because', '-PRON-', 'knock', 'weight', 'off', 'the', 'area', '-PRON-', 'always', 'work', 'on']\n",
      " pos:  ['DET', 'ADV', 'VERB', 'DET', 'PRON', 'VERB', 'PART', 'VERB', 'ADV', 'ADV', 'ADJ', 'ADP', 'ADJ', 'NOUN', 'ADV', 'ADJ', 'NOUN', 'ADP', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADJ', 'ADV', 'VERB', 'ADP']\n",
      "k:  545289\n",
      "len(sent):  19\n",
      "len(sent_pos):  18\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'go', 'out', 'on', 'a', 'limb', 'and', 'say', '-PRON-', 'be', 'not', '34;just', '-PRON-', 'and', '-PRON-', 'mixing&#34']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'NOUN', 'PART', 'ADP', 'DET', 'NOUN', 'CCONJ', 'VERB', 'PRON', 'VERB', 'ADV', 'NUM', 'PRON', 'CCONJ', 'ADJ', 'NOUN']\n",
      "k:  598455\n",
      "len(sent):  8\n",
      "len(sent_pos):  7\n",
      "sent:  ['-PRON-', 'absolutely', 'love', 'migra', 'eeze!pattie', 'in', 'North', 'Carolina']\n",
      " pos:  ['PRON', 'ADV', 'VERB', 'PROPN', 'PROPN', 'ADP', 'PROPN']\n",
      "k:  614578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 822561/3605491 [00:01<00:04, 587422.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  17\n",
      "len(sent_pos):  16\n",
      "sent:  ['but', '-PRON-', 'be', 'enteric', 'coat', 'and', 'enteric', 'coating', 'have', 'some', 'bad', 'effect', 'accord', 'to', '-PRON-', 'North', 'Dakota']\n",
      " pos:  ['CCONJ', 'PRON', 'VERB', 'ADJ', 'VERB', 'CCONJ', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'VERB', 'ADP', 'ADJ', 'PROPN']\n",
      "k:  762711\n",
      "len(sent):  23\n",
      "len(sent_pos):  22\n",
      "sent:  ['so', 'many', 'people', 'be', 'deficient', 'in', 'magnesium', 'and', 'the', 'effect', 'be', 'far', 'reach', 'read', 'the', 'magnesium', 'miracle', 'by', 'carolyn', 'dean', 'm.d.', 'North', 'Dakota']\n",
      " pos:  ['ADV', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'ADP', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'VERB', 'ADV', 'VERB', 'VERB', 'DET', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  767548\n",
      "len(sent):  22\n",
      "len(sent_pos):  21\n",
      "sent:  ['-PRON-', 'tell', 'everyone', 'about', 'this', 'product', 'and', 'refer', '-PRON-', 'to', 'the', 'book', 'the', 'miracle', 'of', 'magnesium', 'by', 'carolyn', 'deane', 'm.d.', 'North', 'Dakota']\n",
      " pos:  ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'CCONJ', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN', 'DET', 'PROPN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  772421\n",
      "len(sent):  14\n",
      "len(sent_pos):  13\n",
      "sent:  ['-PRON-', 'husband', 'be', 'tell', 'to', 'take', 'this', 'for', '-PRON-', 'thyroid', 'by', '-PRON-', 'North', 'Dakota']\n",
      " pos:  ['ADJ', 'NOUN', 'VERB', 'VERB', 'PART', 'VERB', 'DET', 'ADP', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'PROPN']\n",
      "k:  870217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 1123573/3605491 [00:01<00:04, 589629.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  44\n",
      "len(sent_pos):  43\n",
      "sent:  ['michael', 'murray', 'North', 'Dakota', 'author', 'of', '`', 'the', 'pill', 'book', 'guide', 'to', 'natural', 'medicines', 'write', '`', 'subject', 'take', 'carnitine', 'show', 'significant', 'improvement', 'in', 'heart', 'rate', 'blood', 'pressure', 'angina', 'attack', 'rhythm', 'disturbance', 'and', 'clinical', 'sign', 'of', 'impaired', 'heart', 'function', 'compare', 'to', 'the', 'subject', 'take', 'placebo']\n",
      " pos:  ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'ADP', 'PUNCT', 'DET', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'VERB', 'PUNCT', 'NOUN', 'VERB', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'CCONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN']\n",
      "k:  1037336\n",
      "len(sent):  2\n",
      "len(sent_pos):  1\n",
      "sent:  ['North', 'Dakota']\n",
      " pos:  ['PROPN']\n",
      "k:  1060330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 1365770/3605491 [00:02<00:03, 591953.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  28\n",
      "len(sent_pos):  27\n",
      "sent:  ['-PRON-', 'be', 'very', 'happy', 'to', 'find', 'j.crow', \"'s\", 'lugol', \"'s\", '2', 'iodine', 'solution', 'online', 'and', 'at', 'a', 'great', 'price!it', 'be', 'exactly', 'what', '-PRON-', 'North', 'Dakota', 'm.d', 'doctor', 'prescribe']\n",
      " pos:  ['PRON', 'VERB', 'ADV', 'ADJ', 'PART', 'VERB', 'PROPN', 'PART', 'PROPN', 'PART', 'NUM', 'PROPN', 'PROPN', 'ADV', 'CCONJ', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'ADV', 'NOUN', 'ADJ', 'PROPN', 'PROPN', 'NOUN', 'VERB']\n",
      "k:  1248947\n",
      "len(sent):  21\n",
      "len(sent_pos):  20\n",
      "sent:  ['decrease', 'absorptionone', 'of', 'the', 'main', 'health', 'risk', 'of', 'magnesium', 'stearate', 'accord', 'to', 'ron', 'schmid', 'North', 'Dakota', 'in', '-PRON-', 'article', 'dietary', 'supplements']\n",
      " pos:  ['VERB', 'PROPN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'ADJ', 'NOUN', 'PROPN', 'PROPN']\n",
      "k:  1253984\n",
      "len(sent):  28\n",
      "len(sent_pos):  27\n",
      "sent:  ['unlike', 'other', 'quality', 'oil', '-PRON-', 'have', 'purchase', 'this', 'have', 'a', 'yellow', 'tinge', 'and', 'strong', 'oder', 'which', 'accord', 'to', 'the', 'coconut', 'oil', 'miracle', 'by', 'bruce', 'fife', 'c.n.', 'North', 'Dakota']\n",
      " pos:  ['ADP', 'ADJ', 'NOUN', 'NOUN', 'PRON', 'VERB', 'VERB', 'DET', 'VERB', 'DET', 'ADJ', 'NOUN', 'CCONJ', 'ADJ', 'NOUN', 'ADJ', 'VERB', 'ADP', 'DET', 'PROPN', 'NOUN', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  1274570\n",
      "len(sent):  39\n",
      "len(sent_pos):  38\n",
      "sent:  ['apparently', 'rex', 'e.', 'newnham', 'ph', 'd.,d.o.', 'North', 'Dakota', 'have', 'do', 'some', 'research', 'on', 'the', 'level', 'of', 'boron', 'in', 'soil', 'across', 'the', 'world', 'and', 'find', 'that', 'population', 'of', 'people', 'who', 'be', 'deficient', 'in', 'boron', 'have', 'the', 'high', 'incidence', 'of', 'arthritis']\n",
      " pos:  ['ADV', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', 'CCONJ', 'VERB', 'ADP', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'ADP', 'PROPN', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN']\n",
      "k:  1313858\n",
      "len(sent):  50\n",
      "len(sent_pos):  49\n",
      "sent:  ['thought', '-PRON-', 'would', 'mention', 'that', 'review', 'so', 'far', 'seem', 'to', 'favor', 'use', 'samento', 'after', 'develop', 'lyme', 'disease', 'but', '-PRON-', 'North', 'Dakota', 'naturopathic', 'doctor', 'who', 'specialize', 'in', 'lyme', 'disease', 'have', 'post', 'doc', 'training', 'sponsor', 'pd', 'for', 'by', 'the', 'professional', 'lyme', 'org', 'recommend', '-PRON-', 'use', 'samento', 'as', 'a', 'protection', 'against', 'get', 'lyme']\n",
      " pos:  ['NOUN', 'PRON', 'VERB', 'VERB', 'DET', 'NOUN', 'ADV', 'ADV', 'VERB', 'PART', 'VERB', 'VERB', 'PROPN', 'ADP', 'VERB', 'PROPN', 'NOUN', 'CCONJ', 'ADJ', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADP', 'ADP', 'DET', 'ADJ', 'PROPN', 'PROPN', 'VERB', 'PRON', 'VERB', 'PROPN', 'ADP', 'DET', 'NOUN', 'ADP', 'VERB', 'PROPN']\n",
      "k:  1314285\n",
      "len(sent):  23\n",
      "len(sent_pos):  22\n",
      "sent:  ['with', 'that', 'be', 'say', '-PRON-', 'be', 'going', 'to', 'buy', 'another', 'bottle', 'because', '-PRON-', 'be', '87', 'pound', 'heavy', 'before', '-PRON-', 'start', 'the', '100', 'pill']\n",
      " pos:  ['ADP', 'DET', 'VERB', 'VERB', 'PRON', 'VERB', 'DET', 'NOUN', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'NUM', 'NOUN', 'ADJ', 'ADP', 'PRON', 'VERB', 'DET', 'NUM', 'NOUN']\n",
      "k:  1358145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 1486742/3605491 [00:02<00:03, 592732.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  14\n",
      "len(sent_pos):  13\n",
      "sent:  ['manufacture', 'New', 'York', 'in', 'the', 'usai', 'do', 'not', 'see', 'anywhere', 'if', 'this', 'be', 'kosher']\n",
      " pos:  ['VERB', 'PROPN', 'ADP', 'DET', 'PROPN', 'VERB', 'ADV', 'VERB', 'ADV', 'ADP', 'DET', 'VERB', 'PROPN']\n",
      "k:  1375316\n",
      "len(sent):  18\n",
      "len(sent_pos):  17\n",
      "sent:  ['-PRON-', 'be', 'look', 'for', 'this', 'product', 'here', 'in', 'North', 'Carolina', 'and', 'to', '-PRON-', 'surprise', 'no', 'one', 'carry', '-PRON-']\n",
      " pos:  ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', 'ADV', 'ADP', 'PROPN', 'CCONJ', 'ADP', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'PRON']\n",
      "k:  1428448\n",
      "len(sent):  6\n",
      "len(sent_pos):  5\n",
      "sent:  ['cathy', 'hopkins', 'North', 'Dakota', 'cnc', 'bep']\n",
      " pos:  ['PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  1490211\n",
      "len(sent):  5\n",
      "len(sent_pos):  4\n",
      "sent:  ['v', 'gambill', 'hendersonville', 'North', 'Carolina']\n",
      " pos:  ['PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  1492454\n",
      "len(sent):  18\n",
      "len(sent_pos):  17\n",
      "sent:  ['-PRON-', 'next', 'thought', 'be', 'to', 'contact', 'the', 'bbb', 'of', 'New', 'Jersey', 'to', 'see', 'if', '-PRON-', 'could', 'assist', '-PRON-']\n",
      " pos:  ['ADJ', 'ADJ', 'NOUN', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PROPN', 'PART', 'VERB', 'ADP', 'PRON', 'VERB', 'VERB', 'PRON']\n",
      "k:  1493665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 1607644/3605491 [00:02<00:03, 593021.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  6\n",
      "len(sent_pos):  5\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'happy', 'cu']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'ADJ', 'ADP']\n",
      "k:  1514981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 1727903/3605491 [00:02<00:03, 593575.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  26\n",
      "len(sent_pos):  25\n",
      "sent:  ['-PRON-', 'doctor', 'a', 'North', 'Dakota', 'prescribe', 'pgx', 'ultra', 'along', 'with', 'supplement', 'glysen', 'and', 'photoglysen', 'to', 'help', 'control', '-PRON-', 'blood', 'sugar', 'and', 'to', 'help', '-PRON-', 'lose', 'weight']\n",
      " pos:  ['ADJ', 'NOUN', 'DET', 'PROPN', 'VERB', 'PROPN', 'PROPN', 'ADP', 'ADP', 'NOUN', 'PROPN', 'CCONJ', 'PROPN', 'PART', 'VERB', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'CCONJ', 'PART', 'VERB', 'PRON', 'VERB', 'NOUN']\n",
      "k:  1649079\n",
      "len(sent):  12\n",
      "len(sent_pos):  11\n",
      "sent:  ['-PRON-', 'North', 'Dakota', 'say', '-PRON-', 'body', 'process', 'the', 'fish', 'oil', 'better', 'though']\n",
      " pos:  ['ADJ', 'PROPN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADV', 'ADV']\n",
      "k:  1709543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 1849569/3605491 [00:03<00:02, 594523.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  64\n",
      "len(sent_pos):  63\n",
      "sent:  ['well', '20', 'capsules!speaking', 'of', 'which', 'the', 'member', 'of', 'gaia', 'herb', \"'s\", 'scientific', 'advisory', 'board', 'be', 'all', 'say', 'to', 'doctor', 'of', 'naturopathy', 'and', '-PRON-', 'even', 'have', 'the', 'designation', 'North', 'Dakota', 'after', '-PRON-', 'name', 'and', 'call', '-PRON-', 'doctor', 'and', '-PRON-', 'take', 'money', 'for', 'diagnose', 'and', 'treat', 'people', 'people', 'who', 'may', 'really', 'have', 'a', 'disease', 'that', 'real', 'doctor', 'could', 'treat', 'use', 'ludicrous', 'belief', 'that', 'flatly', 'contradict', 'science']\n",
      " pos:  ['INTJ', 'NUM', 'NOUN', 'ADP', 'ADJ', 'DET', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PART', 'PROPN', 'PROPN', 'PROPN', 'VERB', 'DET', 'VERB', 'ADP', 'NOUN', 'ADP', 'PROPN', 'CCONJ', 'PRON', 'ADV', 'VERB', 'DET', 'NOUN', 'PROPN', 'ADP', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'PRON', 'NOUN', 'CCONJ', 'PRON', 'VERB', 'NOUN', 'ADP', 'VERB', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', 'DET', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'ADV', 'VERB', 'NOUN']\n",
      "k:  1778758\n",
      "len(sent):  16\n",
      "len(sent_pos):  15\n",
      "sent:  ['before', 'put', '-PRON-', 'on', 'birth', 'control', 'or', 'other', 'hormone', '-PRON-', 'North', 'Dakota', 'want', 'to', 'try', 'this']\n",
      " pos:  ['ADP', 'VERB', 'PRON', 'ADP', 'NOUN', 'NOUN', 'CCONJ', 'ADJ', 'NOUN', 'ADJ', 'PROPN', 'VERB', 'PART', 'VERB', 'DET']\n",
      "k:  1829183\n",
      "len(sent):  8\n",
      "len(sent_pos):  7\n",
      "sent:  ['-PRON-', 'North', 'Dakota', 'also', 'like', 'this', 'vitamin', 'company']\n",
      " pos:  ['ADJ', 'PROPN', 'ADV', 'VERB', 'DET', 'NOUN', 'NOUN']\n",
      "k:  1852181\n",
      "len(sent):  15\n",
      "len(sent_pos):  14\n",
      "sent:  ['-PRON-', 'North', 'Dakota', 'have', 'prescribe', '-PRON-', 'a', 'similar', 'product', 'which', '-PRON-', 'sell', 'through', '-PRON-', 'office']\n",
      " pos:  ['ADJ', 'PROPN', 'VERB', 'VERB', 'PRON', 'DET', 'ADJ', 'NOUN', 'ADJ', 'PRON', 'VERB', 'ADP', 'ADJ', 'NOUN']\n",
      "k:  1852682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 2031789/3605491 [00:03<00:02, 595396.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  15\n",
      "len(sent_pos):  14\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'skeptic', 'so', '-PRON-', 'can', 'imagine', 'just', 'how', 'high', '-PRON-', 'hope', 'be']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'VERB', 'ADV', 'ADV', 'ADJ', 'ADJ', 'NOUN', 'VERB']\n",
      "k:  1954788\n",
      "len(sent):  11\n",
      "len(sent_pos):  10\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'always', 'buy', 'this', 'juice.cactus', 'juice', 'for', 'life']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'ADV', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADP', 'NOUN']\n",
      "k:  2010305\n",
      "len(sent):  18\n",
      "len(sent_pos):  17\n",
      "sent:  ['-PRON-', 'next', 'thought', 'be', 'to', 'contact', 'the', 'bbb', 'of', 'New', 'Jersey', 'to', 'see', 'if', '-PRON-', 'could', 'assist', '-PRON-']\n",
      " pos:  ['ADJ', 'ADJ', 'NOUN', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PROPN', 'PART', 'VERB', 'ADP', 'PRON', 'VERB', 'VERB', 'PRON']\n",
      "k:  2023289\n",
      "len(sent):  9\n",
      "len(sent_pos):  8\n",
      "sent:  ['this', 'be', 'the', 'one.author', 'jan', 'mcbarron', 'm.d.', 'North', 'Dakota']\n",
      " pos:  ['DET', 'VERB', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'PROPN']\n",
      "k:  2061062\n",
      "len(sent):  20\n",
      "len(sent_pos):  19\n",
      "sent:  ['the', 'packaging', 'say', 'the', 'company', 'be', 'in', 'brooklyn', 'New', 'York', 'but', 'the', 'wording', 'on', 'the', 'package', 'be', 'a', 'little', 'strange']\n",
      " pos:  ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'CCONJ', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ']\n",
      "k:  2070503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 2273434/3605491 [00:03<00:02, 595953.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  19\n",
      "len(sent_pos):  18\n",
      "sent:  ['for', 'other', 'cope', 'with', 'adrenal', 'stress', '-PRON-', 'recommend', 'james', 'l.', 'wilson', 'North', 'Dakota', 'd.c.', 'ph.d.', \"'s\", 'book', 'adrenal', 'fatigue']\n",
      " pos:  ['ADP', 'NOUN', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PRON', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'PART', 'NOUN', 'PROPN', 'PROPN']\n",
      "k:  2185746\n",
      "len(sent):  12\n",
      "len(sent_pos):  11\n",
      "sent:  ['try', 'and', '-PRON-', 'will', 'see', 'what', '-PRON-', 'be', 'going', 'to', 'talkin', 'about']\n",
      " pos:  ['VERB', 'CCONJ', 'PRON', 'VERB', 'VERB', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP']\n",
      "k:  2260921\n",
      "len(sent):  8\n",
      "len(sent_pos):  7\n",
      "sent:  ['-PRON-', 'North', 'Dakota', 'recommend', '-PRON-', 'for', 'candida', 'issue']\n",
      " pos:  ['ADJ', 'PROPN', 'VERB', 'PRON', 'ADP', 'ADJ', 'NOUN']\n",
      "k:  2280891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 2641208/3605491 [00:04<00:01, 597734.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  35\n",
      "len(sent_pos):  34\n",
      "sent:  ['34;the', 'u.s.', 'food', 'and', 'drug', 'administration', 'be', 'warn', 'consumer', 'to', 'immediately', 'stop', 'use', 'hydroxycut', 'product', 'by', 'iovate', 'health', 'sciences', 'inc.', 'of', 'oakville', 'ontario', 'and', 'distribute', 'by', 'iovate', 'health', 'sciences', 'usa', 'inc.', 'of', 'blasdell', 'New', 'York']\n",
      " pos:  ['NUM', 'PROPN', 'PROPN', 'CCONJ', 'PROPN', 'PROPN', 'VERB', 'VERB', 'NOUN', 'PART', 'ADV', 'VERB', 'VERB', 'PROPN', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'CCONJ', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PROPN']\n",
      "k:  2522354\n",
      "len(sent):  42\n",
      "len(sent_pos):  41\n",
      "sent:  ['-PRON-', 'have', 'be', 'on', '500', 'mg', 'of', 'acetyl', 'l', 'carnitine', 'and', '200', 'mg', 'of', 'alpha', 'lipoic', 'acid', 'ala', 'to', 'fight', 'off', 'effect', 'of', 'age', 'on', 'the', 'brain', 'nov', '2003', 'reader', 'digest', 'article', 'the', 'end', 'of', 'aging.\")i', 'stop', 'by', 'the', 'hickory', 'North', 'Carolina']\n",
      " pos:  ['PRON', 'VERB', 'VERB', 'ADP', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'CCONJ', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PART', 'VERB', 'PART', 'NOUN', 'ADP', 'VERB', 'ADP', 'DET', 'NOUN', 'PROPN', 'NUM', 'NOUN', 'PROPN', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'VERB', 'ADP', 'DET', 'PROPN', 'PROPN']\n",
      "k:  2551931\n",
      "len(sent):  28\n",
      "len(sent_pos):  27\n",
      "sent:  ['the', 'drop', 'have', 'to', 'be', 'refrigerate', 'sooooo', 'with', 'that', 'be', 'say', '-PRON-', 'be', 'going', 'to', 'see', 'fine', 'for', 'the', 'summer', 'can', 'not', 'wait', 'to', 'get', '-PRON-', 'real', 'drops;-']\n",
      " pos:  ['DET', 'NOUN', 'VERB', 'PART', 'VERB', 'VERB', 'ADJ', 'ADP', 'DET', 'VERB', 'VERB', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ', 'ADJ', 'NOUN']\n",
      "k:  2641207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 2762711/3605491 [00:04<00:01, 597912.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  7\n",
      "len(sent_pos):  6\n",
      "sent:  ['be', 'going', 'to', 'on', '-PRON-', '3rd', 'patch']\n",
      " pos:  ['VERB', 'DET', 'ADP', 'ADJ', 'ADJ', 'NOUN']\n",
      "k:  2664790\n",
      "len(sent):  13\n",
      "len(sent_pos):  12\n",
      "sent:  ['going', 'to', 'order', '-PRON-', 'again.why', 'pay', 'money', 'for', 'something', 'that', 'do', 'not', 'work']\n",
      " pos:  ['DET', 'NOUN', 'PRON', 'INTJ', 'VERB', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'VERB', 'ADV', 'VERB']\n",
      "k:  2710977\n",
      "len(sent):  18\n",
      "len(sent_pos):  17\n",
      "sent:  ['this', 'brand', 'of', 'magnesium', 'be', 'prescribe', 'to', '-PRON-', 'by', '-PRON-', 'North', 'Dakota', 'and', 'have', 'truly', 'make', 'a', 'difference']\n",
      " pos:  ['DET', 'NOUN', 'ADP', 'NOUN', 'VERB', 'VERB', 'ADP', 'PRON', 'ADP', 'ADJ', 'PROPN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'DET', 'NOUN']\n",
      "k:  2720310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 2945322/3605491 [00:04<00:01, 598397.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  4\n",
      "len(sent_pos):  3\n",
      "sent:  ['love', 'North', 'Carolina', 'wholemega']\n",
      " pos:  ['VERB', 'PROPN', 'PROPN']\n",
      "k:  2870783\n",
      "len(sent):  5\n",
      "len(sent_pos):  4\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'female']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'NOUN']\n",
      "k:  2941161\n",
      "len(sent):  14\n",
      "len(sent_pos):  13\n",
      "sent:  ['unit', 'j', 'greenfield', 'North', 'Carolina', '29607i', 'follow', 'the', 'direction', 'and', 'have', 'absolutely', 'no', 'result']\n",
      " pos:  ['NOUN', 'PROPN', 'NOUN', 'PROPN', 'NOUN', 'VERB', 'DET', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'DET', 'NOUN']\n",
      "k:  2988696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 3127623/3605491 [00:05<00:00, 598794.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  8\n",
      "len(sent_pos):  7\n",
      "sent:  ['no', 'filler', 'and', 'produce', 'in', 'brevard', 'North', 'Carolina']\n",
      " pos:  ['DET', 'NOUN', 'CCONJ', 'VERB', 'ADP', 'PROPN', 'PROPN']\n",
      "k:  3033584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 3370414/3605491 [00:05<00:00, 599077.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  8\n",
      "len(sent_pos):  7\n",
      "sent:  ['but', '-PRON-', 'be', 'going', 'to', 'keep', 'use', '-PRON-']\n",
      " pos:  ['CCONJ', 'PRON', 'VERB', 'DET', 'NOUN', 'VERB', 'PRON']\n",
      "k:  3252070\n",
      "len(sent):  11\n",
      "len(sent_pos):  10\n",
      "sent:  ['going', 'to', 'try', 'one', 'more', 'month', 'and', 'see', 'what', 'happen', 'hopefully']\n",
      " pos:  ['DET', 'NOUN', 'NUM', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'VERB', 'ADV']\n",
      "k:  3275967\n",
      "len(sent):  5\n",
      "len(sent_pos):  4\n",
      "sent:  ['-PRON-', 'be', 'going', 'to', 'fan']\n",
      " pos:  ['PRON', 'VERB', 'DET', 'NOUN']\n",
      "k:  3323704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 3491877/3605491 [00:05<00:00, 598960.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sent):  55\n",
      "len(sent_pos):  54\n",
      "sent:  ['guideline', 'fromm.d.', \"'s\", 'and', 'North', 'Dakota', \"'s\", 'who', 'have', 'use', 'nutritional', 'iodine', 'for', 'year', 'in', '-PRON-', 'own', 'practice', 'not', 'only', 'enable', '-PRON-', 'to', 'understand', 'the', 'process', 'which', '-PRON-', 'appreciate', 'but', 'help', '-PRON-', 'feel', 'confident', 'that', '-PRON-', 'supplementation', 'could', 'be', 'do', 'safely', 'and', 'in', 'the', 'manner', 'most', 'likely', 'to', 'provide', 'the', 'result', '-PRON-', 'be', 'look', 'for']\n",
      " pos:  ['NOUN', 'NOUN', 'PART', 'CCONJ', 'PROPN', 'PART', 'NOUN', 'VERB', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'ADV', 'ADV', 'VERB', 'PRON', 'PART', 'VERB', 'DET', 'NOUN', 'ADJ', 'PRON', 'VERB', 'CCONJ', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADV', 'CCONJ', 'ADP', 'DET', 'NOUN', 'ADV', 'ADJ', 'PART', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', 'ADP']\n",
      "k:  3407908\n",
      "len(sent):  4\n",
      "len(sent_pos):  3\n",
      "sent:  ['john', 'gastonia', 'North', 'Carolina']\n",
      " pos:  ['PROPN', 'PROPN', 'PROPN']\n",
      "k:  3414228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3605491/3605491 [00:06<00:00, 599260.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(preprocessed_reviews))):\n",
    "    filter_pairs(i, sent=unigram_sentences[i], sent_paired=preprocessed_reviews[i], sent_pos=unigram_pos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUG: something weird with the row above where it fails - why is sent smaller than sent_paired???\n",
    "## also, only apply filter_pos to has_paired = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'NUM',\n",
       " 'PROPN',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'do',\n",
       " 'recommend',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'one',\n",
       " 'a_day',\n",
       " 'though',\n",
       " 'with',\n",
       " 'extra',\n",
       " 'calcium']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'do',\n",
       " 'recommend',\n",
       " 'one',\n",
       " 'though',\n",
       " 'with',\n",
       " 'extra',\n",
       " 'calcium',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'a_day',\n",
       " 'though']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605491"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43362695"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size with unigrams\n",
    "len([word for sentence in unigram_sentences for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21960569"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size with trigrams\n",
    "len([word for sentence in trigram_sentences for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_flat = [word for sentence in trigram_sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21960569"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigrams_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dpe', 'job', 'b', 'flax', 'd', 'regular', '-PRON-', 'house', '-PRON-', '-PRON-', 'job', 'simply', 'good', 'result', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_flat[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_words = set([word for word in trigrams_flat if '_' in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203277"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mouth', 'quickly', 'lozenge', 'formula', 'dissolve', 'slowly', 'preferable', 'accord', '-PRON-', 'research', 'this_product', 'great', 'side_effect', '-PRON-', '-PRON-', 'cold', 'sore_throat', 'soon', 'start', '-PRON-', 'every_day', '-PRON-', 'start', 'come', 'cold', '-PRON-', 'usual', 'symptom', 'anticipate', 'sick', 'day', '-PRON-', 'usual', 'pattern', '-PRON-', 'sick', 'anticipate', 'taking', 'this_product', 'reason', '-PRON-', 'come', '-PRON-', 'cold', 'sore_throat', '-PRON-', 'great', '-PRON-', 'recommend', 'this_product']\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_flat[100:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night.not_a_miracle_cure\n",
      "pinot_noir\n",
      "solublenot_certify_kosher_or_halal$8.99\n",
      "count)*****fat_solublenot_certify_kosher\n",
      "240_softgels)****fat_solublenot_certify\n",
      "solublenot_certify_kosher_or_halal$13.78\n",
      "8220;not_hungry&#8221\n",
      "distilledmercury_freenot_enteric_coatednot\n",
      "cholesterolmolecularly_distilledmercury_freenot_enteric\n",
      "estafa!not_worth_the_money\n",
      "34;not_guilty&#34\n",
      "hacking_snot_fill\n",
      "solublenot_certify_kosher_or_halal$27.99\n",
      "each)****triglycerides_formnot_certify_kosher\n",
      "freshness_34;not_rancid&#34\n",
      "formnot_certify_kosher_or_halal$45.82\n",
      "formnot_certify_kosher_or_halal$45.46\n",
      "hungry.not_a_stimulant\n",
      "240_softgels,)fat_solublenot_certify\n",
      "stearateschelatedvegetariannot_enteric_coatedcontain_laxative\n",
      "34;not_work&#34;.\n",
      "enteric_coatednot_vegetarianone\n",
      "supply).)ethyl_ester_formnot_certify\n",
      "22.8=_78.6not_373i_freak\n",
      "cholesterolmolecularly_distilledno_mercurynot_enteric\n",
      "solublenot_certify_kosher_or_halal$27.77\n",
      "90-count)*****ubiquinolfat_solublenot_certify_kosher\n",
      "mercurynot_enteric_coatednot_vegetarianphospholipid\n",
      "120_softgels)*****not_certify_kosher\n",
      "90_count)*****not_certify\n",
      "stearateschelatedvegetariannot_enteric_coatedpossibly_contain\n",
      "dontnot_purches\n",
      "supplement!not_surprisingly_pharmaceutical\n",
      "starchy_substances(not_dilute\n",
      "formnot_certify_kosher_or_halal$16.49\n",
      "soycontains_gmoscontain_cholesterolnot_vegetariannot\n",
      "34;not_addictive&#34;--lie\n",
      "coatednot_vegetarian?ifos_rating_=\n",
      "coatednot_vegetarianphospholipid_=\n",
      "solublenot_certify_kosher_or_halal$10.94\n",
      "magnesium_glycinatewater_solublenot_certify\n",
      "solublenot_certify_kosher_or_halal$15.18\n",
      "solublenot_certify_kosher_or_halal$25.87\n",
      "formnot_certify_kosher_or_halal$10.99\n",
      "solublenot_certify_kosher_or_halal$37.85\n",
      "tauratewater_solublenot_certify_kosher\n",
      "regularly(not_excessive\n",
      "coatednot_vegetarianvitamin_d3\n",
      "glass_of_pinot_grigio\n",
      "softgels)*****fat_solublenot_certify_kosher\n",
      "solublenot_certify_kosher_or_halal$25.99\n",
      "day(not_jittery\n",
      "formnot_certify_kosher_or_halal$39.95\n",
      "sonot_happy-\n",
      "mercurynot_enteric_coatednot_vegetarianikos\n",
      "handle_stress(not_a_cure\n",
      "cholesterolno_mercurymolecularly_distillednot_vegetarianifos\n",
      "softgels)not_certify_kosher_or_halal$25.49\n",
      "day(not_the_nervous\n",
      "34;not_great&#34;.\n",
      "distillednot_enteric_coatednot_vegetarianifos\n",
      "vcaps)*****fat_solublenot_certify_kosher\n",
      "solublenot_certify_kosher_or_halal$25.15\n",
      "solublenot_certify_kosher_or_halal$14.98\n",
      "solublenot_certify_kosher_or_halal$22.95\n",
      "solublenot_certify_kosher_or_halal$29.29\n",
      "22_knot_gust\n",
      "isolate(not_concentrate\n",
      "enteric_coatednot_vegetarianphone_number\n",
      "lol!!!)-not_yet!!!.\n",
      "softgel)triglycerides_formnot_certify_kosher\n",
      "concoctions!not_particularly_worried\n",
      "solublenot_certify_kosher_or_halal$37.99\n",
      "low_carb_diet\".not_necessarily\n",
      "400-count_softgels)triglyceride_formnot_certify\n",
      "34;not_right&#34\n",
      "dead_fisn!not_worth_the_money\n",
      "count)****ubiquinolfat_solublenot_certify_kosher\n",
      "solublenot_certify_kosher_or_halal$13.80\n",
      "formnot_certify_kosher_or_halal$44.45\n",
      "sgels)****ubiquinolwater_solublenot_certify_kosher\n",
      "pill(not_raspberry).i\n",
      "unconfirmednot_enteric_coatednot_vegetarianifos\n",
      "isnot_a_gimmick\n",
      "gel_caps)not_certify\n",
      "solublenot_certify_kosher_or_halal$31.94\n",
      "couldnot_afford\n",
      "acnnot_speak\n",
      "formnot_certify_kosher_or_halal$30.12\n",
      "amazon(not_3rd_party\n",
      "knot_tension_spacing\n",
      "60_count)****not_certify\n",
      "distilledno_mercurynot_enteric_coatednot\n",
      "crucial_andnot_user_friendly\n",
      "34;not_normal.&#34\n",
      "300_softgels)fat_solublenot_certify\n",
      "formnot_certify_kosher_or_halal$19.12\n",
      "formnot_certify_kosher_or_halal$44.95\n",
      "solublenot_certify_kosher_or_halal$11.22\n",
      "34;not_much&#34\n",
      "formnot_certified_kosher_or_halal$11.97\n",
      "tests(not_serotonin\n",
      "34;not_trying&#34\n",
      "proprietary_information)enteric_coatednot_vegetarianifos\n",
      "solublenot_certify_kosher_or_halal$11.85\n",
      "mercurymolecularly_distilledenteric_coatednot_vegetarianifos\n",
      "chelatewater_solublenot_certify_kosher\n",
      "flowersnot_intend\n",
      "count)not_certify_kosher_or_halal$25.19\n",
      "softgel)ubiquinolfat_solublenot_certify_kosher\n",
      "magnesium_citratewater_solublenot_certify\n",
      "gmoscontain_cholesterolmercury_freenot_enteric\n",
      "frustrating!not_anymore\n",
      "label(not_a_sticker).belive\n",
      "count)ubiquinolfat_solublenot_certify_kosher\n",
      "middle_of_the_day(not_unusual\n",
      "say:\"not_intend\n",
      "solublenot_certify_kosher_or_halal$9.14\n",
      "capsules)not_certify_kosher_or_halal$10.79\n",
      "solublenot_certify_kosher_or_halal$23.66\n",
      "mercurynot_enteric_coatednot_vegetarianone\n",
      "180_softgels)*****triglyceride_formnot_certify\n",
      "eating,[not_starve\n",
      "it(not_necessarily\n",
      "vegetariannot_enteric\n",
      "unot_+_20:00\n",
      "120_softgels)*****ubiquinolfat_solublenot_certify\n",
      "bat-----not_noticing\n",
      "formnot_certify_kosher_or_halal$13.99\n",
      "solublenot_certify_kosher_or_halal$13.59\n",
      "it&#8230;not_pleasant\n",
      "magnesium_oxidewater_solublenot_certify\n",
      "gprotein_21_gnot_a_significant\n",
      "powdered_ionic)vegetariannot_enteric_coatedpossibly\n",
      "glycinatefat_solublenot_certify_kosher\n",
      "34;apart.&#34;not_melted_goo\n",
      "60-v_gel_bottle)not_certify\n",
      "soft_gel_60-count)not_certify\n",
      "cholesterolflash_distillationnot_enteric_coatednot\n",
      "co2_technologyenteric_coatednot_vegetarianifos\n",
      "solublenot_certify_kosher_or_halal$11.05\n",
      "34;not_tasty&#34\n",
      "technologyenteric_coatednot_vegetarianifos_rating\n",
      "soyno_gmosno_cholesterolchelatednot_vegetariannot\n",
      "cholesterolno_mercurymolecularly_distillednot_enteric\n",
      "formnot_certify_kosher_or_halal$11.59\n",
      "reviews?cannot_honestly\n",
      "freemolecularly_distilledmercury_freenot_enteric\n",
      "360_softgels)*****fat_solublenot_certify\n",
      "solublenot_certify_kosher_or_halal$31.50\n",
      "34;not_helpful&#34_vote\n",
      "formnot_certify_kosher_or_halal$37.52\n",
      "solublenot_certify_kosher_or_halal$17.86\n",
      "including(not_limit\n",
      "nonot_certify_kosher_or_halal$38.25\n",
      "consistent_with_the_directions,(not_miraculously_tho)it\n",
      "she_cannnot_suck\n",
      "xtra_60ct)triglycerides_formnot_certify\n",
      "carnauba_waxnot_exactly\n",
      "gelatinmisleadingnot_goodnot_vegetarianyeahuh_hu\n",
      "solublenot_certify_kosher_or_halal$\n",
      "enteric_coatednot_vegetariansource_of_astaxanthin\n",
      "enteric_coatednot_vegetarianone_100\n",
      "softgels)***ubiquinonenot_certify_kosher_or_halal$24.33\n",
      "solublenot_certify_kosher_or_halal$11.74\n",
      "guarantee!)****ethyl_ester_formnot_certify\n",
      "34;not_responding&#34\n",
      "300_softgels)****fat_solublenot_certify\n",
      "daynot_dailyyou\n",
      "240_softgels)fat_solublenot_certify\n",
      "solublenot_certify_kosher_or_halal$17.09\n",
      "ki_cannot_doknow\n",
      "solublenot_certify_kosher_or_halal$14.99\n",
      "34;not_hungry&#34;.\n",
      "freenot_enteric_coatednot_vegetarianifos\n",
      "formnot_certify_kosher_or_halal$44.70\n",
      "soft_gel_60-count)*****not_certify\n",
      "rugbynot_coat\n",
      "freeenteric_coatednot_vegetarianphospholipid_=\n",
      "cholesterolno_mercurynot_enteric_coatednot\n",
      "soyno_gmosno_cholesterolvegetariannot_enteric\n",
      "antioxidant))not_certify_kosher_or_halalfat\n",
      "solublenot_certify_kosher_or_halal$13.97\n",
      "solublenot_certify_kosher_or_halal$7.36\n",
      "formnot_certify_kosher_or_halal$23.07\n",
      "coatednot_vegetarianone_100_mg\n",
      "solublenot_certify_kosher_or_halal$11.45\n",
      "4:30_5:00.not_fully\n",
      "150_softgel)*****ubiquinolfat_solublenot_certify\n",
      "herbalife_products.not_impressed\n",
      "japanese_knot_weed\n",
      "vegetariannot_enteric_coatedphone_number\n",
      "re_order--------not_likely\n",
      "drynot_a_fan\n",
      "great.not_gritty\n",
      "180ct)****triglyceride_formnot_certify_kosher\n",
      "stick\".not_forgetting\n",
      "peanuts(not_crush_nut\n",
      "mercurycontain_cholesterolmolecularly_distillednot_vegetarianifos\n",
      "freenot_enteric_coatednot_vegetarianphospholipid\n",
      "appetite*not_good!.\n",
      "orange_juice(not_consentrate\n",
      "ester_formnot_certify_kosher\n",
      "yesnot_certify_kosher_or_halal$260.00\n",
      "formprescription_require_nonot_certified\n",
      "softgels)ubiquinonefat_solublenot_certify_kosher\n",
      "solublenot_certify_kosher_or_halal$32.94\n",
      "solublenot_certify_kosher_or_halal$37.65\n",
      "chelatedvegetariannot_enteric_coatedphone_number\n",
      "softgel)****ubiquinolfat_solublenot_certify_kosher\n",
      "120_softgels)ubiquinolfat_solublenot_certify\n",
      "formnot_certify_kosher_or_halal$50.64\n",
      "240_softgels,)*****fat_solublenot_certify\n",
      "krill_oil_with_astaxanthin)not_certify\n",
      "thyroid_biospies(not_cancer\n",
      "trader_tom(not_trader_joe)are\n",
      "formnot_certify_kosher_or_halal$23.99\n",
      "solublenot_certify_kosher_or_halal$27.56\n",
      "super_antioxidant))*****not_certify_kosher\n",
      "legs(not_itchy\n",
      "reasonable_price!not_a_substitute\n",
      "solublenot_certify_kosher_or_halal$24.75\n",
      "solublenot_certify_kosher_or_halal$15.41\n",
      "solublenot_certify_kosher_or_halal$18.49\n",
      "snot_a_stranger\n",
      "60_capliques)not_certify_kosher\n",
      "tried.not_nasty\n",
      "distillationnot_enteric_coatednot_vegetarianifos\n",
      "energy.not_jittery\n",
      "34;not_farm_raise\n",
      "unconfirmed)not_certified_kosher_or_halal$23.99\n",
      "yesnot_certify_kosher_or_halal$210.00\n",
      "solublenot_certify_kosher_or_halal$15.69\n",
      "chelatedvegetariannot_enteric_coatedcontain_laxative\n",
      "ia_mnot_discourage\n",
      "mercurymolecularly_distillednot_enteric_coatednot\n",
      "pucker_balloon_knot_erupt\n",
      "flora)****triglycerides_formnot_certify_kosher\n",
      "beauty])fat_solublenot_certify_kosher\n",
      "formnot_certify_kosher_or_halal$46.95\n",
      "scary_friendliernot_a_beast\n",
      "mint-(not_papaya),helps\n",
      "formnot_certify_kosher_or_halal$21.99\n",
      "solublenot_certify_kosher_or_halal$9.49\n",
      "again!not_worth_the_money\n",
      "solublenot_certify_kosher_or_halal$20.23\n",
      "formnot_certify_kosher_or_halal$69.95\n",
      "solublenot_certify_kosher_or_halal$17.50\n",
      "enteric_coatednot_vegetarianifos_rating\n",
      "beauty])****fat_solublenot_certify_kosher\n",
      "formnot_certify_kosher_or_halal$45.25\n",
      "vote_34;not_helpful&#34\n",
      "less.[not_affiliate\n",
      "formnot_certified_koshercertified_halal$18.97\n",
      "2,not_drastic_wt\n",
      "formnot_certify_kosher_or_halal$44.75\n",
      "molecularly_distillednot_enteric\n",
      "b3_--not_the_impostor_inositol--\n",
      "soycontains_gmoscholesterol_freenot_vegetariannot\n",
      "solublenot_certify_kosher_or_halal$22.00\n",
      "will!?!not_true\n",
      "180-count)***triglycerides_formnot_certify_kosher\n",
      "softgels)not_certify_kosher_or_halal$22.45\n",
      "best.doesnot_rise\n",
      "formnot_certify_kosher_or_halal$29.65\n",
      "taste)-not_silver_ion\n",
      "immediately.not_irritate\n",
      "freemercury_freenot_enteric_coatednot\n",
      "12mg)not_certify_kosher_or_halalfat\n",
      "alpha_lipoic_acidnot_a_dangerous\n",
      "nnnnnnot_gggggood\n",
      "softgels)fat_solublenot_certify_kosher\n",
      "-not_that!.\n",
      "easily*not_overly_bitter\n",
      "3/19/13not_a_sniffle!i\n",
      "mercuryenteric_coatednot_vegetarianphospholipid_=\n",
      "form!\"not_correct\n",
      "30-count)****ethyl_ester_formnot_certify\n",
      "ubiquinol_coq10)ubiquinolfat_solublenot_certify\n",
      "solublenot_certify_kosher_or_halal$58.40\n",
      "digestnot_a_giant_horse_pillno\n",
      "solublenot_certify_kosher_or_halal$20.22\n",
      "stearatesnot_chelatedvegetariannot_enteric_coatedcompany\n",
      "enteric_coatednot_vegetarianvitamin_d3\n",
      "cholesterolnot_vegetariannot_enteric_coatedphone\n",
      "caps)*****not_certify_kosher_or_halalfat\n",
      "b9-(not_synthetic\n",
      "300.not_ony\n",
      "solublenot_certify_kosher_or_halal$24.99\n",
      "solublenot_certify_kosher_or_halal$24.00\n",
      "either.'not_a_fan\n",
      "citric_acidnot_chelatedvegetariannot_enteric\n",
      "softgels)not_certify_kosher_or_halal$28.95\n",
      "formnot_certify_kosher_or_halal$31.49\n",
      "coatednot_vegetarianifos_rating_=\n",
      "solublenot_certify_kosher_or_halal$6.75\n",
      "solublenot_certify_kosher_or_halal$12.19\n",
      "solublenot_certify_kosher_or_halal$39.30\n",
      "34;not_enough&#34\n",
      "time?not_likely\n",
      "salty_and_bitter(not_fond\n",
      "coq10)ubiquinolfat_solublenot_certify_kosher\n",
      "solublenot_certify_kosher_or_halal$49.49\n",
      "softgels)ubiquinonenot_certify_kosher_or_halal$20.49\n",
      "my_pants.didnot_slip_down.maintained\n",
      "gmosno_cholesterolcontain_stearateschelatedvegetariannot_enteric\n",
      "softgels)not_certify_kosher_or_halalfat\n",
      "34;not_delicious&#34_range\n",
      "gmosno_cholesterolno_mercurynot_enteric\n",
      "oxidefat_solublenot_certify_kosher\n",
      "earlier!not_impressed\n",
      "softgels)not_certify_kosher_or_halal$26.09\n",
      "180-count)***triglyceride_formnot_certify_kosher\n",
      "that.not_purchased\n",
      "freenot_enteric_coatednot_vegetarianone\n",
      "btw.)not_exactly\n",
      "chelatedvegetariannot_enteric_coatedcompany_number\n",
      "softgels)***ethyl_ester_formnot_certify\n",
      "flora)triglyceride_formnot_certify_kosher\n",
      "breathing,(not_a_pulmonary\n",
      "60-count)****ubiquinolfat_solublenot_certify_kosher\n",
      "sodiumnot_the_nutritional_powerhouse\n",
      "cannot_hold_a_candle\n",
      "nnot_agree\n",
      "60-v_gel_bottle)*****not_certify\n",
      "frustration(not_smart\n",
      "us.not_entirely\n",
      "kid.&#34;not_sleeping,&#34\n",
      "solublenot_certify_kosher_or_halal$24.95\n",
      "formprescription_require_nonot_certify\n",
      "soyno_gmoscontain_cholesterolnot_vegetariannot\n",
      "34;not_valid\n",
      "veggie_based.not_derive_from\n",
      "to/_not_und\n",
      "testingnot_surebut\n",
      "snot_nosed\n",
      "couldnot_guarantee\n",
      "solublenot_certify_kosher_or_halal$6.99\n",
      "180-count)ethyl_ester_formnot_certify\n",
      "coatednot_vegetarianone_300_mg\n",
      "formnot_certify_kosher_or_halal$24.95\n",
      "chelatednot_vegetariannot_enteric_coatedcontain\n",
      "doe_snot_interfere\n",
      "softgels_60-count)****ubiquinolfat_solublenot_certify\n",
      "bottle)ubiquinonenot_certify_kosher_or_halal$13.49\n",
      "30-count)ethyl_ester_formnot_certify\n",
      "solublenot_certify_kosher_or_halal$15.50\n",
      "stearate)chelatednot_vegetariannot_enteric_coatedno\n",
      "formnot_certified_koshercertified_halal$17.53\n",
      "34;not_cool&#34\n",
      "soyno_gmosno_cholesterolnot_vegetariannot\n",
      "dietary_life!--not_dramatically\n",
      "solublenot_certify_kosher_or_halal$19.19\n",
      "impurities.(not_distilled)when\n",
      "slightly_sweet.not_exactly\n",
      "eczema(not_apply_directly\n",
      "softgels)*****ethyl_esters_formnot_certify\n",
      "90_softgels)****triglyceride_formnot_certify\n",
      "tart\"-\"citrusy\"-\"not_unpleasant\n",
      "formnot_certify_kosher_or_halal$37.10\n",
      "mercurymolecularly_distillednot_vegetarianifos_rating\n",
      "dgl_licorice.[not_affiliate\n",
      "solublenot_certify_kosher_or_halal$24.57\n",
      "solublenot_certify_kosher_or_halal$23.04\n",
      "unconfirmed)not_enteric_coatednot_vegetarianifos\n",
      "distilledenteric_coatednot_vegetarianifos_rating\n",
      "34;not_hungry&#34\n",
      "softgels)ethyl_ester_formnot_certified\n",
      "esters_formnot_certify_kosher\n",
      "bisglycinatewater_solublenot_certify_kosher\n",
      "xtra_60ct)triglycerides_form****not_certify\n",
      "softgels)ubiquinonenot_certify_kosher_or_halal$24.47\n",
      "solublenot_certify_kosher_or_halal$6.57\n",
      "solublenot_certify_kosher_or_halal$28.39\n",
      "chelatednot_vegetariannot_enteric_coatedphone\n",
      "monot_boostercould\n",
      "donnot_buy!.\n",
      "softgels)not_certify_kosher_or_halal$25.98\n",
      "chelatedvegetariannot_enteric_coatedpossibly_contain\n",
      "products!you_cannot_beat\n",
      "formnot_certify_kosher_or_halal$19.40\n",
      "beauty])*****fat_solublenot_certify_kosher\n",
      "cannot_hurt!.\n",
      "vegetariannot_enteric_coatedpossibly_contain\n",
      "swamp_snot_mixture\n",
      "34;not_bad&#34\n",
      "enteric_coatednot_vegetarianphospholipid_=\n",
      "supply))****ubiquinonefat_solublenot_certify_kosher\n",
      "weightnot_a_magic\n",
      "sgels)***not_certify_kosher_or_halal$45.74\n",
      "crackpot_or_outright_nutters).not_withstand\n",
      "oz.))*****ethyl_ester_formnot_certify\n",
      "digestion.a_must.not_habit\n",
      "34;not_myself&#34\n",
      "pressednot_enteric_coatedvegetarianifos_rating\n",
      "60_capliques)****not_certify_kosher\n",
      "buy;*tiny*butterfinger_tasting*not_the_healthiest\n",
      "30_softgels)****not_certify\n",
      "formnot_certify_kosher_or_halal$22.23\n",
      "30_capsules)***not_certify\n",
      "freenot_enteric_coatedvegetarianifos_rating\n",
      "100,not_the_kyolic\n",
      "solublenot_certify_kosher_or_halal$32.99\n",
      "90_count)****not_certify_kosher\n",
      "formnot_certify_kosher_or_halal$38.15\n",
      "formnot_certify_kosher_or_halal$19.47\n",
      "serving)*****not_certify_kosher_or_halalfat\n",
      "softgels)not_certify_kosher_or_halal$29.21\n",
      "solublenot_certify_kosher_or_halal$14.48\n",
      "count)not_certify_kosher_or_halalfat\n",
      "snot_rocket\n",
      "folic_acid_4%-not_zero\n",
      "negligable_andnot_acceptable\n",
      "capsules_60-count)*****fat_solublenot_certify\n",
      "xtra_8oz)*****triglycerides_formnot_certify\n",
      "formnot_certify_kosher_or_halal$44.59\n",
      "loo_weightnot_workingnot\n",
      "is!(not_a_shot_of_course)i\n",
      "formnot_certify_kosher_or_halal$42.95\n"
     ]
    }
   ],
   "source": [
    "# print trigrams containing 'no' or 'not'\n",
    "for w in paired_words:\n",
    "    if ('_no_' in w or 'not_' in w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_text = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"magnesium malate magnesium glycinatewater solublenot certify kosher or halal$ n a for 120 200 mg capsule on amazonrecommended serving two capsulesprice per gel cap $ n a use amazon 's price)price per 100 mgs magnesium $ n a use amazon 's price)no soyno gmosno cholesterolno stearateschelatedvegetariannot enteric coatedno laxative propertiesno ingredient source from chinaphone number 800 476 3542manufactur in the u.s.a.ingredient magnesium malate chelate magnesium glycinate and vegetarian capsule non gmo plant cellulose)doctor 's good high absorption 100 chelated magnesium\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for one of the weird paired terms in the list above: 'solublenot_certify_kosher'\n",
    "# this shows the review it was a part of before getting paired\n",
    "[sent for sent in unigram_text if 'not certify kosher' in sent][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearly, there was a problem in the unigram terms as well since `soluble` and `not` are joined together (along with other words).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KAL Magnesium Glycinate 400 vs Nine Leading Magnesium Supplements. ***Here is a side-by-side comparison of ten leading magnesium supplements: Nutrigold Magnesium Gold, Doctor's Best High Absorption 100% Chelated Magnesium, JigSaw Magnesium w/SRT, Now Foods Magnesium Citrate (200 mgs), Now Foods Magnesium Capsules (400 mgs), Solgar Magnesium Citrate, Life Extension Magnesium Caps, Thorne Research Magnesium Citrate, Bluebonnet Nutrition Albion Chelated Magnesium, and KAL Magnesium Glycinate 400.Magnesium is needed for more than 300 biochemical reactions in the body. It helps maintain normal muscle and nerve function, keeps heart rhythm steady, supports a healthy immune system, and keeps bones strong. Magnesium also helps regulate blood sugar levels, promotes normal blood pressure, and is known to be involved in energy metabolism and protein synthesis. There is an increased interest in the role of magnesium in preventing and managing disorders such as hypertension, cardiovascular disease, and diabetes.Magnesium is present in all cells of the body. It is a mineral that is critical for energy production and metabolism, muscle contraction, nerve impulse transmission, and bone mineralization. It helps to regulate calcium transport and absorption. By stimulating the secretion of calcitonin, it aids the influx of calcium into bone and promotes optimal bone mineralization. It has been called nature's calcium channel blocker. The idea refers to magnesium's ability to block calcium from entering muscle and heart cells.BREAKDOWN: Here is a breakdown of these supplements.Nutrigold Magnesium Gold (See,Nutrigold Magnesium Gold 120 Vegetarian Capsules)*****Form: magnesium malate / magnesium glycinateWater solubleNot certified Kosher or Halal$ N/A for 120 200 mg capsules on AmazonRecommended Serving: two capsulesPrice per gel cap: $ N/A (using Amazon's price)Price per 100 mgs magnesium: $ N/A (using Amazon's price)No soyNo GMOsNo cholesterolNo stearatesChelatedVegetarianNot enteric c\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the same review in the original unprocessed reviews dataset\n",
    "[sent for sent in text if '$17.09' in sent][0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the unprocessed reviews as well, `soluble` and `not` are joined together (along with other words).  This is a problem with the data itself; not an outcome of the preprocessing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = reviews[reviews.asin.str.contains('B00013YZ1Q')]\n",
    "q2 = q1[q1.summary.str.contains('KAL Magnesium Glycinate 400 vs Nine Leading Magnesium')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['KAL Magnesium Glycinate 400 vs Nine Leading Magnesium Supplements'], dtype=object)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find the product from the review above:\n",
    "q2.summary.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do_not', 268437),\n",
       " ('this_product', 207554),\n",
       " ('seem_to', 45681),\n",
       " ('can_not', 45528),\n",
       " ('great_product', 41158),\n",
       " ('weight_loss', 35438),\n",
       " ('so_far', 29550),\n",
       " ('at_all', 25321),\n",
       " ('this_stuff', 23679),\n",
       " ('highly_recommend', 23157),\n",
       " ('lose_weight', 23118),\n",
       " ('fish_oil', 21909),\n",
       " ('side_effect', 17800),\n",
       " ('as_well', 17148),\n",
       " ('would_recommend', 16050),\n",
       " ('in_the_morning', 15725),\n",
       " ('at_least', 14776),\n",
       " ('will_continue', 14454),\n",
       " ('more_than', 13908),\n",
       " ('more_energy', 13045),\n",
       " ('per_day', 11691),\n",
       " ('every_day', 11147),\n",
       " ('garcinia_cambogia', 10203),\n",
       " ('as_well_as', 9379),\n",
       " ('at_night', 8951),\n",
       " ('very_happy', 8328),\n",
       " ('too_much', 8297),\n",
       " ('year_ago', 8001),\n",
       " ('no_side_effect', 7872),\n",
       " ('high_quality', 7664),\n",
       " ('energy_level', 7583),\n",
       " ('vitamin_d', 7473),\n",
       " ('vitamin_c', 7400),\n",
       " ('year_old', 7201),\n",
       " ('run_out', 7056),\n",
       " ('no_longer', 7043),\n",
       " ('five_star', 6781),\n",
       " ('suffer_from', 6679),\n",
       " ('dr._oz', 6578),\n",
       " ('wake_up', 6439),\n",
       " ('immune_system', 6167),\n",
       " ('twice_a_day', 6086),\n",
       " ('on_the_market', 6069),\n",
       " ('customer_service', 6019),\n",
       " ('even_though', 5759),\n",
       " ('less_than', 5695),\n",
       " ('omega_3', 5605),\n",
       " ('krill_oil', 5506),\n",
       " ('blood_pressure', 5494),\n",
       " ('as_direct', 5476),\n",
       " ('very_pleased', 5396),\n",
       " ('anyone_who', 5368),\n",
       " ('5_star', 5212),\n",
       " ('waste_of_money', 5048),\n",
       " ('go_away', 4994),\n",
       " ('end_up', 4937),\n",
       " ('multi_vitamin', 4887),\n",
       " ('month_ago', 4870),\n",
       " ('green_coffee_bean_extract', 4603),\n",
       " ('great_product!.', 4583),\n",
       " ('exactly_what', 4504),\n",
       " ('come_back', 4417),\n",
       " ('raspberry_ketone', 4414),\n",
       " ('look_forward', 4396),\n",
       " ('every_morning', 4219),\n",
       " ('diet_and_exercise', 4201),\n",
       " ('get_rid', 4201),\n",
       " ('week_ago', 4127),\n",
       " ('people_who', 4125),\n",
       " ('6_month', 3940),\n",
       " ('blood_sugar', 3938),\n",
       " ('throughout_the_day', 3937),\n",
       " ('500_mg', 3719),\n",
       " ('second_bottle', 3601),\n",
       " ('little_bit', 3570),\n",
       " ('those_who', 3559),\n",
       " ('appetite_suppressant', 3509),\n",
       " ('by_far', 3460),\n",
       " ('on_an_empty', 3397),\n",
       " ('vitamin_d3', 3313),\n",
       " ('raspberry_ketones', 3300),\n",
       " ('health_benefit', 3286),\n",
       " ('right_away', 3258),\n",
       " ('long_term', 3252),\n",
       " ('joint_pain', 3218),\n",
       " ('energy_boost', 3198),\n",
       " ('green_tea', 3190),\n",
       " ('hot_flash', 3082),\n",
       " ('rather_than', 3081),\n",
       " ('loose_weight', 3036),\n",
       " ('gain_weight', 2963),\n",
       " ('before_bed', 2921),\n",
       " ('1000_mg', 2666),\n",
       " ('do_the_trick', 2613),\n",
       " ('digestive_system', 2604),\n",
       " ('depend_on', 2581),\n",
       " ('anything_else', 2563),\n",
       " ('fat_burner', 2518),\n",
       " ('clear_up', 2498),\n",
       " ('vitamin_e', 2489)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the 100 most frequent paired words\n",
    "paired_words_frq = Counter([word for word in trigrams_flat if '_' in word])\n",
    "paired_words_frq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('overturn_conventional_wisdom', 1),\n",
       " ('eat&#8221_the_wrong_combo', 1),\n",
       " ('tub_of_humus_with_veggie', 1),\n",
       " ('veep_university', 1),\n",
       " ('consumer_of_cookies!!it', 1),\n",
       " ('portion_veep_university---', 1),\n",
       " ('expereienc_with_veep', 1),\n",
       " ('visual_representation_veep', 1),\n",
       " ('veep_lookcut_program', 1),\n",
       " ('fitness_fanatic_veep_university', 1),\n",
       " ('outdoor_enthusiast_mtn', 1),\n",
       " ('mountain_biking_rowing', 1),\n",
       " ('trx_training', 1),\n",
       " ('lilttle_longer', 1),\n",
       " ('double_decker_cheeseburger', 1),\n",
       " ('marathon_and_a_tri_atholon', 1),\n",
       " ('8220_recommended&#8221', 1),\n",
       " ('trade_show&#8230', 1),\n",
       " ('go!upon_arrival', 1),\n",
       " ('hydroxycitric_acid_hca).this', 1),\n",
       " ('sharp_edges2', 1),\n",
       " ('crash_dieting).in_conclusion', 1),\n",
       " ('nuline_nutritionals_and_tomoson', 1),\n",
       " ('wishful_thinking!ftc_disclosure', 1),\n",
       " ('savor_the_taste).as', 1),\n",
       " ('34;healthy_fat&#34', 1),\n",
       " ('atrail_fibrillationso', 1),\n",
       " ('holy_cr*p', 1),\n",
       " ('w700_and_the_ubersurge', 1),\n",
       " (\"bootle_of_uberday_women_'s\", 1),\n",
       " ('detail_and_a_superior_product!paula', 1),\n",
       " ('deem_morbidly_obese', 1),\n",
       " ('ever!!!highly_recommended', 1),\n",
       " ('bioperene_a_black_pepper', 1),\n",
       " ('neal_a_previous_reviewer', 1),\n",
       " ('george_flansbaum_whom', 1),\n",
       " ('dynamic_nutrition_brand.http://www.amazon.com/pure-forskohlii-standardized-recommended-manufactured/dp/b00k6nzslm/ref=sr_1_3?s=hpc&ie;=utf8&qid;=1405359371&sr;=1-3&keywords;=forskolin',\n",
       "  1),\n",
       " ('young_chronologically', 1),\n",
       " ('side_effects).this', 1),\n",
       " ('onelife_pharma_sound', 1),\n",
       " ('hole_tighter', 1),\n",
       " ('melissa_jones', 1),\n",
       " ('coco_mak_seriously', 1),\n",
       " ('awful!!!._a_group', 1),\n",
       " ('i&#8217;m_assuming', 1),\n",
       " ('yeast_infection_every2', 1),\n",
       " ('a++standadrized_forskolin_excellent!.', 1),\n",
       " ('athlete_and_grappler', 1),\n",
       " ('currently_a_brazilian_jiujitsu', 1),\n",
       " ('minus_the_jitters--', 1),\n",
       " ('wrestling_and_bjj', 1),\n",
       " ('garcinia_cambhogia', 1),\n",
       " ('flood_with_34;snake_oil.&#34', 1),\n",
       " ('hydrolysis_the_protease_enzym', 1),\n",
       " ('camp_inducement', 1),\n",
       " ('air_passage_vasodilation', 1),\n",
       " ('belly_fat!).i', 1),\n",
       " ('amateur_muay_thai_tournament', 1),\n",
       " ('hormone_sensitive_lipase&#8212;which', 1),\n",
       " ('newly_devote', 1),\n",
       " ('deep_hallow_breath', 1),\n",
       " ('34;daily_supplements&#34', 1),\n",
       " ('happier!._there&#8217;s', 1),\n",
       " ('ad_lib', 1),\n",
       " ('sound_8220;hokey&#34', 1),\n",
       " ('rattly_and_uncomfortable', 1),\n",
       " ('burn_fat_and_dlower', 1),\n",
       " ('effectivity_lipolysis', 1),\n",
       " ('rebecca_peagler', 1),\n",
       " ('medical_conditionsthe', 1),\n",
       " ('forskohlli_root', 1),\n",
       " (\"luckily_it't\", 1),\n",
       " ('minus_1.5_lbs)&#9830', 1),\n",
       " ('60-day_trail', 1),\n",
       " ('considerable_drop)fyi', 1),\n",
       " ('8730_surprised_me', 1),\n",
       " ('red_meat_diet!with', 1),\n",
       " ('amazing!https://www.youtube.com_channel_uctciyg3wusbfxkgyjfpz8og', 1),\n",
       " ('garcinic_cambogia_extract', 1),\n",
       " ('virtually_untreatable', 1),\n",
       " ('sample_set_of_naturily', 1),\n",
       " ('osteoarthritis_and_osteomalacia', 1),\n",
       " ('con_su_rodilla', 1),\n",
       " ('mucho_a_mi', 1),\n",
       " ('productmuy_buen_producto_ayuda', 1),\n",
       " ('sodium_free.&#34', 1),\n",
       " ('peanuts_or_tree', 1),\n",
       " ('countless_cortizone_shot', 1),\n",
       " ('tons_of_benefits!.', 1),\n",
       " ('34;clear_out&#34', 1),\n",
       " ('bri_nutrition&#8217;s_unconditional_guarantee', 1),\n",
       " ('dieting_pilling', 1),\n",
       " ('nutrition!._bri_nutrition', 1),\n",
       " ('garage_and_repaint_the_wall', 1),\n",
       " ('bri_nutrition_triphalia', 1),\n",
       " ('crave_34;sweets&#34_and_salty', 1),\n",
       " ('unhygienic_colon', 1),\n",
       " ('slouchy_and_drain', 1),\n",
       " ('stumble_upon_the_colonpro', 1),\n",
       " ('8220;bowel_issues&#8221_lately', 1)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 100 most infrequent paired words\n",
    "paired_words_frq.most_common()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203277"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq)   # number of paired words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 0 ns, total: 29.5 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# we need to learn the full vocabulary of the corpus to be modeled\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.6)\n",
    "trigram_dictionary.compactify()   # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary.save('../vocab_dictionary.dict')     # save vocabulary dict locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary = Dictionary.load('../vocab_dictionary.dict')  # load the finished dictionary from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
