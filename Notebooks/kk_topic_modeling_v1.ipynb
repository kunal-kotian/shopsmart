{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm, tqdm_notebook, tnrange\n",
    "from S3_read_write import load_df_s3, save_df_s3\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas('Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'amazon-reviews-project'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_df_s3(bucket_name, 'amazon_reviews/reviews_data_clean', filetype='text', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585444, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape    # 585,444 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>categories_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>B-flax-D is a re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dpes the job well</td>\n",
       "      <td>Contains Organic...</td>\n",
       "      <td>New Generation B...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Studies show tha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fast shipping, g...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I started taking...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bioavailability ...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I tried Nutrihil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Other Resveratro...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I really liked t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I can't find thi...</td>\n",
       "      <td>Everyone knows t...</td>\n",
       "      <td>Nutrihill Resver...</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful           reviewText  overall              summary  \\\n",
       "0  0929619730  [0, 0]  B-flax-D is a re...      5.0    Dpes the job well   \n",
       "1  0978559088  [1, 1]  Studies show tha...      4.0  Fast shipping, g...   \n",
       "2  0978559088  [1, 1]  I started taking...      5.0  Bioavailability ...   \n",
       "3  0978559088  [0, 1]  I tried Nutrihil...      1.0  Other Resveratro...   \n",
       "4  0978559088  [0, 0]  I really liked t...      5.0  I can't find thi...   \n",
       "\n",
       "           description                title     categories_clean  \n",
       "0  Contains Organic...  New Generation B...  Health & Persona...  \n",
       "1  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "2  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "3  Everyone knows t...  Nutrihill Resver...  Health & Persona...  \n",
       "4  Everyone knows t...  Nutrihill Resver...  Health & Persona...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, Resveratrol',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multivitamins',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Vitamins, Vitamin B, B3 (Niacin)',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Green Tea',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements, Green Coffee Bean Extract',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, CoQ10',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Ginkgo Biloba'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.categories_clean.unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catergories' list indicates that there may be some reviews in the dataset unrelated to health supplements.  Let's get rid of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Liturgy of St. John Chrysostom', 'Origins',\n",
       "       'Sounds of the Earth: Soft Ocean Sounds', 'Bali',\n",
       "       'Tranquil Waters', 'Bach: St. John Passion, BWV 245',\n",
       "       '21st Century Soul', 'Bodies for Strontium', \"John's Bunch\",\n",
       "       'An Evening of Paganini', \"John's Other Bunch\",\n",
       "       'Sus Mas Grandes Exitos', 'Complex Simplicity',\n",
       "       'Kidnapped By Neptune', 'Roman Chant / Easter Vespers', 'Dead 60s',\n",
       "       \"Cilla in the 60's\", 'Chromium', 'Letters From the Vitamin Sea',\n",
       "       'The Stinging Nettles', 'Tendres Annees 60', 'Wehiwehi Hawaii',\n",
       "       'none'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.categories_clean.str.contains('CDs & Vinyl')].title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews[reviews.categories_clean.str.contains('CDs & Vinyl')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product titles shown above are all music albums/songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews[~(reviews.categories_clean.str.contains('CDs & Vinyl'))]   # remove rows with category including 'CDs & Vinyl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, Resveratrol',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multivitamins',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Vitamins, Vitamin B, B3 (Niacin)',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Green Tea',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements, Green Coffee Bean Extract',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Weight Loss, Supplements',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Supplements, Antioxidants, CoQ10',\n",
       "       'Health & Personal Care, Vitamins & Dietary Supplements, Herbal Supplements, Ginkgo Biloba'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt.categories_clean.unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>categories_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>B00009QP4Q</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>The company has ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lives up to its ...</td>\n",
       "      <td>Alpha Five's QLi...</td>\n",
       "      <td>none</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>B0002TIEQQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I ordered this f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>waste of money</td>\n",
       "      <td>Self help tutori...</td>\n",
       "      <td>none</td>\n",
       "      <td>Health &amp; Persona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin helpful           reviewText  overall              summary  \\\n",
       "3639   B00009QP4Q  [2, 2]  The company has ...      5.0  lives up to its ...   \n",
       "50015  B0002TIEQQ  [0, 0]  I ordered this f...      1.0       waste of money   \n",
       "\n",
       "               description title     categories_clean  \n",
       "3639   Alpha Five's QLi...  none  Health & Persona...  \n",
       "50015  Self help tutori...  none  Health & Persona...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt[reviews_filt.categories_clean.str.contains('Software')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filt = reviews_filt[~(reviews_filt.categories_clean.str.contains('Software'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of reviews of pet-related products\n",
    "search_for = [' pet ', ' cat ', ' dog ']\n",
    "pattern = '|'.join(search_for)\n",
    "reviews_filt.title.str.contains(pattern, case=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'Power - Mune Tuna Flavor Pet Herbal Supplement From Vetvittles.com',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'AniMed Witch Hazel 86-Percent Multi-Species Pet Supplement',\n",
       "       'Composure Liquid for Dogs and Cat (188 SERVINGS)'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt[reviews_filt.title.str.contains(pattern, case=False)]['title'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all pet products\n",
    "reviews_filt = reviews_filt[~(reviews_filt.title.str.contains(pattern, case=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned dataframe\n",
    "save_df_s3(df=reviews_filt, bucket_name=bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48501"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filt.asin.nunique()     # 48,535 unique products and 585,179 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine One Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = reviews_filt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0929619730'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.asin     # Amazon Standard Identification Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Generation B-Flax-D'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.title     # this is the product's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Health & Personal Care, Vitamins & Dietary Supplements, Multi & Prenatal Vitamins, Multiple Vitamin-Mineral Supplements'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.categories_clean   # previously filtered/curated categories of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contains Organic Cold-Milled Flaxseed\\nValuable source of soluble and insoluble fiber\\nProvides Omega-3 essential fats, and many other nutrients to help achieve and maintain optimal bowel function.\\n\\nContains Vitamin B12\\nB12 helps prevent nerve damage\\nB12 aids in healthy cell formation.\\nB12 helps prevent anemia\\n\\nContains Vitamin D\\nVitamin D assists the body in the absorption of important minerals like calcium.\\n\\nContains Seleno-yeast\\nA source of selenium, a mineral with powerful anti-viral and disease-fighting properties.\\n\\nContains Vitamin K2\\nMenaQ7TM provides vitamin K2 (menaquinone), extracted and concentrated from natto without solvents. Vitamin K2 prevents arterial calcification and promotes strong bones by improving cross-linking of osteocalcin, a protein found in bones. The amount here has been clinically shown not to interfere with blood anti-coagulant medication. \\n\\nServing Size:\\n1/4 Cup (30 Grams)\\n\\nServings Per Container:\\n30 Servings per container\\n\\nNet Wt. 32 oz / 2 lb (908 g)\\n\\nB-Flax-D is a 100% vegetarian product.\\n\\nDoes Not Contain:\\nContains no artificial colors or preservatives.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.description       # product description provided by the seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dpes the job well'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.summary      # review title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt around. Good product, good price, good results.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.reviewText   # review content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the actual review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.overall     # the rating provided by the reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/amazon_review_screenshot.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"images/amazon_review_screenshot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off using only the title (`summary`) and body (`reviewText`) of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.03 s, sys: 4.25 s, total: 7.28 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = load_df_s3(bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['helpful', 'overall', 'title', 'categories_clean', 'description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>B-flax-D is a regular at our house. It does it...</td>\n",
       "      <td>Dpes the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Studies show that Resveratrol is poorly absorb...</td>\n",
       "      <td>Fast shipping, good communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I started taking this after both my parents di...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I tried Nutrihill, but did not feel any of the...</td>\n",
       "      <td>Other Resveratrol Supplements are Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I really liked this product because it stayed ...</td>\n",
       "      <td>I can't find this product any longer, and I wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                         reviewText  \\\n",
       "0  0929619730  B-flax-D is a regular at our house. It does it...   \n",
       "1  0978559088  Studies show that Resveratrol is poorly absorb...   \n",
       "2  0978559088  I started taking this after both my parents di...   \n",
       "3  0978559088  I tried Nutrihill, but did not feel any of the...   \n",
       "4  0978559088  I really liked this product because it stayed ...   \n",
       "\n",
       "                                             summary  \n",
       "0                                  Dpes the job well  \n",
       "1                  Fast shipping, good communication  \n",
       "2                         Bioavailability is the key  \n",
       "3           Other Resveratrol Supplements are Better  \n",
       "4  I can't find this product any longer, and I wi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each review, concatenate the review title and body\n",
    "df.reviewText = df.summary + '. ' + df.reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...</td>\n",
       "      <td>Dpes the job well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...</td>\n",
       "      <td>Fast shipping, good communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...</td>\n",
       "      <td>Other Resveratrol Supplements are Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0929619730   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  0978559088   \n",
       "4  0978559088   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \\\n",
       "0  Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...   \n",
       "1  Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...   \n",
       "2  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...   \n",
       "3  Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...   \n",
       "4  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...   \n",
       "\n",
       "                                                     summary  \n",
       "0                                          Dpes the job well  \n",
       "1                          Fast shipping, good communication  \n",
       "2                                 Bioavailability is the key  \n",
       "3                   Other Resveratrol Supplements are Better  \n",
       "4  I can't find this product any longer, and I wish I could.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `summary` column now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0929619730</td>\n",
       "      <td>Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0929619730   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  0978559088   \n",
       "4  0978559088   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \n",
       "0  Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt ...  \n",
       "1  Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This ...  \n",
       "2  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...  \n",
       "3  Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the ...  \n",
       "4  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.isnull().sum()    # 73 reviews have neither a review body text, nor a review title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop reviews with no text\n",
    "df = df[~(df.reviewText.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.asin.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few actual review texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice protein powder. Vegan, clean-burning; nice flavor and texture; blends well; low sugar. This is a high-quality protein powder product: I highly recommend it.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So far so good. I have only been on this for a week and have lost a pound so far.  At first I wasn't taking the right dosage but after reading the reviews and usage requirments again on here, I started doing it right.  I am going to continue through the whole first bottle, the free bottle and if I see it is working, I am going to order more.  It has helped with my appetite some but not as much as I had thought.  I do feel better when I take it though.  I have faith and hope to lose much much more.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great product. My son loves these vitamins.  He prefers these over the regular gummy vites (which he loves). He asks for them multiple times during the day.  Much better than the Flinstones.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 584829 entries, 0 to 584901\n",
      "Data columns (total 2 columns):\n",
      "asin          584829 non-null object\n",
      "reviewText    584829 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(df.reviewText.values)    # make an iterable to store only the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent for sent in text if len(sent) == 0]   # there are no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584829"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dpes the job well. B-flax-D is a regular at our house. It does its job simply and with good results. It is reasonable, lasts a long time, and is able to be obtained with free shipping if you hunt around. Good product, good price, good results. \n",
      "\n",
      "Fast shipping, good communication. Studies show that Resveratrol is poorly absorbed when taken by pill, but lozenges are very effectively absorbed. Hardly any companies are selling lozenges. This company promises 99% purity and has fast shipping and good communication. I can't comment on the quality of product because I'm not a chemist but they seem to be legitimate. \n",
      "\n",
      "Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspiring. Doing some research on the Internet, it is indicated that taking resveratrol in lozenge form is preferable as it is broken down by stomach acids.  The ez-melt formula recommended in another review is OK, but it is dissolved in the mouth much more quickly than this lozenge formula, while dissolving more slowly is preferable according to my research.This product has the greatest side effect - since taking it, I haven't had colds or sore throats.  Soon after starting to take it every day, I was starting to come down with a cold, with all my usual symptoms, and was anticipating being very sick the next day, as is my usual pattern.  But I never did get as sick as anticipated - taking this product is the only reason I can come up with.  Since then, I've had no colds or sore throats - it has been great.  I recommend this product to everyone I know, and have given it as gifts to my family. \n",
      "\n",
      "Other Resveratrol Supplements are Better. I tried Nutrihill, but did not feel any of the supposed health benefits. I started reading and realized that even though buccal delivery is the best, the dose is too small in this supplement. I then tried Resveratrol 150 from ezmelts. It's also a lozenge (they call it melt) and it literally melts in your mouth. I have a lot more energy, and haven't been sick at all. I used to get colds and flus all the time before. You can find ezmelts here on amazon, ebay and a bunch of other suppliers. You can find it here:EZ Melts Resveratrol, Natural Grape Flavor, 60 Tablets \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at a few sample reviews\n",
    "for rev in text[:4]:\n",
    "    print(rev, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below are from:\n",
    "\n",
    "http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `gensim`'s `Phrases` class to detect natural combinations of words (like 'vanilla ice cream'), we need to format our text into a list of sentences, with each sentence being a list of words.  This process takes a large amount of processing time (for reference, the times shown under the cells are for running the tasks on a c5.18xlarge EC2 instance (equivalent spot fleet)), so `text` has been split into 3 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Unigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584829"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into 9 parts\n",
    "text_first  = text[:50000]\n",
    "text_second = text[50000:100000]\n",
    "text_third  = text[100000:150000]\n",
    "text_fourth = text[150000:300000]\n",
    "text_fifth  = text[300000:350000]\n",
    "text_sixth  = text[350000:400000]\n",
    "text_seventh= text[400000:450000]\n",
    "text_eighth = text[450000:500000]\n",
    "text_ninth = text[500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:06, 102.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  50000\n",
      "current sent_num:  305895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rev_num = 0    # review tracker\n",
    "sent_num = 0   # sentence tracker\n",
    "unigram_sents_pos = [] # to store lists of lemmatized tokens for each sentence\n",
    "\n",
    "for parsed_review in tqdm(nlp.pipe(text_first, batch_size=10000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305455"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_sents_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, [('dpe', 'NOUN'), ('the', 'DET'), ('job', 'NOUN'), ('well', 'ADV')]]\n",
      "[1, 2, [('b', 'NOUN'), ('flax', 'NOUN'), ('d', 'NOUN'), ('be', 'VERB'), ('a', 'DET'), ('regular', 'ADJ'), ('at', 'ADP'), ('-PRON-', 'ADJ'), ('house', 'NOUN')]]\n",
      "[1, 3, [('-PRON-', 'PRON'), ('do', 'VERB'), ('-PRON-', 'ADJ'), ('job', 'NOUN'), ('simply', 'ADV'), ('and', 'CCONJ'), ('with', 'ADP'), ('good', 'ADJ'), ('result', 'NOUN')]]\n",
      "[1, 4, [('-PRON-', 'PRON'), ('be', 'VERB'), ('reasonable', 'ADJ'), ('last', 'VERB'), ('a', 'DET'), ('long', 'ADJ'), ('time', 'NOUN'), ('and', 'CCONJ'), ('be', 'VERB'), ('able', 'ADJ'), ('to', 'PART'), ('be', 'VERB'), ('obtain', 'VERB'), ('with', 'ADP'), ('free', 'ADJ'), ('shipping', 'NOUN'), ('if', 'ADP'), ('-PRON-', 'PRON'), ('hunt', 'VERB'), ('around', 'ADV')]]\n",
      "[1, 5, [('good', 'ADJ'), ('product', 'NOUN'), ('good', 'ADJ'), ('price', 'NOUN'), ('good', 'ADJ'), ('result', 'NOUN')]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(unigram_sents_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any blank sentences\n",
    "for sent in unigram_sents_pos:\n",
    "    if len(sent[2]) == 0:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:14, 101.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  100000\n",
      "current sent_num:  616751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_second, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615760\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_sents_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:08, 102.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  150000\n",
      "current sent_num:  923642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_third, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [24:10, 103.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  300000\n",
      "current sent_num:  1843092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fourth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:44, 107.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  350000\n",
      "current sent_num:  2144424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fifth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:48, 106.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  400000\n",
      "current sent_num:  2447985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_sixth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [07:49, 106.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  450000\n",
      "current sent_num:  2754623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_seventh, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [08:05, 103.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  500000\n",
      "current sent_num:  3073060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_eighth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84829it [13:31, 104.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  584829\n",
      "current sent_num:  3605491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_ninth, batch_size=20000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T LOAD THIS FILE - there's a _v1 version further down!\n",
    "# del unigram_sentences_savedf\n",
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NOUN+-+||+-+DET+...</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN+-+||+-+NOUN...</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number          unigram_pos    unigram_sentences\n",
       "0              1                1  NOUN+-+||+-+DET+...  dpe+-+||+-+the+-...\n",
       "1              1                2  NOUN+-+||+-+NOUN...  b+-+||+-+flax+-+...\n",
       "2              1                3  PRON+-+||+-+VERB...  -PRON-+-+||+-+do...\n",
       "3              1                4  PRON+-+||+-+VERB...  -PRON-+-+||+-+be...\n",
       "4              1                5  ADJ+-+||+-+NOUN+...  good+-+||+-+prod..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_number, sentence_number, unigram_pos, unigram_sentences]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].head()  # no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(sentence, sentence_pos, sep):\n",
    "    \"\"\"Expects a sentence as a single string as input 1, and its corresponding part-of-speech tags as input 2 (also single string).\n",
    "    sep is the string pattern used to separate words in each sentence string\n",
    "    Cleans it up and returns a single string.\n",
    "    Also updates corresponding part-of-speech string.\n",
    "    \"\"\"\n",
    "    # get rid of webpage links\n",
    "    cond = ['http' in sentence, 'www' in sentence]\n",
    "    if any(cond):\n",
    "        words = sentence.split(sep)\n",
    "        words_pos = sentence_pos.split(sep)\n",
    "        to_remove = []\n",
    "        for i in range(len(words)):\n",
    "            cond_word = ['http' in words[i], 'www' in words[i]]\n",
    "            if any(cond_word):\n",
    "                to_remove.append(i)\n",
    "        # remove words that are links\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del words[j]\n",
    "            del words_pos[j]\n",
    "        # reconstruct sentence after deleting links\n",
    "        sentence = sep.join(words)\n",
    "        sentence_pos = sep.join(words_pos)\n",
    "\n",
    "    # replace underscores with blanks to avoid mix-up with paired words later\n",
    "    # cannot replace with spaces because the strings are split on spaces later \n",
    "    # and this would create new words with no corresponding pos tags\n",
    "    if '_' in sentence:\n",
    "        sentence = sentence.replace('_', '')\n",
    "    return sentence, sentence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!',\n",
       " 'this is a normal sentence',\n",
       " '__ what is this ____ http',\n",
       " '_',\n",
       " 'http']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = ['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!', 'this is a normal sentence', \n",
    "              '__ what is this ____ http', '_', 'http']\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_pos = ['X X X X X X X X X X X X', 'X X X X X', 'X X X X X X', 'X', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy and also BAM! underscoretime!',\n",
       " 'this is a normal sentence',\n",
       " ' what is this ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if clean_up works as expected\n",
    "to_remove = []\n",
    "for i in range(len(test_clean)):\n",
    "    sentence = test_clean[i]\n",
    "    sentence_pos = test_clean_pos[i]\n",
    "    test_clean[i], test_clean_pos[i] = clean_up(sentence, sentence_pos, sep=' ')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if test_clean[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del test_clean[j]\n",
    "    del test_clean_pos[j]\n",
    "\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X X X X X X X X X X X', 'X X X X X', 'X X X X X']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_joined_all = unigram_sentences_savedf.unigram_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599392"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if '_' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'http' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'www' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>1290</td>\n",
       "      <td>7548</td>\n",
       "      <td>X</td>\n",
       "      <td>http://www.amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16153</th>\n",
       "      <td>2775</td>\n",
       "      <td>16180</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>no+-+||+-+jet_la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16602</th>\n",
       "      <td>2837</td>\n",
       "      <td>16629</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22975</th>\n",
       "      <td>3833</td>\n",
       "      <td>23010</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25080</th>\n",
       "      <td>4169</td>\n",
       "      <td>25118</td>\n",
       "      <td>ADJ+-+||+-+PART+...</td>\n",
       "      <td>easy+-+||+-+to+-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_number  sentence_number          unigram_pos  \\\n",
       "7532            1290             7548                    X   \n",
       "16153           2775            16180  DET+-+||+-+NOUN+...   \n",
       "16602           2837            16629  PRON+-+||+-+VERB...   \n",
       "22975           3833            23010  PRON+-+||+-+VERB...   \n",
       "25080           4169            25118  ADJ+-+||+-+PART+...   \n",
       "\n",
       "         unigram_sentences  \n",
       "7532   http://www.amazo...  \n",
       "16153  no+-+||+-+jet_la...  \n",
       "16602  -PRON-+-+||+-+do...  \n",
       "22975  -PRON-+-+||+-+be...  \n",
       "25080  easy+-+||+-+to+-...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences.str.contains('_')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.amazon.com/gp/product/b0000533z8/ref=cm_cr_rev_prod_title',\n",
       " 'no+-+||+-+jet_lag+-+||+-+pill',\n",
       " \"-PRON-+-+||+-+do+-+||+-+recommend+-+||+-+women+-+||+-+'s+-+||+-+one+-+||+-+a_day+-+||+-+though+-+||+-+with+-+||+-+extra+-+||+-+calcium\",\n",
       " '-PRON-+-+||+-+be+-+||+-+less+-+||+-+money+-+||+-+and+-+||+-+good+-+||+-+quality+-+||+-+https://www.amazon.com/review/review-your-purchases/ref=pe_6680_116681230_cm_add_2_star3?_encoding=utf8&asins;=b0000ccw1n%3a3%2cb000sar2dk&channel;=ec_phy&crauthtoken;=ge5g%2bbf%2btr%2f%2fdliytbmmzxn6ajjlfxjdtx902p0aaaadaaaaafnfv%2bbyyxcaaaaa&customerid;=a1pansxlpbgvng#top',\n",
       " 'easy+-+||+-+to+-+||+-+use_work+-+||+-+well',\n",
       " '-PRON-+-+||+-+have+-+||+-+have+-+||+-+pedometer+-+||+-+in+-+||+-+the+-+||+-+past_all+-+||+-+difficult+-+||+-+and+-+||+-+confusing+-+||+-+to+-+||+-+use+-+||+-+to+-+||+-+the+-+||+-+point+-+||+-+-PRON-+-+||+-+simply+-+||+-+give+-+||+-+up+-+||+-+on+-+||+-+-PRON-',\n",
       " 'overall+-+||+-+-PRON-+-+||+-+mother+-+||+-+be+-+||+-+very+-+||+-+satisfied+-+||+-+with+-+||+-+this+-+||+-+product!-d_lionz',\n",
       " 'this+-+||+-+inexpensive+-+||+-+strap+-+||+-+with+-+||+-+a+-+||+-+metal+-+||+-+clip+-+||+-+http://www.amazon.com/gp/product/b000bitymg/ref=oh_details_o00_s00_i00?ie=utf8&psc;=1+-+||+-+be+-+||+-+a+-+||+-+good+-+||+-+replacement+-+||+-+for+-+||+-+the+-+||+-+flimsy+-+||+-+omron+-+||+-+plastic+-+||+-+clip+-+||+-+but+-+||+-+-PRON-+-+||+-+have+-+||+-+not+-+||+-+be+-+||+-+use+-+||+-+-PRON-+-+||+-+long',\n",
       " 'hj_112+-+||+-+digital+-+||+-+pemium+-+||+-+pedometer+-+||+-+update',\n",
       " 'accordingly+-+||+-+for+-+||+-+100gr+-+||+-+serving+-+||+-+size+-+||+-+of;sesame__flax__chia__amaranth__quinoa__hemp__pistachio__almond__cashew__peanutcalories565__534__490__371__368__567__557__575__553__585sat+-+||+-+fat34__18__16__7__4__17__27__19__39__34calcium99__26__63__16__5__0__11__26__4__5magnesium89__98__0__62__49__167__30__67__73__44potassium14__23__5__15__16__167__29__20__19__19zinc48__29__23__19__21__83__15__21__39__22fiber47__109__151__8__11__4+-+||+-+41__49__13__32phytosterols714__0__0__0__0__n+-+||+-+a__214__0__0__0freshness+-+||+-+fresh+-+||+-+mean+-+||+-+nutritious+-+||+-+and+-+||+-+delicious']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence for sentence in words_joined_all if '_' in sentence][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up all unigrams\n",
    "to_remove = []\n",
    "for i in range(len(words_joined_all)):\n",
    "    sentence = words_joined_all[i]\n",
    "    sentence_pos = pos_joined_all[i]\n",
    "    words_joined_all[i], pos_joined_all[i] = clean_up(sentence, sentence_pos, sep='+-+||+-+')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if words_joined_all[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del words_joined_all[j]\n",
    "    del pos_joined_all[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows from unigram_sentences_savedf corresponding to the row numbers (indices) of sentences\n",
    "# that will be blank after the transformation above\n",
    "unigram_sentences_savedf.drop(unigram_sentences_savedf.index[to_remove], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf.drop(['unigram_sentences'], axis=1, inplace=True)\n",
    "unigram_sentences_savedf.drop(['unigram_pos'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf['unigram_sentences'] = words_joined_all\n",
    "unigram_sentences_savedf['unigram_pos'] = pos_joined_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "      <td>NOUN+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "      <td>NOUN+-+||+-+NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  dpe+-+||+-+the+-...  NOUN+-+||+-+DET+...\n",
       "1              1                2  b+-+||+-+flax+-+...  NOUN+-+||+-+NOUN...\n",
       "2              1                3  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...\n",
       "3              1                4  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...\n",
       "4              1                5  good+-+||+-+prod...  ADJ+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599286, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated, cleaned up version of unigram_sentences.feather\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/unigram_sentences_v1.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = [sentence.split('+-+||+-+') for sentence in words_joined_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dpe', 'the', 'job', 'well'], ['b', 'flax', 'd', 'be', 'a', 'regular', 'at', '-PRON-', 'house'], ['-PRON-', 'do', '-PRON-', 'job', 'simply', 'and', 'with', 'good', 'result'], ['-PRON-', 'be', 'reasonable', 'last', 'a', 'long', 'time', 'and', 'be', 'able', 'to', 'be', 'obtain', 'with', 'free', 'shipping', 'if', '-PRON-', 'hunt', 'around']]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sentences[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599286"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 36s, sys: 2.58 s, total: 3min 39s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The common_terms parameter add a way to give special treatment to common terms \n",
    "# (aka stop words) such that their presence between two words won’t prevent bigram detection. \n",
    "# It allows to detect expressions like “bank of america”\n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\"]\n",
    "\n",
    "# Train a first-order phrase detector\n",
    "bigram_model = Phrases(unigram_sentences, threshold=0.6, scoring='npmi', common_terms=common_terms)\n",
    "\n",
    "# Transform unigram sentences into bigram sentences\n",
    "# Paired words are connected by an underscore, e.g. ice_cream\n",
    "bigram_sentences = []\n",
    "for sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 36s, sys: 3.18 s, total: 3min 40s\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a second-order phrase detector\n",
    "# trigram_model = Phrases(bigram_sentences, min_count=5)\n",
    "trigram_model = Phrases(bigram_sentences, threshold=0.5, scoring='npmi')\n",
    "\n",
    "# Transform bigram sentences into trigram sentences\n",
    "trigram_sentences = []\n",
    "for sentence in bigram_sentences:\n",
    "    trigram_sentences.append(trigram_model[sentence])\n",
    "\n",
    "# remove any remaining stopwords\n",
    "# trigram_sentences = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in trigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trigrams will be saved in a dataframe with a single column.\n",
    "# each row is one sentence from any review\n",
    "# each sentence is a single string separated by a single space.\n",
    "trigram_sentences_savedf = pd.DataFrame([u'+-+||+-+'.join(sentence) for sentence in trigram_sentences], columns=['preprocessed_review'])\n",
    "save_df_s3(trigram_sentences_savedf, bucket_name, 'amazon_reviews/preprocessed_reviews.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/preprocessed_reviews.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preprocessed_review\n",
       "0  dpe+-+||+-+the+-...\n",
       "1  b+-+||+-+flax+-+...\n",
       "2  -PRON-+-+||+-+do...\n",
       "3  -PRON-+-+||+-+be...\n",
       "4  good+-+||+-+prod..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram_sentences = trigram_sentences_savedf.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605491"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(trigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unigram_sentences_savedf\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "      <td>NOUN+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "      <td>NOUN+-+||+-+NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  dpe+-+||+-+the+-...  NOUN+-+||+-+DET+...\n",
       "1              1                2  b+-+||+-+flax+-+...  NOUN+-+||+-+NOUN...\n",
       "2              1                3  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...\n",
       "3              1                4  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...\n",
       "4              1                5  good+-+||+-+prod...  ADJ+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df[unigram_sents_pos_df.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599286, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = pd.merge(unigram_sents_pos_df, trigram_sentences_savedf, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "      <td>NOUN+-+||+-+DET+...</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "      <td>NOUN+-+||+-+NOUN...</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>fast+-+||+-+ship...</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "      <td>fast_shipping+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>study+-+||+-+sho...</td>\n",
       "      <td>NOUN+-+||+-+VERB...</td>\n",
       "      <td>study+-+||+-+sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>hardly+-+||+-+an...</td>\n",
       "      <td>ADV+-+||+-+DET+-...</td>\n",
       "      <td>hardly+-+||+-+an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>this+-+||+-+comp...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this+-+||+-+comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  dpe+-+||+-+the+-...  NOUN+-+||+-+DET+...   \n",
       "1              1                2  b+-+||+-+flax+-+...  NOUN+-+||+-+NOUN...   \n",
       "2              1                3  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "3              1                4  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...   \n",
       "4              1                5  good+-+||+-+prod...  ADJ+-+||+-+NOUN+...   \n",
       "5              2                6  fast+-+||+-+ship...  ADJ+-+||+-+NOUN+...   \n",
       "6              2                7  study+-+||+-+sho...  NOUN+-+||+-+VERB...   \n",
       "7              2                8  hardly+-+||+-+an...  ADV+-+||+-+DET+-...   \n",
       "8              2                9  this+-+||+-+comp...  DET+-+||+-+NOUN+...   \n",
       "9              2               10  -PRON-+-+||+-+ca...  PRON+-+||+-+VERB...   \n",
       "\n",
       "   preprocessed_review  \n",
       "0  dpe+-+||+-+the+-...  \n",
       "1  b+-+||+-+flax+-+...  \n",
       "2  -PRON-+-+||+-+do...  \n",
       "3  -PRON-+-+||+-+be...  \n",
       "4  good+-+||+-+prod...  \n",
       "5  fast_shipping+-+...  \n",
       "6  study+-+||+-+sho...  \n",
       "7  hardly+-+||+-+an...  \n",
       "8  this+-+||+-+comp...  \n",
       "9  -PRON-+-+||+-+ca...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_s3(unigram_sents_pos_df, bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/preprocessed_reviews_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599286, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>70</td>\n",
       "      <td>401</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>70</td>\n",
       "      <td>402</td>\n",
       "      <td>-PRON-+-+||+-+th...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>70</td>\n",
       "      <td>403</td>\n",
       "      <td>do+-+||+-+not+-+...</td>\n",
       "      <td>VERB+-+||+-+ADV+...</td>\n",
       "      <td>do_not+-+||+-+re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>71</td>\n",
       "      <td>404</td>\n",
       "      <td>mould+-+||+-+mot...</td>\n",
       "      <td>VERB+-+||+-+PROP...</td>\n",
       "      <td>mould_motion+-+|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>71</td>\n",
       "      <td>405</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>71</td>\n",
       "      <td>406</td>\n",
       "      <td>and+-+||+-+i+-+|...</td>\n",
       "      <td>CCONJ+-+||+-+PRO...</td>\n",
       "      <td>and+-+||+-+i+-+|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>71</td>\n",
       "      <td>407</td>\n",
       "      <td>and+-+||+-+besid...</td>\n",
       "      <td>CCONJ+-+||+-+ADP...</td>\n",
       "      <td>and+-+||+-+besid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>71</td>\n",
       "      <td>408</td>\n",
       "      <td>just+-+||+-+too+...</td>\n",
       "      <td>ADV+-+||+-+ADV+-...</td>\n",
       "      <td>just+-+||+-+too_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>71</td>\n",
       "      <td>409</td>\n",
       "      <td>do+-+||+-+not+-+...</td>\n",
       "      <td>VERB+-+||+-+ADV+...</td>\n",
       "      <td>do_not+-+||+-+buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>72</td>\n",
       "      <td>410</td>\n",
       "      <td>be+-+||+-+a+-+||...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>be+-+||+-+a+-+||...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             70              401  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "401             70              402  -PRON-+-+||+-+th...  PRON+-+||+-+VERB...   \n",
       "402             70              403  do+-+||+-+not+-+...  VERB+-+||+-+ADV+...   \n",
       "403             71              404  mould+-+||+-+mot...  VERB+-+||+-+PROP...   \n",
       "404             71              405  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "405             71              406  and+-+||+-+i+-+|...  CCONJ+-+||+-+PRO...   \n",
       "406             71              407  and+-+||+-+besid...  CCONJ+-+||+-+ADP...   \n",
       "407             71              408  just+-+||+-+too+...  ADV+-+||+-+ADV+-...   \n",
       "408             71              409  do+-+||+-+not+-+...  VERB+-+||+-+ADV+...   \n",
       "409             72              410  be+-+||+-+a+-+||...  VERB+-+||+-+DET+...   \n",
       "\n",
       "     preprocessed_review  \n",
       "400  -PRON-+-+||+-+do...  \n",
       "401  -PRON-+-+||+-+th...  \n",
       "402  do_not+-+||+-+re...  \n",
       "403  mould_motion+-+|...  \n",
       "404  -PRON-+-+||+-+do...  \n",
       "405  and+-+||+-+i+-+|...  \n",
       "406  and+-+||+-+besid...  \n",
       "407  just+-+||+-+too_...  \n",
       "408    do_not+-+||+-+buy  \n",
       "409  be+-+||+-+a+-+||...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df['has_paired_words'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sents_pos_df.loc[unigram_sents_pos_df.preprocessed_review.str.contains('_'), ['has_paired_words']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1565595"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.has_paired_words.sum()  # number of sentences with paired words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "      <td>NOUN+-+||+-+DET+...</td>\n",
       "      <td>dpe+-+||+-+the+-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "      <td>NOUN+-+||+-+NOUN...</td>\n",
       "      <td>b+-+||+-+flax+-+...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "      <td>ADJ+-+||+-+NOUN+...</td>\n",
       "      <td>good+-+||+-+prod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  dpe+-+||+-+the+-...  NOUN+-+||+-+DET+...   \n",
       "1              1                2  b+-+||+-+flax+-+...  NOUN+-+||+-+NOUN...   \n",
       "2              1                3  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "3              1                4  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...   \n",
       "4              1                5  good+-+||+-+prod...  ADJ+-+||+-+NOUN+...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  dpe+-+||+-+the+-...                 0  \n",
       "1  b+-+||+-+flax+-+...                 0  \n",
       "2  -PRON-+-+||+-+do...                 0  \n",
       "3  -PRON-+-+||+-+be...                 1  \n",
       "4  good+-+||+-+prod...                 0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 5.74 s, total: 35.7 s\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigram_sents_pos_df.unigram_pos = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_pos.tolist()]\n",
    "unigram_sents_pos_df.unigram_sentences = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_sentences.tolist()]\n",
    "unigram_sents_pos_df.preprocessed_review = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.preprocessed_review.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>70</td>\n",
       "      <td>401</td>\n",
       "      <td>[-PRON-, do, not...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, do_not,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>70</td>\n",
       "      <td>402</td>\n",
       "      <td>[-PRON-, think, ...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, think, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>70</td>\n",
       "      <td>403</td>\n",
       "      <td>[do, not, recomm...</td>\n",
       "      <td>[VERB, ADV, VERB...</td>\n",
       "      <td>[do_not, recomme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>71</td>\n",
       "      <td>404</td>\n",
       "      <td>[mould, motion, ...</td>\n",
       "      <td>[VERB, PROPN, NU...</td>\n",
       "      <td>[mould_motion, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>71</td>\n",
       "      <td>405</td>\n",
       "      <td>[-PRON-, do, not...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, do_not,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>71</td>\n",
       "      <td>406</td>\n",
       "      <td>[and, i, have, -...</td>\n",
       "      <td>[CCONJ, PRON, VE...</td>\n",
       "      <td>[and, i, have, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>71</td>\n",
       "      <td>407</td>\n",
       "      <td>[and, besides, -...</td>\n",
       "      <td>[CCONJ, ADP, PRO...</td>\n",
       "      <td>[and, besides, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>71</td>\n",
       "      <td>408</td>\n",
       "      <td>[just, too, much...</td>\n",
       "      <td>[ADV, ADV, ADJ, ...</td>\n",
       "      <td>[just, too_much,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>71</td>\n",
       "      <td>409</td>\n",
       "      <td>[do, not, buy]</td>\n",
       "      <td>[VERB, ADV, VERB]</td>\n",
       "      <td>[do_not, buy]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>72</td>\n",
       "      <td>410</td>\n",
       "      <td>[be, a, gift]</td>\n",
       "      <td>[VERB, DET, NOUN]</td>\n",
       "      <td>[be, a, gift]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             70              401  [-PRON-, do, not...  [PRON, VERB, ADV...   \n",
       "401             70              402  [-PRON-, think, ...  [PRON, VERB, ADJ...   \n",
       "402             70              403  [do, not, recomm...  [VERB, ADV, VERB...   \n",
       "403             71              404  [mould, motion, ...  [VERB, PROPN, NU...   \n",
       "404             71              405  [-PRON-, do, not...  [PRON, VERB, ADV...   \n",
       "405             71              406  [and, i, have, -...  [CCONJ, PRON, VE...   \n",
       "406             71              407  [and, besides, -...  [CCONJ, ADP, PRO...   \n",
       "407             71              408  [just, too, much...  [ADV, ADV, ADJ, ...   \n",
       "408             71              409       [do, not, buy]    [VERB, ADV, VERB]   \n",
       "409             72              410        [be, a, gift]    [VERB, DET, NOUN]   \n",
       "\n",
       "     preprocessed_review  has_paired_words  \n",
       "400  [-PRON-, do_not,...                 1  \n",
       "401  [-PRON-, think, ...                 0  \n",
       "402  [do_not, recomme...                 1  \n",
       "403  [mould_motion, 5...                 1  \n",
       "404  [-PRON-, do_not,...                 1  \n",
       "405  [and, i, have, -...                 0  \n",
       "406  [and, besides, -...                 1  \n",
       "407  [just, too_much,...                 1  \n",
       "408        [do_not, buy]                 1  \n",
       "409        [be, a, gift]                 0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "has_paired_words       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an arbitrary sentence and it's transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_sentences.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_pos.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication', 'and', 'restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.preprocessed_review.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramlist = [word for sent in trigram_sentences for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do_not', 268437),\n",
       " ('this_product', 207554),\n",
       " ('can_not', 45528),\n",
       " ('great_product', 41158),\n",
       " ('a_few', 38755),\n",
       " ('weight_loss', 35341),\n",
       " ('so_far', 29550),\n",
       " ('lot_of', 28975),\n",
       " ('as_well', 26527),\n",
       " ('at_all', 25321),\n",
       " ('this_stuff', 23679),\n",
       " ('highly_recommend', 23157),\n",
       " ('lose_weight', 23118),\n",
       " ('fish_oil', 22805),\n",
       " ('side_effect', 17882),\n",
       " ('would_recommend', 16050),\n",
       " ('at_least', 14776),\n",
       " ('will_continue', 14454),\n",
       " ('along_with', 13198),\n",
       " ('per_day', 11691),\n",
       " ('every_day', 11147),\n",
       " ('garcinia_cambogia', 10208),\n",
       " ('at_night', 8951),\n",
       " ('very_happy', 8355),\n",
       " ('too_much', 8303),\n",
       " ('year_ago', 8008),\n",
       " ('no_side_effect', 7872),\n",
       " ('high_quality', 7664),\n",
       " ('energy_level', 7583),\n",
       " ('vitamin_d', 7483),\n",
       " ('vitamin_c', 7408),\n",
       " ('year_old', 7247),\n",
       " ('run_out', 7056),\n",
       " ('no_longer', 7045),\n",
       " ('five_star', 6783),\n",
       " ('dr._oz', 6583),\n",
       " ('suffer_from', 6446),\n",
       " ('wake_up', 6439),\n",
       " ('immune_system', 6191),\n",
       " ('krill_oil', 6048),\n",
       " ('customer_service', 5996),\n",
       " ('even_though', 5805),\n",
       " ('omega_3', 5641),\n",
       " ('as_direct', 5496),\n",
       " ('blood_pressure', 5494),\n",
       " ('very_pleased', 5420),\n",
       " ('anyone_who', 5375),\n",
       " ('5_star', 5222),\n",
       " ('waste_of_money', 5056),\n",
       " ('go_away', 5023),\n",
       " ('end_up', 4976),\n",
       " ('multi_vitamin', 4928),\n",
       " ('month_ago', 4894),\n",
       " ('green_coffee_bean_extract', 4700),\n",
       " ('great_product!.', 4587),\n",
       " ('exactly_what', 4513),\n",
       " ('come_back', 4505),\n",
       " ('raspberry_ketone', 4443),\n",
       " ('look_forward', 4400),\n",
       " ('6_month', 4327),\n",
       " ('get_rid', 4264),\n",
       " ('every_morning', 4247),\n",
       " ('people_who', 4158),\n",
       " ('week_ago', 4127),\n",
       " ('blood_sugar', 4021),\n",
       " ('500_mg', 3763),\n",
       " ('vitamin_d3', 3740),\n",
       " ('food_store', 3652),\n",
       " ('little_bit', 3623),\n",
       " ('second_bottle', 3621),\n",
       " ('those_who', 3581),\n",
       " ('appetite_suppressant', 3514),\n",
       " ('by_far', 3460),\n",
       " ('on_an_empty', 3397),\n",
       " ('health_benefit', 3312),\n",
       " ('raspberry_ketones', 3311),\n",
       " ('hot_flash', 3308),\n",
       " ('right_away', 3270),\n",
       " ('long_term', 3261),\n",
       " ('green_tea', 3229),\n",
       " ('joint_pain', 3227),\n",
       " ('rather_than', 3179),\n",
       " ('loose_weight', 3036),\n",
       " ('gain_weight', 2963),\n",
       " ('before_bed', 2912),\n",
       " ('10_pound', 2821),\n",
       " ('1000_mg', 2720),\n",
       " ('vitamin_e', 2684),\n",
       " ('100_mg', 2613),\n",
       " ('digestive_system', 2611),\n",
       " ('depend_on', 2608),\n",
       " ('gel_cap', 2597),\n",
       " ('anything_else', 2579),\n",
       " ('fat_burner', 2524),\n",
       " ('protein_shake', 2451),\n",
       " ('talk_about', 2422),\n",
       " ('whole_food', 2393),\n",
       " ('come_across', 2317),\n",
       " ('love_it!.', 2271),\n",
       " ('blood_test', 2249)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq = Counter(gramlist)\n",
    "paired_words_frq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('overturn_conventional', 1),\n",
       " ('portion_veep_university---', 1),\n",
       " ('expereienc_with_veep', 1),\n",
       " ('representation_veep', 1),\n",
       " ('veep_lookcut_program', 1),\n",
       " ('enthusiast_mtn_bike', 1),\n",
       " ('mountain_biking_rowing', 1),\n",
       " ('decker_cheeseburger', 1),\n",
       " ('tri_atholon', 1),\n",
       " ('8220_recommended&#8221', 1),\n",
       " ('trade_show&#8230', 1),\n",
       " ('go!upon_arrival', 1),\n",
       " ('sharp_edges2', 1),\n",
       " ('crash_dieting).in_conclusion', 1),\n",
       " ('wishful_thinking!ftc_disclosure', 1),\n",
       " ('34;healthy_fat&#34', 1),\n",
       " ('atrail_fibrillationso', 1),\n",
       " ('holy_cr*p', 1),\n",
       " ('bootle_of_uberday_women', 1),\n",
       " ('superior_product!paula', 1),\n",
       " ('deem_morbidly_obese', 1),\n",
       " ('ever!!!highly_recommended', 1),\n",
       " ('george_flansbaum_whom', 1),\n",
       " ('onelife_pharma', 1),\n",
       " ('melissa_jones', 1),\n",
       " ('coco_mak_seriously', 1),\n",
       " ('i&#8217;m_assuming', 1),\n",
       " ('yeast_infection_every2', 1),\n",
       " ('a++standadrized_forskolin_excellent!.', 1),\n",
       " ('brazilian_jiujitsu_brown', 1),\n",
       " ('wrestling_and_bjj', 1),\n",
       " ('garcinia_cambhogia', 1),\n",
       " ('protease_enzym', 1),\n",
       " ('camp_inducement', 1),\n",
       " ('passage_vasodilation', 1),\n",
       " ('belly_fat!).i', 1),\n",
       " ('sensitive_lipase&#8212;which', 1),\n",
       " ('deep_hallow', 1),\n",
       " ('34;daily_supplements&#34', 1),\n",
       " ('happier!._there&#8217;s', 1),\n",
       " ('ad_lib', 1),\n",
       " ('sound_8220;hokey&#34', 1),\n",
       " ('effectivity_lipolysis', 1),\n",
       " ('rebecca_peagler', 1),\n",
       " ('medical_conditionsthe', 1),\n",
       " ('forskohlli_root', 1),\n",
       " (\"luckily_it't\", 1),\n",
       " ('1.5_lbs)&#9830', 1),\n",
       " ('considerable_drop)fyi', 1),\n",
       " ('8730_surprised', 1),\n",
       " ('red_meat_diet!with', 1),\n",
       " ('channel_uctciyg3wusbfxkgyjfpz8og', 1),\n",
       " ('virtually_untreatable', 1),\n",
       " ('con_su_rodilla', 1),\n",
       " ('ayuda_mucho', 1),\n",
       " ('productmuy_buen_producto', 1),\n",
       " ('sodium_free.&#34', 1),\n",
       " ('countless_cortizone', 1),\n",
       " ('34;clear_out&#34', 1),\n",
       " ('bri_nutrition&#8217;s_unconditional', 1),\n",
       " ('dieting_pilling', 1),\n",
       " ('nutrition!._bri_nutrition', 1),\n",
       " ('garage_and_repaint', 1),\n",
       " ('bri_nutrition_triphalia', 1),\n",
       " ('crave_34;sweets&#34_and_salty', 1),\n",
       " ('8220;bowel_issues&#8221_lately', 1),\n",
       " ('master_cleanse&#34', 1),\n",
       " ('34;all_natural&#34;i', 1),\n",
       " ('occurrence_of_fosmon', 1),\n",
       " ('john_rogan', 1),\n",
       " ('crucial_andnot', 1),\n",
       " ('34;last_diet.&#34', 1),\n",
       " ('34;total_diet&#34', 1),\n",
       " ('defy_reccomemd', 1),\n",
       " ('greg_bastin', 1),\n",
       " ('favorite_navaho_teas', 1),\n",
       " ('unlike_cleartea', 1),\n",
       " ('tea!!._braniac', 1),\n",
       " ('34;truth_of_reality&#34;.', 1),\n",
       " ('definitley_34;up&#34', 1),\n",
       " ('renew_interest!.', 1),\n",
       " ('customer_ssupport', 1),\n",
       " ('semi_pornographic', 1),\n",
       " ('enzyte_commercial', 1),\n",
       " ('hott_natural!!.', 1),\n",
       " ('comment_frombf', 1),\n",
       " ('chinese_herbalists', 1),\n",
       " ('fst_arrival', 1),\n",
       " ('34;roller_coaster&#34;.', 1),\n",
       " ('ashley_sutherland', 1),\n",
       " ('pleasantely_surprised!.', 1),\n",
       " ('sexual_stimulant!!.', 1),\n",
       " ('regard_xanthoparmelia_scabrosa', 1),\n",
       " ('34;xanthoparmelia_cautioni', 1),\n",
       " ('excessive_stimulation.it', 1),\n",
       " ('34;caffeine_blues&#34', 1),\n",
       " ('raw&#8230;.they_cook', 1),\n",
       " ('meek_limp', 1),\n",
       " ('a9_thermogenics', 1),\n",
       " ('science_behind_thermogenics', 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 100 most infrequent paired words\n",
    "paired_words_frq.most_common()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161028"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq)  # number of paired terms  (this drops down to 46,785 after further processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, ...</td>\n",
       "      <td>[NOUN, DET, NOUN...</td>\n",
       "      <td>[dpe, the, job, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be,...</td>\n",
       "      <td>[NOUN, NOUN, NOU...</td>\n",
       "      <td>[b, flax, d, be,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PR...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, do, -PR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, rea...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, be, rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, ...</td>\n",
       "      <td>[ADJ, NOUN, ADJ,...</td>\n",
       "      <td>[good, product, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [dpe, the, job, ...  [NOUN, DET, NOUN...   \n",
       "1              1                2  [b, flax, d, be,...  [NOUN, NOUN, NOU...   \n",
       "2              1                3  [-PRON-, do, -PR...  [PRON, VERB, ADJ...   \n",
       "3              1                4  [-PRON-, be, rea...  [PRON, VERB, ADJ...   \n",
       "4              1                5  [good, product, ...  [ADJ, NOUN, ADJ,...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [dpe, the, job, ...                 0  \n",
       "1  [b, flax, d, be,...                 0  \n",
       "2  [-PRON-, do, -PR...                 0  \n",
       "3  [-PRON-, be, rea...                 1  \n",
       "4  [good, product, ...                 0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove):\n",
    "    # split up paired words failing our format requirements\n",
    "    to_remove.extend([i])\n",
    "    sent_paired.extend(sent[i + skip: i + skip + num_paired])\n",
    "\n",
    "\n",
    "def filter_pairs(sent, sent_paired, sent_pos):\n",
    "    \"\"\"modify sent_paired in place\"\"\"\n",
    "    paired_sent_len = len(sent_paired)\n",
    "    skip = 0\n",
    "    to_remove = []\n",
    "    \n",
    "    for i in range(paired_sent_len):\n",
    "        word = sent_paired[i]\n",
    "        if '_' in word:\n",
    "            num_paired = word.count('_') + 1\n",
    "            \n",
    "            # more than 3 words paired - ignore pairing\n",
    "            if num_paired > 3:\n",
    "                handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                skip += num_paired - 1\n",
    "                continue\n",
    "            \n",
    "            # bigrams: noun/adj, noun\n",
    "            elif num_paired == 2:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_2 == 'NOUN')\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "            \n",
    "            # trigrams: noun/adj, all types, noun/adj\n",
    "            elif num_paired == 3:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                pos_word_3 = sent_pos[i + skip + 2]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_3 in ('NOUN', 'ADJ'))\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "        \n",
    "            # num. of words to skip indexing over sent and sent_pos in the next iter\n",
    "            skip += num_paired - 1\n",
    "        \n",
    "    # remove rejected pairs that are already split and added back individually\n",
    "    if len(to_remove) > 0:\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del sent_paired[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the filtering function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent = ['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "# Expected output:\n",
    "print(['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "sent = ['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'a', 'lot', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, ...</td>\n",
       "      <td>[NOUN, DET, NOUN...</td>\n",
       "      <td>[dpe, the, job, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be,...</td>\n",
       "      <td>[NOUN, NOUN, NOU...</td>\n",
       "      <td>[b, flax, d, be,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PR...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, do, -PR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, rea...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, be, rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, ...</td>\n",
       "      <td>[ADJ, NOUN, ADJ,...</td>\n",
       "      <td>[good, product, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [dpe, the, job, ...  [NOUN, DET, NOUN...   \n",
       "1              1                2  [b, flax, d, be,...  [NOUN, NOUN, NOU...   \n",
       "2              1                3  [-PRON-, do, -PR...  [PRON, VERB, ADJ...   \n",
       "3              1                4  [-PRON-, be, rea...  [PRON, VERB, ADJ...   \n",
       "4              1                5  [good, product, ...  [ADJ, NOUN, ADJ,...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [dpe, the, job, ...                 0  \n",
       "1  [b, flax, d, be,...                 0  \n",
       "2  [-PRON-, do, -PR...                 0  \n",
       "3  [-PRON-, be, rea...                 1  \n",
       "4  [good, product, ...                 0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_reviews = unigram_sents_pos_df.preprocessed_review.tolist()\n",
    "unigram_sentences = unigram_sents_pos_df.unigram_sentences.tolist()\n",
    "unigram_pos = unigram_sents_pos_df.unigram_pos.tolist()\n",
    "has_paired_words = unigram_sents_pos_df.has_paired_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3599286/3599286 [00:07<00:00, 503726.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# get rid of paired words from the corpus which\n",
    "# (1) have more than 3 words joined\n",
    "# (2) bigrams not in the format: noun/adj, noun\n",
    "# (3) trigrams not in the format: noun/adj, all types, noun/adj\n",
    "for i in tqdm(range(len(preprocessed_reviews))):\n",
    "    if has_paired_words[i] == 1:\n",
    "        filter_pairs(sent=unigram_sentences[i], sent_paired=preprocessed_reviews[i], sent_pos=unigram_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]   \n",
       "\n",
       "                                 preprocessed_review  has_paired_words  \n",
       "0                              [dpe, the, job, well]                 0  \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]                 0  \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...                 0  \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...                 1  \n",
       "4         [good, product, good, price, good, result]                 0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599286, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_review_updated = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599286"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_review_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dpe', 'the', 'job', 'well'],\n",
       " ['b', 'flax', 'd', 'be', 'a', 'regular', 'at', '-PRON-', 'house'],\n",
       " ['-PRON-', 'do', '-PRON-', 'job', 'simply', 'and', 'with', 'good', 'result']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_review_updated[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramlist_updated = [word for sent in preprocessed_review_updated for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great_product', 34189),\n",
       " ('weight_loss', 32942),\n",
       " ('fish_oil', 18131),\n",
       " ('side_effect', 17720),\n",
       " ('energy_level', 7491),\n",
       " ('high_quality', 7143),\n",
       " ('vitamin_d', 6331),\n",
       " ('immune_system', 5981),\n",
       " ('blood_pressure', 5431),\n",
       " ('customer_service', 5425),\n",
       " ('anyone_who', 5374),\n",
       " ('vitamin_c', 5201),\n",
       " ('waste_of_money', 4609),\n",
       " ('multi_vitamin', 4236),\n",
       " ('people_who', 4156),\n",
       " ('blood_sugar', 3939),\n",
       " ('little_bit', 3616),\n",
       " ('second_bottle', 3613),\n",
       " ('food_store', 3512),\n",
       " ('health_benefit', 3287),\n",
       " ('hot_flash', 3246),\n",
       " ('long_term', 3235),\n",
       " ('joint_pain', 3143),\n",
       " ('raspberry_ketone', 3095),\n",
       " ('appetite_suppressant', 2795),\n",
       " ('vitamin_e', 2624),\n",
       " ('gel_cap', 2529),\n",
       " ('green_tea', 2501),\n",
       " ('krill_oil', 2492),\n",
       " ('digestive_system', 2487),\n",
       " ('glass_of_water', 2234),\n",
       " ('small_amount', 2179),\n",
       " ('fast_shipping', 2164),\n",
       " ('whole_food', 2146),\n",
       " ('blood_test', 2113),\n",
       " ('expiration_date', 2030),\n",
       " ('fat_burner', 2026),\n",
       " ('huge_difference', 1994),\n",
       " ('protein_powder', 1952),\n",
       " ('acid_reflux', 1902),\n",
       " ('negative_side_effect', 1883),\n",
       " ('someone_who', 1869),\n",
       " ('pre_workout', 1850),\n",
       " ('garcinia_cambogia', 1785),\n",
       " ('prenatal_vitamin', 1750),\n",
       " ('bowel_movement', 1713),\n",
       " ('daily_basis', 1702),\n",
       " ('reasonable_price', 1694),\n",
       " ('upset_stomach', 1691),\n",
       " ('coconut_oil', 1646),\n",
       " ('regular_basis', 1610),\n",
       " ('placebo_effect', 1591),\n",
       " ('belly_fat', 1583),\n",
       " ('soft_gel', 1548),\n",
       " ('b_vitamin', 1540),\n",
       " ('local_health', 1534),\n",
       " ('orange_juice', 1526),\n",
       " ('dietary_supplement', 1478),\n",
       " ('high_blood_pressure', 1464),\n",
       " ('hair_growth', 1386),\n",
       " ('love_it!.', 1385),\n",
       " ('local_store', 1346),\n",
       " ('meal_replacement', 1334),\n",
       " ('digestive_issue', 1316),\n",
       " ('family_member', 1311),\n",
       " ('loose_weight', 1269),\n",
       " ('sore_throat', 1260),\n",
       " ('horse_pill', 1257),\n",
       " ('free_sample', 1251),\n",
       " ('peanut_butter', 1248),\n",
       " ('hair_loss', 1248),\n",
       " ('amino_acid', 1246),\n",
       " ('green_coffee_bean', 1226),\n",
       " ('eating_habit', 1225),\n",
       " ('side_affect', 1222),\n",
       " ('junk_food', 1212),\n",
       " ('vitamin_d3', 1211),\n",
       " ('almond_milk', 1179),\n",
       " ('low_carb', 1172),\n",
       " ('free_shipping', 1163),\n",
       " ('stomach_ache', 1154),\n",
       " ('leg_cramp', 1152),\n",
       " ('real_deal', 1151),\n",
       " ('liquid_form', 1144),\n",
       " ('bottom_line', 1135),\n",
       " ('olive_oil', 1123),\n",
       " ('fish_burps', 1109),\n",
       " ('prescription_drug', 1106),\n",
       " ('fast_delivery', 1097),\n",
       " ('appetite_suppression', 1084),\n",
       " ('sinus_infection', 1051),\n",
       " ('green_coffee', 1046),\n",
       " ('folic_acid', 1035),\n",
       " ('daily_routine', 1034),\n",
       " ('fat_loss', 1029),\n",
       " ('allergic_reaction', 1020),\n",
       " ('yeast_infection', 1018),\n",
       " ('fishy_aftertaste', 1004),\n",
       " ('protein_shake', 996),\n",
       " ('dry_eye', 973)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq_updated = Counter(gramlist_updated)\n",
    "paired_words_frq_updated.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46785"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq_updated)   # final number of cleaned-up paired words in the specified phrase format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Clean-up: Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "\n",
       "                                         unigram_pos  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]   \n",
       "\n",
       "                                 preprocessed_review  has_paired_words  \n",
       "0                              [dpe, the, job, well]                 0  \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]                 0  \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...                 0  \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...                 1  \n",
       "4         [good, product, good, price, good, result]                 0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599286, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_review_final = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in preprocessed_review_updated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>has_paired_words</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dpe, the, job, well]</td>\n",
       "      <td>[NOUN, DET, NOUN, ADV]</td>\n",
       "      <td>0</td>\n",
       "      <td>[dpe, job]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[b, flax, d, be, a, regular, at, -PRON-, house]</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>[b, flax, d, regular, -PRON-, house]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-PRON-, do, -PRON-, job, simply, and, with, g...</td>\n",
       "      <td>[PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-PRON-, -PRON-, job, simply, good, result]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-PRON-, be, reasonable, last, a, long, time, ...</td>\n",
       "      <td>[PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, reasonable, long, time, able, obtain,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "      <td>[ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, product, good, price, good, result]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                              [dpe, the, job, well]   \n",
       "1    [b, flax, d, be, a, regular, at, -PRON-, house]   \n",
       "2  [-PRON-, do, -PRON-, job, simply, and, with, g...   \n",
       "3  [-PRON-, be, reasonable, last, a, long, time, ...   \n",
       "4         [good, product, good, price, good, result]   \n",
       "\n",
       "                                         unigram_pos  has_paired_words  \\\n",
       "0                             [NOUN, DET, NOUN, ADV]                 0   \n",
       "1  [NOUN, NOUN, NOUN, VERB, DET, ADJ, ADP, ADJ, N...                 0   \n",
       "2  [PRON, VERB, ADJ, NOUN, ADV, CCONJ, ADP, ADJ, ...                 0   \n",
       "3  [PRON, VERB, ADJ, VERB, DET, ADJ, NOUN, CCONJ,...                 1   \n",
       "4                  [ADJ, NOUN, ADJ, NOUN, ADJ, NOUN]                 0   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0                                         [dpe, job]  \n",
       "1               [b, flax, d, regular, -PRON-, house]  \n",
       "2        [-PRON-, -PRON-, job, simply, good, result]  \n",
       "3  [-PRON-, reasonable, long, time, able, obtain,...  \n",
       "4         [good, product, good, price, good, result]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.drop(['preprocessed_review'], axis=1, inplace=True)\n",
    "unigram_sents_pos_df['preprocessed_review'] = preprocessed_review_final\n",
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dpe', 'job'],\n",
       " ['b', 'flax', 'd', 'regular', '-PRON-', 'house'],\n",
       " ['-PRON-', '-PRON-', 'job', 'simply', 'good', 'result']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.6 s, sys: 600 ms, total: 32.2 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to learn the full vocabulary of the corpus to be modeled\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "vocab_dictionary = Dictionary(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "vocab_dictionary.filter_extremes(no_below=1000, no_above=0.6)\n",
    "vocab_dictionary.compactify()   # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_s3(vocab_dictionary, bucket_name, filepath='amazon_reviews/vocab_dictionary.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dictionary = Dictionary.load('../vocab_dictionary.dict')  # load the finished dictionary from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = []    # bag-of-words representation of the corpus\n",
    "for review in tokenized_reviews:\n",
    "    bow_corpus.append(vocab_dictionary.doc2bow(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(0, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(6, 3), (7, 1), (14, 1), (15, 1)],\n",
       " [(6, 1), (16, 1)],\n",
       " [(17, 2), (18, 1), (19, 1), (20, 1), (21, 1)],\n",
       " [(22, 1), (23, 1), (24, 1)],\n",
       " [(6, 1), (16, 1), (22, 1), (25, 1), (26, 1)],\n",
       " [(15, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
