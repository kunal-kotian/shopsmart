{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In v3, higher level of filtering of high frequency words is used for constructing the vocab.  Low frequency filtering relaxed from min. 10 docs to 5 docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:51:41.385778Z",
     "start_time": "2018-05-01T22:51:38.252666Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:52:55.980127Z",
     "start_time": "2018-05-01T22:52:53.470300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from six import iteritems\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm, tqdm_notebook, tnrange\n",
    "from S3_read_write import load_df_s3, save_df_s3\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:52:57.210755Z",
     "start_time": "2018-05-01T22:52:57.161488Z"
    }
   },
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas('Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:52:57.633312Z",
     "start_time": "2018-05-01T22:52:57.583413Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = 'amazon-reviews-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start off using only the title (`summary`) and body (`reviewText`) of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 1.94 s, total: 4.05 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = load_df_s3(bucket_name, filepath='amazon_reviews/data_clean_v3', filetype='feather')\n",
    "\n",
    "# df = load_df_s3(bucket_name, filepath='amazon_reviews/reviews_data_clean_v2.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                 int64\n",
       "asin                 object\n",
       "helpful              object\n",
       "reviewText           object\n",
       "overall             float64\n",
       "summary              object\n",
       "description          object\n",
       "title                object\n",
       "categories_clean     object\n",
       "cat1                 object\n",
       "cat2                 object\n",
       "cat3                 object\n",
       "cat4                 object\n",
       "cat5                 object\n",
       "cat6                 object\n",
       "cat7                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[:, ['asin', 'reviewText', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217530, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I started taking this after both my parents di...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I really liked this product because it stayed ...</td>\n",
       "      <td>I can't find this product any longer, and I wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Resveratrol is a polar compound, very insolubl...</td>\n",
       "      <td>Just the Resveratrol product we need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>I bought several of these bracelets for my YMC...</td>\n",
       "      <td>The kids love these bracelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>I bought a few the other week just to see what...</td>\n",
       "      <td>Pleasant Surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                         reviewText  \\\n",
       "0  0978559088  I started taking this after both my parents di...   \n",
       "1  0978559088  I really liked this product because it stayed ...   \n",
       "2  0978559088  Resveratrol is a polar compound, very insolubl...   \n",
       "3  1427600228  I bought several of these bracelets for my YMC...   \n",
       "4  1427600228  I bought a few the other week just to see what...   \n",
       "\n",
       "                                             summary  \n",
       "0                         Bioavailability is the key  \n",
       "1  I can't find this product any longer, and I wi...  \n",
       "2               Just the Resveratrol product we need  \n",
       "3                      The kids love these bracelets  \n",
       "4                                  Pleasant Surprise  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each review, concatenate the review title and body\n",
    "df.reviewText = df.summary + '. ' + df.reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "      <td>Bioavailability is the key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...</td>\n",
       "      <td>Just the Resveratrol product we need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...</td>\n",
       "      <td>The kids love these bracelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...</td>\n",
       "      <td>Pleasant Surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0978559088   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  1427600228   \n",
       "4  1427600228   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \\\n",
       "0  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...   \n",
       "1  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...   \n",
       "2  Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...   \n",
       "3  The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...   \n",
       "4  Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...   \n",
       "\n",
       "                                                     summary  \n",
       "0                                 Bioavailability is the key  \n",
       "1  I can't find this product any longer, and I wish I could.  \n",
       "2                       Just the Resveratrol product we need  \n",
       "3                              The kids love these bracelets  \n",
       "4                                          Pleasant Surprise  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the `summary` column now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0978559088</td>\n",
       "      <td>Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1427600228</td>\n",
       "      <td>Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  0978559088   \n",
       "1  0978559088   \n",
       "2  0978559088   \n",
       "3  1427600228   \n",
       "4  1427600228   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \n",
       "0  Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspirin...  \n",
       "1  I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of...  \n",
       "2  Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in...  \n",
       "3  The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds conf...  \n",
       "4  Pleasant Surprise. I bought a few the other week just to see what they're all about.  The first day I wore one of the bracelets three people asked about it.  They liked the look.  I told them it w...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.isnull().sum()    # 73 reviews have neither a review body text, nor a review title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few actual review texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great product. A friend told me about this and I love it. It works just like it says. Highly recommend this product.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes! It works fast and lowers your BP withi a month.. It works...it works...lowered my blood pressure by 30 points. I take 2 pills in the AM and 2 in the afternoon. My BP was high and now it is borderline.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love it!. I am very sensitive to most things. I can't drink coffee, I can't drink soda, I can't drink most green teas. However with this, I can have just one teaspoon of the Matcha Green Tea with water and I feel a gradual alertness that stays for quite some time with no crashing. I also find it curbs my appetite. Love it!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.iloc[np.random.randint(0, len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217530 entries, 0 to 217529\n",
      "Data columns (total 2 columns):\n",
      "asin          217530 non-null object\n",
      "reviewText    217530 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = list(df.reviewText.values)    # make an iterable to store only the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent for sent in text if len(sent) == 0]   # there are no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioavailability is the key. I started taking this after both my parents died of cancer as it supposed to enhance your immune system - the story on 60 Minutes on resveratrol was incredibly inspiring. Doing some research on the Internet, it is indicated that taking resveratrol in lozenge form is preferable as it is broken down by stomach acids.  The ez-melt formula recommended in another review is OK, but it is dissolved in the mouth much more quickly than this lozenge formula, while dissolving more slowly is preferable according to my research.This product has the greatest side effect - since taking it, I haven't had colds or sore throats.  Soon after starting to take it every day, I was starting to come down with a cold, with all my usual symptoms, and was anticipating being very sick the next day, as is my usual pattern.  But I never did get as sick as anticipated - taking this product is the only reason I can come up with.  Since then, I've had no colds or sore throats - it has been great.  I recommend this product to everyone I know, and have given it as gifts to my family. \n",
      "\n",
      "I can't find this product any longer, and I wish I could.. I really liked this product because it stayed in my mouth for a long time and I felt it was probably doing some good.  I take a number of resveratrol products and this was one of my favorites.  I can't find it anymore and wish I could still find it.When I contacted the company, they said there was no date for re-introduction of this product, and they implied that it would no longer be produced.  I also found someplace online where the product manufacturer was trying to find someone to market it for them.  I hope this happens.  I really enjoyed taking these little pills and felt they were doing some good.  The claims on their website made sense, and I believe that resveratrol is almost a wonder drug/suppliment.  (The 60 Minutes Episode convinced me of that.) \n",
      "\n",
      "Just the Resveratrol product we need. Resveratrol is a polar compound, very insoluble in water and hence saliva.  To get sufficient Resveratrol absorbed a Resveratrol lozenge would have to stay in the mouth and be slowly dissolved.  The lozenge would have to contain enough Resveratrol since most of the Resveratrol will be swallowed with the saliva.  A better known and competing product contains only about a milligram of Resveratrol.  These lozenges contain 20 milligrams.  Nutrihill Resveratrol lozenges dissolve very slowly and contain, IMO, just the right amount of Resveratrol to optimize absorption.  Most of the product will be swallowed but that's OK as just fractions of a milligram need to be absorbed though the mouth's mucosa to produce sustained high levels of Resveratrol in the blood and be absorbed into the cells to do some real good.  I can see where many producers of Resveratrol products like, for example Revgenetics, would not want this product to catch on as their selling point is how well they try to increase absorption of Resveratrol in the digestive system.  Taking Quercetin helps protect Resveratrol when swallowed from first pass liver metabolism, but at an additional financial cost which makes suppliers happy because they can sell more supplements. An additional cost of taking Quercetin with Resveratrol is joint inflammation.I hope this product catches on so it's available from many sellers.Nutrihill Resveratrol Lozenges \n",
      "\n",
      "The kids love these bracelets. I bought several of these bracelets for my YMCA kids. Everyone tells me that it brought good luck. Placebo effect, perhaps but it's a positive effect and builds confidence in kids/teens. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at a few sample reviews\n",
    "for rev in text[:4]:\n",
    "    print(rev, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below are from:\n",
    "\n",
    "http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `gensim`'s `Phrases` class to detect natural combinations of words (like 'vanilla ice cream'), we need to format our text into a list of sentences, with each sentence being a list of words.  This process takes a large amount of processing time (for reference, the times shown under the cells are for running the tasks on a c5.18xlarge EC2 instance (equivalent spot fleet)), so `text` has been split into 3 parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Unigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split text into 9 parts\n",
    "text_first  = text[:50000]\n",
    "text_second = text[50000:100000]\n",
    "text_third  = text[100000:150000]\n",
    "text_fourth = text[150000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:56, 120.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  50000\n",
      "current sent_num:  289565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rev_num = 0    # review tracker\n",
    "sent_num = 0   # sentence tracker\n",
    "unigram_sents_pos = [] # to store lists of lemmatized tokens for each sentence\n",
    "\n",
    "for parsed_review in tqdm(nlp.pipe(text_first, batch_size=10000, n_threads=72)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289158"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_sents_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, [('bioavailability', 'PROPN'), ('be', 'VERB'), ('the', 'DET'), ('key', 'NOUN')]]\n",
      "[1, 2, [('-PRON-', 'PRON'), ('start', 'VERB'), ('take', 'VERB'), ('this', 'DET'), ('after', 'ADP'), ('both', 'CCONJ'), ('-PRON-', 'ADJ'), ('parent', 'NOUN'), ('die', 'VERB'), ('of', 'ADP'), ('cancer', 'NOUN'), ('as', 'ADP'), ('-PRON-', 'PRON'), ('suppose', 'VERB'), ('to', 'PART'), ('enhance', 'VERB'), ('-PRON-', 'ADJ'), ('immune', 'ADJ'), ('system', 'NOUN'), ('the', 'DET'), ('story', 'NOUN'), ('on', 'ADP'), ('60', 'NUM'), ('minutes', 'PROPN'), ('on', 'ADP'), ('resveratrol', 'NOUN'), ('be', 'VERB'), ('incredibly', 'ADV'), ('inspiring', 'ADJ')]]\n",
      "[1, 3, [('do', 'VERB'), ('some', 'DET'), ('research', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('internet', 'NOUN'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('indicate', 'VERB'), ('that', 'ADP'), ('take', 'VERB'), ('resveratrol', 'NOUN'), ('in', 'ADP'), ('lozenge', 'NOUN'), ('form', 'NOUN'), ('be', 'VERB'), ('preferable', 'ADJ'), ('as', 'ADP'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('break', 'VERB'), ('down', 'PART'), ('by', 'ADP'), ('stomach', 'NOUN'), ('acid', 'NOUN')]]\n",
      "[1, 4, [('the', 'DET'), ('ez', 'ADP'), ('melt', 'NOUN'), ('formula', 'NOUN'), ('recommend', 'VERB'), ('in', 'ADP'), ('another', 'DET'), ('review', 'NOUN'), ('be', 'VERB'), ('ok', 'ADJ'), ('but', 'CCONJ'), ('-PRON-', 'PRON'), ('be', 'VERB'), ('dissolve', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('mouth', 'NOUN'), ('much', 'ADV'), ('more', 'ADV'), ('quickly', 'ADV'), ('than', 'ADP'), ('this', 'DET'), ('lozenge', 'NOUN'), ('formula', 'NOUN'), ('while', 'ADP'), ('dissolve', 'VERB'), ('more', 'ADV'), ('slowly', 'ADV'), ('be', 'VERB'), ('preferable', 'ADJ'), ('accord', 'VERB'), ('to', 'ADP'), ('-PRON-', 'ADJ'), ('research', 'NOUN')]]\n",
      "[1, 5, [('this', 'DET'), ('product', 'NOUN'), ('have', 'VERB'), ('the', 'DET'), ('great', 'ADJ'), ('side', 'NOUN'), ('effect', 'NOUN'), ('since', 'ADP'), ('take', 'VERB'), ('-PRON-', 'PRON'), ('-PRON-', 'PRON'), ('have', 'VERB'), ('not', 'ADV'), ('have', 'VERB'), ('cold', 'NOUN'), ('or', 'CCONJ'), ('sore', 'ADJ'), ('throat', 'NOUN')]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(unigram_sents_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if there are any blank sentences\n",
    "for sent in unigram_sents_pos:\n",
    "    if len(sent[2]) == 0:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:55, 120.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  100000\n",
      "current sent_num:  576892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_second, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576034\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_sents_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [06:27, 129.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  150000\n",
      "current sent_num:  855567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_third, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67530it [09:13, 122.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current rev_num:  217530\n",
      "current sent_num:  1243596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for parsed_review in tqdm(nlp.pipe(text_fourth, batch_size=20000, n_threads=36)):\n",
    "    rev_num += 1\n",
    "    for sent in parsed_review.sents:\n",
    "        sent_num += 1\n",
    "        # lemmatize tokens & save corresponding pos tag after filtering whitespace and punctuations\n",
    "        lemmatized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(lemmatized_sent) != 0:\n",
    "            unigram_sents_pos.append([rev_num, sent_num, lemmatized_sent])\n",
    "\n",
    "print('current rev_num: ', rev_num)\n",
    "print('current sent_num: ', sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save progress...\n",
    "review_number = [row[0] for row in unigram_sents_pos]\n",
    "sentence_number = [row[1] for row in unigram_sents_pos]\n",
    "words_joined_all = []\n",
    "pos_joined_all = []\n",
    "for sent in unigram_sents_pos:\n",
    "    word_pos = sent[2]\n",
    "    word_list = [word for word, pos in word_pos]\n",
    "    pos_list = [pos for word, pos in word_pos]\n",
    "    words_joined = '+-+||+-+'.join(word for word in word_list)\n",
    "    pos_joined   = '+-+||+-+'.join(pos for pos in pos_list)\n",
    "    words_joined_all.append(words_joined)\n",
    "    pos_joined_all.append(pos_joined)\n",
    "    \n",
    "unigram_sentences_savedf = pd.DataFrame({'review_number': review_number,\n",
    "                                         'sentence_number': sentence_number,\n",
    "                                         'unigram_sentences': words_joined_all,\n",
    "                                         'unigram_pos': pos_joined_all})\n",
    "\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DON'T LOAD THIS FILE - there's a _v1 version further down!\n",
    "# del unigram_sentences_savedf\n",
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number          unigram_pos    unigram_sentences\n",
       "0              1                1  PROPN+-+||+-+VER...  bioavailability+...\n",
       "1              1                2  PRON+-+||+-+VERB...  -PRON-+-+||+-+st...\n",
       "2              1                3  VERB+-+||+-+DET+...  do+-+||+-+some+-...\n",
       "3              1                4  DET+-+||+-+ADP+-...  the+-+||+-+ez+-+...\n",
       "4              1                5  DET+-+||+-+NOUN+...  this+-+||+-+prod..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_number, sentence_number, unigram_pos, unigram_sentences]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].head()  # no blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up(sentence, sentence_pos, sep):\n",
    "    \"\"\"Expects a sentence as a single string as input 1, and its corresponding part-of-speech tags as input 2 (also single string).\n",
    "    sep is the string pattern used to separate words in each sentence string\n",
    "    Cleans it up and returns a single string.\n",
    "    Also updates corresponding part-of-speech string.\n",
    "    \"\"\"\n",
    "    # get rid of webpage links\n",
    "    cond = ['http' in sentence, 'www' in sentence]\n",
    "    if any(cond):\n",
    "        words = sentence.split(sep)\n",
    "        words_pos = sentence_pos.split(sep)\n",
    "        to_remove = []\n",
    "        for i in range(len(words)):\n",
    "            cond_word = ['http' in words[i], 'www' in words[i]]\n",
    "            if any(cond_word):\n",
    "                to_remove.append(i)\n",
    "        # remove words that are links\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del words[j]\n",
    "            del words_pos[j]\n",
    "        # reconstruct sentence after deleting links\n",
    "        sentence = sep.join(words)\n",
    "        sentence_pos = sep.join(words_pos)\n",
    "\n",
    "    # replace underscores with blanks to avoid mix-up with paired words later\n",
    "    # cannot replace with spaces because the strings are split on spaces later \n",
    "    # and this would create new words with no corresponding pos tags\n",
    "    if '_' in sentence:\n",
    "        sentence = sentence.replace('_', '')\n",
    "    return sentence, sentence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!',\n",
       " 'this is a normal sentence',\n",
       " '__ what is this ____ http',\n",
       " '_',\n",
       " 'http']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = ['whoa watch out for them links boy http://sup.com and also BAM! underscore_time!', 'this is a normal sentence', \n",
    "              '__ what is this ____ http', '_', 'http']\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clean_pos = ['X X X X X X X X X X X X', 'X X X X X', 'X X X X X X', 'X', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 5, 6, 1, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoa watch out for them links boy and also BAM! underscoretime!',\n",
       " 'this is a normal sentence',\n",
       " ' what is this ']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if clean_up works as expected\n",
    "to_remove = []\n",
    "for i in range(len(test_clean)):\n",
    "    sentence = test_clean[i]\n",
    "    sentence_pos = test_clean_pos[i]\n",
    "    test_clean[i], test_clean_pos[i] = clean_up(sentence, sentence_pos, sep=' ')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if test_clean[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del test_clean[j]\n",
    "    del test_clean_pos[j]\n",
    "\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X X X X X X X X X X X', 'X X X X X', 'X X X X X']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.count('X') for e in test_clean_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e.split(' ')) for e in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_joined_all = unigram_sentences_savedf.unigram_pos.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241850"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if '_' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'http' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([sentence for sentence in words_joined_all if 'www' in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>unigram_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>607</td>\n",
       "      <td>3386</td>\n",
       "      <td>X</td>\n",
       "      <td>http://www.amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8051</th>\n",
       "      <td>1454</td>\n",
       "      <td>8071</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>no+-+||+-+jet_la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12199</th>\n",
       "      <td>2166</td>\n",
       "      <td>12224</td>\n",
       "      <td>ADJ+-+||+-+PART+...</td>\n",
       "      <td>easy+-+||+-+to+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12201</th>\n",
       "      <td>2166</td>\n",
       "      <td>12226</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18188</th>\n",
       "      <td>3106</td>\n",
       "      <td>18214</td>\n",
       "      <td>ADV+-+||+-+ADJ+-...</td>\n",
       "      <td>overall+-+||+-+-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_number  sentence_number          unigram_pos  \\\n",
       "3375             607             3386                    X   \n",
       "8051            1454             8071  DET+-+||+-+NOUN+...   \n",
       "12199           2166            12224  ADJ+-+||+-+PART+...   \n",
       "12201           2166            12226  PRON+-+||+-+VERB...   \n",
       "18188           3106            18214  ADV+-+||+-+ADJ+-...   \n",
       "\n",
       "         unigram_sentences  \n",
       "3375   http://www.amazo...  \n",
       "8051   no+-+||+-+jet_la...  \n",
       "12199  easy+-+||+-+to+-...  \n",
       "12201  -PRON-+-+||+-+ha...  \n",
       "18188  overall+-+||+-+-...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf[unigram_sentences_savedf.unigram_sentences.str.contains('_')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.amazon.com/gp/product/b0000533z8/ref=cm_cr_rev_prod_title',\n",
       " 'no+-+||+-+jet_lag+-+||+-+pill',\n",
       " 'easy+-+||+-+to+-+||+-+use_work+-+||+-+well',\n",
       " '-PRON-+-+||+-+have+-+||+-+have+-+||+-+pedometer+-+||+-+in+-+||+-+the+-+||+-+past_all+-+||+-+difficult+-+||+-+and+-+||+-+confusing+-+||+-+to+-+||+-+use+-+||+-+to+-+||+-+the+-+||+-+point+-+||+-+-PRON-+-+||+-+simply+-+||+-+give+-+||+-+up+-+||+-+on+-+||+-+-PRON-',\n",
       " 'overall+-+||+-+-PRON-+-+||+-+mother+-+||+-+be+-+||+-+very+-+||+-+satisfied+-+||+-+with+-+||+-+this+-+||+-+product!-d_lionz',\n",
       " 'this+-+||+-+inexpensive+-+||+-+strap+-+||+-+with+-+||+-+a+-+||+-+metal+-+||+-+clip+-+||+-+http://www.amazon.com/gp/product/b000bitymg/ref=oh_details_o00_s00_i00?ie=utf8&psc;=1+-+||+-+be+-+||+-+a+-+||+-+good+-+||+-+replacement+-+||+-+for+-+||+-+the+-+||+-+flimsy+-+||+-+omron+-+||+-+plastic+-+||+-+clip+-+||+-+but+-+||+-+-PRON-+-+||+-+have+-+||+-+not+-+||+-+be+-+||+-+use+-+||+-+-PRON-+-+||+-+long',\n",
       " 'hj_112+-+||+-+digital+-+||+-+pemium+-+||+-+pedometer+-+||+-+update',\n",
       " 'at+-+||+-+the+-+||+-+end+-+||+-+of+-+||+-+the+-+||+-+2+-+||+-+week+-+||+-+i+-+||+-+get+-+||+-+retest+-+||+-+-PRON-+-+||+-+heamoglobin+-+||+-+be+-+||+-+a+-+||+-+14.2+-+||+-+and+-+||+-+hematocrit+-+||+-+be+-+||+-+a+-+||+-+wopp+-+||+-+44+-+||+-+o_o.',\n",
       " 'and+-+||+-+-PRON-+-+||+-+easy+-+||+-+to+-+||+-+cover+-+||+-+-PRON-+-+||+-+up+-+||+-+with+-+||+-+a+-+||+-+little+-+||+-+orange_pineapple+-+||+-+juice',\n",
       " 'this+-+||+-+work+-+||+-+excellent+-+||+-+for+-+||+-+calm+-+||+-+the+-+||+-+nerve+-+||+-+and+-+||+-+to+-+||+-+ease+-+||+-+stress+-+||+-+^_^']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence for sentence in words_joined_all if '_' in sentence][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up all unigrams\n",
    "to_remove = []\n",
    "for i in range(len(words_joined_all)):\n",
    "    sentence = words_joined_all[i]\n",
    "    sentence_pos = pos_joined_all[i]\n",
    "    words_joined_all[i], pos_joined_all[i] = clean_up(sentence, sentence_pos, sep='+-+||+-+')\n",
    "    \n",
    "    # mark elements to delete if empty\n",
    "    if words_joined_all[i] == '':\n",
    "        to_remove.append(i)\n",
    "\n",
    "# delete elements that are empty\n",
    "for j in sorted(to_remove, reverse=True):\n",
    "    del words_joined_all[j]\n",
    "    del pos_joined_all[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows from unigram_sentences_savedf corresponding to the row numbers (indices) of sentences\n",
    "# that will be blank after the transformation above\n",
    "unigram_sentences_savedf.drop(unigram_sentences_savedf.index[to_remove], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf.drop(['unigram_sentences'], axis=1, inplace=True)\n",
    "unigram_sentences_savedf.drop(['unigram_pos'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf['unigram_sentences'] = words_joined_all\n",
    "unigram_sentences_savedf['unigram_pos'] = pos_joined_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...\n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...\n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...\n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...\n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentences_savedf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updated, cleaned up version of unigram_sentences.feather\n",
    "save_df_s3(unigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_joined_all = unigram_sentences_savedf.unigram_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = [sentence.split('+-+||+-+') for sentence in words_joined_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bioavailability', 'be', 'the', 'key'], ['-PRON-', 'start', 'take', 'this', 'after', 'both', '-PRON-', 'parent', 'die', 'of', 'cancer', 'as', '-PRON-', 'suppose', 'to', 'enhance', '-PRON-', 'immune', 'system', 'the', 'story', 'on', '60', 'minutes', 'on', 'resveratrol', 'be', 'incredibly', 'inspiring'], ['do', 'some', 'research', 'on', 'the', 'internet', '-PRON-', 'be', 'indicate', 'that', 'take', 'resveratrol', 'in', 'lozenge', 'form', 'be', 'preferable', 'as', '-PRON-', 'be', 'break', 'down', 'by', 'stomach', 'acid'], ['the', 'ez', 'melt', 'formula', 'recommend', 'in', 'another', 'review', 'be', 'ok', 'but', '-PRON-', 'be', 'dissolve', 'in', 'the', 'mouth', 'much', 'more', 'quickly', 'than', 'this', 'lozenge', 'formula', 'while', 'dissolve', 'more', 'slowly', 'be', 'preferable', 'accord', 'to', '-PRON-', 'research']]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sentences[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241826"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_joined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 1.12 s, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The common_terms parameter add a way to give special treatment to common terms \n",
    "# (aka stop words) such that their presence between two words won’t prevent bigram detection. \n",
    "# It allows to detect expressions like “bank of america”\n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\"]\n",
    "\n",
    "# Train a first-order phrase detector\n",
    "bigram_model = Phrases(unigram_sentences, threshold=0.6, scoring='npmi', common_terms=common_terms)\n",
    "\n",
    "# Transform unigram sentences into bigram sentences\n",
    "# Paired words are connected by an underscore, e.g. ice_cream\n",
    "bigram_sentences = []\n",
    "for sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 1.29 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a second-order phrase detector\n",
    "# trigram_model = Phrases(bigram_sentences, min_count=5)\n",
    "trigram_model = Phrases(bigram_sentences, threshold=0.5, scoring='npmi')\n",
    "\n",
    "# Transform bigram sentences into trigram sentences\n",
    "trigram_sentences = []\n",
    "for sentence in bigram_sentences:\n",
    "    trigram_sentences.append(trigram_model[sentence])\n",
    "\n",
    "# remove any remaining stopwords\n",
    "# trigram_sentences = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in trigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the trigrams will be saved in a dataframe with a single column.\n",
    "# each row is one sentence from any review\n",
    "# each sentence is a single string separated by a single space.\n",
    "trigram_sentences_savedf = pd.DataFrame([u'+-+||+-+'.join(sentence) for sentence in trigram_sentences], columns=['preprocessed_review'])\n",
    "save_df_s3(trigram_sentences_savedf, bucket_name, 'amazon_reviews/kk/preprocessed_reviews.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_savedf = load_df_s3(bucket_name, 'amazon_reviews/kk/preprocessed_reviews.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this_product+-+|...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preprocessed_review\n",
       "0  bioavailability+...\n",
       "1  -PRON-+-+||+-+st...\n",
       "2  do+-+||+-+some+-...\n",
       "3  the+-+||+-+ez+-+...\n",
       "4  this_product+-+|..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sentences_savedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del unigram_sentences_savedf\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/kk/unigram_sentences_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...\n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...\n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...\n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...\n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df[unigram_sents_pos_df.unigram_pos == ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = pd.merge(unigram_sents_pos_df, trigram_sentences_savedf, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this_product+-+|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>soon+-+||+-+afte...</td>\n",
       "      <td>ADV+-+||+-+ADP+-...</td>\n",
       "      <td>soon+-+||+-+afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "      <td>CCONJ+-+||+-+PRO...</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>since+-+||+-+the...</td>\n",
       "      <td>ADP+-+||+-+ADV+-...</td>\n",
       "      <td>since+-+||+-+the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-PRON-+-+||+-+re...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...   \n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...   \n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...   \n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...   \n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+...   \n",
       "5              1                6  soon+-+||+-+afte...  ADV+-+||+-+ADP+-...   \n",
       "6              1                7  but+-+||+-+-PRON...  CCONJ+-+||+-+PRO...   \n",
       "7              1                8  since+-+||+-+the...  ADP+-+||+-+ADV+-...   \n",
       "8              1                9  -PRON-+-+||+-+re...  PRON+-+||+-+VERB...   \n",
       "9              2               10  -PRON-+-+||+-+ca...  PRON+-+||+-+VERB...   \n",
       "\n",
       "   preprocessed_review  \n",
       "0  bioavailability+...  \n",
       "1  -PRON-+-+||+-+st...  \n",
       "2  do+-+||+-+some+-...  \n",
       "3  the+-+||+-+ez+-+...  \n",
       "4  this_product+-+|...  \n",
       "5  soon+-+||+-+afte...  \n",
       "6  but+-+||+-+-PRON...  \n",
       "7  since+-+||+-+the...  \n",
       "8  -PRON-+-+||+-+re...  \n",
       "9  -PRON-+-+||+-+ca...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(unigram_sents_pos_df, bucket_name, 'amazon_reviews/kk/preprocessed_reviews_v1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df = load_df_s3(bucket_name, 'amazon_reviews/kk/preprocessed_reviews_v1.feather', filetype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>73</td>\n",
       "      <td>401</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>73</td>\n",
       "      <td>402</td>\n",
       "      <td>so+-+||+-+just+-...</td>\n",
       "      <td>ADV+-+||+-+ADV+-...</td>\n",
       "      <td>so+-+||+-+just+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>73</td>\n",
       "      <td>403</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>73</td>\n",
       "      <td>404</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>73</td>\n",
       "      <td>405</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "      <td>CCONJ+-+||+-+PRO...</td>\n",
       "      <td>but+-+||+-+-PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>74</td>\n",
       "      <td>406</td>\n",
       "      <td>excellent+-+||+-...</td>\n",
       "      <td>INTJ+-+||+-+PRON...</td>\n",
       "      <td>excellent+-+||+-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>74</td>\n",
       "      <td>407</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>74</td>\n",
       "      <td>408</td>\n",
       "      <td>-PRON-+-+||+-+ac...</td>\n",
       "      <td>PRON+-+||+-+ADV+...</td>\n",
       "      <td>-PRON-+-+||+-+ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>74</td>\n",
       "      <td>409</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>74</td>\n",
       "      <td>410</td>\n",
       "      <td>one+-+||+-+pill+...</td>\n",
       "      <td>NUM+-+||+-+NOUN+...</td>\n",
       "      <td>one+-+||+-+pill+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             73              401  -PRON-+-+||+-+ha...  PRON+-+||+-+VERB...   \n",
       "401             73              402  so+-+||+-+just+-...  ADV+-+||+-+ADV+-...   \n",
       "402             73              403  -PRON-+-+||+-+ca...  PRON+-+||+-+VERB...   \n",
       "403             73              404  -PRON-+-+||+-+do...  PRON+-+||+-+VERB...   \n",
       "404             73              405  but+-+||+-+-PRON...  CCONJ+-+||+-+PRO...   \n",
       "405             74              406  excellent+-+||+-...  INTJ+-+||+-+PRON...   \n",
       "406             74              407  -PRON-+-+||+-+ha...  PRON+-+||+-+VERB...   \n",
       "407             74              408  -PRON-+-+||+-+ac...  PRON+-+||+-+ADV+...   \n",
       "408             74              409  -PRON-+-+||+-+be...  PRON+-+||+-+VERB...   \n",
       "409             74              410  one+-+||+-+pill+...  NUM+-+||+-+NOUN+...   \n",
       "\n",
       "     preprocessed_review  \n",
       "400  -PRON-+-+||+-+ha...  \n",
       "401  so+-+||+-+just+-...  \n",
       "402  -PRON-+-+||+-+ca...  \n",
       "403  -PRON-+-+||+-+do...  \n",
       "404  but+-+||+-+-PRON...  \n",
       "405  excellent+-+||+-...  \n",
       "406  -PRON-+-+||+-+ha...  \n",
       "407  -PRON-+-+||+-+ac...  \n",
       "408  -PRON-+-+||+-+be...  \n",
       "409  one+-+||+-+pill+...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df['has_paired_words'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df.loc[unigram_sents_pos_df.preprocessed_review.str.contains('_'), ['has_paired_words']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532356"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.has_paired_words.sum()  # number of sentences with paired words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>PROPN+-+||+-+VER...</td>\n",
       "      <td>bioavailability+...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>PRON+-+||+-+VERB...</td>\n",
       "      <td>-PRON-+-+||+-+st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>VERB+-+||+-+DET+...</td>\n",
       "      <td>do+-+||+-+some+-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>DET+-+||+-+ADP+-...</td>\n",
       "      <td>the+-+||+-+ez+-+...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>this+-+||+-+prod...</td>\n",
       "      <td>DET+-+||+-+NOUN+...</td>\n",
       "      <td>this_product+-+|...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  bioavailability+...  PROPN+-+||+-+VER...   \n",
       "1              1                2  -PRON-+-+||+-+st...  PRON+-+||+-+VERB...   \n",
       "2              1                3  do+-+||+-+some+-...  VERB+-+||+-+DET+...   \n",
       "3              1                4  the+-+||+-+ez+-+...  DET+-+||+-+ADP+-...   \n",
       "4              1                5  this+-+||+-+prod...  DET+-+||+-+NOUN+...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  bioavailability+...                 0  \n",
       "1  -PRON-+-+||+-+st...                 1  \n",
       "2  do+-+||+-+some+-...                 1  \n",
       "3  the+-+||+-+ez+-+...                 0  \n",
       "4  this_product+-+|...                 1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 2.31 s, total: 15.8 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigram_sents_pos_df.unigram_pos = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_pos.tolist()]\n",
    "unigram_sents_pos_df.unigram_sentences = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.unigram_sentences.tolist()]\n",
    "unigram_sents_pos_df.preprocessed_review = [sent.split('+-+||+-+') for sent in unigram_sents_pos_df.preprocessed_review.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>73</td>\n",
       "      <td>401</td>\n",
       "      <td>[-PRON-, have, u...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, have, u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>73</td>\n",
       "      <td>402</td>\n",
       "      <td>[so, just, take,...</td>\n",
       "      <td>[ADV, ADV, VERB,...</td>\n",
       "      <td>[so, just, take,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>73</td>\n",
       "      <td>403</td>\n",
       "      <td>[-PRON-, can, no...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, can_not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>73</td>\n",
       "      <td>404</td>\n",
       "      <td>[-PRON-, do, not...</td>\n",
       "      <td>[PRON, VERB, ADV...</td>\n",
       "      <td>[-PRON-, do_not,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>73</td>\n",
       "      <td>405</td>\n",
       "      <td>[but, -PRON-, be...</td>\n",
       "      <td>[CCONJ, PRON, VE...</td>\n",
       "      <td>[but, -PRON-, be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>74</td>\n",
       "      <td>406</td>\n",
       "      <td>[excellent, prod...</td>\n",
       "      <td>[INTJ, PRON, ADJ...</td>\n",
       "      <td>[excellent, prod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>74</td>\n",
       "      <td>407</td>\n",
       "      <td>[-PRON-, have, t...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, have, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>74</td>\n",
       "      <td>408</td>\n",
       "      <td>[-PRON-, actuall...</td>\n",
       "      <td>[PRON, ADV, VERB...</td>\n",
       "      <td>[-PRON-, actuall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>74</td>\n",
       "      <td>409</td>\n",
       "      <td>[-PRON-, be, con...</td>\n",
       "      <td>[PRON, VERB, ADJ...</td>\n",
       "      <td>[-PRON-, be, con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>74</td>\n",
       "      <td>410</td>\n",
       "      <td>[one, pill, seem...</td>\n",
       "      <td>[NUM, NOUN, VERB...</td>\n",
       "      <td>[one, pill, seem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "400             73              401  [-PRON-, have, u...  [PRON, VERB, VER...   \n",
       "401             73              402  [so, just, take,...  [ADV, ADV, VERB,...   \n",
       "402             73              403  [-PRON-, can, no...  [PRON, VERB, ADV...   \n",
       "403             73              404  [-PRON-, do, not...  [PRON, VERB, ADV...   \n",
       "404             73              405  [but, -PRON-, be...  [CCONJ, PRON, VE...   \n",
       "405             74              406  [excellent, prod...  [INTJ, PRON, ADJ...   \n",
       "406             74              407  [-PRON-, have, t...  [PRON, VERB, VER...   \n",
       "407             74              408  [-PRON-, actuall...  [PRON, ADV, VERB...   \n",
       "408             74              409  [-PRON-, be, con...  [PRON, VERB, ADJ...   \n",
       "409             74              410  [one, pill, seem...  [NUM, NOUN, VERB...   \n",
       "\n",
       "     preprocessed_review  has_paired_words  \n",
       "400  [-PRON-, have, u...                 1  \n",
       "401  [so, just, take,...                 0  \n",
       "402  [-PRON-, can_not...                 1  \n",
       "403  [-PRON-, do_not,...                 1  \n",
       "404  [but, -PRON-, be...                 0  \n",
       "405  [excellent, prod...                 0  \n",
       "406  [-PRON-, have, t...                 1  \n",
       "407  [-PRON-, actuall...                 1  \n",
       "408  [-PRON-, be, con...                 0  \n",
       "409  [one, pill, seem...                 1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.iloc[400:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_number          0\n",
       "sentence_number        0\n",
       "unigram_sentences      0\n",
       "unigram_pos            0\n",
       "preprocessed_review    0\n",
       "has_paired_words       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an arbitrary sentence and it's transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'buy', 'nu', 'skin', 'product', 'about', '5', 'year', 'ago', 'from', 'a', 'beauty', 'salon', 'and', '-PRON-', 'love', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_sentences.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'VERB', 'PROPN', 'PROPN', 'NOUN', 'ADV', 'NUM', 'NOUN', 'ADV', 'ADP', 'DET', 'NOUN', 'NOUN', 'CCONJ', 'PRON', 'VERB', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.unigram_pos.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'buy', 'nu', 'skin', 'product', 'about', '5', 'year_ago', 'from', 'a', 'beauty', 'salon', 'and', '-PRON-', 'love', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(unigram_sents_pos_df.preprocessed_review.iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramlist = [word for sent in trigram_sentences for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this_product', 69494),\n",
       " ('do_not', 59653),\n",
       " ('seem_to', 14671),\n",
       " ('can_not', 14216),\n",
       " ('fish_oil', 12919),\n",
       " ('highly_recommend', 12415),\n",
       " ('as_well', 10388),\n",
       " ('lot_of', 9337),\n",
       " ('so_far', 6983),\n",
       " ('would_recommend', 6173),\n",
       " ('will_continue', 5383),\n",
       " ('every_day', 5290),\n",
       " ('side_effect', 5008),\n",
       " ('along_with', 4968),\n",
       " ('vitamin_d', 4601),\n",
       " ('at_least', 4305),\n",
       " ('per_day', 4262),\n",
       " ('high_quality', 4237),\n",
       " ('year_ago', 4147),\n",
       " ('at_night', 3881),\n",
       " ('very_happy', 3827),\n",
       " ('vitamin_c', 3800),\n",
       " ('suffer_from', 3715),\n",
       " ('immune_system', 3494),\n",
       " ('five_star', 3471),\n",
       " ('omega_3', 3426),\n",
       " ('run_out', 3420),\n",
       " ('year_old', 3134),\n",
       " ('long_time', 3126),\n",
       " ('no_longer', 3036),\n",
       " ('krill_oil', 3001),\n",
       " ('make_sure', 2684),\n",
       " ('out_there', 2672),\n",
       " ('very_pleased', 2620),\n",
       " ('wake_up', 2582),\n",
       " ('blood_pressure', 2492),\n",
       " ('energy_level', 2438),\n",
       " ('no_side_effect', 2353),\n",
       " ('get_sick', 2260),\n",
       " ('anyone_who', 2208),\n",
       " ('every_morning', 2101),\n",
       " ('go_away', 2018),\n",
       " ('joint_pain', 1996),\n",
       " ('weight_loss', 1992),\n",
       " ('vitamin_d3', 1952),\n",
       " ('exactly_what', 1941),\n",
       " ('come_back', 1892),\n",
       " ('6_month', 1823),\n",
       " ('hot_flash', 1800),\n",
       " ('better_than', 1785),\n",
       " ('month_ago', 1685),\n",
       " ('by_far', 1668),\n",
       " ('get_rid', 1638),\n",
       " ('lose_weight', 1638),\n",
       " ('even_though', 1605),\n",
       " ('health_benefit', 1584),\n",
       " ('clear_up', 1508),\n",
       " ('blood_sugar', 1495),\n",
       " ('before_bed', 1494),\n",
       " ('digestive_system', 1416),\n",
       " ('no_fishy', 1322),\n",
       " ('very_satisfied', 1290),\n",
       " ('5_star', 1287),\n",
       " ('500_mg', 1263),\n",
       " ('those_who', 1253),\n",
       " ('vitamin_e', 1235),\n",
       " ('right_away', 1227),\n",
       " ('people_who', 1227),\n",
       " ('customer_service', 1210),\n",
       " ('end_up', 1209),\n",
       " ('look_forward', 1182),\n",
       " ('as_describe', 1168),\n",
       " ('love_it!.', 1164),\n",
       " ('acid_reflux', 1149),\n",
       " ('coconut_oil', 1130),\n",
       " ('health_food_store', 1110),\n",
       " ('long_term', 1088),\n",
       " ('an_empty_stomach', 1082),\n",
       " ('b_complex', 1078),\n",
       " ('1000_mg', 1056),\n",
       " ('fast_shipping', 1054),\n",
       " ('little_bit', 1042),\n",
       " ('swear_by', 1035),\n",
       " ('milk_thistle', 1030),\n",
       " ('blood_test', 1029),\n",
       " ('week_ago', 1024),\n",
       " ('rather_than', 1004),\n",
       " ('green_tea', 996),\n",
       " ('as_advertise', 994),\n",
       " ('reasonable_price', 989),\n",
       " ('big_difference', 972),\n",
       " ('huge_difference', 962),\n",
       " ('come_across', 957),\n",
       " ('nature_make', 954),\n",
       " ('worry_about', 942),\n",
       " ('cod_liver_oil', 937),\n",
       " ('through_amazon', 913),\n",
       " ('second_bottle', 909),\n",
       " ('less_expensive', 907),\n",
       " ('multi_vitamin', 901)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq = Counter(gramlist)\n",
    "paired_words_frq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wishful_thinking!ftc_disclosure', 1),\n",
       " ('none_after!.', 1),\n",
       " ('34;healthy_fat&#34', 1),\n",
       " ('atrail_fibrillationso', 1),\n",
       " ('superior_product!paula', 1),\n",
       " ('ever!!!highly_recommended', 1),\n",
       " ('george_flansbaum_whom', 1),\n",
       " ('side_effects).this', 1),\n",
       " ('side_effects!so', 1),\n",
       " ('onelife_pharma_sound', 1),\n",
       " ('coco_mak_seriously', 1),\n",
       " ('34;off_days&#34', 1),\n",
       " ('i&#8217;m_assuming', 1),\n",
       " ('yeast_infection_every2', 1),\n",
       " ('plant_derived', 1),\n",
       " ('protease_enzym', 1),\n",
       " ('virtually_untreatable', 1),\n",
       " ('con_su_rodilla', 1),\n",
       " ('mi_mama', 1),\n",
       " ('ayuda_mucho', 1),\n",
       " ('productmuy_buen_producto', 1),\n",
       " ('sodium_free.&#34_wow', 1),\n",
       " ('1000_mg_serving!.', 1),\n",
       " ('countless_cortizone', 1),\n",
       " ('tons_of_benefits!.', 1),\n",
       " ('34;clear_out&#34', 1),\n",
       " ('bri_nutrition&#8217;s_unconditional_guarantee', 1),\n",
       " ('dieting_pilling', 1),\n",
       " ('nutrition!._bri_nutrition', 1),\n",
       " ('garage_and_repaint', 1),\n",
       " ('bri_nutrition_triphalia', 1),\n",
       " ('unhygienic_colon', 1),\n",
       " ('slouchy_and_drain', 1),\n",
       " ('8220;bowel_issues&#8221_lately', 1),\n",
       " ('feeling_better!!!!!.', 1),\n",
       " ('34;the_master_cleanse&#34', 1),\n",
       " ('34;all_natural&#34;i', 1),\n",
       " ('satisfaction_gaurantee', 1),\n",
       " ('crucial_andnot_user_friendly', 1),\n",
       " ('defy_reccomemd', 1),\n",
       " ('racetam_or_sulbutiamine', 1),\n",
       " ('greg_bastin', 1),\n",
       " ('favorite_navaho_teas', 1),\n",
       " ('braniac_allow', 1),\n",
       " ('unlike_cleartea', 1),\n",
       " ('tea!!._braniac', 1),\n",
       " ('34;truth_of_reality&#34;.', 1),\n",
       " ('definitley_34;up&#34', 1),\n",
       " ('renew_interest!.', 1),\n",
       " ('customer_ssupport', 1),\n",
       " ('semi_pornographic', 1),\n",
       " ('enzyte_commercial', 1),\n",
       " ('hott_natural!!.', 1),\n",
       " ('comment_frombf', 1),\n",
       " ('slam_bam', 1),\n",
       " ('chinese_herbalists', 1),\n",
       " ('fst_arrival', 1),\n",
       " ('pleasantely_surprised!.', 1),\n",
       " ('own_viagra-', 1),\n",
       " ('sexual_stimulant!!.', 1),\n",
       " ('excessive_stimulation.it', 1),\n",
       " ('34;caffeine_blues&#34', 1),\n",
       " ('raw&#8230;.they_cook', 1),\n",
       " ('voracious_sex', 1),\n",
       " ('wrist_clown', 1),\n",
       " ('meek_limp', 1),\n",
       " ('puppy_contract_parvovirus', 1),\n",
       " ('parvovirus_puppy', 1),\n",
       " ('ordinary_slowness', 1),\n",
       " ('internal_34;cleansing&#34', 1),\n",
       " ('eric_meghan', 1),\n",
       " ('mebe_nutrition', 1),\n",
       " ('complementary_ebook', 1),\n",
       " ('overcome_plateaus', 1),\n",
       " ('franklin_forskolin', 1),\n",
       " ('behavioral_functioning', 1),\n",
       " ('triple_amazeball', 1),\n",
       " ('athletic_clientele', 1),\n",
       " ('senior_citizens', 1),\n",
       " ('34;asparagus_like&#34_urine', 1),\n",
       " ('mind_foreskin', 1),\n",
       " ('sun_wrinkel', 1),\n",
       " ('probably_qualitatively', 1),\n",
       " ('ceo_respond', 1),\n",
       " ('contact_aurora', 1),\n",
       " ('secondly_aurora', 1),\n",
       " ('highly_reccommend!!!!!!.', 1),\n",
       " ('lava_lamp', 1),\n",
       " ('milk_thistle_estract', 1),\n",
       " ('health/_regeneration', 1),\n",
       " ('a++quality_milk_thistle', 1),\n",
       " ('milk_thistle_extract!!i', 1),\n",
       " ('thisle_extract!!.', 1),\n",
       " ('acupuncture_eft', 1),\n",
       " ('rigorous_hiking', 1),\n",
       " ('natures_ayurved', 1),\n",
       " ('co@_prevent', 1),\n",
       " ('critical_co2', 1),\n",
       " ('epa_vesisorb_and_sup', 1),\n",
       " ('34;joint_health&#34', 1)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 100 most infrequent paired words\n",
    "paired_words_frq.most_common()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83377"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq)  # number of paired terms  (this drops down to 46,785 after further processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[this_product, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [this_product, h...                 1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove):\n",
    "    # split up paired words failing our format requirements\n",
    "    to_remove.extend([i])\n",
    "    sent_paired.extend(sent[i + skip: i + skip + num_paired])\n",
    "\n",
    "\n",
    "def filter_pairs(sent, sent_paired, sent_pos):\n",
    "    \"\"\"modify sent_paired in place\"\"\"\n",
    "    paired_sent_len = len(sent_paired)\n",
    "    skip = 0\n",
    "    to_remove = []\n",
    "    \n",
    "    for i in range(paired_sent_len):\n",
    "        word = sent_paired[i]\n",
    "        if '_' in word:\n",
    "            num_paired = word.count('_') + 1\n",
    "            \n",
    "            # more than 3 words paired - ignore pairing\n",
    "            if num_paired > 3:\n",
    "                handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                skip += num_paired - 1\n",
    "                continue\n",
    "            \n",
    "            # bigrams: noun/adj, noun\n",
    "            elif num_paired == 2:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_2 == 'NOUN')\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "            \n",
    "            # trigrams: noun/adj, all types, noun/adj\n",
    "            elif num_paired == 3:\n",
    "                pos_word_1 = sent_pos[i + skip]\n",
    "                pos_word_2 = sent_pos[i + skip + 1]\n",
    "                pos_word_3 = sent_pos[i + skip + 2]\n",
    "                cond = (pos_word_1 in ('NOUN', 'ADJ'), pos_word_3 in ('NOUN', 'ADJ'))\n",
    "                if not all(cond):\n",
    "                    handle_failed_pairing(i, skip, num_paired, sent, sent_paired, to_remove)\n",
    "                    skip += num_paired - 1\n",
    "                    continue\n",
    "        \n",
    "            # num. of words to skip indexing over sent and sent_pos in the next iter\n",
    "            skip += num_paired - 1\n",
    "        \n",
    "    # remove rejected pairs that are already split and added back individually\n",
    "    if len(to_remove) > 0:\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del sent_paired[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the filtering function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent = ['liver', 'support', 'supports', 'liver', 'function', 'stimulate', 'des', 'intoxication', 'and', 'restore', 'liver', 'function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['liver', 'support', 'supports', 'liver_function', 'stimulate_des_intoxication_and_restore', 'liver_function', 'eliminate', 'harmful', 'metabolite']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "# Expected output:\n",
    "print(['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liver', 'support', 'supports', 'liver_function', 'liver_function', 'eliminate', 'harmful', 'metabolite', 'stimulate', 'des', 'intoxication', 'and', 'restore']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "sent = ['-PRON-', 'have', 'a', 'lot', 'more', 'energy', 'and', 'have', 'not', 'be', 'sick', 'at', 'all']\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "sent_pos = ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'VERB', 'ADJ', 'ADV', 'ADV']\n",
    "print(sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n"
     ]
    }
   ],
   "source": [
    "sent_paired = ['-PRON-', 'have', 'a_lot', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'at_all']\n",
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_pairs(sent, sent_paired, sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'have', 'more_energy', 'and', 'have', 'not', 'be', 'sick', 'a', 'lot', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(sent_paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[this_product, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [this_product, h...                 1  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_reviews = unigram_sents_pos_df.preprocessed_review.tolist()\n",
    "unigram_sentences = unigram_sents_pos_df.unigram_sentences.tolist()\n",
    "unigram_pos = unigram_sents_pos_df.unigram_pos.tolist()\n",
    "has_paired_words = unigram_sents_pos_df.has_paired_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241826/1241826 [00:03<00:00, 393305.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# get rid of paired words from the corpus which\n",
    "# (1) have more than 3 words joined\n",
    "# (2) bigrams not in the format: noun/adj, noun\n",
    "# (3) trigrams not in the format: noun/adj, all types, noun/adj\n",
    "for i in tqdm(range(len(preprocessed_reviews))):\n",
    "    if has_paired_words[i] == 1:\n",
    "        filter_pairs(sent=unigram_sentences[i], sent_paired=preprocessed_reviews[i], sent_pos=unigram_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v2.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[have, the, grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [have, the, grea...                 1  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_review_updated = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241826"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_review_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bioavailability', 'be', 'the', 'key'],\n",
       " ['-PRON-',\n",
       "  'start',\n",
       "  'take',\n",
       "  'this',\n",
       "  'after',\n",
       "  'both',\n",
       "  '-PRON-',\n",
       "  'parent',\n",
       "  'die',\n",
       "  'of',\n",
       "  'cancer',\n",
       "  'as',\n",
       "  '-PRON-',\n",
       "  'suppose',\n",
       "  'to',\n",
       "  'enhance',\n",
       "  '-PRON-',\n",
       "  'immune_system',\n",
       "  'the',\n",
       "  'story',\n",
       "  'on',\n",
       "  'on',\n",
       "  'resveratrol',\n",
       "  'be',\n",
       "  '60',\n",
       "  'minutes',\n",
       "  'incredibly',\n",
       "  'inspiring'],\n",
       " ['do',\n",
       "  'some',\n",
       "  'research',\n",
       "  'on',\n",
       "  'the',\n",
       "  'internet',\n",
       "  '-PRON-',\n",
       "  'be',\n",
       "  'indicate',\n",
       "  'that',\n",
       "  'take',\n",
       "  'resveratrol',\n",
       "  'in',\n",
       "  'lozenge',\n",
       "  'form',\n",
       "  'be',\n",
       "  'preferable',\n",
       "  'as',\n",
       "  '-PRON-',\n",
       "  'be',\n",
       "  'by',\n",
       "  'stomach',\n",
       "  'acid',\n",
       "  'break',\n",
       "  'down']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_review_updated[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramlist_updated = [word for sent in preprocessed_review_updated for word in sent if '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fish_oil', 10809),\n",
       " ('side_effect', 4969),\n",
       " ('high_quality', 3908),\n",
       " ('vitamin_d', 3852),\n",
       " ('immune_system', 3374),\n",
       " ('long_time', 3112),\n",
       " ('vitamin_c', 2660),\n",
       " ('blood_pressure', 2457),\n",
       " ('energy_level', 2408),\n",
       " ('anyone_who', 2207),\n",
       " ('joint_pain', 1932),\n",
       " ('weight_loss', 1891),\n",
       " ('hot_flash', 1758),\n",
       " ('health_benefit', 1577),\n",
       " ('blood_sugar', 1465),\n",
       " ('digestive_system', 1354),\n",
       " ('krill_oil', 1312),\n",
       " ('people_who', 1227),\n",
       " ('vitamin_e', 1203),\n",
       " ('customer_service', 1112),\n",
       " ('long_term', 1076),\n",
       " ('acid_reflux', 1044),\n",
       " ('health_food_store', 1038),\n",
       " ('little_bit', 1038),\n",
       " ('fast_shipping', 1035),\n",
       " ('blood_test', 956),\n",
       " ('big_difference', 951),\n",
       " ('huge_difference', 950),\n",
       " ('reasonable_price', 950),\n",
       " ('coconut_oil', 928),\n",
       " ('second_bottle', 908),\n",
       " ('small_amount', 867),\n",
       " ('multi_vitamin', 808),\n",
       " ('green_tea', 807),\n",
       " ('daily_basis', 788),\n",
       " ('sore_throat', 785),\n",
       " ('local_health', 784),\n",
       " ('digestive_issue', 781),\n",
       " ('food_store', 763),\n",
       " ('soft_gel', 753),\n",
       " ('love_it!.', 725),\n",
       " ('leg_cramp', 721),\n",
       " ('hair_growth', 706),\n",
       " ('yeast_infection', 701),\n",
       " ('regular_basis', 690),\n",
       " ('orange_juice', 677),\n",
       " ('vitamin_d3', 673),\n",
       " ('dry_eye', 668),\n",
       " ('local_store', 667),\n",
       " ('bowel_movement', 657),\n",
       " ('sinus_infection', 657),\n",
       " ('fishy_aftertaste', 648),\n",
       " ('fish_burps', 637),\n",
       " ('hair_loss', 625),\n",
       " ('b_vitamin', 623),\n",
       " ('family_member', 621),\n",
       " ('high_blood_pressure', 619),\n",
       " ('fishy_taste', 614),\n",
       " ('gel_cap', 598),\n",
       " ('pre_workout', 596),\n",
       " ('digestive_enzyme', 593),\n",
       " ('digestive_problem', 590),\n",
       " ('cod_liver_oil', 590),\n",
       " ('upset_stomach', 588),\n",
       " ('life_saver', 576),\n",
       " ('prescription_drug', 567),\n",
       " ('fast_delivery', 542),\n",
       " ('flu_season', 536),\n",
       " ('protein_powder', 532),\n",
       " ('someone_who', 518),\n",
       " ('liquid_form', 514),\n",
       " ('prescription_medication', 513),\n",
       " ('free_shipping', 511),\n",
       " ('normal_range', 508),\n",
       " ('olive_oil', 508),\n",
       " ('drug_store', 500),\n",
       " ('real_deal', 499),\n",
       " ('whole_food', 498),\n",
       " ('natural_remedy', 494),\n",
       " ('whole_family', 485),\n",
       " ('fishy_burps', 484),\n",
       " ('milk_thistle', 471),\n",
       " ('expiration_date', 454),\n",
       " ('negative_side_effect', 447),\n",
       " ('worth_every_penny', 443),\n",
       " ('essential_oil', 423),\n",
       " ('first_sign', 410),\n",
       " ('colloidal_silver', 409),\n",
       " ('placebo_effect', 393),\n",
       " ('noticeable_difference', 393),\n",
       " ('digestive_tract', 387),\n",
       " ('mood_swing', 373),\n",
       " ('positive_effect', 371),\n",
       " ('tea_tree_oil', 369),\n",
       " ('amino_acid', 366),\n",
       " ('lemon_flavor', 365),\n",
       " ('kidney_stone', 362),\n",
       " ('peppermint_oil', 362),\n",
       " ('aerobic_step', 360),\n",
       " ('prescription_med', 355)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words_frq_updated = Counter(gramlist_updated)\n",
    "paired_words_frq_updated.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27137"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_words_frq_updated)   # final number of cleaned-up paired words in the specified phrase format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Clean-up: Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>has_paired_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>[have, the, grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   preprocessed_review  has_paired_words  \n",
       "0  [bioavailability...                 0  \n",
       "1  [-PRON-, start, ...                 1  \n",
       "2  [do, some, resea...                 1  \n",
       "3  [the, ez, melt, ...                 0  \n",
       "4  [have, the, grea...                 1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241826, 6)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_review_final = [[word for word in sentence if word not in nlp.Defaults.stop_words] for sentence in preprocessed_review_updated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>has_paired_words</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability...</td>\n",
       "      <td>[PROPN, VERB, DE...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bioavailability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "      <td>[PRON, VERB, VER...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, start, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, resea...</td>\n",
       "      <td>[VERB, DET, NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>[research, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, ...</td>\n",
       "      <td>[DET, ADP, NOUN,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ez, melt, formu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, ...</td>\n",
       "      <td>[DET, NOUN, VERB...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, side_eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number    unigram_sentences          unigram_pos  \\\n",
       "0              1                1  [bioavailability...  [PROPN, VERB, DE...   \n",
       "1              1                2  [-PRON-, start, ...  [PRON, VERB, VER...   \n",
       "2              1                3  [do, some, resea...  [VERB, DET, NOUN...   \n",
       "3              1                4  [the, ez, melt, ...  [DET, ADP, NOUN,...   \n",
       "4              1                5  [this, product, ...  [DET, NOUN, VERB...   \n",
       "\n",
       "   has_paired_words  preprocessed_review  \n",
       "0                 0  [bioavailability...  \n",
       "1                 1  [-PRON-, start, ...  \n",
       "2                 1  [research, inter...  \n",
       "3                 0  [ez, melt, formu...  \n",
       "4                 1  [great, side_eff...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.drop(['preprocessed_review'], axis=1, inplace=True)\n",
    "unigram_sents_pos_df['preprocessed_review'] = preprocessed_review_final\n",
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save picked dataframe to S3.  Pickle format allows the columns to store lists\n",
    "save_df_s3(unigram_sents_pos_df, bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from the pickled dataframe on S3\n",
    "unigram_sents_pos_df = load_df_s3(bucket_name, filepath='amazon_reviews/kk/preprocessed_reviews_v3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>unigram_sentences</th>\n",
       "      <th>unigram_pos</th>\n",
       "      <th>has_paired_words</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability, be, the, key]</td>\n",
       "      <td>[PROPN, VERB, DET, NOUN]</td>\n",
       "      <td>0</td>\n",
       "      <td>[bioavailability, key]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-PRON-, start, take, this, after, both, -PRON...</td>\n",
       "      <td>[PRON, VERB, VERB, DET, ADP, CCONJ, ADJ, NOUN,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, start, -PRON-, parent, die, cancer, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[do, some, research, on, the, internet, -PRON-...</td>\n",
       "      <td>[VERB, DET, NOUN, ADP, DET, NOUN, PRON, VERB, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[research, internet, -PRON-, indicate, resvera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, ez, melt, formula, recommend, in, anothe...</td>\n",
       "      <td>[DET, ADP, NOUN, NOUN, VERB, ADP, DET, NOUN, V...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ez, melt, formula, recommend, review, ok, -PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[this, product, have, the, great, side, effect...</td>\n",
       "      <td>[DET, NOUN, VERB, DET, ADJ, NOUN, NOUN, ADP, V...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, side_effect, -PRON-, -PRON-, cold, sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[soon, after, start, to, take, -PRON-, every, ...</td>\n",
       "      <td>[ADV, ADP, VERB, PART, VERB, PRON, DET, NOUN, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[soon, start, -PRON-, -PRON-, start, come, col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[but, -PRON-, never, do, get, as, sick, as, an...</td>\n",
       "      <td>[CCONJ, PRON, ADV, VERB, VERB, ADV, ADJ, ADP, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, sick, anticipate, taking, reason, -PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[since, then, -PRON-, have, have, no, cold, or...</td>\n",
       "      <td>[ADP, ADV, PRON, VERB, VERB, DET, NOUN, CCONJ,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, cold, sore_throat, -PRON-, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[-PRON-, recommend, this, product, to, everyon...</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, ADP, NOUN, PRON, VERB,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, recommend, -PRON-, know, -PRON-, gift...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[-PRON-, can, not, find, this, product, any, l...</td>\n",
       "      <td>[PRON, VERB, ADV, VERB, DET, NOUN, ADV, ADV, C...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-PRON-, find, longer, -PRON-, wish, -PRON-, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "5              1                6   \n",
       "6              1                7   \n",
       "7              1                8   \n",
       "8              1                9   \n",
       "9              2               10   \n",
       "\n",
       "                                   unigram_sentences  \\\n",
       "0                    [bioavailability, be, the, key]   \n",
       "1  [-PRON-, start, take, this, after, both, -PRON...   \n",
       "2  [do, some, research, on, the, internet, -PRON-...   \n",
       "3  [the, ez, melt, formula, recommend, in, anothe...   \n",
       "4  [this, product, have, the, great, side, effect...   \n",
       "5  [soon, after, start, to, take, -PRON-, every, ...   \n",
       "6  [but, -PRON-, never, do, get, as, sick, as, an...   \n",
       "7  [since, then, -PRON-, have, have, no, cold, or...   \n",
       "8  [-PRON-, recommend, this, product, to, everyon...   \n",
       "9  [-PRON-, can, not, find, this, product, any, l...   \n",
       "\n",
       "                                         unigram_pos  has_paired_words  \\\n",
       "0                           [PROPN, VERB, DET, NOUN]                 0   \n",
       "1  [PRON, VERB, VERB, DET, ADP, CCONJ, ADJ, NOUN,...                 1   \n",
       "2  [VERB, DET, NOUN, ADP, DET, NOUN, PRON, VERB, ...                 1   \n",
       "3  [DET, ADP, NOUN, NOUN, VERB, ADP, DET, NOUN, V...                 0   \n",
       "4  [DET, NOUN, VERB, DET, ADJ, NOUN, NOUN, ADP, V...                 1   \n",
       "5  [ADV, ADP, VERB, PART, VERB, PRON, DET, NOUN, ...                 1   \n",
       "6  [CCONJ, PRON, ADV, VERB, VERB, ADV, ADJ, ADP, ...                 1   \n",
       "7  [ADP, ADV, PRON, VERB, VERB, DET, NOUN, CCONJ,...                 1   \n",
       "8  [PRON, VERB, DET, NOUN, ADP, NOUN, PRON, VERB,...                 1   \n",
       "9  [PRON, VERB, ADV, VERB, DET, NOUN, ADV, ADV, C...                 1   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0                             [bioavailability, key]  \n",
       "1  [-PRON-, start, -PRON-, parent, die, cancer, -...  \n",
       "2  [research, internet, -PRON-, indicate, resvera...  \n",
       "3  [ez, melt, formula, recommend, review, ok, -PR...  \n",
       "4  [great, side_effect, -PRON-, -PRON-, cold, sor...  \n",
       "5  [soon, start, -PRON-, -PRON-, start, come, col...  \n",
       "6  [-PRON-, sick, anticipate, taking, reason, -PR...  \n",
       "7         [-PRON-, cold, sore_throat, -PRON-, great]  \n",
       "8  [-PRON-, recommend, -PRON-, know, -PRON-, gift...  \n",
       "9  [-PRON-, find, longer, -PRON-, wish, -PRON-, p...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sents_pos_df.drop(['unigram_sentences', 'unigram_pos', 'has_paired_words'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove '-PRON-' from the token list\n",
    "unigram_sents_pos_df.preprocessed_review = unigram_sents_pos_df.preprocessed_review.map(lambda x: [word for word in x if word != '-PRON-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of tokens that do not contain any alphabets\n",
    "def has_any_alpha(word):\n",
    "    return any([char for char in word if char.isalpha()])\n",
    "\n",
    "unigram_sents_pos_df.preprocessed_review = unigram_sents_pos_df.preprocessed_review.map(lambda x: [word for word in x if has_any_alpha(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove one-character tokens\n",
    "unigram_sents_pos_df.preprocessed_review = unigram_sents_pos_df.preprocessed_review.map(lambda x: [word for word in x if len(word) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_number</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability, key]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[start, parent, die, cancer, suppose, enhance,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[research, internet, indicate, resveratrol, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[ez, melt, formula, recommend, review, ok, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[great, side_effect, cold, sore_throat, product]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_number  sentence_number  \\\n",
       "0              1                1   \n",
       "1              1                2   \n",
       "2              1                3   \n",
       "3              1                4   \n",
       "4              1                5   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0                             [bioavailability, key]  \n",
       "1  [start, parent, die, cancer, suppose, enhance,...  \n",
       "2  [research, internet, indicate, resveratrol, lo...  \n",
       "3  [ez, melt, formula, recommend, review, ok, dis...  \n",
       "4   [great, side_effect, cold, sore_throat, product]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sents_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_num = unigram_sents_pos_df.review_number.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = unigram_sents_pos_df.preprocessed_review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bioavailability', 'key'], ['start', 'parent', 'die', 'cancer', 'suppose', 'enhance', 'immune_system', 'story', 'resveratrol', 'minutes', 'incredibly', 'inspiring'], ['research', 'internet', 'indicate', 'resveratrol', 'lozenge', 'form', 'preferable', 'stomach', 'acid', 'break']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = unigram_sents_pos_df.groupby(('review_number'))['preprocessed_review'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = tokenized_reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bioavailability', 'key', 'start', 'parent', 'die', 'cancer', 'suppose', 'enhance', 'immune_system', 'story', 'resveratrol', 'minutes', 'incredibly', 'inspiring', 'research', 'internet', 'indicate', 'resveratrol', 'lozenge', 'form', 'preferable', 'stomach', 'acid', 'break', 'ez', 'melt', 'formula', 'recommend', 'review', 'ok', 'dissolve', 'mouth', 'quickly', 'lozenge', 'formula', 'dissolve', 'slowly', 'preferable', 'accord', 'research', 'great', 'side_effect', 'cold', 'sore_throat', 'product', 'soon', 'start', 'start', 'come', 'cold', 'usual', 'symptom', 'anticipate', 'sick', 'day', 'usual', 'pattern', 'day', 'sick', 'anticipate', 'taking', 'reason', 'come', 'product', 'cold', 'sore_throat', 'great', 'recommend', 'know', 'gift', 'family', 'product']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_df_s3(tokenized_reviews, bucket_name, filepath='amazon_reviews/kk/tokenized_reviews_v1.pkl', filetype='pickle')\n",
    "tokenized_reviews = load_df_s3(bucket_name, filepath='amazon_reviews/kk/tokenized_reviews_v1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.18 s, sys: 12 ms, total: 9.19 s\n",
      "Wall time: 9.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to learn the full vocabulary of the corpus to be modeled\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "vocab_dictionary = Dictionary(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(133384 unique tokens: ['accord', 'acid', 'anticipate', 'bioavailability', 'break']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "once_ids = [tokenid for tokenid, docfreq in iteritems(vocab_dictionary.dfs) if docfreq == 1]\n",
    "vocab_dictionary.filter_tokens(bad_ids=once_ids)  # remove words that appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42235 unique tokens: ['accord', 'acid', 'anticipate', 'bioavailability', 'break']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "# here we remove tokens which do not appear in at least 10 reviews\n",
    "vocab_dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "vocab_dictionary.compactify()   # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(13850 unique tokens: ['accord', 'acid', 'anticipate', 'bioavailability', 'break']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(vocab_dictionary, bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_v3.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dictionary = load_df_s3(bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_v3.dict', filetype='pickle')\n",
    "# vocab_dictionary = Dictionary.load('../vocab_dictionary.dict')  # load the finished dictionary from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = load_df_s3(bucket_name, filepath='amazon_reviews/kk/tokenized_reviews_v1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag-of-words representation of the corpus/ doc-term matrix\n",
    "bow_corpus = [vocab_dictionary.doc2bow(review) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## First Order LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 0.25\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 4 topics, 3 passes over the supplied corpus of 217530 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6713/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072370008, 0.082538292, 0.11915907, 0.15756291]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.072): 0.023*\"pedometer\" + 0.019*\"step\" + 0.019*\"day\" + 0.016*\"use\" + 0.015*\"great\" + 0.015*\"good\" + 0.015*\"work\" + 0.015*\"like\" + 0.010*\"product\" + 0.009*\"walk\"\n",
      "INFO : topic #1 (0.083): 0.028*\"pedometer\" + 0.022*\"great\" + 0.018*\"work\" + 0.018*\"use\" + 0.015*\"good\" + 0.015*\"easy\" + 0.014*\"product\" + 0.012*\"day\" + 0.010*\"accurate\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.119): 0.036*\"pedometer\" + 0.024*\"good\" + 0.024*\"product\" + 0.023*\"use\" + 0.018*\"great\" + 0.016*\"love\" + 0.016*\"work\" + 0.013*\"step\" + 0.012*\"day\" + 0.012*\"walk\"\n",
      "INFO : topic #3 (0.158): 0.033*\"pedometer\" + 0.026*\"use\" + 0.024*\"day\" + 0.021*\"product\" + 0.020*\"step\" + 0.020*\"great\" + 0.018*\"good\" + 0.014*\"easy\" + 0.014*\"work\" + 0.012*\"pocket\"\n",
      "INFO : topic diff=5.779627, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6988/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082452402, 0.091588706, 0.11311321, 0.097086042]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.082): 0.017*\"good\" + 0.015*\"day\" + 0.014*\"product\" + 0.013*\"work\" + 0.013*\"like\" + 0.013*\"use\" + 0.012*\"help\" + 0.012*\"great\" + 0.009*\"feel\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.092): 0.024*\"product\" + 0.022*\"great\" + 0.019*\"good\" + 0.016*\"use\" + 0.015*\"taste\" + 0.014*\"work\" + 0.009*\"recommend\" + 0.009*\"like\" + 0.009*\"day\" + 0.008*\"help\"\n",
      "INFO : topic #2 (0.113): 0.036*\"product\" + 0.026*\"good\" + 0.024*\"use\" + 0.018*\"great\" + 0.014*\"work\" + 0.013*\"love\" + 0.011*\"pedometer\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"skin\"\n",
      "INFO : topic #3 (0.097): 0.027*\"product\" + 0.025*\"use\" + 0.021*\"day\" + 0.020*\"pedometer\" + 0.019*\"great\" + 0.019*\"good\" + 0.014*\"work\" + 0.012*\"step\" + 0.010*\"time\" + 0.010*\"easy\"\n",
      "INFO : topic diff=1.545039, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090033136, 0.099368781, 0.11270516, 0.084802181]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.090): 0.017*\"good\" + 0.016*\"day\" + 0.015*\"help\" + 0.014*\"work\" + 0.014*\"product\" + 0.012*\"use\" + 0.011*\"like\" + 0.010*\"great\" + 0.010*\"feel\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.099): 0.027*\"product\" + 0.023*\"great\" + 0.022*\"good\" + 0.017*\"use\" + 0.014*\"work\" + 0.014*\"taste\" + 0.009*\"like\" + 0.009*\"help\" + 0.008*\"recommend\" + 0.008*\"price\"\n",
      "INFO : topic #2 (0.113): 0.040*\"product\" + 0.027*\"good\" + 0.026*\"use\" + 0.019*\"great\" + 0.015*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic #3 (0.085): 0.028*\"product\" + 0.025*\"use\" + 0.021*\"day\" + 0.018*\"good\" + 0.018*\"great\" + 0.015*\"work\" + 0.012*\"pedometer\" + 0.011*\"time\" + 0.008*\"year\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.771673, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.094821341, 0.10767336, 0.11680388, 0.079674095]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.095): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"good\" + 0.015*\"work\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"like\" + 0.009*\"great\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.108): 0.028*\"product\" + 0.024*\"great\" + 0.023*\"good\" + 0.017*\"use\" + 0.013*\"taste\" + 0.013*\"work\" + 0.010*\"like\" + 0.009*\"price\" + 0.008*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.042*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.016*\"work\" + 0.013*\"love\" + 0.012*\"skin\" + 0.011*\"oil\" + 0.010*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.080): 0.027*\"product\" + 0.025*\"use\" + 0.020*\"day\" + 0.017*\"great\" + 0.016*\"good\" + 0.016*\"work\" + 0.011*\"time\" + 0.008*\"year\" + 0.008*\"help\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.567345, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10244222, 0.11420026, 0.11670072, 0.077437147]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.102): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.015*\"good\" + 0.014*\"product\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"like\" + 0.009*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.114): 0.029*\"product\" + 0.026*\"good\" + 0.025*\"great\" + 0.016*\"use\" + 0.015*\"taste\" + 0.012*\"work\" + 0.011*\"like\" + 0.010*\"price\" + 0.009*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.045*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.016*\"work\" + 0.013*\"skin\" + 0.013*\"love\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.077): 0.027*\"product\" + 0.024*\"use\" + 0.019*\"day\" + 0.016*\"work\" + 0.016*\"great\" + 0.016*\"good\" + 0.011*\"time\" + 0.009*\"year\" + 0.008*\"help\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.496468, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1093009, 0.11959437, 0.11728799, 0.076797262]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.109): 0.017*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.015*\"good\" + 0.014*\"product\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.120): 0.029*\"product\" + 0.028*\"good\" + 0.025*\"great\" + 0.017*\"use\" + 0.016*\"taste\" + 0.012*\"like\" + 0.010*\"price\" + 0.010*\"work\" + 0.008*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.047*\"product\" + 0.028*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.015*\"work\" + 0.014*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.009*\"oil\"\n",
      "INFO : topic #3 (0.077): 0.027*\"product\" + 0.024*\"use\" + 0.020*\"day\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"great\" + 0.011*\"time\" + 0.011*\"pain\" + 0.009*\"year\" + 0.008*\"help\"\n",
      "INFO : topic diff=0.446620, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11482716, 0.1257468, 0.11888508, 0.077609047]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.115): 0.017*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.126): 0.029*\"product\" + 0.029*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.016*\"use\" + 0.012*\"like\" + 0.010*\"price\" + 0.010*\"work\" + 0.009*\"supplement\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.119): 0.049*\"product\" + 0.029*\"use\" + 0.026*\"good\" + 0.024*\"great\" + 0.015*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.011*\"oil\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #3 (0.078): 0.027*\"product\" + 0.023*\"use\" + 0.019*\"day\" + 0.019*\"work\" + 0.014*\"good\" + 0.013*\"great\" + 0.011*\"time\" + 0.011*\"pain\" + 0.010*\"year\" + 0.008*\"help\"\n",
      "INFO : topic diff=0.416753, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12343414, 0.12966594, 0.11762695, 0.079292007]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.123): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"use\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.130): 0.030*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.010*\"price\" + 0.009*\"work\" + 0.009*\"find\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.118): 0.051*\"product\" + 0.029*\"use\" + 0.027*\"good\" + 0.025*\"great\" + 0.016*\"work\" + 0.016*\"skin\" + 0.013*\"love\" + 0.011*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.079): 0.026*\"product\" + 0.023*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.013*\"good\" + 0.012*\"pain\" + 0.012*\"great\" + 0.011*\"time\" + 0.011*\"uti\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.383540, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13068466, 0.13434196, 0.11937566, 0.078368261]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.131): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.134): 0.032*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"supplement\" + 0.009*\"find\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.119): 0.053*\"product\" + 0.029*\"use\" + 0.028*\"good\" + 0.026*\"great\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.011*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.078): 0.026*\"product\" + 0.022*\"use\" + 0.020*\"work\" + 0.020*\"day\" + 0.013*\"pain\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"time\" + 0.010*\"year\" + 0.009*\"uti\"\n",
      "INFO : topic diff=0.332496, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13711213, 0.13563071, 0.12254435, 0.080282904]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.137): 0.017*\"help\" + 0.017*\"day\" + 0.016*\"work\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.136): 0.032*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"supplement\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.123): 0.054*\"product\" + 0.029*\"use\" + 0.028*\"good\" + 0.027*\"great\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic #3 (0.080): 0.025*\"product\" + 0.021*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.014*\"pain\" + 0.012*\"good\" + 0.011*\"great\" + 0.011*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.339808, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14204738, 0.1419716, 0.12403292, 0.080710754]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.142): 0.018*\"help\" + 0.017*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.142): 0.033*\"good\" + 0.028*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.018*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"love\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.124): 0.054*\"product\" + 0.031*\"use\" + 0.028*\"great\" + 0.026*\"good\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.081): 0.024*\"product\" + 0.022*\"use\" + 0.020*\"day\" + 0.019*\"work\" + 0.015*\"pain\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.309580, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14905578, 0.14514583, 0.12714738, 0.081536062]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.149): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.145): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.127): 0.056*\"product\" + 0.030*\"use\" + 0.028*\"great\" + 0.026*\"good\" + 0.016*\"work\" + 0.013*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"hair\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.082): 0.025*\"product\" + 0.021*\"use\" + 0.020*\"work\" + 0.019*\"day\" + 0.017*\"pain\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.289438, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15631747, 0.14985779, 0.12782843, 0.082744226]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.156): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.150): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.010*\"vitamin\" + 0.010*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.128): 0.057*\"product\" + 0.030*\"use\" + 0.029*\"great\" + 0.027*\"good\" + 0.015*\"work\" + 0.014*\"skin\" + 0.012*\"love\" + 0.012*\"recommend\" + 0.011*\"hair\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.083): 0.024*\"product\" + 0.020*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.018*\"pain\" + 0.011*\"good\" + 0.010*\"great\" + 0.010*\"year\" + 0.010*\"time\" + 0.010*\"week\"\n",
      "INFO : topic diff=0.282694, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15795387, 0.15619229, 0.12993996, 0.083497956]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.158): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.156): 0.035*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.011*\"price\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.130): 0.057*\"product\" + 0.031*\"use\" + 0.030*\"great\" + 0.027*\"good\" + 0.015*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"hair\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.083): 0.023*\"product\" + 0.021*\"day\" + 0.020*\"use\" + 0.020*\"work\" + 0.018*\"pain\" + 0.011*\"good\" + 0.010*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic diff=0.249285, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16313532, 0.15855511, 0.13291879, 0.084954567]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.163): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.159): 0.035*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.009*\"supplement\" + 0.009*\"brand\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.133): 0.058*\"product\" + 0.031*\"use\" + 0.031*\"great\" + 0.027*\"good\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"hair\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.085): 0.022*\"product\" + 0.020*\"day\" + 0.020*\"use\" + 0.020*\"work\" + 0.019*\"pain\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"great\" + 0.009*\"help\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.253378, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16724987, 0.16620769, 0.13625547, 0.085355461]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.167): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.166): 0.034*\"good\" + 0.026*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.136): 0.058*\"product\" + 0.031*\"great\" + 0.030*\"use\" + 0.027*\"good\" + 0.015*\"work\" + 0.013*\"love\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.010*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.085): 0.021*\"product\" + 0.021*\"work\" + 0.020*\"day\" + 0.020*\"use\" + 0.018*\"pain\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"year\" + 0.010*\"great\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.238933, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16898209, 0.17096286, 0.1390024, 0.085833132]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.169): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.171): 0.034*\"good\" + 0.025*\"product\" + 0.024*\"great\" + 0.022*\"taste\" + 0.015*\"like\" + 0.015*\"use\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.139): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.015*\"work\" + 0.013*\"love\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.086): 0.021*\"product\" + 0.021*\"work\" + 0.020*\"day\" + 0.020*\"use\" + 0.017*\"pain\" + 0.010*\"week\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.228935, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16906099, 0.17793798, 0.14387201, 0.085871525]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.169): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.178): 0.035*\"good\" + 0.025*\"product\" + 0.025*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.144): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"skin\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.086): 0.021*\"work\" + 0.021*\"product\" + 0.021*\"day\" + 0.019*\"use\" + 0.017*\"pain\" + 0.011*\"week\" + 0.010*\"good\" + 0.010*\"help\" + 0.010*\"time\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.211883, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.167919, 0.18397315, 0.14783899, 0.086044557]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.168): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.184): 0.035*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.024*\"product\" + 0.015*\"like\" + 0.013*\"use\" + 0.013*\"vitamin\" + 0.011*\"price\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.148): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.029*\"good\" + 0.014*\"oil\" + 0.014*\"love\" + 0.014*\"skin\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.086): 0.022*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.019*\"use\" + 0.016*\"pain\" + 0.011*\"week\" + 0.010*\"help\" + 0.010*\"good\" + 0.010*\"time\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.211599, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17378229, 0.18530473, 0.15064622, 0.087697856]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.174): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.185): 0.035*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.024*\"product\" + 0.015*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"price\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.151): 0.062*\"product\" + 0.032*\"great\" + 0.029*\"good\" + 0.029*\"use\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.088): 0.022*\"work\" + 0.021*\"product\" + 0.021*\"day\" + 0.019*\"use\" + 0.015*\"pain\" + 0.011*\"week\" + 0.010*\"help\" + 0.010*\"time\" + 0.010*\"good\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.216266, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.175501, 0.18868011, 0.15290572, 0.088813968]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.176): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.189): 0.034*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"love\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.153): 0.063*\"product\" + 0.033*\"great\" + 0.030*\"good\" + 0.029*\"use\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.089): 0.022*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.016*\"pain\" + 0.011*\"week\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.202595, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17733867, 0.19178689, 0.15487689, 0.090235047]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.177): 0.018*\"help\" + 0.017*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.192): 0.034*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"price\"\n",
      "INFO : topic #2 (0.155): 0.064*\"product\" + 0.033*\"great\" + 0.030*\"use\" + 0.030*\"good\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.012*\"skin\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.090): 0.023*\"work\" + 0.020*\"product\" + 0.020*\"day\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"start\" + 0.009*\"good\"\n",
      "INFO : topic diff=0.201018, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17716767, 0.18951592, 0.16166346, 0.090444118]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.177): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.190): 0.034*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"price\"\n",
      "INFO : topic #2 (0.162): 0.062*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.017*\"hair\" + 0.013*\"love\" + 0.013*\"work\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.012*\"oil\"\n",
      "INFO : topic #3 (0.090): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"start\" + 0.010*\"time\" + 0.010*\"help\" + 0.009*\"feel\"\n",
      "INFO : topic diff=0.197441, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18067282, 0.19118102, 0.16493705, 0.091609389]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.181): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.191): 0.034*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.165): 0.063*\"product\" + 0.031*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.016*\"hair\" + 0.014*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.092): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"time\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.192417, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18454447, 0.19268326, 0.16975927, 0.092190467]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.185): 0.018*\"help\" + 0.016*\"work\" + 0.015*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.193): 0.033*\"good\" + 0.024*\"great\" + 0.024*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.170): 0.064*\"product\" + 0.032*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"oil\" + 0.013*\"hair\" + 0.013*\"love\" + 0.013*\"skin\" + 0.013*\"work\" + 0.012*\"recommend\"\n",
      "INFO : topic #3 (0.092): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.011*\"help\" + 0.010*\"start\" + 0.010*\"feel\" + 0.010*\"time\"\n",
      "INFO : topic diff=0.182581, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18603575, 0.19468854, 0.17148259, 0.094140485]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.186): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.195): 0.033*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.171): 0.065*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"hair\" + 0.013*\"skin\" + 0.012*\"recommend\"\n",
      "INFO : topic #3 (0.094): 0.023*\"work\" + 0.020*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.016*\"pain\" + 0.013*\"week\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.195012, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1871279, 0.19577834, 0.1724543, 0.096336752]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.187): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.033*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.172): 0.067*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.014*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"hair\" + 0.013*\"recommend\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.096): 0.024*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.183644, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18892746, 0.19620804, 0.17386544, 0.099184059]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.013*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.033*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.174): 0.068*\"product\" + 0.034*\"great\" + 0.030*\"good\" + 0.027*\"use\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.013*\"skin\" + 0.012*\"hair\"\n",
      "INFO : topic #3 (0.099): 0.025*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.184489, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1879535, 0.19437112, 0.17978567, 0.10033237]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.188): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.013*\"feel\" + 0.012*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.194): 0.032*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.180): 0.067*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"work\" + 0.012*\"oil\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.100): 0.024*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.016*\"pain\" + 0.013*\"week\" + 0.012*\"feel\" + 0.011*\"try\" + 0.011*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.178492, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.189362, 0.19849733, 0.18487963, 0.10260747]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"day\" + 0.014*\"feel\" + 0.013*\"good\" + 0.013*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.198): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.185): 0.069*\"product\" + 0.033*\"great\" + 0.028*\"good\" + 0.027*\"use\" + 0.016*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"oil\" + 0.012*\"work\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.103): 0.023*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.017*\"use\" + 0.015*\"pain\" + 0.014*\"week\" + 0.014*\"feel\" + 0.011*\"try\" + 0.011*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.186617, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18876565, 0.19359982, 0.19071725, 0.10616958]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"day\" + 0.014*\"feel\" + 0.014*\"supplement\" + 0.013*\"good\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.194): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.191): 0.069*\"product\" + 0.032*\"great\" + 0.027*\"good\" + 0.026*\"use\" + 0.019*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"work\" + 0.010*\"oil\" + 0.010*\"look\"\n",
      "INFO : topic #3 (0.106): 0.023*\"work\" + 0.022*\"product\" + 0.020*\"day\" + 0.016*\"use\" + 0.015*\"week\" + 0.014*\"feel\" + 0.014*\"pain\" + 0.012*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.182206, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18824422, 0.18603107, 0.19514026, 0.10940558]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.188): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"feel\" + 0.015*\"supplement\" + 0.015*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"try\" + 0.010*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.186): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.015*\"easy\" + 0.014*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.195): 0.072*\"product\" + 0.034*\"great\" + 0.027*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.014*\"recommend\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"look\" + 0.010*\"feel\"\n",
      "INFO : topic #3 (0.109): 0.023*\"product\" + 0.022*\"work\" + 0.019*\"day\" + 0.017*\"pain\" + 0.016*\"use\" + 0.016*\"feel\" + 0.014*\"week\" + 0.013*\"try\" + 0.011*\"help\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.172691, rho=0.176777\n",
      "INFO : PROGRESS: pass 1, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17799795, 0.18255495, 0.1903078, 0.11632661]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.178): 0.019*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.015*\"feel\" + 0.014*\"good\" + 0.014*\"supplement\" + 0.014*\"day\" + 0.011*\"try\" + 0.010*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.183): 0.030*\"good\" + 0.025*\"great\" + 0.021*\"taste\" + 0.021*\"product\" + 0.019*\"easy\" + 0.017*\"like\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"love\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.190): 0.069*\"product\" + 0.034*\"great\" + 0.027*\"good\" + 0.027*\"use\" + 0.018*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.013*\"work\" + 0.010*\"look\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.116): 0.022*\"day\" + 0.021*\"work\" + 0.019*\"pedometer\" + 0.018*\"product\" + 0.018*\"use\" + 0.012*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"step\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.218292, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18480211, 0.18610239, 0.18875685, 0.11519255]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.185): 0.019*\"help\" + 0.016*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"feel\" + 0.014*\"day\" + 0.013*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.186): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"easy\" + 0.017*\"like\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"vitamin\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.189): 0.068*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.027*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.010*\"buy\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.115): 0.022*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.018*\"product\" + 0.017*\"pedometer\" + 0.013*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"great\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.175803, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19179255, 0.18646948, 0.18922505, 0.11470156]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.192): 0.019*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.014*\"feel\" + 0.012*\"supplement\" + 0.010*\"use\" + 0.010*\"try\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.186): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"like\" + 0.016*\"easy\" + 0.012*\"use\" + 0.011*\"supplement\" + 0.011*\"vitamin\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.189): 0.067*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.027*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.115): 0.022*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.018*\"product\" + 0.015*\"pedometer\" + 0.014*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"great\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.157960, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19529314, 0.18715534, 0.1924337, 0.11437142]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.195): 0.020*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.187): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"like\" + 0.015*\"easy\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.192): 0.066*\"product\" + 0.035*\"great\" + 0.029*\"use\" + 0.027*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.011*\"oil\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.114): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.014*\"pain\" + 0.013*\"pedometer\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"great\"\n",
      "INFO : topic diff=0.155190, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20020992, 0.18785389, 0.19151044, 0.11461343]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.200): 0.020*\"help\" + 0.017*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.188): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.192): 0.067*\"product\" + 0.035*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.012*\"price\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.115): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.014*\"pain\" + 0.012*\"week\" + 0.012*\"pedometer\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.151011, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #42000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2048969, 0.1874655, 0.19132914, 0.11475474]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.205): 0.020*\"help\" + 0.017*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.187): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.020*\"product\" + 0.016*\"like\" + 0.014*\"easy\" + 0.014*\"use\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.191): 0.066*\"product\" + 0.034*\"great\" + 0.030*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.012*\"recommend\" + 0.012*\"price\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.115): 0.023*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"try\" + 0.010*\"pedometer\"\n",
      "INFO : topic diff=0.149110, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20697893, 0.18844217, 0.19041695, 0.11595067]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.207): 0.020*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.013*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.190): 0.067*\"product\" + 0.034*\"great\" + 0.031*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"price\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.116): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.149458, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21328223, 0.1881234, 0.18772557, 0.11684494]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.020*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.068*\"product\" + 0.035*\"great\" + 0.031*\"use\" + 0.029*\"good\" + 0.017*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.024*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.148822, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21984485, 0.1883132, 0.18766758, 0.11601531]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.220): 0.020*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.069*\"product\" + 0.036*\"great\" + 0.030*\"use\" + 0.030*\"good\" + 0.017*\"skin\" + 0.015*\"work\" + 0.014*\"love\" + 0.013*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.116): 0.024*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.017*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.136532, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22446886, 0.18594456, 0.18906766, 0.11715781]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.224): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.015*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"year\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.186): 0.032*\"good\" + 0.024*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.069*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.030*\"good\" + 0.016*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.014*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.023*\"day\" + 0.023*\"work\" + 0.018*\"use\" + 0.017*\"pain\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.010*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.146625, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22763981, 0.18724807, 0.18839441, 0.11712882]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.228): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.069*\"product\" + 0.036*\"great\" + 0.032*\"use\" + 0.030*\"good\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.014*\"price\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.023*\"day\" + 0.023*\"work\" + 0.018*\"use\" + 0.017*\"pain\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"start\" + 0.011*\"try\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.141023, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23272769, 0.18646647, 0.18943864, 0.11743854]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.012*\"year\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.186): 0.033*\"good\" + 0.024*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.069*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.030*\"good\" + 0.015*\"skin\" + 0.014*\"price\" + 0.014*\"work\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"hair\"\n",
      "INFO : topic #3 (0.117): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"pain\" + 0.018*\"use\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.137312, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23787811, 0.18779679, 0.18831362, 0.11788799]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.238): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.188): 0.033*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.188): 0.070*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.031*\"good\" + 0.015*\"skin\" + 0.015*\"price\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.023*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.142588, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23718999, 0.19094783, 0.18879527, 0.11786384]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"feel\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.033*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.070*\"product\" + 0.037*\"great\" + 0.031*\"use\" + 0.031*\"good\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.018*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.130692, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24076603, 0.19114582, 0.18952838, 0.11855568]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.241): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.191): 0.032*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.190): 0.070*\"product\" + 0.038*\"great\" + 0.032*\"good\" + 0.031*\"use\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.119): 0.024*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.136184, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24356957, 0.19598214, 0.19123375, 0.11787803]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.244): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.196): 0.032*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.191): 0.070*\"product\" + 0.037*\"great\" + 0.032*\"good\" + 0.031*\"use\" + 0.017*\"price\" + 0.014*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.135668, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24256288, 0.19891818, 0.19216248, 0.11760422]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.243): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.199): 0.032*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.192): 0.070*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.030*\"use\" + 0.017*\"price\" + 0.014*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.133012, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23895404, 0.20425718, 0.19474481, 0.11707886]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.010*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.204): 0.032*\"good\" + 0.026*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.195): 0.070*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.029*\"use\" + 0.018*\"price\" + 0.013*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.024*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.013*\"feel\" + 0.013*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.128591, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23528239, 0.2083039, 0.1967998, 0.11642897]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.235): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.010*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.208): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"product\" + 0.017*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.197): 0.070*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"skin\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"work\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.116): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.013*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.130732, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23970246, 0.20816387, 0.19843741, 0.1173287]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.208): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"product\" + 0.017*\"like\" + 0.013*\"use\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.198): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.117): 0.024*\"day\" + 0.024*\"work\" + 0.016*\"use\" + 0.016*\"pain\" + 0.015*\"product\" + 0.014*\"feel\" + 0.012*\"week\" + 0.011*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.136096, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2392199, 0.21064433, 0.19962873, 0.11766813]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.211): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"like\" + 0.017*\"product\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.200): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.017*\"use\" + 0.017*\"pain\" + 0.015*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.132017, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23943159, 0.21259424, 0.20091364, 0.11853683]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.213): 0.031*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.013*\"easy\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.201): 0.073*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.134634, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23712933, 0.20974047, 0.20717795, 0.11827373]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.210): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.207): 0.070*\"product\" + 0.036*\"great\" + 0.033*\"good\" + 0.029*\"use\" + 0.017*\"price\" + 0.016*\"hair\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.118): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.138794, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2391582, 0.21078502, 0.20954332, 0.11889697]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.211): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.210): 0.071*\"product\" + 0.036*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.015*\"hair\" + 0.013*\"buy\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.134244, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24239142, 0.21119587, 0.21391612, 0.11897726]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.242): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.211): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.014*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.214): 0.071*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.013*\"love\" + 0.013*\"buy\" + 0.013*\"hair\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.132644, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24177086, 0.21242128, 0.21542174, 0.12076033]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.242): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.215): 0.072*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.013*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"hair\"\n",
      "INFO : topic #3 (0.121): 0.025*\"work\" + 0.024*\"day\" + 0.016*\"use\" + 0.016*\"product\" + 0.016*\"pain\" + 0.015*\"feel\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.145183, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24062435, 0.21239218, 0.21611519, 0.12310937]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.241): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.216): 0.074*\"product\" + 0.038*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"hair\"\n",
      "INFO : topic #3 (0.123): 0.025*\"work\" + 0.023*\"day\" + 0.016*\"product\" + 0.016*\"use\" + 0.016*\"feel\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"try\" + 0.012*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.139377, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2403713, 0.21211062, 0.21730782, 0.12612621]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.011*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.217): 0.075*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.027*\"use\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.126): 0.025*\"work\" + 0.023*\"day\" + 0.016*\"feel\" + 0.016*\"product\" + 0.016*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"try\" + 0.011*\"energy\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.143215, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23713008, 0.21013279, 0.22332871, 0.12718305]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.210): 0.030*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.011*\"love\" + 0.011*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.223): 0.074*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.127): 0.025*\"work\" + 0.023*\"day\" + 0.017*\"feel\" + 0.016*\"product\" + 0.016*\"use\" + 0.015*\"pain\" + 0.014*\"week\" + 0.012*\"try\" + 0.011*\"energy\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.143772, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23668925, 0.21377808, 0.22846378, 0.12976378]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.015*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.214): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.228): 0.076*\"product\" + 0.037*\"great\" + 0.032*\"good\" + 0.027*\"use\" + 0.016*\"skin\" + 0.015*\"price\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\" + 0.011*\"work\"\n",
      "INFO : topic #3 (0.130): 0.024*\"work\" + 0.023*\"day\" + 0.019*\"feel\" + 0.017*\"product\" + 0.015*\"use\" + 0.015*\"pain\" + 0.015*\"week\" + 0.013*\"energy\" + 0.012*\"try\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.153638, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23348764, 0.20784597, 0.23483755, 0.13426413]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.019*\"help\" + 0.018*\"product\" + 0.014*\"work\" + 0.014*\"good\" + 0.013*\"supplement\" + 0.012*\"day\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.208): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.013*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.235): 0.075*\"product\" + 0.036*\"great\" + 0.030*\"good\" + 0.027*\"use\" + 0.019*\"skin\" + 0.013*\"price\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\" + 0.011*\"work\"\n",
      "INFO : topic #3 (0.134): 0.024*\"work\" + 0.022*\"day\" + 0.019*\"feel\" + 0.017*\"product\" + 0.015*\"week\" + 0.015*\"use\" + 0.013*\"pain\" + 0.013*\"try\" + 0.013*\"energy\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.154417, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22841106, 0.19824417, 0.2395906, 0.1390768]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.228): 0.020*\"help\" + 0.019*\"product\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.015*\"work\" + 0.013*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.198): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.016*\"easy\" + 0.013*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"fish_oil\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.240): 0.078*\"product\" + 0.038*\"great\" + 0.030*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"price\" + 0.012*\"work\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.139): 0.023*\"work\" + 0.021*\"feel\" + 0.021*\"day\" + 0.019*\"product\" + 0.015*\"pain\" + 0.015*\"week\" + 0.015*\"use\" + 0.014*\"try\" + 0.012*\"start\" + 0.012*\"energy\"\n",
      "INFO : topic diff=0.150953, rho=0.173878\n",
      "INFO : PROGRESS: pass 2, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21458569, 0.19270712, 0.23496395, 0.14824264]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.215): 0.019*\"help\" + 0.019*\"product\" + 0.015*\"good\" + 0.015*\"work\" + 0.014*\"supplement\" + 0.012*\"day\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.193): 0.028*\"good\" + 0.024*\"taste\" + 0.022*\"great\" + 0.021*\"easy\" + 0.018*\"like\" + 0.015*\"product\" + 0.012*\"supplement\" + 0.011*\"vitamin\" + 0.011*\"love\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.235): 0.075*\"product\" + 0.038*\"great\" + 0.031*\"good\" + 0.027*\"use\" + 0.018*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.014*\"price\" + 0.012*\"work\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.148): 0.023*\"day\" + 0.022*\"work\" + 0.020*\"pedometer\" + 0.017*\"use\" + 0.015*\"feel\" + 0.015*\"product\" + 0.012*\"week\" + 0.012*\"try\" + 0.011*\"pain\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.190146, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22231422, 0.19618872, 0.23243958, 0.14582624]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.222): 0.020*\"help\" + 0.018*\"product\" + 0.015*\"work\" + 0.015*\"good\" + 0.013*\"supplement\" + 0.012*\"day\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.196): 0.028*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.018*\"easy\" + 0.018*\"like\" + 0.015*\"product\" + 0.012*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.232): 0.074*\"product\" + 0.038*\"great\" + 0.030*\"good\" + 0.028*\"use\" + 0.018*\"skin\" + 0.014*\"love\" + 0.014*\"price\" + 0.014*\"recommend\" + 0.012*\"work\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.146): 0.024*\"day\" + 0.022*\"work\" + 0.018*\"pedometer\" + 0.017*\"use\" + 0.015*\"feel\" + 0.015*\"product\" + 0.012*\"week\" + 0.012*\"pain\" + 0.011*\"try\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.155764, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23050936, 0.19580041, 0.23232676, 0.14433601]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.196): 0.028*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.017*\"easy\" + 0.015*\"product\" + 0.012*\"vitamin\" + 0.012*\"use\" + 0.011*\"supplement\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.232): 0.073*\"product\" + 0.038*\"great\" + 0.031*\"good\" + 0.028*\"use\" + 0.018*\"skin\" + 0.015*\"price\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.144): 0.024*\"day\" + 0.023*\"work\" + 0.017*\"use\" + 0.016*\"pedometer\" + 0.015*\"feel\" + 0.015*\"product\" + 0.013*\"pain\" + 0.012*\"week\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.139252, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #28000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23451039, 0.19626994, 0.23536308, 0.14315005]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.235): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.011*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.196): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"easy\" + 0.015*\"product\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.235): 0.071*\"product\" + 0.038*\"great\" + 0.031*\"good\" + 0.029*\"use\" + 0.017*\"skin\" + 0.015*\"price\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.024*\"day\" + 0.023*\"work\" + 0.017*\"use\" + 0.015*\"feel\" + 0.014*\"product\" + 0.014*\"pedometer\" + 0.013*\"pain\" + 0.012*\"week\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.136839, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23972242, 0.19666913, 0.23348683, 0.14263166]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.021*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.197): 0.029*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.015*\"product\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.233): 0.072*\"product\" + 0.038*\"great\" + 0.031*\"good\" + 0.029*\"use\" + 0.017*\"skin\" + 0.016*\"price\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.024*\"day\" + 0.023*\"work\" + 0.017*\"use\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"pain\" + 0.012*\"week\" + 0.012*\"pedometer\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.131889, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24492742, 0.19604306, 0.23319668, 0.14213696]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.245): 0.021*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.196): 0.029*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.015*\"easy\" + 0.015*\"product\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.233): 0.071*\"product\" + 0.038*\"great\" + 0.031*\"good\" + 0.030*\"use\" + 0.017*\"skin\" + 0.016*\"price\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.025*\"day\" + 0.023*\"work\" + 0.017*\"use\" + 0.015*\"feel\" + 0.015*\"pain\" + 0.014*\"product\" + 0.013*\"week\" + 0.011*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.131287, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24626581, 0.1967051, 0.23167685, 0.14294389]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.246): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.197): 0.029*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.015*\"product\" + 0.014*\"easy\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.232): 0.072*\"product\" + 0.038*\"great\" + 0.032*\"good\" + 0.030*\"use\" + 0.017*\"skin\" + 0.017*\"price\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.025*\"day\" + 0.024*\"work\" + 0.017*\"use\" + 0.015*\"feel\" + 0.015*\"pain\" + 0.014*\"product\" + 0.013*\"week\" + 0.011*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.131184, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25320399, 0.19580136, 0.22795504, 0.14344348]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.253): 0.020*\"help\" + 0.019*\"product\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.029*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.015*\"product\" + 0.013*\"easy\" + 0.013*\"use\" + 0.010*\"vitamin\" + 0.010*\"love\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.228): 0.073*\"product\" + 0.038*\"great\" + 0.032*\"good\" + 0.030*\"use\" + 0.017*\"price\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.025*\"day\" + 0.025*\"work\" + 0.016*\"use\" + 0.016*\"pain\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"week\" + 0.011*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.130280, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26014662, 0.19556946, 0.22749943, 0.14179344]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.260): 0.020*\"help\" + 0.019*\"product\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.029*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.015*\"product\" + 0.013*\"use\" + 0.013*\"easy\" + 0.011*\"love\" + 0.010*\"vitamin\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.227): 0.073*\"product\" + 0.039*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.016*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.026*\"day\" + 0.024*\"work\" + 0.016*\"use\" + 0.016*\"pain\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"week\" + 0.011*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.121129, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26536697, 0.19318822, 0.22881299, 0.14290427]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.265): 0.020*\"help\" + 0.019*\"product\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.193): 0.029*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.015*\"product\" + 0.013*\"use\" + 0.013*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.229): 0.073*\"product\" + 0.039*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.016*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.026*\"day\" + 0.024*\"work\" + 0.016*\"pain\" + 0.016*\"use\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.129865, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26880488, 0.19444463, 0.22753686, 0.14240426]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.269): 0.020*\"help\" + 0.019*\"product\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.010*\"try\" + 0.010*\"feel\"\n",
      "INFO : topic #1 (0.194): 0.030*\"good\" + 0.027*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.015*\"product\" + 0.015*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"water\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.228): 0.073*\"product\" + 0.039*\"great\" + 0.034*\"good\" + 0.031*\"use\" + 0.018*\"price\" + 0.015*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.014*\"buy\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.026*\"day\" + 0.024*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.125525, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2740505, 0.19319363, 0.22788836, 0.14241326]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.274): 0.020*\"help\" + 0.019*\"product\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.010*\"try\" + 0.010*\"feel\"\n",
      "INFO : topic #1 (0.193): 0.030*\"good\" + 0.027*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"product\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"water\"\n",
      "INFO : topic #2 (0.228): 0.073*\"product\" + 0.039*\"great\" + 0.034*\"good\" + 0.031*\"use\" + 0.019*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.014*\"buy\" + 0.012*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.025*\"day\" + 0.024*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"feel\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.121602, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27901757, 0.19421963, 0.22640878, 0.14246573]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.279): 0.020*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"supplement\" + 0.010*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic #1 (0.194): 0.030*\"good\" + 0.025*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"product\" + 0.014*\"use\" + 0.013*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.226): 0.073*\"product\" + 0.039*\"great\" + 0.035*\"good\" + 0.031*\"use\" + 0.020*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.026*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.016*\"feel\" + 0.016*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.126670, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27717516, 0.19727324, 0.22696939, 0.1418633]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.277): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"supplement\" + 0.010*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic #1 (0.197): 0.030*\"good\" + 0.025*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.015*\"use\" + 0.014*\"product\" + 0.012*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.227): 0.074*\"product\" + 0.040*\"great\" + 0.035*\"good\" + 0.031*\"use\" + 0.020*\"price\" + 0.014*\"skin\" + 0.014*\"buy\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.142): 0.026*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.016*\"feel\" + 0.016*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.116610, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28092802, 0.1977594, 0.22764108, 0.14256048]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.281): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.198): 0.030*\"good\" + 0.025*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.015*\"use\" + 0.014*\"product\" + 0.012*\"easy\" + 0.012*\"vitamin\" + 0.010*\"love\" + 0.009*\"water\"\n",
      "INFO : topic #2 (0.228): 0.073*\"product\" + 0.040*\"great\" + 0.035*\"good\" + 0.031*\"use\" + 0.020*\"price\" + 0.014*\"skin\" + 0.014*\"buy\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.143): 0.026*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.016*\"feel\" + 0.016*\"use\" + 0.013*\"product\" + 0.012*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.120818, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28323939, 0.20267898, 0.22883849, 0.14112218]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.283): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"supplement\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.203): 0.030*\"good\" + 0.025*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"use\" + 0.014*\"product\" + 0.013*\"easy\" + 0.012*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.229): 0.073*\"product\" + 0.040*\"great\" + 0.036*\"good\" + 0.031*\"use\" + 0.021*\"price\" + 0.015*\"buy\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.141): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"pain\" + 0.016*\"feel\" + 0.016*\"use\" + 0.013*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.122365, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28135812, 0.2058349, 0.22942612, 0.14030412]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.281): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.206): 0.030*\"good\" + 0.027*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"product\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.013*\"use\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.229): 0.074*\"product\" + 0.040*\"great\" + 0.036*\"good\" + 0.030*\"use\" + 0.021*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.140): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"pain\" + 0.017*\"feel\" + 0.015*\"use\" + 0.013*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.119941, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27598378, 0.21219967, 0.23192279, 0.13940133]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.276): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.028*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.014*\"product\" + 0.013*\"easy\" + 0.013*\"use\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.232): 0.074*\"product\" + 0.040*\"great\" + 0.037*\"good\" + 0.029*\"use\" + 0.022*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.139): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"feel\" + 0.017*\"pain\" + 0.015*\"use\" + 0.013*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.116512, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27137631, 0.21665281, 0.23407607, 0.13821967]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.271): 0.020*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"year\" + 0.012*\"use\" + 0.010*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.217): 0.030*\"good\" + 0.029*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.013*\"product\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.234): 0.074*\"product\" + 0.040*\"great\" + 0.037*\"good\" + 0.029*\"use\" + 0.022*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.014*\"skin\" + 0.013*\"recommend\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.138): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.013*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.117771, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27596006, 0.21651216, 0.23604231, 0.13898662]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.276): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"year\" + 0.012*\"use\" + 0.010*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.217): 0.030*\"good\" + 0.029*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.013*\"product\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.236): 0.075*\"product\" + 0.041*\"great\" + 0.038*\"good\" + 0.029*\"use\" + 0.022*\"price\" + 0.016*\"buy\" + 0.015*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.139): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.015*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"try\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.122855, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27506191, 0.21932158, 0.23719278, 0.13914813]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.275): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.219): 0.030*\"good\" + 0.029*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.237): 0.076*\"product\" + 0.041*\"great\" + 0.038*\"good\" + 0.029*\"use\" + 0.022*\"price\" + 0.016*\"buy\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.139): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.118676, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27506787, 0.22142692, 0.23850569, 0.13976122]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.275): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.221): 0.029*\"good\" + 0.029*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"product\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.239): 0.076*\"product\" + 0.041*\"great\" + 0.038*\"good\" + 0.030*\"use\" + 0.021*\"price\" + 0.016*\"buy\" + 0.015*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.140): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.121004, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2718482, 0.21880072, 0.24551004, 0.13934395]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.272): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.219): 0.029*\"good\" + 0.028*\"taste\" + 0.019*\"great\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.246): 0.073*\"product\" + 0.039*\"great\" + 0.036*\"good\" + 0.029*\"use\" + 0.020*\"price\" + 0.016*\"hair\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.139): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"week\" + 0.014*\"product\" + 0.013*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.125696, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27394632, 0.22000165, 0.24795429, 0.13991594]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.274): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.220): 0.029*\"good\" + 0.028*\"taste\" + 0.019*\"great\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.248): 0.074*\"product\" + 0.039*\"great\" + 0.037*\"good\" + 0.028*\"use\" + 0.020*\"price\" + 0.015*\"buy\" + 0.015*\"hair\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.140): 0.026*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"week\" + 0.014*\"product\" + 0.013*\"start\" + 0.012*\"help\" + 0.012*\"try\"\n",
      "INFO : topic diff=0.121559, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27730432, 0.22048528, 0.25230038, 0.1399399]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.277): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.012*\"day\" + 0.012*\"use\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.220): 0.029*\"good\" + 0.028*\"taste\" + 0.020*\"great\" + 0.017*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.013*\"product\" + 0.011*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.252): 0.074*\"product\" + 0.039*\"great\" + 0.036*\"good\" + 0.028*\"use\" + 0.020*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"skin\" + 0.013*\"hair\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.140): 0.026*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"week\" + 0.013*\"start\" + 0.012*\"help\" + 0.012*\"try\"\n",
      "INFO : topic diff=0.119832, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27591929, 0.22176954, 0.25395092, 0.14188725]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.276): 0.019*\"help\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.012*\"use\" + 0.012*\"day\" + 0.011*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.222): 0.028*\"good\" + 0.028*\"taste\" + 0.020*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.013*\"product\" + 0.011*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.254): 0.075*\"product\" + 0.039*\"great\" + 0.036*\"good\" + 0.028*\"use\" + 0.020*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.012*\"hair\"\n",
      "INFO : topic #3 (0.142): 0.025*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.015*\"use\" + 0.015*\"pain\" + 0.014*\"product\" + 0.014*\"week\" + 0.013*\"start\" + 0.013*\"energy\" + 0.012*\"try\"\n",
      "INFO : topic diff=0.132182, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27353805, 0.22166435, 0.25467801, 0.14466748]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.274): 0.019*\"product\" + 0.019*\"help\" + 0.016*\"work\" + 0.014*\"good\" + 0.012*\"use\" + 0.012*\"day\" + 0.011*\"year\" + 0.011*\"recommend\" + 0.010*\"supplement\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.222): 0.029*\"taste\" + 0.028*\"good\" + 0.020*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.013*\"product\" + 0.011*\"love\" + 0.011*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.255): 0.077*\"product\" + 0.040*\"great\" + 0.036*\"good\" + 0.027*\"use\" + 0.020*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.145): 0.026*\"work\" + 0.025*\"day\" + 0.019*\"feel\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"pain\" + 0.014*\"week\" + 0.013*\"energy\" + 0.012*\"start\" + 0.012*\"try\"\n",
      "INFO : topic diff=0.126809, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27296397, 0.22152685, 0.25601372, 0.14818649]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.273): 0.019*\"product\" + 0.019*\"help\" + 0.016*\"work\" + 0.014*\"good\" + 0.012*\"use\" + 0.012*\"day\" + 0.011*\"year\" + 0.011*\"supplement\" + 0.010*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.222): 0.028*\"good\" + 0.028*\"taste\" + 0.020*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"product\" + 0.011*\"love\" + 0.011*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.256): 0.078*\"product\" + 0.041*\"great\" + 0.037*\"good\" + 0.027*\"use\" + 0.019*\"price\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"skin\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.148): 0.026*\"work\" + 0.025*\"day\" + 0.020*\"feel\" + 0.015*\"product\" + 0.015*\"use\" + 0.014*\"pain\" + 0.014*\"week\" + 0.013*\"energy\" + 0.012*\"try\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.130719, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26851949, 0.22013901, 0.26231906, 0.14949369]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.269): 0.019*\"product\" + 0.019*\"help\" + 0.015*\"work\" + 0.014*\"good\" + 0.012*\"use\" + 0.012*\"day\" + 0.011*\"supplement\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.220): 0.028*\"taste\" + 0.028*\"good\" + 0.020*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"product\" + 0.011*\"love\" + 0.011*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.262): 0.077*\"product\" + 0.039*\"great\" + 0.036*\"good\" + 0.028*\"use\" + 0.018*\"price\" + 0.015*\"skin\" + 0.014*\"buy\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.149): 0.025*\"work\" + 0.025*\"day\" + 0.021*\"feel\" + 0.015*\"product\" + 0.015*\"use\" + 0.014*\"week\" + 0.014*\"pain\" + 0.013*\"energy\" + 0.013*\"try\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.131093, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26721358, 0.22406621, 0.26820895, 0.15265098]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.267): 0.019*\"product\" + 0.019*\"help\" + 0.015*\"work\" + 0.014*\"good\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"day\" + 0.010*\"recommend\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.224): 0.027*\"taste\" + 0.026*\"good\" + 0.019*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.013*\"product\" + 0.012*\"supplement\" + 0.010*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.268): 0.079*\"product\" + 0.040*\"great\" + 0.035*\"good\" + 0.027*\"use\" + 0.017*\"price\" + 0.015*\"skin\" + 0.013*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.153): 0.024*\"work\" + 0.024*\"day\" + 0.022*\"feel\" + 0.015*\"product\" + 0.015*\"week\" + 0.015*\"energy\" + 0.014*\"use\" + 0.014*\"pain\" + 0.013*\"try\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.141649, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26270211, 0.21797508, 0.2756969, 0.15836807]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.263): 0.019*\"product\" + 0.019*\"help\" + 0.014*\"work\" + 0.014*\"good\" + 0.013*\"supplement\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"recommend\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.218): 0.026*\"taste\" + 0.026*\"good\" + 0.019*\"great\" + 0.018*\"like\" + 0.016*\"easy\" + 0.013*\"vitamin\" + 0.013*\"product\" + 0.012*\"supplement\" + 0.010*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.276): 0.078*\"product\" + 0.038*\"great\" + 0.033*\"good\" + 0.026*\"use\" + 0.019*\"skin\" + 0.015*\"price\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.158): 0.024*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.016*\"week\" + 0.016*\"product\" + 0.014*\"energy\" + 0.014*\"use\" + 0.014*\"try\" + 0.013*\"start\" + 0.012*\"pain\"\n",
      "INFO : topic diff=0.142422, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25593588, 0.20855919, 0.28067058, 0.16415279]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.256): 0.020*\"help\" + 0.020*\"product\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.014*\"work\" + 0.011*\"use\" + 0.011*\"day\" + 0.011*\"recommend\" + 0.010*\"feel\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.209): 0.027*\"taste\" + 0.026*\"good\" + 0.020*\"great\" + 0.018*\"like\" + 0.016*\"easy\" + 0.013*\"supplement\" + 0.013*\"product\" + 0.013*\"vitamin\" + 0.012*\"fish_oil\" + 0.011*\"pill\"\n",
      "INFO : topic #2 (0.281): 0.081*\"product\" + 0.040*\"great\" + 0.033*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.015*\"price\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.012*\"buy\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.164): 0.025*\"feel\" + 0.023*\"work\" + 0.022*\"day\" + 0.017*\"product\" + 0.015*\"week\" + 0.014*\"try\" + 0.014*\"pain\" + 0.014*\"energy\" + 0.014*\"use\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.139095, rho=0.171308\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=4, decay=0.5, chunksize=7000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.16684973066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217520/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 1/1 [11:12<00:00, 672.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_4.html\n",
      "CPU times: user 11min 12s, sys: 1.35 s, total: 11min 13s\n",
      "Wall time: 11min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [4]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 3\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n",
      "Calculated coherence score...:  -1.94634373959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 1/1 [06:40<00:00, 400.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_4.html\n",
      "CPU times: user 6min 53s, sys: 25.9 s, total: 7min 19s\n",
      "Wall time: 6min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# setting alpha and eta manually\n",
    "# Set training parameters.\n",
    "num_topics_list = [4]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 5\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = 0.5e-2\n",
    "alpha = 1e-2\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda = LdaMulticore(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667]\n",
      "INFO : using symmetric eta at 0.16666666666666666\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 6 topics, 2 passes over the supplied corpus of 217530 documents, updating model once every 5000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4885/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.081580229, 0.082519785, 0.10786959, 0.11670941, 0.10751451, 0.071371369]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #5 (0.071): 0.029*\"work\" + 0.020*\"great\" + 0.016*\"use\" + 0.015*\"good\" + 0.013*\"product\" + 0.012*\"try\" + 0.011*\"buy\" + 0.008*\"year\" + 0.008*\"help\" + 0.008*\"easy\"\n",
      "INFO : topic #0 (0.082): 0.025*\"pedometer\" + 0.021*\"step\" + 0.019*\"day\" + 0.018*\"like\" + 0.015*\"good\" + 0.015*\"great\" + 0.015*\"use\" + 0.014*\"work\" + 0.011*\"walk\" + 0.010*\"product\"\n",
      "INFO : topic #4 (0.108): 0.042*\"pedometer\" + 0.030*\"use\" + 0.022*\"step\" + 0.020*\"easy\" + 0.019*\"day\" + 0.017*\"great\" + 0.017*\"omron\" + 0.016*\"love\" + 0.015*\"good\" + 0.015*\"pocket\"\n",
      "INFO : topic #2 (0.108): 0.035*\"pedometer\" + 0.025*\"product\" + 0.024*\"good\" + 0.022*\"use\" + 0.018*\"great\" + 0.015*\"work\" + 0.014*\"love\" + 0.013*\"walk\" + 0.013*\"day\" + 0.012*\"step\"\n",
      "INFO : topic #3 (0.117): 0.031*\"pedometer\" + 0.025*\"day\" + 0.025*\"use\" + 0.025*\"product\" + 0.020*\"great\" + 0.018*\"good\" + 0.018*\"step\" + 0.014*\"work\" + 0.012*\"like\" + 0.011*\"walk\"\n",
      "INFO : topic diff=7.882243, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4993/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.08214917, 0.082063362, 0.10822404, 0.099500418, 0.093595825, 0.083547436]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #1 (0.082): 0.024*\"great\" + 0.023*\"product\" + 0.022*\"good\" + 0.019*\"taste\" + 0.015*\"use\" + 0.013*\"work\" + 0.012*\"like\" + 0.011*\"pedometer\" + 0.011*\"recommend\" + 0.010*\"price\"\n",
      "INFO : topic #0 (0.082): 0.018*\"day\" + 0.018*\"good\" + 0.016*\"like\" + 0.013*\"product\" + 0.012*\"use\" + 0.012*\"great\" + 0.011*\"help\" + 0.010*\"work\" + 0.008*\"supplement\" + 0.008*\"recommend\"\n",
      "INFO : topic #4 (0.094): 0.048*\"pedometer\" + 0.028*\"use\" + 0.024*\"step\" + 0.021*\"easy\" + 0.020*\"day\" + 0.017*\"great\" + 0.016*\"love\" + 0.015*\"omron\" + 0.015*\"good\" + 0.014*\"pocket\"\n",
      "INFO : topic #3 (0.100): 0.028*\"product\" + 0.025*\"day\" + 0.024*\"use\" + 0.023*\"pedometer\" + 0.019*\"great\" + 0.019*\"good\" + 0.014*\"step\" + 0.014*\"work\" + 0.012*\"time\" + 0.011*\"like\"\n",
      "INFO : topic #2 (0.108): 0.035*\"product\" + 0.026*\"good\" + 0.025*\"use\" + 0.018*\"great\" + 0.014*\"pedometer\" + 0.012*\"work\" + 0.012*\"love\" + 0.012*\"recommend\" + 0.011*\"day\" + 0.011*\"skin\"\n",
      "INFO : topic diff=1.392192, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089187667, 0.087025493, 0.11075857, 0.092349991, 0.077964678, 0.10174695]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.041*\"pedometer\" + 0.028*\"use\" + 0.021*\"step\" + 0.020*\"day\" + 0.019*\"easy\" + 0.016*\"great\" + 0.015*\"love\" + 0.015*\"good\" + 0.013*\"omron\" + 0.013*\"work\"\n",
      "INFO : topic #1 (0.087): 0.029*\"product\" + 0.027*\"great\" + 0.025*\"good\" + 0.022*\"taste\" + 0.015*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.012*\"work\" + 0.010*\"recommend\" + 0.010*\"find\"\n",
      "INFO : topic #3 (0.092): 0.032*\"product\" + 0.025*\"day\" + 0.024*\"use\" + 0.019*\"good\" + 0.019*\"great\" + 0.014*\"work\" + 0.013*\"pedometer\" + 0.011*\"time\" + 0.011*\"like\" + 0.008*\"step\"\n",
      "INFO : topic #5 (0.102): 0.020*\"work\" + 0.020*\"product\" + 0.018*\"good\" + 0.015*\"great\" + 0.015*\"help\" + 0.015*\"use\" + 0.012*\"try\" + 0.011*\"feel\" + 0.011*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.111): 0.042*\"product\" + 0.028*\"good\" + 0.025*\"use\" + 0.019*\"great\" + 0.013*\"love\" + 0.012*\"work\" + 0.012*\"skin\" + 0.012*\"recommend\" + 0.010*\"year\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.962333, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4998/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093232781, 0.090508245, 0.11662935, 0.08940839, 0.070322976, 0.11885891]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.070): 0.033*\"pedometer\" + 0.028*\"use\" + 0.020*\"day\" + 0.018*\"easy\" + 0.017*\"step\" + 0.015*\"great\" + 0.015*\"love\" + 0.014*\"good\" + 0.013*\"work\" + 0.012*\"product\"\n",
      "INFO : topic #3 (0.089): 0.031*\"product\" + 0.026*\"day\" + 0.024*\"use\" + 0.018*\"good\" + 0.017*\"great\" + 0.015*\"work\" + 0.012*\"time\" + 0.010*\"like\" + 0.009*\"pain\" + 0.008*\"recommend\"\n",
      "INFO : topic #0 (0.093): 0.018*\"good\" + 0.017*\"day\" + 0.014*\"product\" + 0.014*\"help\" + 0.013*\"like\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"great\" + 0.009*\"work\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.045*\"product\" + 0.028*\"use\" + 0.027*\"good\" + 0.020*\"great\" + 0.015*\"skin\" + 0.012*\"love\" + 0.012*\"work\" + 0.011*\"recommend\" + 0.010*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.119): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"good\" + 0.016*\"help\" + 0.015*\"use\" + 0.014*\"great\" + 0.012*\"try\" + 0.011*\"feel\" + 0.011*\"year\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.641621, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095448039, 0.096145362, 0.12530029, 0.088130891, 0.065539673, 0.13179821]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.066): 0.028*\"use\" + 0.025*\"pedometer\" + 0.018*\"day\" + 0.017*\"easy\" + 0.015*\"love\" + 0.015*\"great\" + 0.013*\"good\" + 0.013*\"work\" + 0.013*\"step\" + 0.011*\"product\"\n",
      "INFO : topic #3 (0.088): 0.029*\"product\" + 0.024*\"use\" + 0.024*\"day\" + 0.016*\"great\" + 0.016*\"good\" + 0.016*\"work\" + 0.012*\"time\" + 0.011*\"pain\" + 0.010*\"like\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.096): 0.034*\"product\" + 0.032*\"great\" + 0.032*\"good\" + 0.020*\"taste\" + 0.016*\"use\" + 0.015*\"price\" + 0.015*\"like\" + 0.012*\"vitamin\" + 0.010*\"find\" + 0.010*\"work\"\n",
      "INFO : topic #2 (0.125): 0.044*\"product\" + 0.031*\"use\" + 0.025*\"good\" + 0.023*\"great\" + 0.015*\"skin\" + 0.015*\"oil\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"mouse\" + 0.010*\"recommend\"\n",
      "INFO : topic #5 (0.132): 0.022*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.016*\"good\" + 0.015*\"use\" + 0.014*\"great\" + 0.012*\"try\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.480036, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10159654, 0.10123654, 0.1262411, 0.088439733, 0.062454596, 0.1467755]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.027*\"use\" + 0.019*\"day\" + 0.017*\"pedometer\" + 0.015*\"easy\" + 0.015*\"love\" + 0.014*\"great\" + 0.013*\"work\" + 0.012*\"good\" + 0.011*\"product\" + 0.009*\"step\"\n",
      "INFO : topic #3 (0.088): 0.029*\"product\" + 0.025*\"day\" + 0.023*\"use\" + 0.016*\"work\" + 0.015*\"good\" + 0.014*\"great\" + 0.012*\"pain\" + 0.012*\"time\" + 0.009*\"like\" + 0.008*\"help\"\n",
      "INFO : topic #0 (0.102): 0.017*\"good\" + 0.015*\"help\" + 0.015*\"day\" + 0.014*\"product\" + 0.011*\"supplement\" + 0.011*\"like\" + 0.011*\"use\" + 0.009*\"great\" + 0.009*\"recommend\" + 0.008*\"work\"\n",
      "INFO : topic #2 (0.126): 0.050*\"product\" + 0.030*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"oil\" + 0.011*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #5 (0.147): 0.022*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.016*\"good\" + 0.015*\"use\" + 0.013*\"feel\" + 0.013*\"great\" + 0.012*\"try\" + 0.011*\"year\" + 0.010*\"start\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.435455, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10611774, 0.10967626, 0.12968247, 0.088895626, 0.060573161, 0.15795206]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.061): 0.026*\"use\" + 0.018*\"day\" + 0.014*\"easy\" + 0.014*\"love\" + 0.013*\"great\" + 0.012*\"work\" + 0.011*\"pedometer\" + 0.011*\"good\" + 0.010*\"product\" + 0.008*\"purchase\"\n",
      "INFO : topic #3 (0.089): 0.028*\"product\" + 0.024*\"day\" + 0.023*\"use\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"great\" + 0.011*\"time\" + 0.011*\"pain\" + 0.009*\"like\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.110): 0.037*\"product\" + 0.036*\"good\" + 0.033*\"great\" + 0.022*\"taste\" + 0.018*\"price\" + 0.015*\"use\" + 0.015*\"like\" + 0.012*\"vitamin\" + 0.011*\"find\" + 0.010*\"recommend\"\n",
      "INFO : topic #2 (0.130): 0.049*\"product\" + 0.030*\"use\" + 0.025*\"good\" + 0.024*\"great\" + 0.016*\"skin\" + 0.014*\"love\" + 0.012*\"work\" + 0.012*\"oil\" + 0.011*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #5 (0.158): 0.022*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.015*\"good\" + 0.014*\"use\" + 0.014*\"feel\" + 0.013*\"try\" + 0.012*\"great\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.392525, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11090781, 0.11734419, 0.13220033, 0.090926334, 0.059298549, 0.1644734]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.026*\"use\" + 0.017*\"day\" + 0.013*\"love\" + 0.013*\"dry_eye\" + 0.012*\"easy\" + 0.012*\"work\" + 0.011*\"great\" + 0.010*\"good\" + 0.009*\"product\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #3 (0.091): 0.028*\"product\" + 0.024*\"day\" + 0.021*\"use\" + 0.019*\"pain\" + 0.016*\"work\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"time\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic #1 (0.117): 0.038*\"good\" + 0.037*\"product\" + 0.033*\"great\" + 0.023*\"taste\" + 0.019*\"price\" + 0.015*\"use\" + 0.014*\"like\" + 0.011*\"vitamin\" + 0.011*\"find\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.132): 0.050*\"product\" + 0.031*\"use\" + 0.025*\"good\" + 0.023*\"great\" + 0.018*\"skin\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"oil\" + 0.010*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #5 (0.164): 0.022*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.015*\"good\" + 0.014*\"use\" + 0.014*\"feel\" + 0.013*\"try\" + 0.012*\"year\" + 0.012*\"great\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.361759, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11617849, 0.12463831, 0.13679545, 0.092752576, 0.058343943, 0.17461607]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.027*\"use\" + 0.017*\"day\" + 0.013*\"love\" + 0.012*\"easy\" + 0.011*\"work\" + 0.010*\"great\" + 0.010*\"dry_eye\" + 0.009*\"good\" + 0.009*\"product\" + 0.009*\"tablespoon\"\n",
      "INFO : topic #3 (0.093): 0.028*\"product\" + 0.025*\"day\" + 0.022*\"use\" + 0.020*\"pain\" + 0.016*\"work\" + 0.013*\"good\" + 0.011*\"time\" + 0.011*\"great\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic #1 (0.125): 0.040*\"good\" + 0.038*\"product\" + 0.034*\"great\" + 0.027*\"taste\" + 0.018*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"find\" + 0.010*\"love\" + 0.010*\"buy\"\n",
      "INFO : topic #2 (0.137): 0.052*\"product\" + 0.033*\"use\" + 0.024*\"good\" + 0.024*\"great\" + 0.018*\"skin\" + 0.016*\"oil\" + 0.014*\"love\" + 0.012*\"work\" + 0.010*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.175): 0.023*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.015*\"use\" + 0.015*\"good\" + 0.015*\"try\" + 0.014*\"feel\" + 0.012*\"day\" + 0.012*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.317767, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12157632, 0.13210043, 0.13733187, 0.096879184, 0.057329893, 0.17983107]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.025*\"use\" + 0.016*\"day\" + 0.012*\"love\" + 0.011*\"easy\" + 0.011*\"work\" + 0.009*\"great\" + 0.009*\"good\" + 0.008*\"product\" + 0.008*\"dry_eye\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #3 (0.097): 0.028*\"product\" + 0.024*\"day\" + 0.021*\"use\" + 0.018*\"work\" + 0.018*\"pain\" + 0.013*\"good\" + 0.011*\"time\" + 0.011*\"great\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic #1 (0.132): 0.041*\"good\" + 0.039*\"product\" + 0.035*\"great\" + 0.027*\"taste\" + 0.017*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"find\" + 0.011*\"vitamin\" + 0.010*\"buy\"\n",
      "INFO : topic #2 (0.137): 0.053*\"product\" + 0.033*\"use\" + 0.025*\"great\" + 0.024*\"good\" + 0.019*\"skin\" + 0.014*\"oil\" + 0.014*\"love\" + 0.012*\"work\" + 0.010*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.180): 0.024*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.015*\"try\" + 0.015*\"use\" + 0.015*\"good\" + 0.015*\"feel\" + 0.013*\"day\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.318124, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12937367, 0.13811581, 0.13767362, 0.10036745, 0.05686703, 0.18874297]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.023*\"use\" + 0.016*\"day\" + 0.013*\"bladder\" + 0.012*\"love\" + 0.011*\"work\" + 0.011*\"easy\" + 0.010*\"tablespoon\" + 0.008*\"great\" + 0.008*\"good\" + 0.008*\"product\"\n",
      "INFO : topic #3 (0.100): 0.026*\"product\" + 0.024*\"day\" + 0.020*\"pain\" + 0.020*\"use\" + 0.018*\"work\" + 0.012*\"good\" + 0.011*\"time\" + 0.010*\"year\" + 0.010*\"great\" + 0.010*\"help\"\n",
      "INFO : topic #2 (0.138): 0.055*\"product\" + 0.033*\"use\" + 0.025*\"great\" + 0.024*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.013*\"oil\" + 0.012*\"work\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.138): 0.042*\"good\" + 0.039*\"product\" + 0.034*\"great\" + 0.027*\"taste\" + 0.018*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"find\" + 0.010*\"vitamin\" + 0.010*\"buy\"\n",
      "INFO : topic #5 (0.189): 0.025*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.015*\"try\" + 0.015*\"use\" + 0.014*\"feel\" + 0.014*\"good\" + 0.014*\"day\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.291290, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13400745, 0.14391246, 0.14143509, 0.10067084, 0.056245688, 0.19316404]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.023*\"use\" + 0.015*\"day\" + 0.012*\"bladder\" + 0.011*\"love\" + 0.011*\"tablespoon\" + 0.010*\"work\" + 0.010*\"easy\" + 0.008*\"product\" + 0.008*\"great\" + 0.008*\"good\"\n",
      "INFO : topic #3 (0.101): 0.026*\"product\" + 0.024*\"day\" + 0.020*\"use\" + 0.019*\"pain\" + 0.018*\"work\" + 0.012*\"good\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"great\"\n",
      "INFO : topic #2 (0.141): 0.056*\"product\" + 0.034*\"use\" + 0.026*\"great\" + 0.024*\"good\" + 0.019*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.012*\"oil\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.144): 0.044*\"good\" + 0.040*\"product\" + 0.035*\"great\" + 0.026*\"taste\" + 0.019*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"find\" + 0.011*\"brand\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.193): 0.026*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.015*\"try\" + 0.015*\"feel\" + 0.015*\"use\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.251383, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13935791, 0.14799905, 0.14442571, 0.10349681, 0.05606015, 0.20089561]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.056): 0.022*\"use\" + 0.014*\"day\" + 0.010*\"tablespoon\" + 0.010*\"love\" + 0.010*\"bladder\" + 0.010*\"work\" + 0.009*\"easy\" + 0.007*\"product\" + 0.007*\"good\" + 0.007*\"great\"\n",
      "INFO : topic #3 (0.103): 0.026*\"product\" + 0.023*\"day\" + 0.021*\"pain\" + 0.019*\"use\" + 0.018*\"work\" + 0.012*\"good\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"time\" + 0.009*\"great\"\n",
      "INFO : topic #2 (0.144): 0.057*\"product\" + 0.033*\"use\" + 0.026*\"great\" + 0.024*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.012*\"oil\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.148): 0.045*\"good\" + 0.040*\"product\" + 0.035*\"great\" + 0.025*\"taste\" + 0.019*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"find\" + 0.011*\"brand\" + 0.011*\"love\"\n",
      "INFO : topic #5 (0.201): 0.025*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.016*\"feel\" + 0.015*\"try\" + 0.015*\"day\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.247012, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14455867, 0.15345137, 0.14870495, 0.10599019, 0.056211304, 0.20756878]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.026*\"camera\" + 0.021*\"use\" + 0.012*\"day\" + 0.009*\"love\" + 0.009*\"easy\" + 0.008*\"tablespoon\" + 0.008*\"bladder\" + 0.008*\"work\" + 0.007*\"great\" + 0.007*\"good\"\n",
      "INFO : topic #3 (0.106): 0.026*\"product\" + 0.024*\"day\" + 0.022*\"pain\" + 0.018*\"work\" + 0.018*\"use\" + 0.011*\"good\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"time\" + 0.010*\"week\"\n",
      "INFO : topic #2 (0.149): 0.058*\"product\" + 0.033*\"use\" + 0.027*\"great\" + 0.024*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.011*\"recommend\" + 0.010*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.153): 0.045*\"good\" + 0.040*\"product\" + 0.035*\"great\" + 0.024*\"taste\" + 0.019*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.012*\"find\" + 0.011*\"brand\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.208): 0.026*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.016*\"day\" + 0.016*\"feel\" + 0.015*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.267547, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14935628, 0.15967555, 0.15063471, 0.10770284, 0.05611096, 0.21297486]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.021*\"use\" + 0.020*\"camera\" + 0.012*\"day\" + 0.009*\"easy\" + 0.009*\"love\" + 0.008*\"tablespoon\" + 0.008*\"bladder\" + 0.008*\"work\" + 0.007*\"good\" + 0.007*\"great\"\n",
      "INFO : topic #3 (0.108): 0.025*\"product\" + 0.023*\"day\" + 0.023*\"pain\" + 0.018*\"use\" + 0.018*\"work\" + 0.011*\"year\" + 0.011*\"good\" + 0.011*\"help\" + 0.010*\"week\" + 0.009*\"time\"\n",
      "INFO : topic #2 (0.151): 0.058*\"product\" + 0.035*\"use\" + 0.027*\"great\" + 0.024*\"good\" + 0.016*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.011*\"recommend\" + 0.011*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.160): 0.045*\"good\" + 0.039*\"product\" + 0.034*\"great\" + 0.025*\"taste\" + 0.018*\"price\" + 0.017*\"use\" + 0.016*\"like\" + 0.012*\"brand\" + 0.012*\"find\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.213): 0.026*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.016*\"day\" + 0.016*\"feel\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.223525, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15295318, 0.16705748, 0.15544912, 0.11078174, 0.056038421, 0.21717027]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.021*\"use\" + 0.016*\"camera\" + 0.011*\"day\" + 0.010*\"tablespoon\" + 0.009*\"easy\" + 0.008*\"love\" + 0.007*\"bladder\" + 0.007*\"work\" + 0.006*\"good\" + 0.006*\"great\"\n",
      "INFO : topic #3 (0.111): 0.026*\"product\" + 0.024*\"pain\" + 0.022*\"day\" + 0.018*\"use\" + 0.018*\"work\" + 0.011*\"good\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"week\" + 0.009*\"time\"\n",
      "INFO : topic #2 (0.155): 0.059*\"product\" + 0.037*\"use\" + 0.028*\"great\" + 0.023*\"good\" + 0.018*\"skin\" + 0.013*\"love\" + 0.013*\"work\" + 0.012*\"oil\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.167): 0.046*\"good\" + 0.038*\"product\" + 0.034*\"great\" + 0.026*\"taste\" + 0.018*\"price\" + 0.017*\"use\" + 0.016*\"like\" + 0.012*\"brand\" + 0.012*\"find\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.217): 0.026*\"work\" + 0.017*\"product\" + 0.016*\"help\" + 0.016*\"day\" + 0.016*\"feel\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.216942, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16103911, 0.17455006, 0.15824018, 0.11241518, 0.056121964, 0.22356686]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.019*\"use\" + 0.012*\"camera\" + 0.010*\"day\" + 0.009*\"tablespoon\" + 0.008*\"easy\" + 0.007*\"love\" + 0.007*\"bladder\" + 0.007*\"work\" + 0.006*\"good\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.112): 0.025*\"product\" + 0.024*\"pain\" + 0.022*\"day\" + 0.018*\"use\" + 0.017*\"work\" + 0.011*\"help\" + 0.010*\"good\" + 0.010*\"year\" + 0.010*\"week\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.161): 0.018*\"supplement\" + 0.016*\"help\" + 0.014*\"good\" + 0.014*\"product\" + 0.010*\"recommend\" + 0.010*\"day\" + 0.008*\"doctor\" + 0.008*\"use\" + 0.008*\"body\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.175): 0.047*\"good\" + 0.038*\"product\" + 0.033*\"great\" + 0.025*\"taste\" + 0.019*\"price\" + 0.016*\"use\" + 0.016*\"like\" + 0.013*\"brand\" + 0.012*\"find\" + 0.011*\"vitamin\"\n",
      "INFO : topic #5 (0.224): 0.026*\"work\" + 0.017*\"help\" + 0.017*\"feel\" + 0.017*\"day\" + 0.017*\"product\" + 0.016*\"try\" + 0.014*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.213211, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16821957, 0.18110827, 0.16027606, 0.11477552, 0.056101177, 0.22432813]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.018*\"use\" + 0.010*\"day\" + 0.009*\"camera\" + 0.008*\"tablespoon\" + 0.008*\"easy\" + 0.008*\"eye\" + 0.007*\"love\" + 0.006*\"bladder\" + 0.006*\"work\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.115): 0.025*\"pain\" + 0.025*\"product\" + 0.022*\"day\" + 0.017*\"use\" + 0.017*\"work\" + 0.011*\"help\" + 0.010*\"year\" + 0.010*\"week\" + 0.010*\"good\" + 0.010*\"joint\"\n",
      "INFO : topic #0 (0.168): 0.020*\"supplement\" + 0.016*\"help\" + 0.014*\"good\" + 0.013*\"product\" + 0.010*\"recommend\" + 0.010*\"day\" + 0.008*\"health\" + 0.008*\"doctor\" + 0.008*\"body\" + 0.008*\"use\"\n",
      "INFO : topic #1 (0.181): 0.046*\"good\" + 0.037*\"product\" + 0.034*\"great\" + 0.024*\"taste\" + 0.020*\"price\" + 0.016*\"use\" + 0.016*\"like\" + 0.013*\"brand\" + 0.012*\"vitamin\" + 0.012*\"find\"\n",
      "INFO : topic #5 (0.224): 0.027*\"work\" + 0.017*\"feel\" + 0.017*\"day\" + 0.017*\"help\" + 0.016*\"product\" + 0.016*\"try\" + 0.014*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.203239, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17395775, 0.19135794, 0.16083758, 0.11714975, 0.056189675, 0.22710218]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.017*\"use\" + 0.010*\"day\" + 0.010*\"tablespoon\" + 0.009*\"seed\" + 0.008*\"eye\" + 0.007*\"camera\" + 0.007*\"easy\" + 0.006*\"love\" + 0.006*\"second_bottle\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.117): 0.026*\"pain\" + 0.024*\"product\" + 0.023*\"day\" + 0.017*\"use\" + 0.017*\"work\" + 0.011*\"help\" + 0.010*\"week\" + 0.010*\"good\" + 0.010*\"year\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.174): 0.020*\"supplement\" + 0.016*\"help\" + 0.014*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.008*\"body\" + 0.008*\"health\" + 0.008*\"doctor\" + 0.008*\"use\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.191): 0.047*\"good\" + 0.037*\"product\" + 0.035*\"great\" + 0.024*\"taste\" + 0.020*\"price\" + 0.016*\"use\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"brand\" + 0.011*\"find\"\n",
      "INFO : topic #5 (0.227): 0.027*\"work\" + 0.019*\"feel\" + 0.019*\"day\" + 0.017*\"help\" + 0.016*\"product\" + 0.015*\"try\" + 0.014*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.179252, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17590527, 0.19929582, 0.1652644, 0.11814775, 0.056747705, 0.22837983]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.018*\"use\" + 0.017*\"seed\" + 0.017*\"coal\" + 0.010*\"tablespoon\" + 0.010*\"eye\" + 0.010*\"day\" + 0.007*\"light\" + 0.006*\"easy\" + 0.006*\"love\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.118): 0.025*\"pain\" + 0.024*\"product\" + 0.022*\"day\" + 0.018*\"use\" + 0.017*\"work\" + 0.011*\"help\" + 0.010*\"good\" + 0.010*\"week\" + 0.010*\"year\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.176): 0.020*\"supplement\" + 0.016*\"help\" + 0.014*\"good\" + 0.014*\"product\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.009*\"doctor\" + 0.008*\"body\" + 0.008*\"health\" + 0.008*\"use\"\n",
      "INFO : topic #1 (0.199): 0.047*\"good\" + 0.036*\"product\" + 0.035*\"great\" + 0.025*\"taste\" + 0.019*\"price\" + 0.017*\"use\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"brand\" + 0.011*\"find\"\n",
      "INFO : topic #5 (0.228): 0.027*\"work\" + 0.019*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.016*\"product\" + 0.015*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.187932, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17958891, 0.20352559, 0.16929722, 0.11962985, 0.056768838, 0.23763596]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.017*\"use\" + 0.016*\"seed\" + 0.014*\"coal\" + 0.010*\"tablespoon\" + 0.009*\"eye\" + 0.009*\"day\" + 0.006*\"easy\" + 0.006*\"light\" + 0.006*\"time\" + 0.006*\"love\"\n",
      "INFO : topic #3 (0.120): 0.027*\"pain\" + 0.023*\"product\" + 0.022*\"day\" + 0.018*\"use\" + 0.017*\"work\" + 0.012*\"help\" + 0.010*\"week\" + 0.010*\"good\" + 0.010*\"year\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.180): 0.019*\"supplement\" + 0.016*\"help\" + 0.014*\"good\" + 0.014*\"product\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.009*\"doctor\" + 0.008*\"health\" + 0.008*\"body\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.204): 0.047*\"good\" + 0.036*\"product\" + 0.035*\"great\" + 0.024*\"taste\" + 0.020*\"price\" + 0.017*\"use\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"brand\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.238): 0.028*\"work\" + 0.019*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.016*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.170536, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18603955, 0.21728499, 0.17119485, 0.12046049, 0.057070695, 0.24046858]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.016*\"use\" + 0.014*\"seed\" + 0.011*\"eye\" + 0.011*\"coal\" + 0.009*\"day\" + 0.009*\"tablespoon\" + 0.006*\"easy\" + 0.006*\"time\" + 0.006*\"light\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #3 (0.120): 0.026*\"pain\" + 0.023*\"product\" + 0.022*\"day\" + 0.017*\"use\" + 0.017*\"work\" + 0.012*\"help\" + 0.010*\"week\" + 0.010*\"year\" + 0.010*\"good\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.186): 0.021*\"supplement\" + 0.016*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.010*\"mg\" + 0.010*\"day\" + 0.009*\"doctor\" + 0.008*\"health\" + 0.008*\"body\"\n",
      "INFO : topic #1 (0.217): 0.046*\"good\" + 0.036*\"product\" + 0.034*\"great\" + 0.024*\"taste\" + 0.020*\"price\" + 0.016*\"use\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"brand\" + 0.012*\"easy\"\n",
      "INFO : topic #5 (0.240): 0.028*\"work\" + 0.020*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.167041, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18858087, 0.22794333, 0.17728843, 0.12082786, 0.057388302, 0.24010608]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.015*\"use\" + 0.014*\"seed\" + 0.012*\"eye\" + 0.010*\"tablespoon\" + 0.009*\"day\" + 0.009*\"coal\" + 0.006*\"light\" + 0.006*\"easy\" + 0.006*\"orange\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.121): 0.025*\"pain\" + 0.023*\"product\" + 0.022*\"day\" + 0.017*\"use\" + 0.017*\"work\" + 0.012*\"help\" + 0.010*\"week\" + 0.010*\"year\" + 0.009*\"good\" + 0.009*\"time\"\n",
      "INFO : topic #0 (0.189): 0.021*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.010*\"doctor\" + 0.010*\"mg\" + 0.010*\"day\" + 0.008*\"'s\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.228): 0.046*\"good\" + 0.035*\"product\" + 0.034*\"great\" + 0.027*\"taste\" + 0.020*\"price\" + 0.016*\"like\" + 0.015*\"use\" + 0.014*\"vitamin\" + 0.013*\"brand\" + 0.013*\"easy\"\n",
      "INFO : topic #5 (0.240): 0.028*\"work\" + 0.020*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.152144, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19238959, 0.23682222, 0.18116677, 0.12228386, 0.057399526, 0.24430361]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.015*\"use\" + 0.013*\"seed\" + 0.011*\"eye\" + 0.009*\"day\" + 0.009*\"tablespoon\" + 0.007*\"coal\" + 0.007*\"orange\" + 0.006*\"second_bottle\" + 0.006*\"time\" + 0.006*\"easy\"\n",
      "INFO : topic #3 (0.122): 0.024*\"pain\" + 0.023*\"product\" + 0.021*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.012*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"good\" + 0.009*\"start\"\n",
      "INFO : topic #0 (0.192): 0.021*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.010*\"doctor\" + 0.010*\"mg\" + 0.010*\"day\" + 0.009*\"'s\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.237): 0.047*\"good\" + 0.034*\"product\" + 0.034*\"great\" + 0.028*\"taste\" + 0.020*\"price\" + 0.017*\"like\" + 0.016*\"vitamin\" + 0.014*\"use\" + 0.013*\"easy\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.244): 0.028*\"work\" + 0.021*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.150726, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19605415, 0.24956158, 0.18709886, 0.12341043, 0.057405412, 0.24208152]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.014*\"use\" + 0.013*\"seed\" + 0.012*\"eye\" + 0.009*\"day\" + 0.008*\"tablespoon\" + 0.007*\"orange\" + 0.006*\"second_bottle\" + 0.006*\"dry_eye\" + 0.006*\"coal\" + 0.006*\"time\"\n",
      "INFO : topic #3 (0.123): 0.024*\"pain\" + 0.023*\"product\" + 0.021*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.012*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"good\" + 0.009*\"start\"\n",
      "INFO : topic #0 (0.196): 0.021*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.011*\"doctor\" + 0.010*\"mg\" + 0.010*\"day\" + 0.009*\"'s\" + 0.009*\"level\"\n",
      "INFO : topic #5 (0.242): 0.029*\"work\" + 0.021*\"day\" + 0.018*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.250): 0.047*\"good\" + 0.034*\"great\" + 0.034*\"product\" + 0.028*\"taste\" + 0.020*\"price\" + 0.017*\"like\" + 0.016*\"vitamin\" + 0.014*\"use\" + 0.013*\"brand\" + 0.013*\"easy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.139327, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19771701, 0.26169908, 0.18924558, 0.12436403, 0.057545397, 0.24735048]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.013*\"use\" + 0.012*\"seed\" + 0.011*\"eye\" + 0.008*\"day\" + 0.008*\"tablespoon\" + 0.008*\"osteoporosis\" + 0.006*\"orange\" + 0.006*\"second_bottle\" + 0.006*\"dry_eye\" + 0.005*\"easy\"\n",
      "INFO : topic #3 (0.124): 0.023*\"pain\" + 0.023*\"product\" + 0.021*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.013*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"good\" + 0.009*\"start\"\n",
      "INFO : topic #0 (0.198): 0.021*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.011*\"doctor\" + 0.009*\"mg\" + 0.009*\"day\" + 0.009*\"'s\" + 0.009*\"level\"\n",
      "INFO : topic #5 (0.247): 0.029*\"work\" + 0.021*\"day\" + 0.019*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.262): 0.047*\"good\" + 0.035*\"great\" + 0.034*\"product\" + 0.029*\"taste\" + 0.020*\"price\" + 0.017*\"like\" + 0.016*\"vitamin\" + 0.013*\"easy\" + 0.013*\"use\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.132891, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19851001, 0.2712326, 0.19797511, 0.12598461, 0.058329672, 0.2513836]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.018*\"seed\" + 0.015*\"flax\" + 0.013*\"use\" + 0.012*\"tablespoon\" + 0.011*\"eye\" + 0.008*\"day\" + 0.007*\"orange\" + 0.007*\"dry_eye\" + 0.007*\"second_bottle\" + 0.006*\"osteoporosis\"\n",
      "INFO : topic #3 (0.126): 0.022*\"product\" + 0.022*\"pain\" + 0.020*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.012*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"good\" + 0.009*\"start\"\n",
      "INFO : topic #0 (0.199): 0.021*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.011*\"recommend\" + 0.011*\"doctor\" + 0.009*\"'s\" + 0.009*\"mg\" + 0.009*\"day\" + 0.009*\"health\"\n",
      "INFO : topic #5 (0.251): 0.029*\"work\" + 0.021*\"day\" + 0.019*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.271): 0.047*\"good\" + 0.035*\"great\" + 0.033*\"product\" + 0.031*\"taste\" + 0.019*\"price\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.013*\"use\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.149541, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20233835, 0.27658489, 0.2020264, 0.127758, 0.05928098, 0.26076743]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.024*\"gaia\" + 0.020*\"seed\" + 0.013*\"flax\" + 0.013*\"use\" + 0.011*\"tablespoon\" + 0.009*\"eye\" + 0.007*\"day\" + 0.006*\"second_bottle\" + 0.006*\"orange\" + 0.005*\"easy\"\n",
      "INFO : topic #3 (0.128): 0.023*\"product\" + 0.021*\"pain\" + 0.020*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.012*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"start\" + 0.009*\"good\"\n",
      "INFO : topic #0 (0.202): 0.020*\"supplement\" + 0.015*\"help\" + 0.013*\"good\" + 0.013*\"product\" + 0.012*\"recommend\" + 0.011*\"doctor\" + 0.010*\"health\" + 0.009*\"'s\" + 0.009*\"day\" + 0.009*\"level\"\n",
      "INFO : topic #5 (0.261): 0.028*\"work\" + 0.021*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.016*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.277): 0.047*\"good\" + 0.035*\"great\" + 0.033*\"product\" + 0.031*\"taste\" + 0.019*\"price\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.013*\"use\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.143726, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20604603, 0.28736052, 0.20740362, 0.12850928, 0.059556842, 0.26352516]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.060): 0.020*\"gaia\" + 0.018*\"seed\" + 0.012*\"use\" + 0.011*\"flax\" + 0.010*\"tablespoon\" + 0.009*\"eye\" + 0.008*\"orange\" + 0.007*\"day\" + 0.006*\"second_bottle\" + 0.006*\"dry_eye\"\n",
      "INFO : topic #3 (0.129): 0.022*\"product\" + 0.022*\"pain\" + 0.020*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.012*\"help\" + 0.011*\"week\" + 0.010*\"year\" + 0.009*\"start\" + 0.009*\"month\"\n",
      "INFO : topic #2 (0.207): 0.065*\"product\" + 0.035*\"use\" + 0.030*\"great\" + 0.023*\"good\" + 0.020*\"oil\" + 0.016*\"skin\" + 0.015*\"love\" + 0.012*\"recommend\" + 0.011*\"work\" + 0.010*\"buy\"\n",
      "INFO : topic #5 (0.264): 0.028*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.287): 0.047*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.031*\"taste\" + 0.018*\"price\" + 0.018*\"like\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.013*\"love\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.127974, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20965751, 0.29450089, 0.21140094, 0.13073452, 0.060052205, 0.26861244]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.060): 0.017*\"gaia\" + 0.015*\"seed\" + 0.012*\"use\" + 0.011*\"eye\" + 0.010*\"tablespoon\" + 0.010*\"flax\" + 0.008*\"orange\" + 0.007*\"day\" + 0.007*\"dry_eye\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #3 (0.131): 0.023*\"pain\" + 0.022*\"product\" + 0.020*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.010*\"year\" + 0.009*\"start\" + 0.009*\"month\"\n",
      "INFO : topic #2 (0.211): 0.067*\"product\" + 0.036*\"use\" + 0.030*\"great\" + 0.023*\"good\" + 0.019*\"oil\" + 0.015*\"skin\" + 0.015*\"love\" + 0.013*\"recommend\" + 0.011*\"work\" + 0.010*\"buy\"\n",
      "INFO : topic #5 (0.269): 0.029*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.295): 0.046*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.031*\"taste\" + 0.018*\"price\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.129035, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21346562, 0.30011445, 0.21574344, 0.13272214, 0.060507476, 0.27029309]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.061): 0.015*\"seed\" + 0.014*\"gaia\" + 0.014*\"eye\" + 0.012*\"use\" + 0.010*\"tablespoon\" + 0.009*\"flax\" + 0.008*\"orange\" + 0.007*\"dry_eye\" + 0.006*\"day\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #3 (0.133): 0.024*\"pain\" + 0.023*\"product\" + 0.019*\"day\" + 0.017*\"work\" + 0.017*\"use\" + 0.012*\"help\" + 0.012*\"week\" + 0.009*\"start\" + 0.009*\"year\" + 0.009*\"month\"\n",
      "INFO : topic #2 (0.216): 0.067*\"product\" + 0.036*\"use\" + 0.030*\"great\" + 0.023*\"good\" + 0.019*\"oil\" + 0.016*\"skin\" + 0.015*\"love\" + 0.012*\"recommend\" + 0.011*\"work\" + 0.010*\"hair\"\n",
      "INFO : topic #5 (0.270): 0.029*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.300): 0.046*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.030*\"taste\" + 0.018*\"like\" + 0.018*\"price\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.119782, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21661909, 0.30166483, 0.22547412, 0.13254324, 0.06104983, 0.27418196]\n",
      "DEBUG : updating topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.061): 0.014*\"eye\" + 0.014*\"seed\" + 0.011*\"gaia\" + 0.011*\"use\" + 0.010*\"grow\" + 0.009*\"butter\" + 0.009*\"tablespoon\" + 0.008*\"flax\" + 0.007*\"orange\" + 0.006*\"dry_eye\"\n",
      "INFO : topic #3 (0.133): 0.024*\"pain\" + 0.022*\"product\" + 0.020*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.010*\"month\" + 0.009*\"start\" + 0.009*\"year\"\n",
      "INFO : topic #2 (0.225): 0.066*\"product\" + 0.035*\"use\" + 0.030*\"great\" + 0.022*\"good\" + 0.021*\"hair\" + 0.018*\"oil\" + 0.016*\"skin\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.274): 0.029*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.302): 0.046*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.029*\"taste\" + 0.019*\"price\" + 0.018*\"like\" + 0.015*\"easy\" + 0.015*\"vitamin\" + 0.013*\"love\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.122042, rho=0.176777\n",
      "INFO : PROGRESS: pass 0, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22141863, 0.31047016, 0.23211369, 0.133696, 0.061398406, 0.27775171]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.061): 0.016*\"eye\" + 0.012*\"seed\" + 0.010*\"use\" + 0.010*\"grow\" + 0.010*\"gaia\" + 0.008*\"butter\" + 0.008*\"tablespoon\" + 0.008*\"dry_eye\" + 0.007*\"flax\" + 0.007*\"memory\"\n",
      "INFO : topic #3 (0.134): 0.024*\"pain\" + 0.022*\"product\" + 0.020*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.010*\"month\" + 0.009*\"start\" + 0.009*\"year\"\n",
      "INFO : topic #2 (0.232): 0.067*\"product\" + 0.034*\"use\" + 0.029*\"great\" + 0.022*\"good\" + 0.021*\"oil\" + 0.019*\"hair\" + 0.016*\"skin\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.010*\"work\"\n",
      "INFO : topic #5 (0.278): 0.029*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.310): 0.046*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.029*\"taste\" + 0.018*\"price\" + 0.018*\"like\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.013*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.123158, rho=0.174078\n",
      "INFO : PROGRESS: pass 0, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22714524, 0.3158412, 0.23646295, 0.136088, 0.061647035, 0.28234482]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.015*\"eye\" + 0.012*\"seed\" + 0.010*\"grow\" + 0.010*\"use\" + 0.009*\"gaia\" + 0.007*\"butter\" + 0.007*\"tablespoon\" + 0.007*\"memory\" + 0.007*\"dry_eye\" + 0.007*\"flax\"\n",
      "INFO : topic #3 (0.136): 0.024*\"pain\" + 0.022*\"product\" + 0.020*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.009*\"month\" + 0.009*\"start\" + 0.009*\"year\"\n",
      "INFO : topic #2 (0.236): 0.068*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.022*\"good\" + 0.020*\"oil\" + 0.019*\"hair\" + 0.016*\"skin\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.282): 0.029*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.018*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.316): 0.047*\"good\" + 0.034*\"great\" + 0.032*\"product\" + 0.029*\"taste\" + 0.019*\"price\" + 0.018*\"like\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.013*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.115320, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2302257, 0.32068294, 0.24385139, 0.13718572, 0.062014408, 0.28937876]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.015*\"eye\" + 0.012*\"seed\" + 0.011*\"gaia\" + 0.010*\"use\" + 0.009*\"grow\" + 0.007*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"memory\" + 0.007*\"dry_eye\" + 0.006*\"butter\"\n",
      "INFO : topic #3 (0.137): 0.024*\"pain\" + 0.022*\"product\" + 0.019*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.009*\"month\" + 0.009*\"start\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.244): 0.069*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.022*\"good\" + 0.022*\"oil\" + 0.016*\"skin\" + 0.016*\"hair\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.010*\"work\"\n",
      "INFO : topic #5 (0.289): 0.030*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.018*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.321): 0.046*\"good\" + 0.035*\"great\" + 0.032*\"product\" + 0.029*\"taste\" + 0.018*\"price\" + 0.018*\"like\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.013*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.108938, rho=0.169031\n",
      "INFO : PROGRESS: pass 0, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23301272, 0.3304522, 0.24704844, 0.13960195, 0.062418222, 0.29463115]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.014*\"eye\" + 0.013*\"seed\" + 0.009*\"use\" + 0.009*\"gaia\" + 0.009*\"grow\" + 0.008*\"tablespoon\" + 0.008*\"memory\" + 0.007*\"second_bottle\" + 0.006*\"orange\" + 0.006*\"dry_eye\"\n",
      "INFO : topic #3 (0.140): 0.023*\"pain\" + 0.022*\"product\" + 0.019*\"day\" + 0.017*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.009*\"start\" + 0.009*\"lose\" + 0.009*\"month\"\n",
      "INFO : topic #2 (0.247): 0.071*\"product\" + 0.034*\"use\" + 0.031*\"great\" + 0.022*\"good\" + 0.021*\"oil\" + 0.016*\"skin\" + 0.015*\"hair\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.295): 0.030*\"work\" + 0.022*\"day\" + 0.021*\"feel\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.330): 0.046*\"good\" + 0.035*\"great\" + 0.032*\"product\" + 0.030*\"taste\" + 0.019*\"like\" + 0.018*\"price\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.110165, rho=0.166667\n",
      "INFO : PROGRESS: pass 0, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23583376, 0.33564579, 0.25157285, 0.1425941, 0.06315244, 0.30064851]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.014*\"eye\" + 0.013*\"seed\" + 0.009*\"use\" + 0.009*\"memory\" + 0.008*\"grow\" + 0.008*\"gaia\" + 0.008*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"dry_eye\" + 0.006*\"orange\"\n",
      "INFO : topic #3 (0.143): 0.023*\"product\" + 0.022*\"pain\" + 0.019*\"day\" + 0.018*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.252): 0.072*\"product\" + 0.033*\"use\" + 0.031*\"great\" + 0.022*\"good\" + 0.020*\"oil\" + 0.016*\"skin\" + 0.014*\"hair\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.301): 0.030*\"work\" + 0.022*\"feel\" + 0.022*\"day\" + 0.017*\"help\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"use\" + 0.013*\"good\" + 0.013*\"start\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.336): 0.046*\"good\" + 0.036*\"great\" + 0.032*\"product\" + 0.030*\"taste\" + 0.019*\"like\" + 0.018*\"price\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.014*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.121170, rho=0.164399\n",
      "INFO : PROGRESS: pass 0, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23717757, 0.34239501, 0.25613758, 0.14603578, 0.063699923, 0.30628315]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.064): 0.014*\"eye\" + 0.012*\"seed\" + 0.009*\"use\" + 0.008*\"memory\" + 0.008*\"grow\" + 0.008*\"second_bottle\" + 0.008*\"tablespoon\" + 0.007*\"orange\" + 0.006*\"gaia\" + 0.006*\"dry_eye\"\n",
      "INFO : topic #3 (0.146): 0.023*\"product\" + 0.021*\"pain\" + 0.019*\"work\" + 0.018*\"day\" + 0.016*\"use\" + 0.013*\"help\" + 0.012*\"week\" + 0.010*\"lose\" + 0.009*\"try\" + 0.009*\"feel\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.256): 0.073*\"product\" + 0.033*\"use\" + 0.032*\"great\" + 0.022*\"good\" + 0.019*\"oil\" + 0.016*\"skin\" + 0.015*\"hair\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.306): 0.030*\"work\" + 0.023*\"feel\" + 0.022*\"day\" + 0.017*\"try\" + 0.017*\"help\" + 0.016*\"product\" + 0.014*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.342): 0.046*\"good\" + 0.036*\"great\" + 0.032*\"product\" + 0.031*\"taste\" + 0.019*\"like\" + 0.018*\"price\" + 0.016*\"easy\" + 0.014*\"love\" + 0.014*\"vitamin\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.108045, rho=0.162221\n",
      "INFO : PROGRESS: pass 0, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24111222, 0.34753379, 0.26078072, 0.14934625, 0.06426584, 0.31103921]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.064): 0.014*\"eye\" + 0.011*\"seed\" + 0.008*\"use\" + 0.008*\"second_bottle\" + 0.008*\"memory\" + 0.008*\"grow\" + 0.008*\"tablespoon\" + 0.007*\"orange\" + 0.006*\"fit\" + 0.006*\"review\"\n",
      "INFO : topic #3 (0.149): 0.022*\"product\" + 0.022*\"pain\" + 0.019*\"work\" + 0.018*\"day\" + 0.016*\"use\" + 0.013*\"week\" + 0.013*\"help\" + 0.010*\"try\" + 0.009*\"feel\" + 0.009*\"lose\"\n",
      "INFO : topic #2 (0.261): 0.074*\"product\" + 0.033*\"use\" + 0.032*\"great\" + 0.023*\"good\" + 0.018*\"oil\" + 0.016*\"skin\" + 0.014*\"hair\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.311): 0.031*\"work\" + 0.024*\"feel\" + 0.023*\"day\" + 0.017*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.014*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.348): 0.046*\"good\" + 0.036*\"great\" + 0.032*\"product\" + 0.030*\"taste\" + 0.019*\"like\" + 0.018*\"price\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.014*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.109913, rho=0.160128\n",
      "INFO : PROGRESS: pass 0, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24468972, 0.35372511, 0.26852563, 0.15164855, 0.064825974, 0.31387389]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.014*\"eye\" + 0.011*\"seed\" + 0.009*\"memory\" + 0.008*\"second_bottle\" + 0.008*\"use\" + 0.008*\"grow\" + 0.007*\"tablespoon\" + 0.007*\"orange\" + 0.006*\"fit\" + 0.006*\"review\"\n",
      "INFO : topic #3 (0.152): 0.023*\"pain\" + 0.022*\"product\" + 0.018*\"work\" + 0.018*\"day\" + 0.016*\"use\" + 0.013*\"week\" + 0.013*\"help\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"lose\"\n",
      "INFO : topic #2 (0.269): 0.075*\"product\" + 0.032*\"use\" + 0.032*\"great\" + 0.023*\"good\" + 0.017*\"oil\" + 0.015*\"skin\" + 0.013*\"hair\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.011*\"order\"\n",
      "INFO : topic #5 (0.314): 0.030*\"work\" + 0.024*\"feel\" + 0.023*\"day\" + 0.018*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.354): 0.046*\"good\" + 0.036*\"great\" + 0.031*\"product\" + 0.030*\"taste\" + 0.020*\"like\" + 0.018*\"price\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.100171, rho=0.158114\n",
      "INFO : PROGRESS: pass 0, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24547683, 0.3526431, 0.28003934, 0.15449247, 0.065362088, 0.31601435]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.015*\"eye\" + 0.010*\"seed\" + 0.009*\"memory\" + 0.008*\"second_bottle\" + 0.008*\"email\" + 0.008*\"use\" + 0.007*\"grow\" + 0.007*\"tablespoon\" + 0.006*\"fit\" + 0.006*\"orange\"\n",
      "INFO : topic #3 (0.154): 0.023*\"product\" + 0.022*\"pain\" + 0.018*\"day\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"week\" + 0.013*\"help\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.280): 0.075*\"product\" + 0.033*\"use\" + 0.031*\"great\" + 0.022*\"good\" + 0.020*\"skin\" + 0.016*\"oil\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"hair\" + 0.011*\"order\"\n",
      "INFO : topic #5 (0.316): 0.030*\"work\" + 0.026*\"feel\" + 0.023*\"day\" + 0.018*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.353): 0.045*\"good\" + 0.035*\"great\" + 0.032*\"product\" + 0.029*\"taste\" + 0.020*\"like\" + 0.017*\"price\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.105215, rho=0.156174\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25150868, 0.36387849, 0.29051283, 0.15883581, 0.065953255, 0.31770244]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.066): 0.014*\"eye\" + 0.009*\"seed\" + 0.009*\"memory\" + 0.008*\"email\" + 0.008*\"second_bottle\" + 0.007*\"use\" + 0.007*\"grow\" + 0.007*\"tablespoon\" + 0.006*\"review\" + 0.006*\"fit\"\n",
      "INFO : topic #3 (0.159): 0.023*\"product\" + 0.021*\"pain\" + 0.017*\"day\" + 0.017*\"work\" + 0.015*\"use\" + 0.015*\"week\" + 0.013*\"help\" + 0.011*\"feel\" + 0.010*\"lose\" + 0.010*\"try\"\n",
      "INFO : topic #2 (0.291): 0.075*\"product\" + 0.033*\"use\" + 0.031*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.016*\"oil\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.010*\"order\" + 0.010*\"hair\"\n",
      "INFO : topic #5 (0.318): 0.029*\"work\" + 0.027*\"feel\" + 0.023*\"day\" + 0.018*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"use\" + 0.014*\"start\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.364): 0.044*\"good\" + 0.035*\"great\" + 0.032*\"product\" + 0.028*\"taste\" + 0.020*\"like\" + 0.017*\"easy\" + 0.016*\"price\" + 0.015*\"vitamin\" + 0.013*\"love\" + 0.011*\"brand\"\n",
      "INFO : topic diff=0.110499, rho=0.154303\n",
      "INFO : PROGRESS: pass 0, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25158286, 0.36142117, 0.30233893, 0.16353489, 0.066419937, 0.32270133]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.066): 0.014*\"eye\" + 0.009*\"seed\" + 0.009*\"memory\" + 0.009*\"second_bottle\" + 0.008*\"email\" + 0.007*\"use\" + 0.007*\"review\" + 0.007*\"grow\" + 0.006*\"fit\" + 0.006*\"tablespoon\"\n",
      "INFO : topic #3 (0.164): 0.023*\"product\" + 0.020*\"pain\" + 0.017*\"day\" + 0.017*\"work\" + 0.016*\"week\" + 0.015*\"use\" + 0.013*\"help\" + 0.012*\"lose\" + 0.011*\"feel\" + 0.010*\"try\"\n",
      "INFO : topic #2 (0.302): 0.075*\"product\" + 0.032*\"use\" + 0.031*\"great\" + 0.023*\"skin\" + 0.021*\"good\" + 0.014*\"oil\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"look\" + 0.010*\"order\"\n",
      "INFO : topic #5 (0.323): 0.029*\"work\" + 0.028*\"feel\" + 0.023*\"day\" + 0.019*\"try\" + 0.017*\"help\" + 0.016*\"product\" + 0.014*\"good\" + 0.014*\"start\" + 0.014*\"use\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.361): 0.043*\"good\" + 0.036*\"great\" + 0.032*\"product\" + 0.027*\"taste\" + 0.020*\"like\" + 0.018*\"easy\" + 0.016*\"price\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.011*\"brand\"\n",
      "INFO : topic diff=0.102121, rho=0.152499\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25190127, 0.35605896, 0.31300044, 0.1689847, 0.066697121, 0.33055675]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.067): 0.014*\"eye\" + 0.009*\"second_bottle\" + 0.009*\"memory\" + 0.008*\"email\" + 0.008*\"seed\" + 0.007*\"fit\" + 0.007*\"review\" + 0.007*\"use\" + 0.006*\"grow\" + 0.006*\"assist\"\n",
      "INFO : topic #3 (0.169): 0.023*\"product\" + 0.019*\"pain\" + 0.017*\"work\" + 0.017*\"day\" + 0.016*\"week\" + 0.015*\"lose\" + 0.015*\"use\" + 0.014*\"help\" + 0.012*\"weight\" + 0.011*\"try\"\n",
      "INFO : topic #2 (0.313): 0.076*\"product\" + 0.031*\"great\" + 0.031*\"use\" + 0.026*\"skin\" + 0.021*\"good\" + 0.013*\"love\" + 0.013*\"look\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.010*\"order\"\n",
      "INFO : topic #5 (0.331): 0.029*\"feel\" + 0.029*\"work\" + 0.023*\"day\" + 0.020*\"try\" + 0.017*\"help\" + 0.016*\"product\" + 0.015*\"good\" + 0.014*\"start\" + 0.013*\"use\" + 0.012*\"like\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.356): 0.043*\"good\" + 0.035*\"great\" + 0.032*\"product\" + 0.027*\"taste\" + 0.020*\"like\" + 0.019*\"easy\" + 0.016*\"price\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.107200, rho=0.150756\n",
      "INFO : PROGRESS: pass 1, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23912522, 0.35130459, 0.3055982, 0.17293626, 0.071652003, 0.31624436]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.072): 0.075*\"pedometer\" + 0.034*\"step\" + 0.021*\"accurate\" + 0.021*\"pocket\" + 0.019*\"omron\" + 0.016*\"clip\" + 0.012*\"use\" + 0.010*\"day\" + 0.010*\"set\" + 0.009*\"easy\"\n",
      "INFO : topic #3 (0.173): 0.022*\"product\" + 0.021*\"day\" + 0.018*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.016*\"lose\" + 0.015*\"week\" + 0.013*\"help\" + 0.011*\"weight\" + 0.011*\"try\"\n",
      "INFO : topic #2 (0.306): 0.074*\"product\" + 0.033*\"use\" + 0.032*\"great\" + 0.025*\"skin\" + 0.021*\"good\" + 0.015*\"love\" + 0.013*\"recommend\" + 0.013*\"look\" + 0.012*\"oil\" + 0.010*\"work\"\n",
      "INFO : topic #5 (0.316): 0.031*\"work\" + 0.028*\"feel\" + 0.024*\"day\" + 0.020*\"try\" + 0.016*\"help\" + 0.015*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"like\"\n",
      "INFO : topic #1 (0.351): 0.044*\"good\" + 0.037*\"great\" + 0.031*\"product\" + 0.025*\"taste\" + 0.022*\"easy\" + 0.021*\"like\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"vitamin\" + 0.012*\"buy\"\n",
      "INFO : topic diff=0.256289, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23986366, 0.35487774, 0.30732411, 0.17267331, 0.073504388, 0.32119986]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.077*\"pedometer\" + 0.036*\"step\" + 0.021*\"accurate\" + 0.020*\"pocket\" + 0.019*\"omron\" + 0.017*\"clip\" + 0.014*\"use\" + 0.012*\"easy\" + 0.012*\"day\" + 0.010*\"count\"\n",
      "INFO : topic #3 (0.173): 0.021*\"day\" + 0.021*\"product\" + 0.019*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"lose\" + 0.015*\"week\" + 0.013*\"help\" + 0.012*\"walk\" + 0.011*\"weight\"\n",
      "INFO : topic #2 (0.307): 0.073*\"product\" + 0.034*\"use\" + 0.032*\"great\" + 0.025*\"skin\" + 0.021*\"good\" + 0.015*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.012*\"look\" + 0.010*\"work\"\n",
      "INFO : topic #5 (0.321): 0.031*\"work\" + 0.027*\"feel\" + 0.024*\"day\" + 0.019*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.355): 0.044*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.026*\"taste\" + 0.022*\"easy\" + 0.020*\"like\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"vitamin\" + 0.012*\"buy\"\n",
      "INFO : topic diff=0.112784, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24478988, 0.36163506, 0.31005502, 0.17338368, 0.073244698, 0.33572486]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.073): 0.074*\"pedometer\" + 0.035*\"step\" + 0.021*\"accurate\" + 0.020*\"pocket\" + 0.018*\"omron\" + 0.016*\"clip\" + 0.014*\"use\" + 0.012*\"day\" + 0.012*\"easy\" + 0.010*\"count\"\n",
      "INFO : topic #3 (0.173): 0.021*\"day\" + 0.021*\"product\" + 0.019*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.015*\"lose\" + 0.015*\"week\" + 0.014*\"help\" + 0.011*\"walk\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.310): 0.073*\"product\" + 0.034*\"use\" + 0.032*\"great\" + 0.024*\"skin\" + 0.021*\"good\" + 0.014*\"love\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.012*\"look\" + 0.011*\"hair\"\n",
      "INFO : topic #5 (0.336): 0.031*\"work\" + 0.026*\"feel\" + 0.024*\"day\" + 0.018*\"try\" + 0.017*\"help\" + 0.015*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.362): 0.045*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"like\" + 0.020*\"easy\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"vitamin\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.102837, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24859214, 0.3642419, 0.31568182, 0.17404974, 0.073140278, 0.348414]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.073): 0.071*\"pedometer\" + 0.033*\"step\" + 0.020*\"accurate\" + 0.019*\"pocket\" + 0.017*\"omron\" + 0.015*\"clip\" + 0.013*\"use\" + 0.012*\"day\" + 0.011*\"easy\" + 0.010*\"count\"\n",
      "INFO : topic #3 (0.174): 0.021*\"day\" + 0.021*\"product\" + 0.020*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.014*\"week\" + 0.014*\"help\" + 0.014*\"lose\" + 0.010*\"try\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.316): 0.072*\"product\" + 0.035*\"use\" + 0.032*\"great\" + 0.024*\"skin\" + 0.021*\"good\" + 0.014*\"love\" + 0.014*\"oil\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.011*\"look\"\n",
      "INFO : topic #5 (0.348): 0.031*\"work\" + 0.024*\"feel\" + 0.024*\"day\" + 0.018*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.015*\"good\" + 0.013*\"start\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.364): 0.046*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"like\" + 0.019*\"easy\" + 0.018*\"price\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.095800, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24943927, 0.3688485, 0.32507598, 0.174132, 0.073062271, 0.36016163]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.073): 0.067*\"pedometer\" + 0.032*\"step\" + 0.019*\"accurate\" + 0.018*\"pocket\" + 0.016*\"omron\" + 0.014*\"clip\" + 0.013*\"use\" + 0.011*\"day\" + 0.011*\"easy\" + 0.009*\"count\"\n",
      "INFO : topic #3 (0.174): 0.021*\"pain\" + 0.021*\"day\" + 0.020*\"product\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.014*\"week\" + 0.013*\"lose\" + 0.010*\"try\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.325): 0.070*\"product\" + 0.036*\"use\" + 0.032*\"great\" + 0.023*\"skin\" + 0.021*\"good\" + 0.016*\"oil\" + 0.014*\"love\" + 0.012*\"hair\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #5 (0.360): 0.031*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.018*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.014*\"product\" + 0.013*\"start\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.369): 0.046*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.027*\"taste\" + 0.020*\"like\" + 0.018*\"easy\" + 0.018*\"price\" + 0.013*\"love\" + 0.013*\"vitamin\" + 0.013*\"use\"\n",
      "INFO : topic diff=0.090952, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25481012, 0.3718645, 0.32817918, 0.17529522, 0.073024757, 0.37640405]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.073): 0.063*\"pedometer\" + 0.030*\"step\" + 0.018*\"accurate\" + 0.017*\"pocket\" + 0.015*\"omron\" + 0.014*\"clip\" + 0.013*\"use\" + 0.011*\"day\" + 0.011*\"easy\" + 0.009*\"count\"\n",
      "INFO : topic #3 (0.175): 0.021*\"pain\" + 0.021*\"day\" + 0.020*\"product\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.014*\"week\" + 0.013*\"lose\" + 0.010*\"try\" + 0.009*\"weight\"\n",
      "INFO : topic #2 (0.328): 0.071*\"product\" + 0.036*\"use\" + 0.032*\"great\" + 0.021*\"skin\" + 0.021*\"good\" + 0.014*\"oil\" + 0.014*\"love\" + 0.013*\"hair\" + 0.013*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.372): 0.046*\"good\" + 0.037*\"great\" + 0.031*\"product\" + 0.028*\"taste\" + 0.020*\"like\" + 0.019*\"price\" + 0.017*\"easy\" + 0.013*\"love\" + 0.013*\"use\" + 0.013*\"vitamin\"\n",
      "INFO : topic #5 (0.376): 0.031*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.014*\"product\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.092741, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25845379, 0.37902841, 0.33270025, 0.17660987, 0.073290743, 0.38882735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.073): 0.057*\"pedometer\" + 0.028*\"step\" + 0.016*\"accurate\" + 0.015*\"pocket\" + 0.014*\"omron\" + 0.013*\"use\" + 0.012*\"clip\" + 0.011*\"day\" + 0.010*\"easy\" + 0.008*\"set\"\n",
      "INFO : topic #3 (0.177): 0.021*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.014*\"week\" + 0.012*\"lose\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.333): 0.071*\"product\" + 0.036*\"use\" + 0.032*\"great\" + 0.022*\"skin\" + 0.021*\"good\" + 0.015*\"oil\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.379): 0.046*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"like\" + 0.019*\"price\" + 0.017*\"easy\" + 0.013*\"love\" + 0.013*\"use\" + 0.013*\"vitamin\"\n",
      "INFO : topic #5 (0.389): 0.031*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.089475, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26293769, 0.38645345, 0.336667, 0.17848611, 0.073521897, 0.39763758]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.052*\"pedometer\" + 0.025*\"step\" + 0.015*\"accurate\" + 0.014*\"pocket\" + 0.013*\"use\" + 0.013*\"omron\" + 0.011*\"clip\" + 0.011*\"day\" + 0.010*\"easy\" + 0.008*\"eye\"\n",
      "INFO : topic #3 (0.178): 0.024*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.011*\"lose\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.337): 0.071*\"product\" + 0.036*\"use\" + 0.032*\"great\" + 0.022*\"skin\" + 0.021*\"good\" + 0.014*\"oil\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.386): 0.047*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.016*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.398): 0.030*\"work\" + 0.023*\"day\" + 0.022*\"feel\" + 0.019*\"help\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.014*\"product\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.084037, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26743573, 0.39318529, 0.34278479, 0.17975895, 0.073764935, 0.41102397]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.048*\"pedometer\" + 0.023*\"step\" + 0.014*\"accurate\" + 0.013*\"use\" + 0.013*\"pocket\" + 0.012*\"omron\" + 0.011*\"day\" + 0.010*\"clip\" + 0.010*\"easy\" + 0.008*\"seed\"\n",
      "INFO : topic #3 (0.180): 0.025*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.011*\"lose\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.343): 0.071*\"product\" + 0.037*\"use\" + 0.032*\"great\" + 0.021*\"skin\" + 0.021*\"good\" + 0.016*\"oil\" + 0.014*\"love\" + 0.012*\"hair\" + 0.012*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.393): 0.047*\"good\" + 0.037*\"great\" + 0.030*\"product\" + 0.030*\"taste\" + 0.019*\"like\" + 0.019*\"price\" + 0.016*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.411): 0.031*\"work\" + 0.024*\"day\" + 0.022*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.083856, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27170813, 0.39976785, 0.34509158, 0.18342714, 0.073846072, 0.42095721]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.044*\"pedometer\" + 0.022*\"step\" + 0.013*\"use\" + 0.013*\"accurate\" + 0.012*\"pocket\" + 0.011*\"omron\" + 0.011*\"seed\" + 0.011*\"day\" + 0.010*\"clip\" + 0.009*\"easy\"\n",
      "INFO : topic #3 (0.183): 0.024*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.345): 0.072*\"product\" + 0.038*\"use\" + 0.032*\"great\" + 0.021*\"skin\" + 0.021*\"good\" + 0.015*\"oil\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.400): 0.047*\"good\" + 0.036*\"great\" + 0.031*\"product\" + 0.029*\"taste\" + 0.019*\"like\" + 0.019*\"price\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.421): 0.031*\"work\" + 0.024*\"day\" + 0.022*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.086724, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27994153, 0.40528825, 0.34555739, 0.18670402, 0.074004725, 0.43536156]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.040*\"pedometer\" + 0.020*\"step\" + 0.013*\"use\" + 0.012*\"accurate\" + 0.011*\"pocket\" + 0.011*\"seed\" + 0.010*\"day\" + 0.010*\"omron\" + 0.009*\"easy\" + 0.009*\"clip\"\n",
      "INFO : topic #3 (0.187): 0.025*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.009*\"try\" + 0.009*\"lose\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.346): 0.074*\"product\" + 0.038*\"use\" + 0.032*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.015*\"oil\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.011*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.405): 0.047*\"good\" + 0.036*\"great\" + 0.030*\"product\" + 0.030*\"taste\" + 0.019*\"like\" + 0.019*\"price\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.435): 0.031*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.090824, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28507662, 0.41171128, 0.35238624, 0.1866731, 0.074066594, 0.44865504]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.036*\"pedometer\" + 0.019*\"step\" + 0.012*\"use\" + 0.011*\"seed\" + 0.011*\"accurate\" + 0.010*\"pocket\" + 0.010*\"day\" + 0.009*\"omron\" + 0.009*\"easy\" + 0.008*\"clip\"\n",
      "INFO : topic #3 (0.187): 0.025*\"pain\" + 0.020*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.352): 0.075*\"product\" + 0.038*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.014*\"oil\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.011*\"work\" + 0.010*\"order\"\n",
      "INFO : topic #1 (0.412): 0.048*\"good\" + 0.036*\"great\" + 0.031*\"product\" + 0.029*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.449): 0.032*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.077522, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29198042, 0.41463745, 0.3581484, 0.18975671, 0.074327253, 0.46362194]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.032*\"pedometer\" + 0.017*\"step\" + 0.012*\"use\" + 0.011*\"seed\" + 0.010*\"day\" + 0.010*\"accurate\" + 0.009*\"pocket\" + 0.008*\"easy\" + 0.008*\"omron\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #3 (0.190): 0.026*\"pain\" + 0.019*\"day\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"start\" + 0.009*\"try\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.358): 0.075*\"product\" + 0.038*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.014*\"love\" + 0.014*\"oil\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.415): 0.049*\"good\" + 0.036*\"great\" + 0.031*\"product\" + 0.029*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.464): 0.032*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.076053, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29891965, 0.4185507, 0.36663327, 0.19227718, 0.074893519, 0.47810018]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.027*\"pedometer\" + 0.015*\"step\" + 0.013*\"camera\" + 0.012*\"use\" + 0.011*\"seed\" + 0.009*\"day\" + 0.008*\"easy\" + 0.008*\"accurate\" + 0.008*\"pocket\" + 0.007*\"eye\"\n",
      "INFO : topic #3 (0.192): 0.027*\"pain\" + 0.020*\"day\" + 0.018*\"product\" + 0.018*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"start\" + 0.010*\"lose\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.367): 0.076*\"product\" + 0.038*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.019*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.013*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.419): 0.049*\"good\" + 0.036*\"great\" + 0.031*\"product\" + 0.029*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.478): 0.032*\"work\" + 0.025*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.085608, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30490503, 0.42505118, 0.37137142, 0.19365785, 0.075296976, 0.49171737]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.023*\"pedometer\" + 0.014*\"step\" + 0.012*\"camera\" + 0.012*\"use\" + 0.011*\"seed\" + 0.009*\"day\" + 0.008*\"easy\" + 0.008*\"eye\" + 0.008*\"accurate\" + 0.007*\"tablespoon\"\n",
      "INFO : topic #3 (0.194): 0.027*\"pain\" + 0.019*\"day\" + 0.018*\"product\" + 0.017*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"start\" + 0.009*\"lose\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.371): 0.076*\"product\" + 0.039*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.425): 0.049*\"good\" + 0.035*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"use\" + 0.014*\"easy\" + 0.013*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.492): 0.032*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"year\" + 0.014*\"start\"\n",
      "INFO : topic diff=0.074591, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30843681, 0.43594575, 0.3807058, 0.19699292, 0.075630769, 0.5043267]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.021*\"pedometer\" + 0.012*\"step\" + 0.012*\"use\" + 0.011*\"seed\" + 0.011*\"camera\" + 0.009*\"day\" + 0.008*\"tablespoon\" + 0.008*\"easy\" + 0.008*\"eye\" + 0.007*\"accurate\"\n",
      "INFO : topic #3 (0.197): 0.028*\"pain\" + 0.019*\"day\" + 0.018*\"product\" + 0.017*\"work\" + 0.016*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.009*\"try\" + 0.009*\"start\" + 0.009*\"lose\"\n",
      "INFO : topic #2 (0.381): 0.076*\"product\" + 0.040*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.019*\"skin\" + 0.014*\"love\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.436): 0.050*\"good\" + 0.035*\"great\" + 0.030*\"product\" + 0.029*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"use\" + 0.014*\"easy\" + 0.013*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.504): 0.032*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.014*\"good\" + 0.014*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.076353, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31989098, 0.4450759, 0.38597116, 0.19852428, 0.07606788, 0.51889658]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.018*\"pedometer\" + 0.011*\"step\" + 0.011*\"use\" + 0.011*\"seed\" + 0.009*\"camera\" + 0.008*\"eye\" + 0.008*\"day\" + 0.008*\"tablespoon\" + 0.008*\"memory\" + 0.007*\"easy\"\n",
      "INFO : topic #3 (0.199): 0.028*\"pain\" + 0.019*\"day\" + 0.018*\"product\" + 0.017*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.009*\"joint\" + 0.009*\"start\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.386): 0.077*\"product\" + 0.040*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.018*\"skin\" + 0.014*\"hair\" + 0.013*\"love\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.445): 0.050*\"good\" + 0.035*\"great\" + 0.030*\"product\" + 0.028*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"use\" + 0.014*\"easy\" + 0.013*\"brand\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.519): 0.032*\"work\" + 0.024*\"day\" + 0.021*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.014*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.078717, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.33020985, 0.45376778, 0.39169648, 0.20051658, 0.076430596, 0.5260058]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.016*\"pedometer\" + 0.014*\"eye\" + 0.011*\"seed\" + 0.011*\"use\" + 0.010*\"step\" + 0.008*\"day\" + 0.008*\"camera\" + 0.008*\"tablespoon\" + 0.008*\"memory\" + 0.007*\"easy\"\n",
      "INFO : topic #3 (0.201): 0.029*\"pain\" + 0.019*\"day\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"joint\" + 0.010*\"start\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.392): 0.077*\"product\" + 0.040*\"use\" + 0.033*\"great\" + 0.021*\"good\" + 0.018*\"skin\" + 0.014*\"hair\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"oil\" + 0.011*\"work\"\n",
      "INFO : topic #1 (0.454): 0.050*\"good\" + 0.035*\"great\" + 0.029*\"product\" + 0.027*\"taste\" + 0.021*\"price\" + 0.019*\"like\" + 0.014*\"easy\" + 0.014*\"use\" + 0.013*\"brand\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.526): 0.033*\"work\" + 0.025*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.014*\"start\" + 0.014*\"year\"\n",
      "INFO : topic diff=0.077601, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.33845019, 0.47098836, 0.39583057, 0.20319143, 0.076754831, 0.53847337]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.077): 0.014*\"pedometer\" + 0.014*\"eye\" + 0.013*\"seed\" + 0.010*\"use\" + 0.010*\"step\" + 0.009*\"tablespoon\" + 0.008*\"day\" + 0.007*\"memory\" + 0.007*\"camera\" + 0.007*\"easy\"\n",
      "INFO : topic #3 (0.203): 0.030*\"pain\" + 0.019*\"day\" + 0.018*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.009*\"start\" + 0.009*\"joint\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.396): 0.079*\"product\" + 0.040*\"use\" + 0.034*\"great\" + 0.021*\"good\" + 0.017*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"hair\" + 0.012*\"oil\" + 0.011*\"order\"\n",
      "INFO : topic #1 (0.471): 0.051*\"good\" + 0.036*\"great\" + 0.029*\"product\" + 0.027*\"taste\" + 0.021*\"price\" + 0.019*\"like\" + 0.014*\"use\" + 0.014*\"easy\" + 0.014*\"vitamin\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.538): 0.033*\"work\" + 0.026*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.014*\"start\" + 0.013*\"year\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.072230, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3419916, 0.4844873, 0.40530962, 0.20382956, 0.077529393, 0.54477942]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.018*\"seed\" + 0.016*\"eye\" + 0.012*\"pedometer\" + 0.011*\"coal\" + 0.011*\"use\" + 0.009*\"tablespoon\" + 0.009*\"step\" + 0.008*\"day\" + 0.007*\"memory\" + 0.006*\"easy\"\n",
      "INFO : topic #3 (0.204): 0.030*\"pain\" + 0.019*\"day\" + 0.017*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.009*\"start\" + 0.009*\"try\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.405): 0.078*\"product\" + 0.041*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.013*\"hair\" + 0.011*\"order\"\n",
      "INFO : topic #1 (0.484): 0.050*\"good\" + 0.036*\"great\" + 0.028*\"product\" + 0.027*\"taste\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"use\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.545): 0.033*\"work\" + 0.026*\"day\" + 0.022*\"feel\" + 0.019*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.074937, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.34796447, 0.49273777, 0.41454482, 0.20534481, 0.077822417, 0.5660612]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.018*\"seed\" + 0.016*\"eye\" + 0.010*\"pedometer\" + 0.010*\"use\" + 0.010*\"coal\" + 0.009*\"tablespoon\" + 0.008*\"step\" + 0.008*\"day\" + 0.007*\"orange\" + 0.006*\"memory\"\n",
      "INFO : topic #3 (0.205): 0.031*\"pain\" + 0.019*\"day\" + 0.017*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.012*\"week\" + 0.009*\"start\" + 0.009*\"joint\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.415): 0.079*\"product\" + 0.041*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.012*\"hair\" + 0.011*\"order\"\n",
      "INFO : topic #1 (0.493): 0.051*\"good\" + 0.036*\"great\" + 0.028*\"product\" + 0.027*\"taste\" + 0.021*\"price\" + 0.019*\"like\" + 0.015*\"use\" + 0.014*\"easy\" + 0.013*\"brand\" + 0.013*\"vitamin\"\n",
      "INFO : topic #5 (0.566): 0.033*\"work\" + 0.025*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"good\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic diff=0.073117, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.35905617, 0.51574731, 0.42054701, 0.20582214, 0.07838162, 0.57532823]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.018*\"eye\" + 0.017*\"seed\" + 0.010*\"use\" + 0.009*\"pedometer\" + 0.009*\"tablespoon\" + 0.008*\"coal\" + 0.008*\"day\" + 0.007*\"step\" + 0.007*\"memory\" + 0.007*\"orange\"\n",
      "INFO : topic #3 (0.206): 0.030*\"pain\" + 0.019*\"day\" + 0.017*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.012*\"week\" + 0.009*\"start\" + 0.009*\"try\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.421): 0.079*\"product\" + 0.040*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.017*\"skin\" + 0.014*\"oil\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"hair\" + 0.011*\"order\"\n",
      "INFO : topic #1 (0.516): 0.050*\"good\" + 0.035*\"great\" + 0.028*\"product\" + 0.027*\"taste\" + 0.021*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.015*\"use\" + 0.014*\"vitamin\" + 0.014*\"brand\"\n",
      "INFO : topic #5 (0.575): 0.034*\"work\" + 0.026*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.075087, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.36505336, 0.53582865, 0.43378651, 0.20577581, 0.079048619, 0.5808658]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.018*\"eye\" + 0.016*\"seed\" + 0.010*\"use\" + 0.009*\"tablespoon\" + 0.008*\"orange\" + 0.008*\"pedometer\" + 0.007*\"day\" + 0.007*\"coal\" + 0.007*\"step\" + 0.006*\"memory\"\n",
      "INFO : topic #3 (0.206): 0.030*\"pain\" + 0.019*\"day\" + 0.016*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.014*\"help\" + 0.012*\"week\" + 0.009*\"start\" + 0.009*\"lose\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.434): 0.080*\"product\" + 0.040*\"use\" + 0.034*\"great\" + 0.021*\"good\" + 0.016*\"skin\" + 0.016*\"oil\" + 0.013*\"recommend\" + 0.013*\"love\" + 0.012*\"order\" + 0.011*\"hair\"\n",
      "INFO : topic #1 (0.536): 0.050*\"good\" + 0.035*\"great\" + 0.028*\"taste\" + 0.028*\"product\" + 0.021*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.014*\"use\" + 0.014*\"brand\"\n",
      "INFO : topic #5 (0.581): 0.034*\"work\" + 0.026*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"year\" + 0.014*\"start\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.068500, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.37227091, 0.55221248, 0.44311598, 0.20711182, 0.079290837, 0.59396106]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.018*\"eye\" + 0.015*\"seed\" + 0.009*\"orange\" + 0.009*\"use\" + 0.009*\"tablespoon\" + 0.007*\"day\" + 0.007*\"memory\" + 0.007*\"pedometer\" + 0.006*\"coal\" + 0.006*\"flax\"\n",
      "INFO : topic #3 (0.207): 0.029*\"pain\" + 0.018*\"day\" + 0.017*\"product\" + 0.015*\"work\" + 0.014*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"start\" + 0.009*\"weight\"\n",
      "INFO : topic #2 (0.443): 0.080*\"product\" + 0.039*\"use\" + 0.034*\"great\" + 0.021*\"good\" + 0.016*\"skin\" + 0.015*\"oil\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"hair\" + 0.012*\"order\"\n",
      "INFO : topic #1 (0.552): 0.051*\"good\" + 0.035*\"great\" + 0.029*\"taste\" + 0.027*\"product\" + 0.021*\"price\" + 0.019*\"like\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.013*\"brand\" + 0.013*\"use\"\n",
      "INFO : topic #5 (0.594): 0.034*\"work\" + 0.027*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.068461, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.38030103, 0.57599139, 0.45625979, 0.2084455, 0.079490557, 0.59700567]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.018*\"eye\" + 0.016*\"seed\" + 0.009*\"use\" + 0.009*\"orange\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.007*\"memory\" + 0.006*\"second_bottle\" + 0.006*\"pedometer\" + 0.006*\"flax\"\n",
      "INFO : topic #3 (0.208): 0.029*\"pain\" + 0.018*\"day\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"start\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.456): 0.081*\"product\" + 0.038*\"use\" + 0.034*\"great\" + 0.021*\"good\" + 0.016*\"skin\" + 0.015*\"oil\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.012*\"hair\" + 0.012*\"order\"\n",
      "INFO : topic #1 (0.576): 0.051*\"good\" + 0.035*\"great\" + 0.030*\"taste\" + 0.027*\"product\" + 0.021*\"price\" + 0.019*\"like\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.597): 0.034*\"work\" + 0.027*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.068108, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.38535768, 0.60015762, 0.46447352, 0.2091994, 0.079863943, 0.61168468]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.080): 0.018*\"eye\" + 0.014*\"seed\" + 0.009*\"orange\" + 0.009*\"use\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.007*\"osteoporosis\" + 0.007*\"grow\" + 0.006*\"flax\" + 0.006*\"memory\"\n",
      "INFO : topic #3 (0.209): 0.028*\"pain\" + 0.018*\"day\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"help\" + 0.014*\"use\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"joint\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.464): 0.083*\"product\" + 0.038*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.016*\"skin\" + 0.015*\"oil\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.012*\"order\" + 0.011*\"hair\"\n",
      "INFO : topic #1 (0.600): 0.051*\"good\" + 0.035*\"great\" + 0.030*\"taste\" + 0.026*\"product\" + 0.021*\"price\" + 0.019*\"like\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.612): 0.034*\"work\" + 0.027*\"day\" + 0.022*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.014*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.066872, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3884708, 0.61875713, 0.4840287, 0.21086384, 0.080992818, 0.62277627]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.081): 0.019*\"seed\" + 0.018*\"eye\" + 0.014*\"flax\" + 0.011*\"tablespoon\" + 0.009*\"orange\" + 0.008*\"use\" + 0.006*\"second_bottle\" + 0.006*\"day\" + 0.006*\"memory\" + 0.006*\"dry_eye\"\n",
      "INFO : topic #3 (0.211): 0.027*\"pain\" + 0.018*\"day\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"help\" + 0.014*\"use\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"joint\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.484): 0.082*\"product\" + 0.039*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.019*\"oil\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.012*\"order\" + 0.012*\"hair\"\n",
      "INFO : topic #1 (0.619): 0.051*\"good\" + 0.035*\"great\" + 0.032*\"taste\" + 0.026*\"product\" + 0.021*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.015*\"vitamin\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.623): 0.034*\"work\" + 0.027*\"day\" + 0.023*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"year\" + 0.014*\"start\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.076923, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.39665404, 0.63023978, 0.49691457, 0.21300508, 0.082220361, 0.64709836]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.082): 0.020*\"seed\" + 0.019*\"gaia\" + 0.016*\"eye\" + 0.013*\"flax\" + 0.011*\"tablespoon\" + 0.008*\"orange\" + 0.008*\"use\" + 0.007*\"memory\" + 0.006*\"second_bottle\" + 0.006*\"review\"\n",
      "INFO : topic #3 (0.213): 0.027*\"pain\" + 0.017*\"day\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.011*\"lose\" + 0.009*\"weight\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.497): 0.084*\"product\" + 0.039*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.018*\"oil\" + 0.016*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.012*\"order\" + 0.011*\"hair\"\n",
      "INFO : topic #1 (0.630): 0.051*\"good\" + 0.035*\"great\" + 0.032*\"taste\" + 0.026*\"product\" + 0.020*\"price\" + 0.019*\"like\" + 0.015*\"easy\" + 0.015*\"vitamin\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.647): 0.033*\"work\" + 0.027*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.018*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.074587, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.40524665, 0.65202188, 0.51098752, 0.21363245, 0.082752846, 0.65761924]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.083): 0.019*\"seed\" + 0.018*\"gaia\" + 0.016*\"eye\" + 0.012*\"flax\" + 0.010*\"tablespoon\" + 0.010*\"orange\" + 0.008*\"use\" + 0.007*\"memory\" + 0.006*\"second_bottle\" + 0.006*\"review\"\n",
      "INFO : topic #3 (0.214): 0.027*\"pain\" + 0.017*\"day\" + 0.016*\"product\" + 0.014*\"work\" + 0.014*\"help\" + 0.014*\"use\" + 0.013*\"week\" + 0.010*\"lose\" + 0.009*\"start\" + 0.009*\"weight\"\n",
      "INFO : topic #2 (0.511): 0.085*\"product\" + 0.039*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.019*\"oil\" + 0.016*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.012*\"order\" + 0.010*\"buy\"\n",
      "INFO : topic #1 (0.652): 0.051*\"good\" + 0.035*\"great\" + 0.032*\"taste\" + 0.025*\"product\" + 0.020*\"price\" + 0.020*\"like\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.658): 0.033*\"work\" + 0.027*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.018*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.069027, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.41336006, 0.6670118, 0.5243336, 0.21645857, 0.08351136, 0.67406636]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.084): 0.017*\"eye\" + 0.017*\"seed\" + 0.015*\"gaia\" + 0.010*\"flax\" + 0.010*\"tablespoon\" + 0.010*\"orange\" + 0.008*\"use\" + 0.007*\"memory\" + 0.007*\"dry_eye\" + 0.006*\"review\"\n",
      "INFO : topic #3 (0.216): 0.028*\"pain\" + 0.017*\"day\" + 0.015*\"product\" + 0.014*\"work\" + 0.014*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.010*\"lose\" + 0.010*\"joint\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.524): 0.086*\"product\" + 0.040*\"use\" + 0.035*\"great\" + 0.021*\"good\" + 0.018*\"oil\" + 0.015*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.012*\"order\" + 0.010*\"buy\"\n",
      "INFO : topic #1 (0.667): 0.050*\"good\" + 0.035*\"great\" + 0.032*\"taste\" + 0.025*\"product\" + 0.020*\"like\" + 0.019*\"price\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.674): 0.034*\"work\" + 0.027*\"day\" + 0.023*\"feel\" + 0.019*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.067064, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.42186096, 0.67957914, 0.53879595, 0.21881607, 0.084239878, 0.68454528]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.084): 0.021*\"eye\" + 0.017*\"seed\" + 0.013*\"gaia\" + 0.010*\"tablespoon\" + 0.010*\"orange\" + 0.010*\"flax\" + 0.007*\"use\" + 0.007*\"memory\" + 0.007*\"dry_eye\" + 0.007*\"grow\"\n",
      "INFO : topic #3 (0.219): 0.029*\"pain\" + 0.017*\"day\" + 0.015*\"product\" + 0.014*\"work\" + 0.014*\"use\" + 0.013*\"help\" + 0.013*\"week\" + 0.010*\"joint\" + 0.010*\"lose\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.539): 0.087*\"product\" + 0.041*\"use\" + 0.035*\"great\" + 0.020*\"good\" + 0.018*\"oil\" + 0.016*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"order\" + 0.011*\"hair\"\n",
      "INFO : topic #1 (0.680): 0.051*\"good\" + 0.035*\"great\" + 0.031*\"taste\" + 0.024*\"product\" + 0.020*\"like\" + 0.019*\"price\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.685): 0.034*\"work\" + 0.027*\"day\" + 0.024*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.065736, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.43022826, 0.68455958, 0.56328291, 0.21822351, 0.08508984, 0.69632572]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.085): 0.020*\"eye\" + 0.016*\"seed\" + 0.014*\"grow\" + 0.011*\"gaia\" + 0.009*\"tablespoon\" + 0.009*\"flax\" + 0.009*\"orange\" + 0.009*\"butter\" + 0.007*\"use\" + 0.007*\"second_bottle\"\n",
      "INFO : topic #3 (0.218): 0.029*\"pain\" + 0.017*\"day\" + 0.015*\"product\" + 0.014*\"work\" + 0.013*\"help\" + 0.013*\"use\" + 0.013*\"week\" + 0.011*\"lose\" + 0.010*\"joint\" + 0.009*\"start\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.563): 0.085*\"product\" + 0.039*\"use\" + 0.034*\"great\" + 0.021*\"hair\" + 0.020*\"good\" + 0.017*\"oil\" + 0.016*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic #1 (0.685): 0.051*\"good\" + 0.034*\"great\" + 0.030*\"taste\" + 0.024*\"product\" + 0.020*\"like\" + 0.020*\"price\" + 0.017*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.696): 0.034*\"work\" + 0.027*\"day\" + 0.023*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"start\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"product\"\n",
      "INFO : topic diff=0.072746, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.44178617, 0.70242864, 0.58115679, 0.21949846, 0.085680574, 0.70948011]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.022*\"eye\" + 0.014*\"seed\" + 0.013*\"grow\" + 0.010*\"gaia\" + 0.009*\"memory\" + 0.009*\"tablespoon\" + 0.008*\"orange\" + 0.008*\"flax\" + 0.008*\"butter\" + 0.008*\"dry_eye\"\n",
      "INFO : topic #3 (0.219): 0.029*\"pain\" + 0.017*\"day\" + 0.015*\"product\" + 0.014*\"work\" + 0.013*\"help\" + 0.013*\"use\" + 0.013*\"week\" + 0.011*\"lose\" + 0.011*\"joint\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.581): 0.086*\"product\" + 0.038*\"use\" + 0.034*\"great\" + 0.020*\"good\" + 0.020*\"oil\" + 0.019*\"hair\" + 0.016*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\"\n",
      "INFO : topic #1 (0.702): 0.051*\"good\" + 0.034*\"great\" + 0.030*\"taste\" + 0.024*\"product\" + 0.020*\"like\" + 0.020*\"price\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.709): 0.034*\"work\" + 0.028*\"day\" + 0.024*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"start\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"time\"\n",
      "INFO : topic diff=0.072467, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.45442191, 0.7130847, 0.59508073, 0.22280514, 0.086122975, 0.72373462]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.021*\"eye\" + 0.014*\"seed\" + 0.013*\"grow\" + 0.010*\"gaia\" + 0.009*\"memory\" + 0.008*\"orange\" + 0.008*\"tablespoon\" + 0.008*\"flax\" + 0.007*\"butter\" + 0.007*\"second_bottle\"\n",
      "INFO : topic #3 (0.223): 0.030*\"pain\" + 0.017*\"day\" + 0.015*\"product\" + 0.014*\"work\" + 0.013*\"help\" + 0.013*\"use\" + 0.013*\"week\" + 0.011*\"lose\" + 0.010*\"joint\" + 0.009*\"start\"\n",
      "INFO : topic #2 (0.595): 0.088*\"product\" + 0.039*\"use\" + 0.035*\"great\" + 0.020*\"good\" + 0.019*\"oil\" + 0.019*\"hair\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\"\n",
      "INFO : topic #1 (0.713): 0.052*\"good\" + 0.035*\"great\" + 0.030*\"taste\" + 0.024*\"product\" + 0.020*\"like\" + 0.020*\"price\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.724): 0.035*\"work\" + 0.028*\"day\" + 0.024*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"start\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"time\"\n",
      "INFO : topic diff=0.070263, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.46306929, 0.72483051, 0.61674368, 0.22411908, 0.086708322, 0.74499142]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.087): 0.021*\"eye\" + 0.014*\"seed\" + 0.013*\"grow\" + 0.011*\"gaia\" + 0.009*\"memory\" + 0.008*\"tablespoon\" + 0.008*\"orange\" + 0.007*\"second_bottle\" + 0.007*\"flax\" + 0.007*\"dry_eye\"\n",
      "INFO : topic #3 (0.224): 0.030*\"pain\" + 0.016*\"day\" + 0.014*\"product\" + 0.014*\"work\" + 0.014*\"help\" + 0.013*\"use\" + 0.013*\"week\" + 0.011*\"lose\" + 0.010*\"joint\" + 0.009*\"weight\"\n",
      "INFO : topic #2 (0.617): 0.089*\"product\" + 0.039*\"use\" + 0.035*\"great\" + 0.021*\"oil\" + 0.020*\"good\" + 0.016*\"hair\" + 0.016*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\"\n",
      "INFO : topic #1 (0.725): 0.052*\"good\" + 0.035*\"great\" + 0.030*\"taste\" + 0.024*\"product\" + 0.021*\"like\" + 0.020*\"price\" + 0.018*\"easy\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.745): 0.035*\"work\" + 0.028*\"day\" + 0.024*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.014*\"start\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"good\" + 0.012*\"time\"\n",
      "INFO : topic diff=0.067403, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.47065905, 0.74556899, 0.63068694, 0.22749913, 0.087361544, 0.7621367]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.087): 0.020*\"eye\" + 0.014*\"seed\" + 0.012*\"grow\" + 0.010*\"memory\" + 0.009*\"gaia\" + 0.009*\"tablespoon\" + 0.007*\"orange\" + 0.007*\"second_bottle\" + 0.007*\"flax\" + 0.007*\"fit\"\n",
      "INFO : topic #3 (0.227): 0.029*\"pain\" + 0.016*\"day\" + 0.014*\"product\" + 0.014*\"work\" + 0.013*\"help\" + 0.013*\"use\" + 0.013*\"week\" + 0.012*\"lose\" + 0.009*\"weight\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.631): 0.091*\"product\" + 0.039*\"use\" + 0.036*\"great\" + 0.020*\"good\" + 0.020*\"oil\" + 0.016*\"skin\" + 0.015*\"hair\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\"\n",
      "INFO : topic #1 (0.746): 0.052*\"good\" + 0.035*\"great\" + 0.031*\"taste\" + 0.023*\"product\" + 0.021*\"like\" + 0.019*\"price\" + 0.018*\"easy\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.013*\"buy\"\n",
      "INFO : topic #5 (0.762): 0.035*\"work\" + 0.028*\"day\" + 0.024*\"feel\" + 0.020*\"help\" + 0.018*\"try\" + 0.015*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.013*\"year\" + 0.012*\"product\"\n",
      "INFO : topic diff=0.066718, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.47814879, 0.75591642, 0.64601696, 0.2318026, 0.088453464, 0.7795819]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.088): 0.020*\"eye\" + 0.015*\"seed\" + 0.011*\"grow\" + 0.011*\"memory\" + 0.008*\"tablespoon\" + 0.008*\"gaia\" + 0.008*\"second_bottle\" + 0.008*\"orange\" + 0.007*\"dry_eye\" + 0.006*\"fit\"\n",
      "INFO : topic #3 (0.232): 0.028*\"pain\" + 0.016*\"day\" + 0.014*\"product\" + 0.014*\"work\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"use\" + 0.012*\"lose\" + 0.010*\"weight\" + 0.009*\"joint\"\n",
      "INFO : topic #2 (0.646): 0.093*\"product\" + 0.038*\"use\" + 0.036*\"great\" + 0.020*\"good\" + 0.019*\"oil\" + 0.015*\"skin\" + 0.015*\"hair\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.014*\"order\"\n",
      "INFO : topic #1 (0.756): 0.052*\"good\" + 0.036*\"great\" + 0.031*\"taste\" + 0.023*\"product\" + 0.021*\"like\" + 0.020*\"price\" + 0.017*\"easy\" + 0.015*\"vitamin\" + 0.015*\"love\" + 0.013*\"buy\"\n",
      "INFO : topic #5 (0.780): 0.036*\"work\" + 0.028*\"day\" + 0.025*\"feel\" + 0.020*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.013*\"year\" + 0.012*\"time\"\n",
      "INFO : topic diff=0.079273, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.48319817, 0.76976919, 0.66174531, 0.23711734, 0.089379132, 0.79691541]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.089): 0.019*\"eye\" + 0.013*\"seed\" + 0.011*\"grow\" + 0.010*\"memory\" + 0.008*\"tablespoon\" + 0.008*\"second_bottle\" + 0.008*\"orange\" + 0.007*\"gaia\" + 0.007*\"review\" + 0.006*\"fit\"\n",
      "INFO : topic #3 (0.237): 0.027*\"pain\" + 0.015*\"day\" + 0.014*\"product\" + 0.014*\"work\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"use\" + 0.012*\"lose\" + 0.010*\"weight\" + 0.010*\"workout\"\n",
      "INFO : topic #2 (0.662): 0.094*\"product\" + 0.039*\"use\" + 0.037*\"great\" + 0.020*\"good\" + 0.018*\"oil\" + 0.015*\"hair\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.014*\"order\"\n",
      "INFO : topic #1 (0.770): 0.052*\"good\" + 0.036*\"great\" + 0.031*\"taste\" + 0.023*\"product\" + 0.022*\"like\" + 0.019*\"price\" + 0.017*\"easy\" + 0.015*\"vitamin\" + 0.015*\"love\" + 0.013*\"buy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.797): 0.036*\"work\" + 0.028*\"day\" + 0.025*\"feel\" + 0.020*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.012*\"time\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.070540, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.49327335, 0.7810486, 0.67562515, 0.24221852, 0.090214066, 0.81299794]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.090): 0.020*\"eye\" + 0.013*\"seed\" + 0.010*\"memory\" + 0.010*\"grow\" + 0.009*\"second_bottle\" + 0.009*\"orange\" + 0.008*\"tablespoon\" + 0.007*\"fit\" + 0.007*\"review\" + 0.006*\"gaia\"\n",
      "INFO : topic #3 (0.242): 0.027*\"pain\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"product\" + 0.014*\"week\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"lose\" + 0.010*\"workout\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.676): 0.095*\"product\" + 0.038*\"use\" + 0.037*\"great\" + 0.020*\"good\" + 0.017*\"oil\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.014*\"hair\" + 0.014*\"love\" + 0.014*\"order\"\n",
      "INFO : topic #1 (0.781): 0.052*\"good\" + 0.036*\"great\" + 0.031*\"taste\" + 0.022*\"product\" + 0.022*\"like\" + 0.019*\"price\" + 0.017*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.813): 0.037*\"work\" + 0.028*\"day\" + 0.027*\"feel\" + 0.020*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"good\" + 0.013*\"time\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.073265, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.50413656, 0.7954371, 0.69703496, 0.24574694, 0.091101401, 0.82450306]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.091): 0.020*\"eye\" + 0.013*\"seed\" + 0.012*\"memory\" + 0.010*\"grow\" + 0.009*\"second_bottle\" + 0.008*\"orange\" + 0.008*\"tablespoon\" + 0.007*\"fit\" + 0.007*\"review\" + 0.006*\"gaia\"\n",
      "INFO : topic #3 (0.246): 0.029*\"pain\" + 0.015*\"day\" + 0.014*\"week\" + 0.014*\"product\" + 0.013*\"work\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"lose\" + 0.010*\"joint\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.697): 0.097*\"product\" + 0.037*\"use\" + 0.037*\"great\" + 0.020*\"good\" + 0.016*\"oil\" + 0.015*\"recommend\" + 0.014*\"skin\" + 0.014*\"order\" + 0.014*\"love\" + 0.013*\"hair\"\n",
      "INFO : topic #1 (0.795): 0.053*\"good\" + 0.036*\"great\" + 0.030*\"taste\" + 0.022*\"product\" + 0.022*\"like\" + 0.019*\"price\" + 0.017*\"easy\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.825): 0.036*\"work\" + 0.028*\"day\" + 0.027*\"feel\" + 0.020*\"help\" + 0.020*\"try\" + 0.014*\"start\" + 0.014*\"use\" + 0.013*\"good\" + 0.013*\"time\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.067628, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.50793344, 0.79245007, 0.72774065, 0.24963419, 0.091996185, 0.83506471]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.092): 0.021*\"eye\" + 0.011*\"memory\" + 0.011*\"seed\" + 0.009*\"grow\" + 0.009*\"second_bottle\" + 0.008*\"email\" + 0.008*\"tablespoon\" + 0.007*\"fit\" + 0.007*\"orange\" + 0.007*\"review\"\n",
      "INFO : topic #3 (0.250): 0.028*\"pain\" + 0.015*\"day\" + 0.015*\"week\" + 0.013*\"product\" + 0.013*\"work\" + 0.013*\"help\" + 0.012*\"lose\" + 0.012*\"use\" + 0.011*\"joint\" + 0.010*\"weight\"\n",
      "INFO : topic #2 (0.728): 0.096*\"product\" + 0.038*\"use\" + 0.036*\"great\" + 0.020*\"good\" + 0.019*\"skin\" + 0.016*\"oil\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\" + 0.012*\"hair\"\n",
      "INFO : topic #1 (0.792): 0.052*\"good\" + 0.035*\"great\" + 0.030*\"taste\" + 0.022*\"like\" + 0.022*\"product\" + 0.019*\"price\" + 0.018*\"easy\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #5 (0.835): 0.036*\"work\" + 0.029*\"feel\" + 0.028*\"day\" + 0.020*\"try\" + 0.020*\"help\" + 0.015*\"start\" + 0.014*\"use\" + 0.013*\"good\" + 0.012*\"time\" + 0.012*\"like\"\n",
      "INFO : topic diff=0.074039, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.52186072, 0.81383032, 0.75556254, 0.25620583, 0.09288992, 0.84149808]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.093): 0.020*\"eye\" + 0.012*\"memory\" + 0.011*\"seed\" + 0.009*\"email\" + 0.009*\"second_bottle\" + 0.009*\"grow\" + 0.007*\"tablespoon\" + 0.007*\"fit\" + 0.007*\"review\" + 0.007*\"orange\"\n",
      "INFO : topic #3 (0.256): 0.027*\"pain\" + 0.015*\"week\" + 0.014*\"day\" + 0.013*\"lose\" + 0.013*\"product\" + 0.013*\"help\" + 0.012*\"work\" + 0.012*\"use\" + 0.011*\"weight\" + 0.011*\"joint\"\n",
      "INFO : topic #2 (0.756): 0.097*\"product\" + 0.037*\"use\" + 0.036*\"great\" + 0.019*\"good\" + 0.019*\"skin\" + 0.016*\"oil\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"order\" + 0.011*\"look\"\n",
      "INFO : topic #1 (0.814): 0.050*\"good\" + 0.035*\"great\" + 0.029*\"taste\" + 0.022*\"like\" + 0.022*\"product\" + 0.019*\"easy\" + 0.018*\"price\" + 0.016*\"vitamin\" + 0.014*\"love\" + 0.012*\"brand\"\n",
      "INFO : topic #5 (0.841): 0.035*\"work\" + 0.031*\"feel\" + 0.028*\"day\" + 0.020*\"try\" + 0.020*\"help\" + 0.015*\"start\" + 0.014*\"use\" + 0.013*\"good\" + 0.012*\"time\" + 0.012*\"like\"\n",
      "INFO : topic diff=0.081217, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.52473056, 0.80847812, 0.7887702, 0.26351988, 0.093610361, 0.86020762]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.094): 0.020*\"eye\" + 0.012*\"memory\" + 0.011*\"seed\" + 0.010*\"second_bottle\" + 0.009*\"email\" + 0.008*\"grow\" + 0.008*\"review\" + 0.007*\"fit\" + 0.007*\"tablespoon\" + 0.006*\"orange\"\n",
      "INFO : topic #3 (0.264): 0.026*\"pain\" + 0.016*\"week\" + 0.015*\"lose\" + 0.014*\"day\" + 0.013*\"product\" + 0.013*\"help\" + 0.012*\"weight\" + 0.012*\"work\" + 0.011*\"use\" + 0.011*\"joint\"\n",
      "INFO : topic #2 (0.789): 0.096*\"product\" + 0.036*\"use\" + 0.036*\"great\" + 0.022*\"skin\" + 0.019*\"good\" + 0.014*\"recommend\" + 0.014*\"oil\" + 0.014*\"love\" + 0.013*\"order\" + 0.013*\"look\"\n",
      "INFO : topic #1 (0.808): 0.050*\"good\" + 0.035*\"great\" + 0.028*\"taste\" + 0.023*\"like\" + 0.022*\"product\" + 0.019*\"easy\" + 0.018*\"price\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.012*\"brand\"\n",
      "INFO : topic #5 (0.860): 0.035*\"work\" + 0.031*\"feel\" + 0.028*\"day\" + 0.021*\"try\" + 0.020*\"help\" + 0.015*\"start\" + 0.013*\"use\" + 0.013*\"good\" + 0.012*\"like\" + 0.012*\"time\"\n",
      "INFO : topic diff=0.076070, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.52769017, 0.79912066, 0.81719315, 0.2725794, 0.094084889, 0.888596]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.094): 0.020*\"eye\" + 0.012*\"memory\" + 0.010*\"second_bottle\" + 0.010*\"seed\" + 0.009*\"email\" + 0.009*\"fit\" + 0.008*\"review\" + 0.008*\"grow\" + 0.006*\"assist\" + 0.006*\"tablespoon\"\n",
      "INFO : topic #3 (0.273): 0.024*\"pain\" + 0.020*\"lose\" + 0.016*\"week\" + 0.015*\"weight\" + 0.014*\"day\" + 0.013*\"help\" + 0.013*\"product\" + 0.012*\"work\" + 0.011*\"joint\" + 0.011*\"use\"\n",
      "INFO : topic #1 (0.799): 0.050*\"good\" + 0.035*\"great\" + 0.028*\"taste\" + 0.023*\"like\" + 0.022*\"product\" + 0.020*\"easy\" + 0.017*\"price\" + 0.015*\"vitamin\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic #2 (0.817): 0.097*\"product\" + 0.036*\"great\" + 0.035*\"use\" + 0.025*\"skin\" + 0.018*\"good\" + 0.015*\"recommend\" + 0.015*\"look\" + 0.014*\"love\" + 0.013*\"order\" + 0.013*\"oil\"\n",
      "INFO : topic #5 (0.889): 0.035*\"work\" + 0.033*\"feel\" + 0.028*\"day\" + 0.023*\"try\" + 0.020*\"help\" + 0.015*\"start\" + 0.013*\"good\" + 0.013*\"use\" + 0.013*\"like\" + 0.012*\"time\"\n",
      "INFO : topic diff=0.083712, rho=0.148240\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=6, decay=0.5, chunksize=5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 6 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -3.09180302341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217530/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 50%|█████     | 1/2 [07:00<07:00, 420.73s/it]INFO : using autotuned alpha, starting with [0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715]\n",
      "INFO : using symmetric eta at 0.14285714285714285\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 7 topics, 2 passes over the supplied corpus of 217530 documents, updating model once every 5000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_6.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 4901/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.078899696, 0.078232959, 0.099672362, 0.10354438, 0.097108215, 0.071659699, 0.094206348]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #5 (0.072): 0.026*\"work\" + 0.017*\"great\" + 0.017*\"use\" + 0.016*\"good\" + 0.014*\"try\" + 0.013*\"buy\" + 0.012*\"product\" + 0.009*\"day\" + 0.008*\"brand\" + 0.008*\"pedometer\"\n",
      "INFO : topic #1 (0.078): 0.030*\"pedometer\" + 0.021*\"great\" + 0.018*\"good\" + 0.017*\"product\" + 0.017*\"work\" + 0.015*\"use\" + 0.012*\"day\" + 0.012*\"accurate\" + 0.011*\"recommend\" + 0.010*\"easy\"\n",
      "INFO : topic #4 (0.097): 0.041*\"pedometer\" + 0.030*\"use\" + 0.022*\"step\" + 0.021*\"day\" + 0.018*\"easy\" + 0.017*\"omron\" + 0.017*\"love\" + 0.016*\"great\" + 0.015*\"good\" + 0.015*\"pocket\"\n",
      "INFO : topic #2 (0.100): 0.038*\"pedometer\" + 0.028*\"product\" + 0.025*\"good\" + 0.024*\"use\" + 0.017*\"great\" + 0.015*\"love\" + 0.015*\"work\" + 0.014*\"day\" + 0.013*\"step\" + 0.013*\"walk\"\n",
      "INFO : topic #3 (0.104): 0.032*\"pedometer\" + 0.028*\"day\" + 0.027*\"product\" + 0.025*\"use\" + 0.019*\"great\" + 0.019*\"step\" + 0.019*\"good\" + 0.013*\"work\" + 0.013*\"walk\" + 0.010*\"like\"\n",
      "INFO : topic diff=8.761256, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4994/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.077099167, 0.078175493, 0.10153522, 0.092793211, 0.087585419, 0.08339671, 0.087169558]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.077): 0.019*\"day\" + 0.018*\"good\" + 0.017*\"like\" + 0.013*\"use\" + 0.012*\"product\" + 0.011*\"work\" + 0.011*\"help\" + 0.011*\"great\" + 0.009*\"pedometer\" + 0.009*\"step\"\n",
      "INFO : topic #1 (0.078): 0.027*\"product\" + 0.023*\"good\" + 0.021*\"great\" + 0.014*\"taste\" + 0.013*\"work\" + 0.013*\"use\" + 0.011*\"recommend\" + 0.010*\"price\" + 0.010*\"find\" + 0.010*\"like\"\n",
      "INFO : topic #4 (0.088): 0.048*\"pedometer\" + 0.028*\"use\" + 0.025*\"step\" + 0.021*\"day\" + 0.019*\"easy\" + 0.016*\"omron\" + 0.016*\"love\" + 0.016*\"great\" + 0.015*\"good\" + 0.014*\"pocket\"\n",
      "INFO : topic #3 (0.093): 0.030*\"product\" + 0.027*\"day\" + 0.023*\"use\" + 0.023*\"pedometer\" + 0.020*\"good\" + 0.019*\"great\" + 0.015*\"step\" + 0.012*\"work\" + 0.010*\"walk\" + 0.010*\"time\"\n",
      "INFO : topic #2 (0.102): 0.037*\"product\" + 0.027*\"good\" + 0.027*\"use\" + 0.018*\"great\" + 0.015*\"pedometer\" + 0.013*\"love\" + 0.012*\"work\" + 0.012*\"recommend\" + 0.012*\"day\" + 0.011*\"skin\"\n",
      "INFO : topic diff=1.433705, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.080350399, 0.084939264, 0.10439484, 0.086912617, 0.075421065, 0.10164704, 0.084912926]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.040*\"pedometer\" + 0.027*\"use\" + 0.021*\"step\" + 0.021*\"day\" + 0.017*\"easy\" + 0.016*\"love\" + 0.015*\"great\" + 0.015*\"good\" + 0.014*\"omron\" + 0.013*\"product\"\n",
      "INFO : topic #0 (0.080): 0.019*\"good\" + 0.018*\"day\" + 0.016*\"like\" + 0.013*\"help\" + 0.013*\"product\" + 0.012*\"use\" + 0.010*\"work\" + 0.010*\"great\" + 0.009*\"feel\" + 0.008*\"recommend\"\n",
      "INFO : topic #3 (0.087): 0.034*\"product\" + 0.027*\"day\" + 0.023*\"use\" + 0.020*\"good\" + 0.019*\"great\" + 0.013*\"pedometer\" + 0.012*\"work\" + 0.010*\"time\" + 0.009*\"like\" + 0.008*\"step\"\n",
      "INFO : topic #5 (0.102): 0.019*\"product\" + 0.019*\"work\" + 0.018*\"good\" + 0.014*\"use\" + 0.014*\"help\" + 0.014*\"great\" + 0.013*\"try\" + 0.011*\"feel\" + 0.011*\"year\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.104): 0.045*\"product\" + 0.029*\"good\" + 0.028*\"use\" + 0.019*\"great\" + 0.013*\"love\" + 0.012*\"skin\" + 0.012*\"recommend\" + 0.012*\"work\" + 0.011*\"year\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.964099, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4998/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082380161, 0.089927785, 0.10981563, 0.084463708, 0.068612866, 0.11692598, 0.085360482]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.069): 0.031*\"pedometer\" + 0.028*\"use\" + 0.021*\"day\" + 0.016*\"step\" + 0.015*\"easy\" + 0.015*\"love\" + 0.014*\"great\" + 0.014*\"good\" + 0.012*\"product\" + 0.012*\"work\"\n",
      "INFO : topic #0 (0.082): 0.018*\"day\" + 0.018*\"good\" + 0.014*\"like\" + 0.014*\"help\" + 0.012*\"use\" + 0.012*\"product\" + 0.010*\"work\" + 0.009*\"great\" + 0.008*\"feel\" + 0.007*\"supplement\"\n",
      "INFO : topic #1 (0.090): 0.036*\"product\" + 0.030*\"good\" + 0.024*\"great\" + 0.013*\"taste\" + 0.013*\"price\" + 0.012*\"use\" + 0.011*\"vitamin\" + 0.011*\"work\" + 0.011*\"find\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.110): 0.047*\"product\" + 0.031*\"use\" + 0.028*\"good\" + 0.020*\"great\" + 0.016*\"skin\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"year\" + 0.011*\"oil\" + 0.011*\"recommend\"\n",
      "INFO : topic #5 (0.117): 0.020*\"work\" + 0.018*\"product\" + 0.017*\"good\" + 0.015*\"help\" + 0.014*\"use\" + 0.013*\"try\" + 0.013*\"great\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.011*\"year\"\n",
      "INFO : topic diff=0.627472, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.083757438, 0.095780909, 0.11718391, 0.083996005, 0.064154781, 0.12806934, 0.085773222]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.064): 0.028*\"use\" + 0.024*\"pedometer\" + 0.019*\"day\" + 0.016*\"love\" + 0.014*\"great\" + 0.014*\"easy\" + 0.013*\"good\" + 0.013*\"step\" + 0.012*\"work\" + 0.012*\"product\"\n",
      "INFO : topic #0 (0.084): 0.018*\"good\" + 0.016*\"day\" + 0.016*\"help\" + 0.014*\"like\" + 0.012*\"use\" + 0.011*\"product\" + 0.010*\"work\" + 0.008*\"feel\" + 0.008*\"great\" + 0.007*\"supplement\"\n",
      "INFO : topic #1 (0.096): 0.039*\"product\" + 0.033*\"good\" + 0.028*\"great\" + 0.015*\"price\" + 0.013*\"use\" + 0.013*\"taste\" + 0.011*\"find\" + 0.011*\"work\" + 0.011*\"like\" + 0.010*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.045*\"product\" + 0.034*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.016*\"skin\" + 0.016*\"oil\" + 0.014*\"love\" + 0.013*\"work\" + 0.012*\"mouse\" + 0.010*\"recommend\"\n",
      "INFO : topic #5 (0.128): 0.020*\"work\" + 0.017*\"product\" + 0.017*\"good\" + 0.016*\"help\" + 0.014*\"use\" + 0.013*\"try\" + 0.012*\"great\" + 0.011*\"feel\" + 0.011*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.463660, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.087877363, 0.10231636, 0.11630774, 0.085152149, 0.062480595, 0.14061739, 0.086486228]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.032*\"swanson\" + 0.027*\"use\" + 0.019*\"day\" + 0.015*\"love\" + 0.015*\"pedometer\" + 0.013*\"great\" + 0.013*\"product\" + 0.013*\"work\" + 0.013*\"good\" + 0.012*\"easy\"\n",
      "INFO : topic #3 (0.085): 0.031*\"product\" + 0.026*\"day\" + 0.023*\"use\" + 0.014*\"work\" + 0.014*\"good\" + 0.014*\"pain\" + 0.013*\"great\" + 0.011*\"time\" + 0.010*\"help\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.102): 0.044*\"product\" + 0.036*\"good\" + 0.028*\"great\" + 0.018*\"price\" + 0.014*\"taste\" + 0.014*\"use\" + 0.012*\"find\" + 0.011*\"recommend\" + 0.010*\"work\" + 0.010*\"like\"\n",
      "INFO : topic #2 (0.116): 0.050*\"product\" + 0.034*\"use\" + 0.026*\"good\" + 0.024*\"great\" + 0.015*\"skin\" + 0.015*\"love\" + 0.013*\"oil\" + 0.012*\"work\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.141): 0.020*\"work\" + 0.018*\"product\" + 0.016*\"help\" + 0.016*\"good\" + 0.014*\"use\" + 0.014*\"try\" + 0.013*\"feel\" + 0.011*\"great\" + 0.011*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.424523, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090476662, 0.11127699, 0.11786371, 0.086196736, 0.060638338, 0.15164649, 0.0882378]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.061): 0.027*\"use\" + 0.021*\"swanson\" + 0.018*\"day\" + 0.014*\"love\" + 0.012*\"great\" + 0.012*\"product\" + 0.012*\"work\" + 0.012*\"good\" + 0.010*\"pedometer\" + 0.010*\"easy\"\n",
      "INFO : topic #3 (0.086): 0.030*\"product\" + 0.026*\"day\" + 0.022*\"use\" + 0.014*\"work\" + 0.013*\"good\" + 0.013*\"pain\" + 0.012*\"great\" + 0.011*\"time\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.111): 0.044*\"product\" + 0.037*\"good\" + 0.030*\"great\" + 0.018*\"price\" + 0.014*\"taste\" + 0.013*\"use\" + 0.012*\"find\" + 0.011*\"recommend\" + 0.010*\"vitamin\" + 0.010*\"like\"\n",
      "INFO : topic #2 (0.118): 0.049*\"product\" + 0.034*\"use\" + 0.025*\"good\" + 0.024*\"great\" + 0.019*\"skin\" + 0.015*\"love\" + 0.014*\"oil\" + 0.012*\"work\" + 0.011*\"recommend\" + 0.011*\"year\"\n",
      "INFO : topic #5 (0.152): 0.021*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.016*\"good\" + 0.015*\"try\" + 0.013*\"use\" + 0.013*\"feel\" + 0.012*\"year\" + 0.012*\"start\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.377985, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093541734, 0.11890174, 0.12004835, 0.089023672, 0.059011783, 0.15805067, 0.090603188]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.026*\"use\" + 0.017*\"day\" + 0.016*\"swanson\" + 0.014*\"love\" + 0.012*\"product\" + 0.011*\"work\" + 0.011*\"great\" + 0.011*\"good\" + 0.009*\"purchase\" + 0.009*\"easy\"\n",
      "INFO : topic #3 (0.089): 0.029*\"product\" + 0.025*\"day\" + 0.020*\"pain\" + 0.020*\"use\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"time\" + 0.011*\"great\" + 0.011*\"help\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.119): 0.045*\"product\" + 0.040*\"good\" + 0.030*\"great\" + 0.020*\"price\" + 0.014*\"taste\" + 0.013*\"use\" + 0.012*\"find\" + 0.011*\"recommend\" + 0.010*\"like\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.120): 0.049*\"product\" + 0.035*\"use\" + 0.025*\"good\" + 0.024*\"great\" + 0.021*\"skin\" + 0.016*\"love\" + 0.013*\"oil\" + 0.012*\"work\" + 0.011*\"year\" + 0.011*\"recommend\"\n",
      "INFO : topic #5 (0.158): 0.020*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.016*\"good\" + 0.015*\"try\" + 0.013*\"feel\" + 0.013*\"use\" + 0.012*\"year\" + 0.012*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.350623, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.096873425, 0.12538373, 0.12360359, 0.091468357, 0.057986811, 0.16836932, 0.092529066]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.027*\"use\" + 0.017*\"day\" + 0.013*\"love\" + 0.012*\"product\" + 0.011*\"swanson\" + 0.011*\"work\" + 0.010*\"great\" + 0.010*\"good\" + 0.009*\"tablespoon\" + 0.009*\"easy\"\n",
      "INFO : topic #3 (0.091): 0.029*\"product\" + 0.027*\"day\" + 0.022*\"pain\" + 0.020*\"use\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"time\" + 0.011*\"help\" + 0.010*\"great\"\n",
      "INFO : topic #2 (0.124): 0.049*\"product\" + 0.038*\"use\" + 0.024*\"great\" + 0.024*\"good\" + 0.021*\"skin\" + 0.019*\"oil\" + 0.016*\"love\" + 0.012*\"work\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.125): 0.048*\"product\" + 0.041*\"good\" + 0.032*\"great\" + 0.020*\"price\" + 0.016*\"taste\" + 0.014*\"use\" + 0.012*\"find\" + 0.011*\"recommend\" + 0.010*\"like\" + 0.009*\"buy\"\n",
      "INFO : topic #5 (0.168): 0.021*\"work\" + 0.017*\"product\" + 0.016*\"help\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.012*\"day\" + 0.012*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.304915, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10070077, 0.13292551, 0.1236062, 0.094670445, 0.057163522, 0.17400202, 0.095720306]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.025*\"use\" + 0.016*\"day\" + 0.012*\"love\" + 0.011*\"work\" + 0.011*\"product\" + 0.010*\"good\" + 0.010*\"great\" + 0.008*\"swanson\" + 0.008*\"tablespoon\" + 0.008*\"easy\"\n",
      "INFO : topic #3 (0.095): 0.029*\"product\" + 0.027*\"day\" + 0.020*\"pain\" + 0.020*\"use\" + 0.017*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"time\" + 0.011*\"help\" + 0.010*\"month\"\n",
      "INFO : topic #2 (0.124): 0.050*\"product\" + 0.038*\"use\" + 0.024*\"great\" + 0.024*\"good\" + 0.022*\"skin\" + 0.016*\"love\" + 0.016*\"oil\" + 0.012*\"work\" + 0.012*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.133): 0.051*\"product\" + 0.043*\"good\" + 0.033*\"great\" + 0.020*\"price\" + 0.015*\"taste\" + 0.014*\"use\" + 0.012*\"find\" + 0.011*\"recommend\" + 0.010*\"like\" + 0.010*\"vitamin\"\n",
      "INFO : topic #5 (0.174): 0.022*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"good\" + 0.014*\"feel\" + 0.013*\"use\" + 0.013*\"day\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.306657, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1076337, 0.13903053, 0.12199922, 0.099195525, 0.056636382, 0.18048653, 0.097895958]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.024*\"use\" + 0.015*\"day\" + 0.014*\"bladder\" + 0.012*\"love\" + 0.011*\"product\" + 0.010*\"work\" + 0.010*\"tablespoon\" + 0.009*\"good\" + 0.009*\"great\" + 0.007*\"easy\"\n",
      "INFO : topic #6 (0.098): 0.024*\"taste\" + 0.022*\"great\" + 0.022*\"like\" + 0.018*\"good\" + 0.015*\"easy\" + 0.014*\"use\" + 0.013*\"pill\" + 0.012*\"product\" + 0.011*\"work\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.122): 0.051*\"product\" + 0.038*\"use\" + 0.024*\"great\" + 0.024*\"good\" + 0.021*\"skin\" + 0.017*\"love\" + 0.016*\"oil\" + 0.012*\"work\" + 0.012*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.139): 0.052*\"product\" + 0.044*\"good\" + 0.033*\"great\" + 0.021*\"price\" + 0.014*\"use\" + 0.014*\"taste\" + 0.013*\"find\" + 0.011*\"recommend\" + 0.010*\"buy\" + 0.010*\"brand\"\n",
      "INFO : topic #5 (0.180): 0.023*\"work\" + 0.017*\"product\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"feel\" + 0.013*\"use\" + 0.013*\"day\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.291258, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1103549, 0.14490767, 0.12386268, 0.099991873, 0.056105897, 0.18794189, 0.099489212]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.024*\"use\" + 0.014*\"day\" + 0.013*\"bladder\" + 0.012*\"love\" + 0.011*\"tablespoon\" + 0.011*\"product\" + 0.010*\"work\" + 0.008*\"good\" + 0.008*\"great\" + 0.007*\"purchase\"\n",
      "INFO : topic #6 (0.099): 0.026*\"taste\" + 0.023*\"like\" + 0.022*\"great\" + 0.018*\"good\" + 0.016*\"easy\" + 0.014*\"use\" + 0.012*\"pill\" + 0.011*\"product\" + 0.010*\"swallow\" + 0.010*\"work\"\n",
      "INFO : topic #2 (0.124): 0.051*\"product\" + 0.039*\"use\" + 0.025*\"great\" + 0.024*\"good\" + 0.023*\"skin\" + 0.017*\"love\" + 0.015*\"oil\" + 0.012*\"work\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.145): 0.055*\"product\" + 0.047*\"good\" + 0.035*\"great\" + 0.022*\"price\" + 0.014*\"use\" + 0.013*\"find\" + 0.013*\"taste\" + 0.011*\"recommend\" + 0.011*\"brand\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.188): 0.024*\"work\" + 0.018*\"product\" + 0.017*\"try\" + 0.017*\"help\" + 0.015*\"good\" + 0.014*\"feel\" + 0.014*\"day\" + 0.013*\"use\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.242292, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11248666, 0.14998394, 0.1247951, 0.10408481, 0.055854175, 0.19429608, 0.1028002]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.023*\"use\" + 0.013*\"day\" + 0.011*\"love\" + 0.011*\"tablespoon\" + 0.010*\"product\" + 0.010*\"bladder\" + 0.010*\"work\" + 0.008*\"good\" + 0.007*\"great\" + 0.006*\"purchase\"\n",
      "INFO : topic #6 (0.103): 0.025*\"taste\" + 0.023*\"like\" + 0.021*\"great\" + 0.018*\"good\" + 0.016*\"easy\" + 0.014*\"magnesium\" + 0.013*\"use\" + 0.013*\"pill\" + 0.011*\"product\" + 0.011*\"water\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.125): 0.050*\"product\" + 0.039*\"use\" + 0.025*\"great\" + 0.023*\"good\" + 0.023*\"skin\" + 0.017*\"love\" + 0.014*\"oil\" + 0.012*\"work\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.150): 0.056*\"product\" + 0.048*\"good\" + 0.035*\"great\" + 0.022*\"price\" + 0.014*\"use\" + 0.014*\"find\" + 0.012*\"recommend\" + 0.012*\"taste\" + 0.011*\"brand\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.194): 0.024*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.017*\"try\" + 0.015*\"good\" + 0.015*\"feel\" + 0.014*\"day\" + 0.013*\"use\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic diff=0.240678, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11530176, 0.15719478, 0.12678698, 0.10799877, 0.056053787, 0.20044579, 0.10465965]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.027*\"camera\" + 0.022*\"use\" + 0.012*\"day\" + 0.010*\"love\" + 0.009*\"tablespoon\" + 0.008*\"work\" + 0.008*\"bladder\" + 0.008*\"product\" + 0.008*\"great\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.105): 0.026*\"taste\" + 0.023*\"like\" + 0.021*\"great\" + 0.018*\"good\" + 0.016*\"easy\" + 0.013*\"pill\" + 0.013*\"use\" + 0.012*\"magnesium\" + 0.011*\"product\" + 0.011*\"water\"\n",
      "INFO : topic #2 (0.127): 0.050*\"product\" + 0.039*\"use\" + 0.026*\"great\" + 0.023*\"good\" + 0.022*\"skin\" + 0.017*\"love\" + 0.013*\"oil\" + 0.012*\"work\" + 0.012*\"hair\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.157): 0.057*\"product\" + 0.048*\"good\" + 0.036*\"great\" + 0.022*\"price\" + 0.015*\"use\" + 0.014*\"find\" + 0.012*\"recommend\" + 0.012*\"buy\" + 0.012*\"brand\" + 0.010*\"taste\"\n",
      "INFO : topic #5 (0.200): 0.025*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.016*\"try\" + 0.015*\"feel\" + 0.015*\"day\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.251949, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11786495, 0.16501652, 0.12700221, 0.11062978, 0.056208849, 0.20780733, 0.10675431]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.021*\"use\" + 0.020*\"camera\" + 0.011*\"day\" + 0.010*\"iodoral\" + 0.009*\"love\" + 0.008*\"tablespoon\" + 0.008*\"product\" + 0.008*\"work\" + 0.008*\"bladder\" + 0.007*\"good\"\n",
      "INFO : topic #6 (0.107): 0.027*\"taste\" + 0.023*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.016*\"easy\" + 0.014*\"use\" + 0.012*\"pill\" + 0.011*\"magnesium\" + 0.010*\"product\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.127): 0.049*\"product\" + 0.041*\"use\" + 0.025*\"great\" + 0.022*\"good\" + 0.021*\"skin\" + 0.018*\"love\" + 0.014*\"oil\" + 0.012*\"work\" + 0.012*\"hair\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.165): 0.057*\"product\" + 0.048*\"good\" + 0.035*\"great\" + 0.022*\"price\" + 0.017*\"use\" + 0.014*\"find\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"recommend\" + 0.011*\"taste\"\n",
      "INFO : topic #5 (0.208): 0.025*\"work\" + 0.017*\"product\" + 0.017*\"try\" + 0.017*\"help\" + 0.015*\"feel\" + 0.015*\"day\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.213930, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1206995, 0.17202079, 0.12963815, 0.11364957, 0.056186292, 0.20982902, 0.11014105]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.021*\"use\" + 0.016*\"camera\" + 0.011*\"day\" + 0.010*\"tablespoon\" + 0.009*\"love\" + 0.008*\"product\" + 0.008*\"iodoral\" + 0.008*\"work\" + 0.007*\"bladder\" + 0.007*\"good\"\n",
      "INFO : topic #6 (0.110): 0.029*\"taste\" + 0.023*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.016*\"easy\" + 0.014*\"use\" + 0.012*\"pill\" + 0.012*\"water\" + 0.011*\"mix\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.130): 0.048*\"product\" + 0.043*\"use\" + 0.025*\"great\" + 0.023*\"skin\" + 0.021*\"good\" + 0.017*\"love\" + 0.015*\"oil\" + 0.012*\"work\" + 0.012*\"hair\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.172): 0.058*\"product\" + 0.049*\"good\" + 0.036*\"great\" + 0.022*\"price\" + 0.017*\"use\" + 0.014*\"find\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"recommend\" + 0.011*\"taste\"\n",
      "INFO : topic #5 (0.210): 0.025*\"work\" + 0.018*\"product\" + 0.017*\"try\" + 0.017*\"help\" + 0.016*\"feel\" + 0.015*\"day\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.207910, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12570122, 0.17973182, 0.13063605, 0.1158846, 0.056242079, 0.21783119, 0.11355096]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.020*\"use\" + 0.012*\"camera\" + 0.010*\"day\" + 0.009*\"tablespoon\" + 0.008*\"love\" + 0.008*\"product\" + 0.007*\"bladder\" + 0.007*\"work\" + 0.006*\"good\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #6 (0.114): 0.028*\"taste\" + 0.023*\"like\" + 0.020*\"great\" + 0.020*\"good\" + 0.016*\"easy\" + 0.013*\"pill\" + 0.013*\"use\" + 0.011*\"water\" + 0.010*\"mix\" + 0.010*\"swallow\"\n",
      "INFO : topic #2 (0.131): 0.047*\"product\" + 0.041*\"use\" + 0.024*\"great\" + 0.022*\"skin\" + 0.021*\"good\" + 0.017*\"hair\" + 0.016*\"love\" + 0.016*\"oil\" + 0.012*\"work\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.180): 0.059*\"product\" + 0.050*\"good\" + 0.036*\"great\" + 0.023*\"price\" + 0.017*\"use\" + 0.014*\"brand\" + 0.014*\"find\" + 0.012*\"buy\" + 0.012*\"recommend\" + 0.010*\"supplement\"\n",
      "INFO : topic #5 (0.218): 0.025*\"work\" + 0.017*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.016*\"feel\" + 0.016*\"day\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.204412, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13047171, 0.18760729, 0.13100092, 0.11860226, 0.056268595, 0.22093832, 0.11684328]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.018*\"use\" + 0.010*\"day\" + 0.010*\"camera\" + 0.008*\"tablespoon\" + 0.008*\"love\" + 0.007*\"product\" + 0.007*\"work\" + 0.006*\"bladder\" + 0.006*\"second_bottle\" + 0.006*\"good\"\n",
      "INFO : topic #6 (0.117): 0.028*\"taste\" + 0.022*\"like\" + 0.019*\"great\" + 0.019*\"good\" + 0.018*\"easy\" + 0.013*\"pill\" + 0.012*\"use\" + 0.012*\"swallow\" + 0.011*\"water\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.131): 0.046*\"product\" + 0.041*\"use\" + 0.024*\"great\" + 0.024*\"skin\" + 0.020*\"good\" + 0.017*\"hair\" + 0.016*\"love\" + 0.015*\"oil\" + 0.012*\"work\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.188): 0.059*\"product\" + 0.050*\"good\" + 0.036*\"great\" + 0.024*\"price\" + 0.017*\"use\" + 0.015*\"brand\" + 0.014*\"find\" + 0.012*\"buy\" + 0.012*\"recommend\" + 0.011*\"supplement\"\n",
      "INFO : topic #5 (0.221): 0.025*\"work\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.016*\"feel\" + 0.016*\"day\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.193060, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13408293, 0.19853608, 0.12994269, 0.12030446, 0.056686744, 0.22326241, 0.12075138]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.018*\"use\" + 0.015*\"swanson\" + 0.010*\"tablespoon\" + 0.010*\"day\" + 0.009*\"optimum\" + 0.008*\"camera\" + 0.007*\"love\" + 0.007*\"product\" + 0.006*\"work\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #3 (0.120): 0.027*\"day\" + 0.027*\"product\" + 0.025*\"pain\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.012*\"year\" + 0.012*\"week\" + 0.011*\"start\" + 0.010*\"time\"\n",
      "INFO : topic #0 (0.134): 0.018*\"help\" + 0.015*\"supplement\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"body\" + 0.009*\"product\" + 0.009*\"health\" + 0.009*\"'s\" + 0.008*\"use\" + 0.008*\"food\"\n",
      "INFO : topic #1 (0.199): 0.060*\"product\" + 0.051*\"good\" + 0.039*\"great\" + 0.024*\"price\" + 0.017*\"use\" + 0.014*\"brand\" + 0.014*\"find\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.012*\"vitamin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.223): 0.026*\"work\" + 0.018*\"feel\" + 0.017*\"help\" + 0.017*\"day\" + 0.017*\"product\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.172384, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13609295, 0.20708205, 0.13225482, 0.12153839, 0.05695549, 0.22499208, 0.12452893]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.017*\"use\" + 0.012*\"swanson\" + 0.011*\"tablespoon\" + 0.009*\"day\" + 0.007*\"optimum\" + 0.007*\"love\" + 0.006*\"product\" + 0.006*\"second_bottle\" + 0.006*\"camera\" + 0.006*\"work\"\n",
      "INFO : topic #3 (0.122): 0.026*\"day\" + 0.026*\"product\" + 0.025*\"pain\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.012*\"year\" + 0.012*\"week\" + 0.011*\"time\" + 0.010*\"start\"\n",
      "INFO : topic #0 (0.136): 0.017*\"help\" + 0.015*\"supplement\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"body\" + 0.009*\"health\" + 0.009*\"product\" + 0.009*\"'s\" + 0.008*\"use\" + 0.008*\"food\"\n",
      "INFO : topic #1 (0.207): 0.059*\"product\" + 0.051*\"good\" + 0.039*\"great\" + 0.024*\"price\" + 0.018*\"use\" + 0.015*\"brand\" + 0.014*\"find\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.225): 0.026*\"work\" + 0.017*\"help\" + 0.017*\"feel\" + 0.017*\"product\" + 0.017*\"day\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.170387, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13821194, 0.21287084, 0.13335329, 0.1245851, 0.057040244, 0.23224643, 0.12741059]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.017*\"use\" + 0.010*\"tablespoon\" + 0.009*\"swanson\" + 0.009*\"day\" + 0.007*\"optimum\" + 0.007*\"love\" + 0.006*\"product\" + 0.006*\"second_bottle\" + 0.006*\"work\" + 0.006*\"want\"\n",
      "INFO : topic #3 (0.125): 0.026*\"day\" + 0.025*\"product\" + 0.025*\"pain\" + 0.018*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.012*\"week\" + 0.012*\"year\" + 0.011*\"time\" + 0.011*\"start\"\n",
      "INFO : topic #0 (0.138): 0.018*\"help\" + 0.015*\"supplement\" + 0.011*\"good\" + 0.010*\"body\" + 0.010*\"day\" + 0.009*\"health\" + 0.009*\"'s\" + 0.009*\"product\" + 0.008*\"use\" + 0.008*\"food\"\n",
      "INFO : topic #1 (0.213): 0.061*\"product\" + 0.052*\"good\" + 0.040*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.015*\"brand\" + 0.014*\"find\" + 0.014*\"buy\" + 0.012*\"recommend\" + 0.011*\"vitamin\"\n",
      "INFO : topic #5 (0.232): 0.027*\"work\" + 0.018*\"help\" + 0.017*\"feel\" + 0.017*\"day\" + 0.017*\"product\" + 0.017*\"try\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.160449, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14107314, 0.22531638, 0.13297012, 0.12653401, 0.057319626, 0.23610932, 0.1334767]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.016*\"use\" + 0.010*\"tablespoon\" + 0.009*\"day\" + 0.008*\"swanson\" + 0.006*\"love\" + 0.006*\"optimum\" + 0.006*\"second_bottle\" + 0.006*\"product\" + 0.006*\"want\" + 0.005*\"work\"\n",
      "INFO : topic #3 (0.127): 0.026*\"day\" + 0.025*\"product\" + 0.024*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.012*\"year\" + 0.012*\"week\" + 0.011*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #0 (0.141): 0.018*\"help\" + 0.016*\"supplement\" + 0.011*\"good\" + 0.010*\"body\" + 0.010*\"day\" + 0.009*\"health\" + 0.009*\"'s\" + 0.008*\"product\" + 0.008*\"use\" + 0.007*\"food\"\n",
      "INFO : topic #1 (0.225): 0.061*\"product\" + 0.051*\"good\" + 0.039*\"great\" + 0.026*\"price\" + 0.018*\"use\" + 0.015*\"find\" + 0.015*\"brand\" + 0.014*\"buy\" + 0.012*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.236): 0.027*\"work\" + 0.018*\"help\" + 0.018*\"day\" + 0.017*\"feel\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.159636, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14168322, 0.23529111, 0.13507709, 0.12744534, 0.057487074, 0.23713084, 0.13946448]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.015*\"use\" + 0.010*\"tablespoon\" + 0.009*\"day\" + 0.007*\"swanson\" + 0.006*\"love\" + 0.006*\"osteoporosis\" + 0.006*\"second_bottle\" + 0.006*\"bladder\" + 0.006*\"product\" + 0.006*\"want\"\n",
      "INFO : topic #3 (0.127): 0.026*\"day\" + 0.025*\"product\" + 0.023*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.012*\"year\" + 0.012*\"week\" + 0.011*\"time\" + 0.011*\"start\"\n",
      "INFO : topic #0 (0.142): 0.017*\"help\" + 0.016*\"supplement\" + 0.011*\"good\" + 0.010*\"body\" + 0.010*\"day\" + 0.010*\"'s\" + 0.010*\"health\" + 0.008*\"product\" + 0.008*\"use\" + 0.007*\"food\"\n",
      "INFO : topic #1 (0.235): 0.063*\"product\" + 0.052*\"good\" + 0.040*\"great\" + 0.026*\"price\" + 0.018*\"use\" + 0.015*\"brand\" + 0.015*\"find\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.237): 0.027*\"work\" + 0.018*\"help\" + 0.018*\"day\" + 0.017*\"feel\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic diff=0.143539, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14338049, 0.24156342, 0.13664754, 0.12907177, 0.057589192, 0.24107374, 0.14500068]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.015*\"use\" + 0.009*\"tablespoon\" + 0.008*\"grow\" + 0.008*\"day\" + 0.007*\"swanson\" + 0.006*\"second_bottle\" + 0.006*\"love\" + 0.006*\"want\" + 0.006*\"bladder\" + 0.006*\"product\"\n",
      "INFO : topic #3 (0.129): 0.026*\"day\" + 0.025*\"product\" + 0.022*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.014*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.011*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.145): 0.035*\"taste\" + 0.024*\"like\" + 0.020*\"good\" + 0.019*\"easy\" + 0.018*\"great\" + 0.016*\"pill\" + 0.014*\"vitamin\" + 0.013*\"swallow\" + 0.013*\"fish_oil\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.241): 0.027*\"work\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"feel\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.242): 0.064*\"product\" + 0.053*\"good\" + 0.040*\"great\" + 0.027*\"price\" + 0.017*\"use\" + 0.015*\"find\" + 0.015*\"brand\" + 0.015*\"buy\" + 0.013*\"recommend\" + 0.013*\"vitamin\"\n",
      "INFO : topic diff=0.139560, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14488722, 0.25269485, 0.1379853, 0.13074532, 0.057635933, 0.23919092, 0.15271401]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.014*\"use\" + 0.009*\"grow\" + 0.008*\"tablespoon\" + 0.008*\"day\" + 0.007*\"swanson\" + 0.007*\"second_bottle\" + 0.006*\"want\" + 0.006*\"assist\" + 0.006*\"love\" + 0.006*\"optimum\"\n",
      "INFO : topic #3 (0.131): 0.026*\"day\" + 0.025*\"product\" + 0.022*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.011*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.153): 0.036*\"taste\" + 0.023*\"like\" + 0.020*\"good\" + 0.019*\"easy\" + 0.019*\"great\" + 0.016*\"pill\" + 0.015*\"fish_oil\" + 0.014*\"vitamin\" + 0.013*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.239): 0.028*\"work\" + 0.019*\"day\" + 0.018*\"help\" + 0.018*\"feel\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.253): 0.065*\"product\" + 0.054*\"good\" + 0.041*\"great\" + 0.027*\"price\" + 0.017*\"use\" + 0.016*\"brand\" + 0.015*\"buy\" + 0.015*\"find\" + 0.014*\"recommend\" + 0.013*\"vitamin\"\n",
      "INFO : topic diff=0.132294, rho=0.200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14544031, 0.26361319, 0.13742417, 0.13208283, 0.057838157, 0.24378586, 0.15865144]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.013*\"use\" + 0.011*\"grow\" + 0.008*\"tablespoon\" + 0.008*\"osteoporosis\" + 0.007*\"day\" + 0.006*\"swanson\" + 0.006*\"second_bottle\" + 0.006*\"want\" + 0.005*\"love\" + 0.005*\"product\"\n",
      "INFO : topic #3 (0.132): 0.025*\"day\" + 0.025*\"product\" + 0.021*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.011*\"start\" + 0.011*\"month\"\n",
      "INFO : topic #6 (0.159): 0.037*\"taste\" + 0.024*\"like\" + 0.020*\"good\" + 0.019*\"easy\" + 0.019*\"great\" + 0.016*\"pill\" + 0.014*\"fish_oil\" + 0.014*\"vitamin\" + 0.013*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.244): 0.028*\"work\" + 0.019*\"day\" + 0.018*\"feel\" + 0.018*\"help\" + 0.017*\"try\" + 0.017*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.264): 0.066*\"product\" + 0.055*\"good\" + 0.042*\"great\" + 0.027*\"price\" + 0.017*\"use\" + 0.016*\"buy\" + 0.016*\"brand\" + 0.015*\"find\" + 0.014*\"recommend\" + 0.013*\"vitamin\"\n",
      "INFO : topic diff=0.124672, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14633562, 0.27173102, 0.14175645, 0.13427418, 0.058367822, 0.24520954, 0.16372174]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.013*\"use\" + 0.013*\"tablespoon\" + 0.010*\"grow\" + 0.007*\"day\" + 0.007*\"second_bottle\" + 0.006*\"osteoporosis\" + 0.006*\"swanson\" + 0.005*\"want\" + 0.005*\"love\" + 0.005*\"meet\"\n",
      "INFO : topic #3 (0.134): 0.025*\"product\" + 0.024*\"day\" + 0.020*\"pain\" + 0.019*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.011*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.164): 0.041*\"taste\" + 0.024*\"like\" + 0.020*\"good\" + 0.019*\"easy\" + 0.019*\"great\" + 0.017*\"fish_oil\" + 0.015*\"pill\" + 0.013*\"swallow\" + 0.012*\"vitamin\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.245): 0.028*\"work\" + 0.019*\"day\" + 0.018*\"feel\" + 0.018*\"help\" + 0.017*\"try\" + 0.017*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.272): 0.067*\"product\" + 0.055*\"good\" + 0.042*\"great\" + 0.027*\"price\" + 0.017*\"use\" + 0.016*\"buy\" + 0.016*\"brand\" + 0.015*\"find\" + 0.014*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic diff=0.135807, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14888635, 0.28213924, 0.14234422, 0.13738097, 0.059230205, 0.25465047, 0.16668402]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.025*\"gaia\" + 0.013*\"use\" + 0.012*\"tablespoon\" + 0.009*\"grow\" + 0.007*\"second_bottle\" + 0.007*\"day\" + 0.005*\"osteoporosis\" + 0.005*\"want\" + 0.005*\"new\" + 0.005*\"meet\"\n",
      "INFO : topic #3 (0.137): 0.025*\"product\" + 0.024*\"day\" + 0.019*\"work\" + 0.019*\"pain\" + 0.016*\"use\" + 0.015*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.012*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.167): 0.041*\"taste\" + 0.024*\"like\" + 0.020*\"good\" + 0.019*\"easy\" + 0.019*\"great\" + 0.015*\"pill\" + 0.014*\"fish_oil\" + 0.012*\"love\" + 0.012*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.255): 0.028*\"work\" + 0.020*\"feel\" + 0.019*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.282): 0.068*\"product\" + 0.055*\"good\" + 0.042*\"great\" + 0.026*\"price\" + 0.017*\"use\" + 0.016*\"buy\" + 0.016*\"brand\" + 0.015*\"find\" + 0.014*\"recommend\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.134076, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1512146, 0.29148549, 0.14433919, 0.1385846, 0.059654251, 0.25610271, 0.17277756]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.060): 0.022*\"gaia\" + 0.012*\"use\" + 0.011*\"tablespoon\" + 0.009*\"grow\" + 0.007*\"second_bottle\" + 0.006*\"day\" + 0.005*\"meet\" + 0.005*\"osteoporosis\" + 0.005*\"want\" + 0.005*\"new\"\n",
      "INFO : topic #3 (0.139): 0.025*\"product\" + 0.024*\"day\" + 0.020*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.013*\"week\" + 0.012*\"year\" + 0.012*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.173): 0.041*\"taste\" + 0.024*\"like\" + 0.020*\"good\" + 0.020*\"easy\" + 0.019*\"great\" + 0.014*\"pill\" + 0.014*\"fish_oil\" + 0.013*\"love\" + 0.013*\"vitamin\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.256): 0.028*\"work\" + 0.020*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.291): 0.069*\"product\" + 0.055*\"good\" + 0.042*\"great\" + 0.026*\"price\" + 0.017*\"use\" + 0.017*\"buy\" + 0.016*\"brand\" + 0.015*\"find\" + 0.014*\"recommend\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.117485, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15384191, 0.3003839, 0.14498022, 0.14161514, 0.060207665, 0.26001853, 0.17652437]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.060): 0.018*\"gaia\" + 0.012*\"use\" + 0.011*\"tablespoon\" + 0.010*\"grow\" + 0.006*\"second_bottle\" + 0.006*\"day\" + 0.006*\"meet\" + 0.005*\"new\" + 0.005*\"normal_range\" + 0.005*\"want\"\n",
      "INFO : topic #3 (0.142): 0.025*\"product\" + 0.024*\"day\" + 0.021*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.177): 0.041*\"taste\" + 0.024*\"like\" + 0.020*\"easy\" + 0.020*\"good\" + 0.019*\"great\" + 0.015*\"pill\" + 0.013*\"fish_oil\" + 0.013*\"love\" + 0.012*\"vitamin\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.260): 0.028*\"work\" + 0.020*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.300): 0.070*\"product\" + 0.054*\"good\" + 0.043*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"brand\" + 0.015*\"find\" + 0.015*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.116645, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15599313, 0.30907696, 0.14609799, 0.14388902, 0.060737438, 0.26236781, 0.17958917]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.061): 0.015*\"grow\" + 0.015*\"gaia\" + 0.011*\"use\" + 0.010*\"tablespoon\" + 0.006*\"second_bottle\" + 0.006*\"meet\" + 0.006*\"day\" + 0.006*\"assist\" + 0.005*\"new\" + 0.005*\"osteoporosis\"\n",
      "INFO : topic #3 (0.144): 0.025*\"product\" + 0.023*\"day\" + 0.022*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.180): 0.041*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.015*\"pill\" + 0.013*\"love\" + 0.012*\"fish_oil\" + 0.012*\"vitamin\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.262): 0.029*\"work\" + 0.020*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.309): 0.071*\"product\" + 0.054*\"good\" + 0.043*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"find\" + 0.015*\"brand\" + 0.014*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.108459, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15703635, 0.31510231, 0.15006436, 0.14496708, 0.061690137, 0.26472244, 0.18255892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.038*\"grow\" + 0.011*\"gaia\" + 0.010*\"use\" + 0.010*\"nutrigold\" + 0.009*\"tablespoon\" + 0.007*\"inch\" + 0.006*\"second_bottle\" + 0.006*\"meet\" + 0.006*\"osteoporosis\" + 0.006*\"day\"\n",
      "INFO : topic #3 (0.145): 0.025*\"product\" + 0.024*\"day\" + 0.021*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.012*\"month\" + 0.011*\"year\"\n",
      "INFO : topic #6 (0.183): 0.039*\"taste\" + 0.024*\"like\" + 0.021*\"easy\" + 0.019*\"good\" + 0.018*\"great\" + 0.016*\"pill\" + 0.013*\"love\" + 0.013*\"fish_oil\" + 0.013*\"vitamin\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.265): 0.029*\"work\" + 0.020*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.018*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic #1 (0.315): 0.072*\"product\" + 0.054*\"good\" + 0.043*\"great\" + 0.026*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"find\" + 0.015*\"brand\" + 0.014*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic diff=0.114580, rho=0.176777\n",
      "INFO : PROGRESS: pass 0, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15984271, 0.32687497, 0.15199389, 0.14677978, 0.062168173, 0.26930737, 0.18689184]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.034*\"grow\" + 0.011*\"nutrigold\" + 0.010*\"use\" + 0.010*\"gaia\" + 0.008*\"tablespoon\" + 0.007*\"second_bottle\" + 0.006*\"inch\" + 0.006*\"meet\" + 0.006*\"new\" + 0.005*\"day\"\n",
      "INFO : topic #3 (0.147): 0.025*\"product\" + 0.024*\"day\" + 0.021*\"pain\" + 0.020*\"work\" + 0.016*\"use\" + 0.015*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"month\" + 0.011*\"year\"\n",
      "INFO : topic #6 (0.187): 0.039*\"taste\" + 0.024*\"like\" + 0.021*\"easy\" + 0.019*\"good\" + 0.018*\"great\" + 0.017*\"pill\" + 0.014*\"fish_oil\" + 0.013*\"love\" + 0.013*\"vitamin\" + 0.013*\"swallow\"\n",
      "INFO : topic #5 (0.269): 0.029*\"work\" + 0.020*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic #1 (0.327): 0.073*\"product\" + 0.055*\"good\" + 0.043*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"find\" + 0.015*\"brand\" + 0.015*\"recommend\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.111926, rho=0.174078\n",
      "INFO : PROGRESS: pass 0, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16272897, 0.33588454, 0.15298493, 0.14987269, 0.062791608, 0.27464852, 0.1901741]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.036*\"grow\" + 0.015*\"nutrigold\" + 0.010*\"use\" + 0.009*\"gaia\" + 0.007*\"tablespoon\" + 0.006*\"second_bottle\" + 0.006*\"meet\" + 0.006*\"inch\" + 0.006*\"new\" + 0.005*\"want\"\n",
      "INFO : topic #3 (0.150): 0.024*\"product\" + 0.023*\"day\" + 0.021*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.015*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"month\" + 0.011*\"year\"\n",
      "INFO : topic #6 (0.190): 0.039*\"taste\" + 0.024*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.017*\"pill\" + 0.014*\"fish_oil\" + 0.013*\"vitamin\" + 0.013*\"love\" + 0.013*\"swallow\"\n",
      "INFO : topic #5 (0.275): 0.029*\"work\" + 0.021*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.017*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic #1 (0.336): 0.075*\"product\" + 0.055*\"good\" + 0.044*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.016*\"find\" + 0.015*\"brand\" + 0.015*\"recommend\" + 0.012*\"vitamin\"\n",
      "INFO : topic diff=0.104702, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16480853, 0.34561738, 0.15535954, 0.15236045, 0.063209616, 0.28081623, 0.19344065]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.033*\"grow\" + 0.019*\"nutrigold\" + 0.010*\"gaia\" + 0.009*\"use\" + 0.007*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"meet\" + 0.006*\"new\" + 0.005*\"inch\" + 0.005*\"normal_range\"\n",
      "INFO : topic #3 (0.152): 0.025*\"product\" + 0.023*\"day\" + 0.021*\"pain\" + 0.020*\"work\" + 0.017*\"use\" + 0.016*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"month\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.193): 0.039*\"taste\" + 0.025*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.016*\"pill\" + 0.014*\"swallow\" + 0.013*\"love\" + 0.013*\"vitamin\" + 0.013*\"fish_oil\"\n",
      "INFO : topic #5 (0.281): 0.029*\"work\" + 0.021*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.018*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"year\"\n",
      "INFO : topic #1 (0.346): 0.077*\"product\" + 0.054*\"good\" + 0.045*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.016*\"find\" + 0.015*\"brand\" + 0.015*\"recommend\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.096877, rho=0.169031\n",
      "INFO : PROGRESS: pass 0, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16766113, 0.35732779, 0.15554099, 0.15481076, 0.063796461, 0.2848216, 0.19887063]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.064): 0.032*\"grow\" + 0.019*\"nutrigold\" + 0.009*\"use\" + 0.009*\"gaia\" + 0.008*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"meet\" + 0.006*\"new\" + 0.006*\"inch\" + 0.005*\"want\"\n",
      "INFO : topic #3 (0.155): 0.025*\"product\" + 0.023*\"day\" + 0.021*\"work\" + 0.020*\"pain\" + 0.017*\"use\" + 0.016*\"help\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"month\" + 0.011*\"time\"\n",
      "INFO : topic #6 (0.199): 0.040*\"taste\" + 0.025*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.016*\"pill\" + 0.014*\"love\" + 0.013*\"vitamin\" + 0.013*\"swallow\" + 0.011*\"fish_oil\"\n",
      "INFO : topic #5 (0.285): 0.030*\"work\" + 0.021*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.018*\"product\" + 0.017*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.357): 0.078*\"product\" + 0.054*\"good\" + 0.046*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"find\" + 0.015*\"recommend\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.099524, rho=0.166667\n",
      "INFO : PROGRESS: pass 0, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17021722, 0.36876166, 0.15609489, 0.15864493, 0.064663142, 0.29047325, 0.2024565]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.030*\"grow\" + 0.020*\"nutrigold\" + 0.009*\"use\" + 0.008*\"gaia\" + 0.007*\"tablespoon\" + 0.007*\"second_bottle\" + 0.006*\"meet\" + 0.006*\"new\" + 0.006*\"inch\" + 0.005*\"expectation\"\n",
      "INFO : topic #2 (0.156): 0.038*\"use\" + 0.035*\"product\" + 0.032*\"oil\" + 0.026*\"skin\" + 0.025*\"hair\" + 0.018*\"great\" + 0.016*\"love\" + 0.015*\"good\" + 0.013*\"krill\" + 0.012*\"nail\"\n",
      "INFO : topic #6 (0.202): 0.041*\"taste\" + 0.025*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.015*\"pill\" + 0.014*\"love\" + 0.012*\"vitamin\" + 0.012*\"swallow\" + 0.011*\"fish_oil\"\n",
      "INFO : topic #5 (0.290): 0.030*\"work\" + 0.022*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.018*\"product\" + 0.018*\"try\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.369): 0.079*\"product\" + 0.055*\"good\" + 0.047*\"great\" + 0.025*\"price\" + 0.018*\"use\" + 0.017*\"buy\" + 0.015*\"find\" + 0.015*\"recommend\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.107906, rho=0.164399\n",
      "INFO : PROGRESS: pass 0, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17225678, 0.38012064, 0.15694851, 0.1630441, 0.065447696, 0.29587686, 0.20594823]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.030*\"grow\" + 0.017*\"nutrigold\" + 0.008*\"use\" + 0.008*\"second_bottle\" + 0.008*\"tablespoon\" + 0.006*\"meet\" + 0.006*\"gaia\" + 0.006*\"new\" + 0.006*\"expectation\" + 0.005*\"inch\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.157): 0.038*\"use\" + 0.034*\"product\" + 0.031*\"oil\" + 0.026*\"hair\" + 0.026*\"skin\" + 0.018*\"great\" + 0.016*\"love\" + 0.015*\"good\" + 0.012*\"nail\" + 0.011*\"krill\"\n",
      "INFO : topic #6 (0.206): 0.041*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.015*\"pill\" + 0.014*\"love\" + 0.012*\"swallow\" + 0.012*\"vitamin\" + 0.012*\"fish_oil\"\n",
      "INFO : topic #5 (0.296): 0.030*\"work\" + 0.023*\"feel\" + 0.021*\"day\" + 0.018*\"help\" + 0.018*\"try\" + 0.018*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.012*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.380): 0.081*\"product\" + 0.055*\"good\" + 0.048*\"great\" + 0.024*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.095200, rho=0.162221\n",
      "INFO : PROGRESS: pass 0, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17622827, 0.39252996, 0.15767874, 0.167403, 0.066186897, 0.30073452, 0.20843329]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.066): 0.028*\"grow\" + 0.016*\"nutrigold\" + 0.008*\"use\" + 0.008*\"second_bottle\" + 0.008*\"tablespoon\" + 0.006*\"new\" + 0.006*\"meet\" + 0.006*\"expectation\" + 0.005*\"review\" + 0.005*\"gaia\"\n",
      "INFO : topic #2 (0.158): 0.037*\"use\" + 0.034*\"product\" + 0.030*\"oil\" + 0.027*\"skin\" + 0.025*\"hair\" + 0.018*\"great\" + 0.016*\"love\" + 0.015*\"good\" + 0.012*\"nail\" + 0.010*\"krill\"\n",
      "INFO : topic #6 (0.208): 0.041*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.020*\"good\" + 0.018*\"great\" + 0.015*\"pill\" + 0.014*\"love\" + 0.013*\"fish_oil\" + 0.012*\"vitamin\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.301): 0.031*\"work\" + 0.024*\"feel\" + 0.021*\"day\" + 0.018*\"help\" + 0.018*\"try\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.012*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.393): 0.081*\"product\" + 0.055*\"good\" + 0.048*\"great\" + 0.023*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"find\" + 0.015*\"recommend\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.099211, rho=0.160128\n",
      "INFO : PROGRESS: pass 0, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17925896, 0.40763208, 0.15863182, 0.17048456, 0.066818461, 0.30469254, 0.21317808]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.067): 0.028*\"grow\" + 0.014*\"nutrigold\" + 0.008*\"second_bottle\" + 0.008*\"use\" + 0.007*\"meet\" + 0.007*\"expectation\" + 0.007*\"tablespoon\" + 0.006*\"new\" + 0.006*\"review\" + 0.005*\"gaia\"\n",
      "INFO : topic #2 (0.159): 0.037*\"use\" + 0.034*\"product\" + 0.030*\"oil\" + 0.027*\"skin\" + 0.025*\"hair\" + 0.017*\"great\" + 0.016*\"love\" + 0.015*\"good\" + 0.012*\"nail\" + 0.010*\"look\"\n",
      "INFO : topic #6 (0.213): 0.040*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.019*\"good\" + 0.018*\"great\" + 0.016*\"pill\" + 0.016*\"fish_oil\" + 0.014*\"love\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.305): 0.031*\"work\" + 0.024*\"feel\" + 0.021*\"day\" + 0.018*\"help\" + 0.018*\"try\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.012*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.408): 0.082*\"product\" + 0.055*\"good\" + 0.048*\"great\" + 0.023*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.089542, rho=0.158114\n",
      "INFO : PROGRESS: pass 0, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18102843, 0.41804597, 0.16264567, 0.17344047, 0.067693613, 0.30711716, 0.21364415]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.068): 0.026*\"grow\" + 0.013*\"naturewise\" + 0.012*\"nutrigold\" + 0.008*\"second_bottle\" + 0.007*\"use\" + 0.007*\"expectation\" + 0.007*\"meet\" + 0.007*\"tablespoon\" + 0.006*\"new\" + 0.006*\"review\"\n",
      "INFO : topic #2 (0.163): 0.039*\"use\" + 0.034*\"skin\" + 0.034*\"product\" + 0.028*\"oil\" + 0.021*\"hair\" + 0.017*\"great\" + 0.016*\"love\" + 0.014*\"good\" + 0.012*\"look\" + 0.010*\"nail\"\n",
      "INFO : topic #6 (0.214): 0.040*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.019*\"good\" + 0.017*\"great\" + 0.017*\"fish_oil\" + 0.016*\"pill\" + 0.014*\"love\" + 0.013*\"vitamin\" + 0.013*\"swallow\"\n",
      "INFO : topic #5 (0.307): 0.030*\"work\" + 0.026*\"feel\" + 0.021*\"day\" + 0.018*\"try\" + 0.018*\"help\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.418): 0.084*\"product\" + 0.054*\"good\" + 0.048*\"great\" + 0.022*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"recommend\" + 0.014*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.098348, rho=0.156174\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18517855, 0.43671823, 0.16415729, 0.17722306, 0.068803221, 0.30975783, 0.22059162]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.069): 0.035*\"naturewise\" + 0.024*\"grow\" + 0.010*\"nutrigold\" + 0.008*\"second_bottle\" + 0.007*\"expectation\" + 0.007*\"use\" + 0.007*\"meet\" + 0.006*\"tablespoon\" + 0.006*\"new\" + 0.006*\"review\"\n",
      "INFO : topic #2 (0.164): 0.039*\"use\" + 0.035*\"skin\" + 0.034*\"product\" + 0.029*\"oil\" + 0.019*\"hair\" + 0.016*\"great\" + 0.016*\"love\" + 0.014*\"good\" + 0.013*\"look\" + 0.010*\"krill\"\n",
      "INFO : topic #6 (0.221): 0.039*\"taste\" + 0.026*\"like\" + 0.023*\"easy\" + 0.018*\"good\" + 0.017*\"great\" + 0.017*\"fish_oil\" + 0.017*\"pill\" + 0.013*\"vitamin\" + 0.013*\"love\" + 0.013*\"swallow\"\n",
      "INFO : topic #5 (0.310): 0.029*\"work\" + 0.027*\"feel\" + 0.021*\"day\" + 0.018*\"try\" + 0.018*\"help\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.437): 0.085*\"product\" + 0.051*\"good\" + 0.048*\"great\" + 0.021*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"recommend\" + 0.014*\"find\" + 0.013*\"brand\" + 0.013*\"quality\"\n",
      "INFO : topic diff=0.101053, rho=0.154303\n",
      "INFO : PROGRESS: pass 0, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18741997, 0.4481248, 0.16735449, 0.18298614, 0.069654264, 0.31424507, 0.21991089]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.070): 0.030*\"naturewise\" + 0.024*\"grow\" + 0.008*\"second_bottle\" + 0.008*\"nutrigold\" + 0.008*\"inch\" + 0.008*\"expectation\" + 0.007*\"meet\" + 0.007*\"use\" + 0.007*\"review\" + 0.006*\"new\"\n",
      "INFO : topic #2 (0.167): 0.041*\"skin\" + 0.038*\"use\" + 0.034*\"product\" + 0.026*\"oil\" + 0.019*\"hair\" + 0.015*\"love\" + 0.015*\"great\" + 0.015*\"look\" + 0.013*\"good\" + 0.009*\"face\"\n",
      "INFO : topic #6 (0.220): 0.038*\"taste\" + 0.026*\"like\" + 0.024*\"easy\" + 0.018*\"good\" + 0.017*\"pill\" + 0.017*\"great\" + 0.016*\"fish_oil\" + 0.014*\"swallow\" + 0.013*\"love\" + 0.013*\"vitamin\"\n",
      "INFO : topic #5 (0.314): 0.029*\"work\" + 0.028*\"feel\" + 0.021*\"day\" + 0.019*\"try\" + 0.018*\"help\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (0.448): 0.087*\"product\" + 0.050*\"good\" + 0.049*\"great\" + 0.020*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.015*\"recommend\" + 0.014*\"find\" + 0.013*\"quality\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.093779, rho=0.152499\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18949999, 0.45735911, 0.16873315, 0.19036019, 0.070272975, 0.3220014, 0.21878013]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.070): 0.025*\"naturewise\" + 0.023*\"grow\" + 0.009*\"second_bottle\" + 0.008*\"inch\" + 0.008*\"expectation\" + 0.008*\"yacon\" + 0.008*\"meet\" + 0.007*\"nutrigold\" + 0.007*\"use\" + 0.006*\"review\"\n",
      "INFO : topic #2 (0.169): 0.046*\"skin\" + 0.036*\"use\" + 0.034*\"product\" + 0.023*\"oil\" + 0.017*\"look\" + 0.017*\"hair\" + 0.015*\"love\" + 0.015*\"great\" + 0.013*\"good\" + 0.010*\"face\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #6 (0.219): 0.038*\"taste\" + 0.026*\"like\" + 0.025*\"easy\" + 0.019*\"pill\" + 0.018*\"fish_oil\" + 0.018*\"good\" + 0.017*\"great\" + 0.014*\"swallow\" + 0.013*\"love\" + 0.013*\"vitamin\"\n",
      "INFO : topic #5 (0.322): 0.030*\"feel\" + 0.029*\"work\" + 0.021*\"day\" + 0.020*\"try\" + 0.018*\"help\" + 0.017*\"product\" + 0.015*\"good\" + 0.013*\"use\" + 0.013*\"start\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (0.457): 0.089*\"product\" + 0.050*\"good\" + 0.049*\"great\" + 0.019*\"price\" + 0.017*\"use\" + 0.016*\"recommend\" + 0.015*\"buy\" + 0.014*\"find\" + 0.013*\"brand\" + 0.013*\"quality\"\n",
      "INFO : topic diff=0.093293, rho=0.150756\n",
      "INFO : PROGRESS: pass 1, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18311672, 0.4575769, 0.16428433, 0.19565295, 0.075519793, 0.30568177, 0.21445806]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.077*\"pedometer\" + 0.035*\"step\" + 0.020*\"accurate\" + 0.020*\"omron\" + 0.018*\"pocket\" + 0.017*\"clip\" + 0.011*\"use\" + 0.010*\"set\" + 0.009*\"track\" + 0.009*\"naturewise\"\n",
      "INFO : topic #2 (0.164): 0.045*\"skin\" + 0.038*\"use\" + 0.034*\"product\" + 0.022*\"oil\" + 0.017*\"look\" + 0.016*\"hair\" + 0.016*\"love\" + 0.015*\"great\" + 0.013*\"good\" + 0.011*\"face\"\n",
      "INFO : topic #6 (0.214): 0.035*\"taste\" + 0.029*\"easy\" + 0.027*\"like\" + 0.018*\"pill\" + 0.018*\"good\" + 0.017*\"great\" + 0.016*\"fish_oil\" + 0.014*\"love\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.306): 0.031*\"work\" + 0.029*\"feel\" + 0.022*\"day\" + 0.020*\"try\" + 0.017*\"help\" + 0.017*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (0.458): 0.085*\"product\" + 0.051*\"great\" + 0.051*\"good\" + 0.020*\"price\" + 0.019*\"use\" + 0.017*\"buy\" + 0.016*\"recommend\" + 0.014*\"find\" + 0.013*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.225649, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18300745, 0.46112609, 0.1647307, 0.1965639, 0.077537864, 0.31016362, 0.21589746]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.080*\"pedometer\" + 0.038*\"step\" + 0.021*\"accurate\" + 0.020*\"omron\" + 0.018*\"pocket\" + 0.017*\"clip\" + 0.013*\"use\" + 0.010*\"set\" + 0.010*\"track\" + 0.010*\"day\"\n",
      "INFO : topic #2 (0.165): 0.044*\"skin\" + 0.039*\"use\" + 0.032*\"product\" + 0.024*\"oil\" + 0.016*\"hair\" + 0.016*\"love\" + 0.016*\"look\" + 0.015*\"great\" + 0.013*\"good\" + 0.011*\"face\"\n",
      "INFO : topic #6 (0.216): 0.038*\"taste\" + 0.028*\"easy\" + 0.026*\"like\" + 0.018*\"good\" + 0.017*\"pill\" + 0.017*\"great\" + 0.015*\"fish_oil\" + 0.014*\"love\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.310): 0.031*\"work\" + 0.027*\"feel\" + 0.022*\"day\" + 0.019*\"try\" + 0.018*\"help\" + 0.016*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.461): 0.084*\"product\" + 0.052*\"great\" + 0.052*\"good\" + 0.021*\"price\" + 0.020*\"use\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.015*\"find\" + 0.013*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.099181, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18572251, 0.4687373, 0.16500548, 0.198688, 0.077579819, 0.32365754, 0.22023697]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.076*\"pedometer\" + 0.036*\"step\" + 0.020*\"accurate\" + 0.019*\"omron\" + 0.018*\"pocket\" + 0.016*\"clip\" + 0.013*\"use\" + 0.010*\"set\" + 0.010*\"day\" + 0.010*\"track\"\n",
      "INFO : topic #2 (0.165): 0.042*\"skin\" + 0.039*\"use\" + 0.032*\"product\" + 0.023*\"oil\" + 0.020*\"hair\" + 0.015*\"love\" + 0.015*\"look\" + 0.015*\"great\" + 0.013*\"good\" + 0.010*\"face\"\n",
      "INFO : topic #6 (0.220): 0.040*\"taste\" + 0.026*\"like\" + 0.026*\"easy\" + 0.018*\"good\" + 0.016*\"great\" + 0.016*\"pill\" + 0.014*\"love\" + 0.013*\"fish_oil\" + 0.012*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.324): 0.031*\"work\" + 0.026*\"feel\" + 0.022*\"day\" + 0.018*\"help\" + 0.018*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.012*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.469): 0.084*\"product\" + 0.053*\"good\" + 0.052*\"great\" + 0.021*\"price\" + 0.020*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.013*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.092257, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18794569, 0.47479475, 0.16740336, 0.20080808, 0.077443689, 0.33422744, 0.22182204]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.077): 0.073*\"pedometer\" + 0.035*\"step\" + 0.019*\"accurate\" + 0.018*\"omron\" + 0.017*\"pocket\" + 0.016*\"clip\" + 0.013*\"use\" + 0.011*\"grow\" + 0.010*\"set\" + 0.010*\"day\"\n",
      "INFO : topic #2 (0.167): 0.041*\"skin\" + 0.040*\"use\" + 0.031*\"product\" + 0.024*\"oil\" + 0.022*\"hair\" + 0.015*\"love\" + 0.014*\"great\" + 0.014*\"look\" + 0.012*\"good\" + 0.010*\"face\"\n",
      "INFO : topic #6 (0.222): 0.040*\"taste\" + 0.026*\"like\" + 0.025*\"easy\" + 0.018*\"good\" + 0.016*\"pill\" + 0.016*\"great\" + 0.013*\"love\" + 0.012*\"swallow\" + 0.012*\"vitamin\" + 0.012*\"water\"\n",
      "INFO : topic #5 (0.334): 0.031*\"work\" + 0.025*\"feel\" + 0.022*\"day\" + 0.019*\"help\" + 0.017*\"try\" + 0.016*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.475): 0.084*\"product\" + 0.053*\"good\" + 0.052*\"great\" + 0.021*\"price\" + 0.021*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.013*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.085593, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18912828, 0.48219764, 0.17117049, 0.20172688, 0.077410109, 0.34183586, 0.2239875]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.077): 0.069*\"pedometer\" + 0.033*\"step\" + 0.018*\"accurate\" + 0.017*\"omron\" + 0.016*\"pocket\" + 0.015*\"clip\" + 0.012*\"use\" + 0.011*\"grow\" + 0.009*\"day\" + 0.009*\"set\"\n",
      "INFO : topic #2 (0.171): 0.041*\"use\" + 0.038*\"skin\" + 0.029*\"product\" + 0.026*\"oil\" + 0.021*\"hair\" + 0.015*\"love\" + 0.015*\"great\" + 0.013*\"look\" + 0.012*\"good\" + 0.009*\"work\"\n",
      "INFO : topic #6 (0.224): 0.040*\"taste\" + 0.026*\"like\" + 0.024*\"easy\" + 0.018*\"good\" + 0.016*\"great\" + 0.016*\"pill\" + 0.013*\"love\" + 0.013*\"water\" + 0.012*\"mix\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.342): 0.031*\"work\" + 0.024*\"feel\" + 0.022*\"day\" + 0.020*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.482): 0.084*\"product\" + 0.054*\"good\" + 0.053*\"great\" + 0.022*\"price\" + 0.021*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.083417, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19246171, 0.49035239, 0.17071983, 0.20462553, 0.077913254, 0.35545292, 0.22553636]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.063*\"pedometer\" + 0.030*\"step\" + 0.017*\"accurate\" + 0.016*\"omron\" + 0.015*\"pocket\" + 0.014*\"clip\" + 0.013*\"grow\" + 0.012*\"use\" + 0.009*\"day\" + 0.009*\"set\"\n",
      "INFO : topic #2 (0.171): 0.041*\"use\" + 0.036*\"skin\" + 0.028*\"product\" + 0.025*\"oil\" + 0.022*\"hair\" + 0.015*\"love\" + 0.014*\"great\" + 0.013*\"look\" + 0.012*\"good\" + 0.009*\"work\"\n",
      "INFO : topic #6 (0.226): 0.040*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.018*\"good\" + 0.016*\"great\" + 0.015*\"pill\" + 0.014*\"water\" + 0.013*\"drink\" + 0.013*\"love\" + 0.013*\"mix\"\n",
      "INFO : topic #5 (0.355): 0.031*\"work\" + 0.024*\"feel\" + 0.022*\"day\" + 0.020*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"good\" + 0.015*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.490): 0.086*\"product\" + 0.054*\"good\" + 0.053*\"great\" + 0.023*\"price\" + 0.022*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.082314, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19464611, 0.50118607, 0.17111044, 0.20747124, 0.078043945, 0.36648041, 0.22928113]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.059*\"pedometer\" + 0.028*\"step\" + 0.016*\"accurate\" + 0.014*\"omron\" + 0.014*\"pocket\" + 0.013*\"clip\" + 0.012*\"use\" + 0.012*\"grow\" + 0.009*\"day\" + 0.009*\"set\"\n",
      "INFO : topic #2 (0.171): 0.041*\"use\" + 0.037*\"skin\" + 0.028*\"product\" + 0.025*\"oil\" + 0.020*\"hair\" + 0.015*\"love\" + 0.014*\"great\" + 0.012*\"look\" + 0.012*\"good\" + 0.009*\"face\"\n",
      "INFO : topic #6 (0.229): 0.040*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.018*\"good\" + 0.016*\"great\" + 0.015*\"drink\" + 0.015*\"pill\" + 0.014*\"water\" + 0.013*\"love\" + 0.013*\"mix\"\n",
      "INFO : topic #5 (0.366): 0.031*\"work\" + 0.024*\"feel\" + 0.021*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"good\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.501): 0.085*\"product\" + 0.055*\"good\" + 0.053*\"great\" + 0.023*\"price\" + 0.022*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.080882, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19773728, 0.51184332, 0.17223965, 0.21073657, 0.078155056, 0.37315241, 0.23364881]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.054*\"pedometer\" + 0.026*\"step\" + 0.015*\"accurate\" + 0.013*\"grow\" + 0.013*\"omron\" + 0.013*\"pocket\" + 0.012*\"use\" + 0.012*\"clip\" + 0.009*\"day\" + 0.008*\"set\"\n",
      "INFO : topic #2 (0.172): 0.041*\"use\" + 0.037*\"skin\" + 0.027*\"product\" + 0.024*\"oil\" + 0.021*\"hair\" + 0.014*\"love\" + 0.014*\"great\" + 0.012*\"look\" + 0.012*\"good\" + 0.009*\"work\"\n",
      "INFO : topic #6 (0.234): 0.040*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.016*\"great\" + 0.015*\"drink\" + 0.014*\"pill\" + 0.013*\"love\" + 0.012*\"mix\"\n",
      "INFO : topic #5 (0.373): 0.030*\"work\" + 0.023*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"good\" + 0.015*\"product\" + 0.014*\"use\" + 0.013*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.512): 0.085*\"product\" + 0.056*\"good\" + 0.053*\"great\" + 0.024*\"price\" + 0.022*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.075569, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20039801, 0.51972234, 0.17377841, 0.21342407, 0.078334965, 0.38549417, 0.23774888]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.049*\"pedometer\" + 0.025*\"step\" + 0.014*\"grow\" + 0.014*\"accurate\" + 0.012*\"omron\" + 0.012*\"use\" + 0.012*\"pocket\" + 0.011*\"clip\" + 0.009*\"day\" + 0.008*\"set\"\n",
      "INFO : topic #2 (0.174): 0.042*\"use\" + 0.036*\"skin\" + 0.028*\"oil\" + 0.026*\"product\" + 0.022*\"hair\" + 0.014*\"love\" + 0.014*\"great\" + 0.012*\"look\" + 0.011*\"good\" + 0.009*\"work\"\n",
      "INFO : topic #6 (0.238): 0.043*\"taste\" + 0.026*\"like\" + 0.020*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.016*\"great\" + 0.014*\"drink\" + 0.014*\"pill\" + 0.013*\"love\" + 0.012*\"mix\"\n",
      "INFO : topic #5 (0.385): 0.031*\"work\" + 0.023*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"good\" + 0.015*\"use\" + 0.014*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.520): 0.087*\"product\" + 0.056*\"good\" + 0.053*\"great\" + 0.023*\"price\" + 0.023*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.074453, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20392627, 0.5311237, 0.17341846, 0.21848992, 0.078582667, 0.395702, 0.23998642]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.045*\"pedometer\" + 0.023*\"step\" + 0.015*\"grow\" + 0.012*\"accurate\" + 0.012*\"use\" + 0.011*\"omron\" + 0.011*\"pocket\" + 0.010*\"clip\" + 0.009*\"day\" + 0.008*\"set\"\n",
      "INFO : topic #2 (0.173): 0.042*\"use\" + 0.036*\"skin\" + 0.027*\"oil\" + 0.026*\"product\" + 0.021*\"hair\" + 0.014*\"love\" + 0.013*\"great\" + 0.012*\"look\" + 0.011*\"good\" + 0.009*\"cream\"\n",
      "INFO : topic #6 (0.240): 0.043*\"taste\" + 0.026*\"like\" + 0.019*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.015*\"great\" + 0.015*\"drink\" + 0.014*\"pill\" + 0.013*\"mix\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.396): 0.031*\"work\" + 0.023*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"product\" + 0.015*\"use\" + 0.015*\"good\" + 0.014*\"year\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.531): 0.088*\"product\" + 0.057*\"good\" + 0.053*\"great\" + 0.023*\"price\" + 0.023*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.015*\"find\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.076440, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2114113, 0.53940403, 0.17175443, 0.22385031, 0.078831233, 0.40679544, 0.2429653]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.041*\"pedometer\" + 0.021*\"step\" + 0.014*\"grow\" + 0.012*\"use\" + 0.011*\"accurate\" + 0.010*\"omron\" + 0.010*\"pocket\" + 0.009*\"clip\" + 0.009*\"day\" + 0.008*\"set\"\n",
      "INFO : topic #2 (0.172): 0.041*\"use\" + 0.035*\"skin\" + 0.026*\"oil\" + 0.026*\"product\" + 0.021*\"hair\" + 0.014*\"love\" + 0.013*\"great\" + 0.011*\"look\" + 0.011*\"good\" + 0.009*\"work\"\n",
      "INFO : topic #6 (0.243): 0.043*\"taste\" + 0.026*\"like\" + 0.019*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.015*\"drink\" + 0.015*\"great\" + 0.013*\"pill\" + 0.013*\"mix\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.407): 0.032*\"work\" + 0.022*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.014*\"year\" + 0.014*\"good\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.539): 0.088*\"product\" + 0.057*\"good\" + 0.053*\"great\" + 0.023*\"price\" + 0.023*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.016*\"find\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.086451, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21507217, 0.55306304, 0.17190324, 0.22465418, 0.079084456, 0.42072466, 0.24444042]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.037*\"pedometer\" + 0.019*\"step\" + 0.014*\"grow\" + 0.011*\"use\" + 0.010*\"accurate\" + 0.009*\"pocket\" + 0.009*\"omron\" + 0.008*\"day\" + 0.008*\"clip\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #2 (0.172): 0.042*\"use\" + 0.036*\"skin\" + 0.026*\"oil\" + 0.025*\"product\" + 0.019*\"hair\" + 0.014*\"love\" + 0.013*\"great\" + 0.011*\"look\" + 0.011*\"good\" + 0.010*\"cream\"\n",
      "INFO : topic #6 (0.244): 0.043*\"taste\" + 0.026*\"like\" + 0.018*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.015*\"drink\" + 0.015*\"great\" + 0.013*\"pill\" + 0.013*\"mix\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.421): 0.032*\"work\" + 0.022*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"year\" + 0.014*\"good\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.553): 0.089*\"product\" + 0.058*\"good\" + 0.053*\"great\" + 0.024*\"price\" + 0.024*\"use\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.016*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.069454, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : optimized alpha [0.21813913, 0.56173187, 0.17241912, 0.23018533, 0.07957831, 0.43500128, 0.24662979]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.080): 0.033*\"pedometer\" + 0.018*\"grow\" + 0.017*\"step\" + 0.011*\"use\" + 0.009*\"accurate\" + 0.008*\"pocket\" + 0.008*\"omron\" + 0.008*\"day\" + 0.008*\"tablespoon\" + 0.007*\"clip\"\n",
      "INFO : topic #2 (0.172): 0.041*\"use\" + 0.036*\"skin\" + 0.025*\"oil\" + 0.025*\"product\" + 0.022*\"hair\" + 0.013*\"love\" + 0.013*\"great\" + 0.011*\"look\" + 0.011*\"good\" + 0.009*\"cream\"\n",
      "INFO : topic #6 (0.247): 0.043*\"taste\" + 0.026*\"like\" + 0.018*\"easy\" + 0.018*\"good\" + 0.018*\"water\" + 0.015*\"drink\" + 0.015*\"great\" + 0.013*\"pill\" + 0.013*\"love\" + 0.013*\"mix\"\n",
      "INFO : topic #5 (0.435): 0.032*\"work\" + 0.022*\"feel\" + 0.022*\"day\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"year\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.562): 0.090*\"product\" + 0.059*\"good\" + 0.053*\"great\" + 0.024*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.065948, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22194804, 0.57548702, 0.17310338, 0.23509105, 0.08029817, 0.44939673, 0.2475093]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.080): 0.027*\"pedometer\" + 0.017*\"grow\" + 0.015*\"step\" + 0.014*\"camera\" + 0.011*\"use\" + 0.008*\"accurate\" + 0.008*\"day\" + 0.007*\"tablespoon\" + 0.007*\"pocket\" + 0.007*\"omron\"\n",
      "INFO : topic #2 (0.173): 0.041*\"use\" + 0.035*\"skin\" + 0.024*\"product\" + 0.024*\"hair\" + 0.024*\"oil\" + 0.013*\"love\" + 0.012*\"great\" + 0.012*\"look\" + 0.011*\"good\" + 0.009*\"cream\"\n",
      "INFO : topic #6 (0.248): 0.042*\"taste\" + 0.026*\"like\" + 0.018*\"easy\" + 0.018*\"good\" + 0.018*\"water\" + 0.015*\"drink\" + 0.014*\"great\" + 0.013*\"pill\" + 0.013*\"mix\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.449): 0.032*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"year\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.575): 0.090*\"product\" + 0.059*\"good\" + 0.053*\"great\" + 0.024*\"use\" + 0.023*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"find\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.075864, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22522017, 0.58854461, 0.17272931, 0.23811641, 0.080937549, 0.46442091, 0.25021917]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.081): 0.024*\"pedometer\" + 0.016*\"grow\" + 0.014*\"step\" + 0.012*\"camera\" + 0.011*\"use\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.007*\"accurate\" + 0.006*\"bladder\" + 0.006*\"pocket\"\n",
      "INFO : topic #2 (0.173): 0.042*\"use\" + 0.034*\"skin\" + 0.024*\"oil\" + 0.024*\"product\" + 0.023*\"hair\" + 0.013*\"love\" + 0.012*\"great\" + 0.011*\"look\" + 0.010*\"good\" + 0.009*\"cream\"\n",
      "INFO : topic #6 (0.250): 0.043*\"taste\" + 0.026*\"like\" + 0.018*\"good\" + 0.018*\"easy\" + 0.017*\"water\" + 0.015*\"drink\" + 0.014*\"great\" + 0.013*\"pill\" + 0.012*\"love\" + 0.012*\"mix\"\n",
      "INFO : topic #5 (0.464): 0.032*\"work\" + 0.023*\"day\" + 0.022*\"feel\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.589): 0.089*\"product\" + 0.058*\"good\" + 0.053*\"great\" + 0.025*\"use\" + 0.023*\"price\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.066861, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22789115, 0.60299683, 0.1746842, 0.24257059, 0.081369087, 0.4746626, 0.25504264]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.081): 0.021*\"pedometer\" + 0.017*\"grow\" + 0.013*\"step\" + 0.011*\"camera\" + 0.010*\"use\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.006*\"accurate\" + 0.006*\"bladder\" + 0.006*\"easy\"\n",
      "INFO : topic #2 (0.175): 0.043*\"use\" + 0.034*\"skin\" + 0.024*\"oil\" + 0.023*\"product\" + 0.022*\"hair\" + 0.013*\"love\" + 0.012*\"great\" + 0.011*\"look\" + 0.010*\"good\" + 0.009*\"face\"\n",
      "INFO : topic #6 (0.255): 0.044*\"taste\" + 0.025*\"like\" + 0.018*\"good\" + 0.017*\"water\" + 0.017*\"easy\" + 0.016*\"drink\" + 0.014*\"great\" + 0.014*\"mix\" + 0.012*\"pill\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.475): 0.032*\"work\" + 0.023*\"day\" + 0.022*\"feel\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.015*\"use\" + 0.014*\"good\" + 0.014*\"product\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.603): 0.090*\"product\" + 0.059*\"good\" + 0.053*\"great\" + 0.026*\"use\" + 0.023*\"price\" + 0.018*\"buy\" + 0.016*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.067595, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23523936, 0.61504042, 0.17521681, 0.24508289, 0.081873067, 0.48992458, 0.25805148]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.082): 0.021*\"grow\" + 0.018*\"pedometer\" + 0.012*\"step\" + 0.010*\"use\" + 0.009*\"camera\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.006*\"bladder\" + 0.006*\"accurate\" + 0.006*\"set\"\n",
      "INFO : topic #2 (0.175): 0.042*\"use\" + 0.033*\"skin\" + 0.026*\"hair\" + 0.024*\"oil\" + 0.022*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.010*\"good\" + 0.008*\"nail\"\n",
      "INFO : topic #6 (0.258): 0.043*\"taste\" + 0.026*\"like\" + 0.018*\"good\" + 0.017*\"easy\" + 0.016*\"water\" + 0.015*\"drink\" + 0.014*\"great\" + 0.013*\"mix\" + 0.013*\"pill\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.490): 0.032*\"work\" + 0.023*\"day\" + 0.022*\"feel\" + 0.021*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"product\" + 0.013*\"start\"\n",
      "INFO : topic #1 (0.615): 0.090*\"product\" + 0.060*\"good\" + 0.053*\"great\" + 0.026*\"use\" + 0.023*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.070907, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2420786, 0.62908798, 0.17547061, 0.24796431, 0.082342811, 0.4994868, 0.2613115]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.082): 0.022*\"grow\" + 0.016*\"pedometer\" + 0.011*\"step\" + 0.010*\"use\" + 0.008*\"camera\" + 0.008*\"tablespoon\" + 0.007*\"day\" + 0.006*\"bladder\" + 0.006*\"second_bottle\" + 0.006*\"easy\"\n",
      "INFO : topic #2 (0.175): 0.041*\"use\" + 0.034*\"skin\" + 0.026*\"hair\" + 0.023*\"oil\" + 0.022*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.010*\"good\" + 0.010*\"nail\"\n",
      "INFO : topic #6 (0.261): 0.042*\"taste\" + 0.025*\"like\" + 0.018*\"easy\" + 0.018*\"good\" + 0.016*\"water\" + 0.015*\"drink\" + 0.013*\"great\" + 0.013*\"pill\" + 0.013*\"mix\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.499): 0.032*\"work\" + 0.023*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"product\"\n",
      "INFO : topic #1 (0.629): 0.090*\"product\" + 0.060*\"good\" + 0.053*\"great\" + 0.026*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"brand\" + 0.016*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.069674, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24732579, 0.65217966, 0.17403826, 0.2504935, 0.083079778, 0.51205212, 0.26694423]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.083): 0.021*\"grow\" + 0.014*\"pedometer\" + 0.011*\"swanson\" + 0.010*\"step\" + 0.009*\"use\" + 0.009*\"tablespoon\" + 0.007*\"camera\" + 0.007*\"optimum\" + 0.007*\"day\" + 0.006*\"bladder\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.174): 0.041*\"use\" + 0.034*\"skin\" + 0.025*\"hair\" + 0.023*\"oil\" + 0.022*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.009*\"good\" + 0.009*\"nail\"\n",
      "INFO : topic #6 (0.267): 0.042*\"taste\" + 0.025*\"like\" + 0.018*\"easy\" + 0.018*\"good\" + 0.017*\"water\" + 0.015*\"drink\" + 0.013*\"mix\" + 0.013*\"pill\" + 0.013*\"great\" + 0.011*\"add\"\n",
      "INFO : topic #5 (0.512): 0.033*\"work\" + 0.024*\"day\" + 0.024*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"year\" + 0.014*\"good\" + 0.013*\"start\" + 0.013*\"product\"\n",
      "INFO : topic #1 (0.652): 0.089*\"product\" + 0.060*\"good\" + 0.054*\"great\" + 0.026*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"brand\" + 0.016*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.066723, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25054413, 0.66817456, 0.1758914, 0.25147977, 0.083574079, 0.52022195, 0.27381575]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.084): 0.021*\"grow\" + 0.013*\"pedometer\" + 0.010*\"tablespoon\" + 0.010*\"swanson\" + 0.009*\"step\" + 0.009*\"use\" + 0.006*\"day\" + 0.006*\"camera\" + 0.006*\"optimum\" + 0.006*\"meet\"\n",
      "INFO : topic #2 (0.176): 0.042*\"use\" + 0.034*\"skin\" + 0.025*\"oil\" + 0.025*\"hair\" + 0.021*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.009*\"good\" + 0.009*\"face\"\n",
      "INFO : topic #6 (0.274): 0.043*\"taste\" + 0.025*\"like\" + 0.017*\"good\" + 0.017*\"easy\" + 0.017*\"water\" + 0.015*\"drink\" + 0.013*\"great\" + 0.013*\"mix\" + 0.012*\"pill\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.520): 0.033*\"work\" + 0.024*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"year\" + 0.013*\"good\" + 0.013*\"start\" + 0.013*\"product\"\n",
      "INFO : topic #1 (0.668): 0.088*\"product\" + 0.060*\"good\" + 0.054*\"great\" + 0.026*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"brand\" + 0.016*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.067211, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25329947, 0.67878169, 0.17695969, 0.25532937, 0.083929822, 0.53905338, 0.27730465]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.084): 0.022*\"grow\" + 0.011*\"pedometer\" + 0.010*\"tablespoon\" + 0.009*\"use\" + 0.009*\"step\" + 0.008*\"swanson\" + 0.006*\"optimum\" + 0.006*\"day\" + 0.006*\"expectation\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #2 (0.177): 0.042*\"use\" + 0.034*\"skin\" + 0.025*\"oil\" + 0.024*\"hair\" + 0.020*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.009*\"good\" + 0.009*\"nail\"\n",
      "INFO : topic #6 (0.277): 0.042*\"taste\" + 0.025*\"like\" + 0.018*\"easy\" + 0.017*\"good\" + 0.017*\"water\" + 0.017*\"drink\" + 0.013*\"great\" + 0.013*\"mix\" + 0.012*\"pill\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.539): 0.033*\"work\" + 0.024*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"year\" + 0.013*\"good\" + 0.013*\"start\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.679): 0.089*\"product\" + 0.061*\"good\" + 0.055*\"great\" + 0.027*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"brand\" + 0.016*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.063199, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25849178, 0.70088136, 0.17656343, 0.25680819, 0.08444678, 0.5506146, 0.28676069]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.084): 0.022*\"grow\" + 0.010*\"pedometer\" + 0.010*\"tablespoon\" + 0.009*\"use\" + 0.008*\"step\" + 0.008*\"swanson\" + 0.006*\"meet\" + 0.006*\"day\" + 0.006*\"second_bottle\" + 0.006*\"optimum\"\n",
      "INFO : topic #2 (0.177): 0.042*\"use\" + 0.033*\"skin\" + 0.027*\"oil\" + 0.025*\"hair\" + 0.020*\"product\" + 0.012*\"love\" + 0.011*\"great\" + 0.011*\"look\" + 0.009*\"nail\" + 0.009*\"good\"\n",
      "INFO : topic #6 (0.287): 0.041*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.015*\"drink\" + 0.015*\"water\" + 0.014*\"pill\" + 0.012*\"great\" + 0.012*\"swallow\" + 0.011*\"mix\"\n",
      "INFO : topic #5 (0.551): 0.033*\"work\" + 0.024*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.701): 0.087*\"product\" + 0.060*\"good\" + 0.053*\"great\" + 0.026*\"use\" + 0.024*\"price\" + 0.018*\"buy\" + 0.017*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.070277, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26004937, 0.72220844, 0.17813292, 0.25721321, 0.084916718, 0.55832654, 0.29695618]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.085): 0.022*\"grow\" + 0.010*\"tablespoon\" + 0.008*\"pedometer\" + 0.008*\"use\" + 0.007*\"swanson\" + 0.007*\"step\" + 0.006*\"meet\" + 0.006*\"bladder\" + 0.006*\"day\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #2 (0.178): 0.041*\"use\" + 0.033*\"skin\" + 0.032*\"oil\" + 0.024*\"hair\" + 0.019*\"product\" + 0.011*\"love\" + 0.011*\"great\" + 0.010*\"look\" + 0.009*\"nail\" + 0.009*\"good\"\n",
      "INFO : topic #6 (0.297): 0.044*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.014*\"drink\" + 0.014*\"water\" + 0.014*\"pill\" + 0.013*\"fish_oil\" + 0.012*\"swallow\" + 0.012*\"great\"\n",
      "INFO : topic #5 (0.558): 0.033*\"work\" + 0.024*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.722): 0.088*\"product\" + 0.061*\"good\" + 0.053*\"great\" + 0.026*\"use\" + 0.024*\"price\" + 0.019*\"buy\" + 0.017*\"recommend\" + 0.017*\"brand\" + 0.016*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.064248, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26255313, 0.73556763, 0.17942563, 0.25895682, 0.085366085, 0.57108903, 0.30568132]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.085): 0.027*\"grow\" + 0.009*\"tablespoon\" + 0.008*\"use\" + 0.007*\"pedometer\" + 0.007*\"swanson\" + 0.007*\"step\" + 0.006*\"meet\" + 0.006*\"second_bottle\" + 0.006*\"bladder\" + 0.006*\"expectation\"\n",
      "INFO : topic #2 (0.179): 0.040*\"use\" + 0.033*\"skin\" + 0.031*\"oil\" + 0.026*\"hair\" + 0.019*\"product\" + 0.011*\"nail\" + 0.011*\"love\" + 0.010*\"look\" + 0.010*\"great\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.306): 0.044*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.015*\"pill\" + 0.014*\"fish_oil\" + 0.014*\"water\" + 0.014*\"drink\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.571): 0.033*\"work\" + 0.025*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.736): 0.088*\"product\" + 0.061*\"good\" + 0.054*\"great\" + 0.025*\"use\" + 0.025*\"price\" + 0.019*\"buy\" + 0.017*\"recommend\" + 0.017*\"find\" + 0.016*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.058582, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2655651, 0.75881678, 0.18041708, 0.26085371, 0.085628562, 0.57517862, 0.31904948]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.027*\"grow\" + 0.009*\"tablespoon\" + 0.008*\"use\" + 0.007*\"swanson\" + 0.007*\"meet\" + 0.007*\"second_bottle\" + 0.006*\"pedometer\" + 0.006*\"step\" + 0.006*\"expectation\" + 0.006*\"optimum\"\n",
      "INFO : topic #2 (0.180): 0.039*\"use\" + 0.034*\"skin\" + 0.032*\"oil\" + 0.026*\"hair\" + 0.018*\"product\" + 0.012*\"nail\" + 0.011*\"love\" + 0.010*\"look\" + 0.010*\"great\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.319): 0.045*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.016*\"fish_oil\" + 0.015*\"pill\" + 0.013*\"water\" + 0.013*\"drink\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.575): 0.033*\"work\" + 0.025*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.759): 0.088*\"product\" + 0.062*\"good\" + 0.054*\"great\" + 0.025*\"use\" + 0.025*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.017*\"brand\" + 0.017*\"find\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.064277, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26715297, 0.7798472, 0.17969243, 0.26192242, 0.086073428, 0.58847183, 0.33023456]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.029*\"grow\" + 0.009*\"tablespoon\" + 0.007*\"meet\" + 0.007*\"use\" + 0.007*\"osteoporosis\" + 0.007*\"swanson\" + 0.006*\"second_bottle\" + 0.006*\"step\" + 0.006*\"expectation\" + 0.006*\"pedometer\"\n",
      "INFO : topic #2 (0.180): 0.039*\"use\" + 0.034*\"skin\" + 0.033*\"oil\" + 0.025*\"hair\" + 0.018*\"product\" + 0.011*\"nail\" + 0.011*\"love\" + 0.010*\"look\" + 0.010*\"great\" + 0.008*\"dry\"\n",
      "INFO : topic #6 (0.330): 0.046*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.016*\"fish_oil\" + 0.015*\"pill\" + 0.014*\"drink\" + 0.013*\"water\" + 0.013*\"swallow\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.588): 0.034*\"work\" + 0.025*\"day\" + 0.023*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.780): 0.089*\"product\" + 0.062*\"good\" + 0.054*\"great\" + 0.025*\"price\" + 0.025*\"use\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.017*\"brand\" + 0.016*\"find\" + 0.012*\"love\"\n",
      "INFO : topic diff=0.062935, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26880866, 0.7965374, 0.18379004, 0.2640059, 0.086796857, 0.59533787, 0.33972734]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.087): 0.027*\"grow\" + 0.012*\"tablespoon\" + 0.008*\"meet\" + 0.007*\"use\" + 0.007*\"second_bottle\" + 0.007*\"expectation\" + 0.006*\"swanson\" + 0.006*\"osteoporosis\" + 0.006*\"step\" + 0.005*\"review\"\n",
      "INFO : topic #2 (0.184): 0.041*\"oil\" + 0.041*\"use\" + 0.036*\"skin\" + 0.026*\"hair\" + 0.017*\"product\" + 0.013*\"love\" + 0.010*\"nail\" + 0.010*\"look\" + 0.010*\"great\" + 0.009*\"smell\"\n",
      "INFO : topic #6 (0.340): 0.048*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.018*\"fish_oil\" + 0.017*\"good\" + 0.014*\"pill\" + 0.013*\"drink\" + 0.013*\"water\" + 0.012*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.595): 0.033*\"work\" + 0.025*\"day\" + 0.024*\"feel\" + 0.022*\"help\" + 0.018*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.013*\"good\" + 0.011*\"month\"\n",
      "INFO : topic #1 (0.797): 0.089*\"product\" + 0.063*\"good\" + 0.055*\"great\" + 0.025*\"use\" + 0.025*\"price\" + 0.020*\"buy\" + 0.018*\"recommend\" + 0.017*\"brand\" + 0.016*\"find\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.065983, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27333689, 0.81623077, 0.18437238, 0.26821861, 0.087982789, 0.61769593, 0.34536618]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.088): 0.026*\"grow\" + 0.021*\"gaia\" + 0.012*\"tablespoon\" + 0.008*\"meet\" + 0.007*\"second_bottle\" + 0.007*\"use\" + 0.007*\"expectation\" + 0.006*\"step\" + 0.006*\"swanson\" + 0.005*\"review\"\n",
      "INFO : topic #2 (0.184): 0.040*\"use\" + 0.039*\"oil\" + 0.035*\"skin\" + 0.025*\"hair\" + 0.016*\"product\" + 0.012*\"love\" + 0.010*\"nail\" + 0.010*\"look\" + 0.009*\"great\" + 0.009*\"dry\"\n",
      "INFO : topic #6 (0.345): 0.048*\"taste\" + 0.025*\"like\" + 0.019*\"easy\" + 0.017*\"good\" + 0.016*\"fish_oil\" + 0.014*\"pill\" + 0.014*\"drink\" + 0.013*\"water\" + 0.012*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.618): 0.033*\"work\" + 0.025*\"day\" + 0.025*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"good\" + 0.011*\"month\"\n",
      "INFO : topic #1 (0.816): 0.089*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.025*\"use\" + 0.024*\"price\" + 0.020*\"buy\" + 0.018*\"recommend\" + 0.017*\"brand\" + 0.016*\"find\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.066857, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27724019, 0.8336277, 0.18656671, 0.26886779, 0.088558517, 0.62570554, 0.3566893]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.089): 0.025*\"grow\" + 0.019*\"gaia\" + 0.011*\"tablespoon\" + 0.008*\"meet\" + 0.007*\"second_bottle\" + 0.007*\"expectation\" + 0.007*\"use\" + 0.006*\"review\" + 0.006*\"step\" + 0.005*\"osteoporosis\"\n",
      "INFO : topic #2 (0.187): 0.040*\"oil\" + 0.040*\"use\" + 0.035*\"skin\" + 0.023*\"hair\" + 0.016*\"product\" + 0.012*\"love\" + 0.010*\"look\" + 0.009*\"nail\" + 0.009*\"great\" + 0.009*\"dry\"\n",
      "INFO : topic #6 (0.357): 0.049*\"taste\" + 0.025*\"like\" + 0.020*\"easy\" + 0.016*\"good\" + 0.016*\"fish_oil\" + 0.014*\"pill\" + 0.013*\"drink\" + 0.012*\"love\" + 0.012*\"water\" + 0.012*\"flavor\"\n",
      "INFO : topic #5 (0.626): 0.033*\"work\" + 0.026*\"day\" + 0.025*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.013*\"start\" + 0.012*\"good\" + 0.012*\"month\"\n",
      "INFO : topic #1 (0.834): 0.089*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.025*\"use\" + 0.024*\"price\" + 0.020*\"buy\" + 0.018*\"recommend\" + 0.016*\"brand\" + 0.016*\"find\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.060665, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28194132, 0.85185361, 0.1871247, 0.27286586, 0.089342773, 0.63938504, 0.3639423]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.089): 0.025*\"grow\" + 0.017*\"gaia\" + 0.011*\"tablespoon\" + 0.008*\"meet\" + 0.007*\"expectation\" + 0.007*\"second_bottle\" + 0.006*\"use\" + 0.006*\"review\" + 0.006*\"swanson\" + 0.005*\"step\"\n",
      "INFO : topic #2 (0.187): 0.041*\"use\" + 0.040*\"oil\" + 0.034*\"skin\" + 0.022*\"hair\" + 0.015*\"product\" + 0.012*\"love\" + 0.010*\"look\" + 0.010*\"nail\" + 0.009*\"dry\" + 0.009*\"great\"\n",
      "INFO : topic #6 (0.364): 0.048*\"taste\" + 0.025*\"like\" + 0.020*\"easy\" + 0.016*\"good\" + 0.015*\"fish_oil\" + 0.014*\"pill\" + 0.013*\"drink\" + 0.012*\"love\" + 0.012*\"water\" + 0.012*\"flavor\"\n",
      "INFO : topic #5 (0.639): 0.034*\"work\" + 0.026*\"day\" + 0.025*\"feel\" + 0.022*\"help\" + 0.017*\"try\" + 0.015*\"year\" + 0.014*\"use\" + 0.014*\"start\" + 0.012*\"good\" + 0.012*\"month\"\n",
      "INFO : topic #1 (0.852): 0.090*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.026*\"use\" + 0.023*\"price\" + 0.020*\"buy\" + 0.019*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.058309, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28589219, 0.8677842, 0.1887932, 0.27590805, 0.090152912, 0.64952129, 0.3696613]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.090): 0.031*\"grow\" + 0.014*\"gaia\" + 0.011*\"tablespoon\" + 0.008*\"meet\" + 0.007*\"expectation\" + 0.007*\"second_bottle\" + 0.006*\"use\" + 0.006*\"review\" + 0.006*\"assist\" + 0.005*\"osteoporosis\"\n",
      "INFO : topic #2 (0.189): 0.041*\"use\" + 0.039*\"oil\" + 0.035*\"skin\" + 0.025*\"hair\" + 0.015*\"product\" + 0.012*\"love\" + 0.010*\"nail\" + 0.010*\"look\" + 0.010*\"dry\" + 0.009*\"great\"\n",
      "INFO : topic #6 (0.370): 0.048*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.016*\"good\" + 0.014*\"fish_oil\" + 0.014*\"pill\" + 0.013*\"drink\" + 0.012*\"love\" + 0.012*\"water\" + 0.012*\"swallow\"\n",
      "INFO : topic #5 (0.650): 0.034*\"work\" + 0.026*\"day\" + 0.025*\"feel\" + 0.022*\"help\" + 0.018*\"try\" + 0.014*\"year\" + 0.014*\"use\" + 0.014*\"start\" + 0.012*\"month\" + 0.012*\"good\"\n",
      "INFO : topic #1 (0.868): 0.091*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.026*\"use\" + 0.023*\"price\" + 0.020*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.014*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.057841, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28822196, 0.87946379, 0.19371641, 0.27630833, 0.091692202, 0.65908152, 0.37419283]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.092): 0.057*\"grow\" + 0.011*\"gaia\" + 0.009*\"tablespoon\" + 0.008*\"nutrigold\" + 0.008*\"inch\" + 0.008*\"meet\" + 0.007*\"second_bottle\" + 0.006*\"expectation\" + 0.006*\"review\" + 0.006*\"osteoporosis\"\n",
      "INFO : topic #2 (0.194): 0.044*\"hair\" + 0.039*\"use\" + 0.036*\"oil\" + 0.034*\"skin\" + 0.020*\"nail\" + 0.015*\"product\" + 0.011*\"love\" + 0.010*\"look\" + 0.009*\"dry\" + 0.008*\"great\"\n",
      "INFO : topic #6 (0.374): 0.046*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.015*\"good\" + 0.015*\"pill\" + 0.015*\"fish_oil\" + 0.013*\"drink\" + 0.013*\"water\" + 0.013*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.659): 0.034*\"work\" + 0.026*\"day\" + 0.025*\"feel\" + 0.023*\"help\" + 0.018*\"try\" + 0.014*\"year\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"month\" + 0.012*\"good\"\n",
      "INFO : topic #1 (0.879): 0.092*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.026*\"use\" + 0.023*\"price\" + 0.020*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.069113, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29393932, 0.89984798, 0.19616424, 0.2784856, 0.092400506, 0.67234057, 0.38315567]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.092): 0.053*\"grow\" + 0.010*\"gaia\" + 0.009*\"nutrigold\" + 0.009*\"tablespoon\" + 0.009*\"meet\" + 0.008*\"second_bottle\" + 0.007*\"inch\" + 0.007*\"expectation\" + 0.006*\"review\" + 0.005*\"osteoporosis\"\n",
      "INFO : topic #2 (0.196): 0.043*\"oil\" + 0.041*\"hair\" + 0.038*\"use\" + 0.034*\"skin\" + 0.018*\"nail\" + 0.015*\"product\" + 0.013*\"krill\" + 0.011*\"love\" + 0.010*\"look\" + 0.008*\"dry\"\n",
      "INFO : topic #6 (0.383): 0.046*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.016*\"fish_oil\" + 0.015*\"pill\" + 0.015*\"good\" + 0.013*\"swallow\" + 0.013*\"drink\" + 0.012*\"love\" + 0.012*\"water\"\n",
      "INFO : topic #5 (0.672): 0.034*\"work\" + 0.026*\"day\" + 0.025*\"feel\" + 0.023*\"help\" + 0.018*\"try\" + 0.014*\"year\" + 0.014*\"start\" + 0.014*\"use\" + 0.013*\"month\" + 0.011*\"good\"\n",
      "INFO : topic #1 (0.900): 0.092*\"product\" + 0.062*\"good\" + 0.054*\"great\" + 0.025*\"use\" + 0.023*\"price\" + 0.019*\"buy\" + 0.019*\"recommend\" + 0.016*\"find\" + 0.016*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.062664, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29998687, 0.91586077, 0.19761047, 0.28289309, 0.093275614, 0.68681914, 0.38917044]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.093): 0.054*\"grow\" + 0.013*\"nutrigold\" + 0.010*\"gaia\" + 0.009*\"meet\" + 0.008*\"tablespoon\" + 0.007*\"inch\" + 0.007*\"second_bottle\" + 0.007*\"expectation\" + 0.006*\"review\" + 0.006*\"osteoporosis\"\n",
      "INFO : topic #2 (0.198): 0.042*\"oil\" + 0.041*\"hair\" + 0.037*\"use\" + 0.034*\"skin\" + 0.019*\"nail\" + 0.014*\"product\" + 0.012*\"krill\" + 0.011*\"love\" + 0.010*\"look\" + 0.008*\"biotin\"\n",
      "INFO : topic #6 (0.389): 0.046*\"taste\" + 0.025*\"like\" + 0.021*\"easy\" + 0.016*\"fish_oil\" + 0.016*\"pill\" + 0.015*\"good\" + 0.013*\"swallow\" + 0.013*\"drink\" + 0.013*\"water\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.687): 0.035*\"work\" + 0.026*\"day\" + 0.026*\"feel\" + 0.023*\"help\" + 0.018*\"try\" + 0.014*\"year\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"month\" + 0.011*\"good\"\n",
      "INFO : topic #1 (0.916): 0.093*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.025*\"use\" + 0.022*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.060820, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30439538, 0.93641692, 0.20074056, 0.28609025, 0.093888946, 0.70513827, 0.39596561]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.094): 0.051*\"grow\" + 0.016*\"nutrigold\" + 0.011*\"gaia\" + 0.009*\"meet\" + 0.008*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"inch\" + 0.007*\"expectation\" + 0.006*\"review\" + 0.006*\"osteoporosis\"\n",
      "INFO : topic #2 (0.201): 0.046*\"oil\" + 0.037*\"use\" + 0.036*\"hair\" + 0.035*\"skin\" + 0.017*\"nail\" + 0.016*\"krill\" + 0.014*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.009*\"strong\"\n",
      "INFO : topic #6 (0.396): 0.046*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.016*\"pill\" + 0.016*\"fish_oil\" + 0.015*\"good\" + 0.014*\"swallow\" + 0.013*\"water\" + 0.013*\"drink\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.705): 0.035*\"work\" + 0.026*\"day\" + 0.026*\"feel\" + 0.023*\"help\" + 0.018*\"try\" + 0.014*\"use\" + 0.014*\"year\" + 0.014*\"start\" + 0.012*\"month\" + 0.011*\"good\"\n",
      "INFO : topic #1 (0.936): 0.095*\"product\" + 0.062*\"good\" + 0.056*\"great\" + 0.025*\"use\" + 0.022*\"price\" + 0.019*\"buy\" + 0.019*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.058090, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30982646, 0.95688426, 0.20131448, 0.28951734, 0.094684884, 0.7183668, 0.40691671]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.095): 0.048*\"grow\" + 0.015*\"nutrigold\" + 0.010*\"gaia\" + 0.009*\"meet\" + 0.009*\"tablespoon\" + 0.007*\"second_bottle\" + 0.007*\"inch\" + 0.006*\"expectation\" + 0.006*\"review\" + 0.005*\"fit\"\n",
      "INFO : topic #2 (0.201): 0.045*\"oil\" + 0.037*\"use\" + 0.035*\"skin\" + 0.035*\"hair\" + 0.017*\"nail\" + 0.015*\"krill\" + 0.013*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.009*\"strong\"\n",
      "INFO : topic #6 (0.407): 0.047*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.015*\"good\" + 0.015*\"pill\" + 0.014*\"fish_oil\" + 0.013*\"drink\" + 0.013*\"water\" + 0.013*\"swallow\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.718): 0.035*\"work\" + 0.027*\"day\" + 0.026*\"feel\" + 0.023*\"help\" + 0.018*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.012*\"month\" + 0.011*\"good\"\n",
      "INFO : topic #1 (0.957): 0.096*\"product\" + 0.062*\"good\" + 0.057*\"great\" + 0.026*\"use\" + 0.022*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.015*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.060123, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31513384, 0.97564697, 0.20217925, 0.29549935, 0.095923461, 0.73545104, 0.41329256]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.096): 0.045*\"grow\" + 0.016*\"nutrigold\" + 0.009*\"meet\" + 0.008*\"tablespoon\" + 0.008*\"gaia\" + 0.008*\"second_bottle\" + 0.007*\"expectation\" + 0.007*\"inch\" + 0.006*\"review\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.202): 0.044*\"oil\" + 0.037*\"use\" + 0.035*\"skin\" + 0.034*\"hair\" + 0.016*\"krill\" + 0.016*\"nail\" + 0.013*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.008*\"strong\"\n",
      "INFO : topic #6 (0.413): 0.047*\"taste\" + 0.026*\"like\" + 0.022*\"easy\" + 0.015*\"good\" + 0.015*\"pill\" + 0.014*\"fish_oil\" + 0.014*\"drink\" + 0.013*\"water\" + 0.013*\"swallow\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.735): 0.036*\"work\" + 0.027*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.014*\"year\" + 0.012*\"month\" + 0.011*\"good\"\n",
      "INFO : topic #1 (0.976): 0.096*\"product\" + 0.062*\"good\" + 0.057*\"great\" + 0.025*\"use\" + 0.022*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.014*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.067226, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : optimized alpha [0.31939468, 0.99470508, 0.20337807, 0.30283764, 0.097008906, 0.7510457, 0.42089275]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.097): 0.045*\"grow\" + 0.014*\"nutrigold\" + 0.009*\"meet\" + 0.008*\"tablespoon\" + 0.008*\"second_bottle\" + 0.007*\"expectation\" + 0.007*\"gaia\" + 0.007*\"inch\" + 0.006*\"review\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.203): 0.042*\"oil\" + 0.037*\"use\" + 0.036*\"hair\" + 0.035*\"skin\" + 0.016*\"nail\" + 0.015*\"krill\" + 0.013*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.008*\"strong\"\n",
      "INFO : topic #6 (0.421): 0.048*\"taste\" + 0.027*\"like\" + 0.022*\"easy\" + 0.015*\"good\" + 0.015*\"pill\" + 0.014*\"drink\" + 0.014*\"fish_oil\" + 0.014*\"water\" + 0.013*\"swallow\" + 0.013*\"love\"\n",
      "INFO : topic #5 (0.751): 0.036*\"work\" + 0.027*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"year\" + 0.013*\"month\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.995): 0.097*\"product\" + 0.062*\"good\" + 0.058*\"great\" + 0.026*\"use\" + 0.021*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.014*\"love\" + 0.014*\"brand\"\n",
      "INFO : topic diff=0.059366, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.327075, 1.0135248, 0.20428067, 0.31001073, 0.097959347, 0.76604748, 0.42648271]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.098): 0.042*\"grow\" + 0.014*\"nutrigold\" + 0.009*\"second_bottle\" + 0.008*\"meet\" + 0.008*\"tablespoon\" + 0.007*\"expectation\" + 0.006*\"inch\" + 0.006*\"gaia\" + 0.006*\"review\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.204): 0.041*\"oil\" + 0.037*\"use\" + 0.036*\"skin\" + 0.035*\"hair\" + 0.016*\"nail\" + 0.013*\"krill\" + 0.012*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.008*\"strong\"\n",
      "INFO : topic #6 (0.426): 0.047*\"taste\" + 0.027*\"like\" + 0.022*\"easy\" + 0.015*\"fish_oil\" + 0.015*\"pill\" + 0.015*\"good\" + 0.014*\"drink\" + 0.014*\"water\" + 0.013*\"swallow\" + 0.012*\"flavor\"\n",
      "INFO : topic #5 (0.766): 0.036*\"work\" + 0.029*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"year\" + 0.013*\"month\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (1.014): 0.096*\"product\" + 0.062*\"good\" + 0.057*\"great\" + 0.025*\"use\" + 0.021*\"price\" + 0.019*\"buy\" + 0.018*\"recommend\" + 0.016*\"find\" + 0.014*\"love\" + 0.014*\"brand\"\n",
      "INFO : topic diff=0.064748, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.33389223, 1.0401255, 0.20561574, 0.31495059, 0.098878495, 0.77812022, 0.43647626]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.099): 0.041*\"grow\" + 0.012*\"nutrigold\" + 0.010*\"meet\" + 0.009*\"second_bottle\" + 0.008*\"expectation\" + 0.008*\"tablespoon\" + 0.006*\"review\" + 0.006*\"inch\" + 0.006*\"gaia\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.206): 0.042*\"oil\" + 0.037*\"skin\" + 0.036*\"use\" + 0.035*\"hair\" + 0.016*\"nail\" + 0.013*\"krill\" + 0.012*\"product\" + 0.011*\"look\" + 0.010*\"love\" + 0.008*\"dry\"\n",
      "INFO : topic #6 (0.436): 0.047*\"taste\" + 0.027*\"like\" + 0.022*\"easy\" + 0.019*\"fish_oil\" + 0.016*\"pill\" + 0.014*\"good\" + 0.013*\"drink\" + 0.013*\"water\" + 0.013*\"swallow\" + 0.012*\"flavor\"\n",
      "INFO : topic #5 (0.778): 0.036*\"work\" + 0.029*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.019*\"try\" + 0.014*\"use\" + 0.014*\"start\" + 0.013*\"year\" + 0.013*\"month\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (1.040): 0.097*\"product\" + 0.062*\"good\" + 0.057*\"great\" + 0.025*\"use\" + 0.020*\"price\" + 0.019*\"recommend\" + 0.018*\"buy\" + 0.016*\"find\" + 0.014*\"brand\" + 0.014*\"love\"\n",
      "INFO : topic diff=0.059077, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.33753091, 1.0580518, 0.21086681, 0.31926563, 0.10001253, 0.78872985, 0.43686625]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.100): 0.038*\"grow\" + 0.013*\"naturewise\" + 0.010*\"nutrigold\" + 0.009*\"meet\" + 0.009*\"second_bottle\" + 0.009*\"expectation\" + 0.008*\"tablespoon\" + 0.006*\"review\" + 0.006*\"inch\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.211): 0.047*\"skin\" + 0.039*\"oil\" + 0.038*\"use\" + 0.029*\"hair\" + 0.014*\"nail\" + 0.014*\"look\" + 0.012*\"product\" + 0.012*\"krill\" + 0.011*\"face\" + 0.010*\"love\"\n",
      "INFO : topic #6 (0.437): 0.046*\"taste\" + 0.027*\"like\" + 0.022*\"easy\" + 0.019*\"fish_oil\" + 0.016*\"pill\" + 0.014*\"good\" + 0.013*\"drink\" + 0.013*\"swallow\" + 0.013*\"water\" + 0.012*\"love\"\n",
      "INFO : topic #5 (0.789): 0.036*\"work\" + 0.031*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.019*\"try\" + 0.014*\"start\" + 0.014*\"use\" + 0.013*\"energy\" + 0.013*\"year\" + 0.012*\"month\"\n",
      "INFO : topic #1 (1.058): 0.099*\"product\" + 0.061*\"good\" + 0.057*\"great\" + 0.025*\"use\" + 0.020*\"price\" + 0.018*\"recommend\" + 0.018*\"buy\" + 0.015*\"find\" + 0.014*\"love\" + 0.014*\"brand\"\n",
      "INFO : topic diff=0.069959, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3460744, 1.0889983, 0.21290004, 0.3248015, 0.10158114, 0.79679775, 0.4514623]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.102): 0.038*\"naturewise\" + 0.034*\"grow\" + 0.009*\"meet\" + 0.009*\"second_bottle\" + 0.009*\"expectation\" + 0.008*\"nutrigold\" + 0.007*\"tablespoon\" + 0.006*\"review\" + 0.006*\"inch\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.213): 0.048*\"skin\" + 0.040*\"oil\" + 0.038*\"use\" + 0.027*\"hair\" + 0.014*\"look\" + 0.013*\"krill\" + 0.013*\"nail\" + 0.012*\"product\" + 0.012*\"face\" + 0.010*\"love\"\n",
      "INFO : topic #6 (0.451): 0.044*\"taste\" + 0.027*\"like\" + 0.024*\"easy\" + 0.019*\"fish_oil\" + 0.016*\"pill\" + 0.013*\"swallow\" + 0.013*\"good\" + 0.013*\"drink\" + 0.013*\"water\" + 0.012*\"vitamin\"\n",
      "INFO : topic #5 (0.797): 0.035*\"work\" + 0.033*\"feel\" + 0.027*\"day\" + 0.023*\"help\" + 0.020*\"try\" + 0.014*\"start\" + 0.014*\"use\" + 0.013*\"energy\" + 0.012*\"year\" + 0.012*\"month\"\n",
      "INFO : topic #1 (1.089): 0.100*\"product\" + 0.059*\"good\" + 0.057*\"great\" + 0.025*\"use\" + 0.018*\"recommend\" + 0.018*\"price\" + 0.018*\"buy\" + 0.015*\"find\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.072516, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.35070407, 1.1091684, 0.21746302, 0.3343631, 0.10276154, 0.81388229, 0.44980919]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.103): 0.035*\"grow\" + 0.032*\"naturewise\" + 0.010*\"meet\" + 0.009*\"second_bottle\" + 0.009*\"inch\" + 0.009*\"expectation\" + 0.007*\"nutrigold\" + 0.007*\"review\" + 0.006*\"tablespoon\" + 0.006*\"fit\"\n",
      "INFO : topic #2 (0.217): 0.056*\"skin\" + 0.037*\"use\" + 0.036*\"oil\" + 0.027*\"hair\" + 0.017*\"look\" + 0.013*\"face\" + 0.013*\"krill\" + 0.012*\"product\" + 0.011*\"nail\" + 0.010*\"serum\"\n",
      "INFO : topic #6 (0.450): 0.043*\"taste\" + 0.027*\"like\" + 0.024*\"easy\" + 0.019*\"fish_oil\" + 0.017*\"pill\" + 0.014*\"swallow\" + 0.013*\"drink\" + 0.013*\"water\" + 0.013*\"good\" + 0.012*\"mix\"\n",
      "INFO : topic #5 (0.814): 0.035*\"work\" + 0.034*\"feel\" + 0.027*\"day\" + 0.022*\"help\" + 0.021*\"try\" + 0.015*\"start\" + 0.014*\"use\" + 0.014*\"energy\" + 0.012*\"month\" + 0.012*\"year\"\n",
      "INFO : topic #1 (1.109): 0.101*\"product\" + 0.057*\"good\" + 0.057*\"great\" + 0.024*\"use\" + 0.018*\"recommend\" + 0.018*\"price\" + 0.017*\"buy\" + 0.015*\"find\" + 0.014*\"love\" + 0.014*\"order\"\n",
      "INFO : topic diff=0.069354, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.35463563, 1.1247667, 0.21960489, 0.34735, 0.1035834, 0.84048575, 0.44741234]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.104): 0.033*\"grow\" + 0.027*\"naturewise\" + 0.010*\"inch\" + 0.010*\"meet\" + 0.010*\"second_bottle\" + 0.010*\"expectation\" + 0.009*\"yacon\" + 0.007*\"fit\" + 0.007*\"review\" + 0.006*\"assist\"\n",
      "INFO : topic #2 (0.220): 0.063*\"skin\" + 0.036*\"use\" + 0.032*\"oil\" + 0.024*\"hair\" + 0.019*\"look\" + 0.014*\"face\" + 0.012*\"product\" + 0.011*\"krill\" + 0.010*\"dry\" + 0.010*\"nail\"\n",
      "INFO : topic #6 (0.447): 0.043*\"taste\" + 0.027*\"like\" + 0.025*\"easy\" + 0.020*\"fish_oil\" + 0.018*\"pill\" + 0.014*\"swallow\" + 0.013*\"good\" + 0.013*\"drink\" + 0.012*\"water\" + 0.012*\"mix\"\n",
      "INFO : topic #5 (0.840): 0.036*\"feel\" + 0.035*\"work\" + 0.027*\"day\" + 0.022*\"help\" + 0.022*\"try\" + 0.015*\"start\" + 0.014*\"energy\" + 0.013*\"use\" + 0.011*\"like\" + 0.011*\"month\"\n",
      "INFO : topic #1 (1.125): 0.104*\"product\" + 0.057*\"great\" + 0.057*\"good\" + 0.024*\"use\" + 0.020*\"recommend\" + 0.017*\"buy\" + 0.017*\"price\" + 0.015*\"find\" + 0.014*\"love\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.071119, rho=0.148240\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=7, decay=0.5, chunksize=5000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 7 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -3.16515368355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217530/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 2/2 [14:11<00:00, 425.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_7.html\n",
      "CPU times: user 14min 10s, sys: 3.03 s, total: 14min 13s\n",
      "Wall time: 14min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [6, 7]\n",
    "chunksize = 5000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "INFO : using symmetric eta at 0.125\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 8 topics, 2 passes over the supplied corpus of 217530 documents, updating model once every 5000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4910/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.076057054, 0.076216474, 0.093472309, 0.097475961, 0.090470284, 0.071344614, 0.088717267, 0.074614719]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #5 (0.071): 0.026*\"work\" + 0.020*\"use\" + 0.018*\"good\" + 0.016*\"great\" + 0.013*\"product\" + 0.012*\"buy\" + 0.011*\"try\" + 0.009*\"year\" + 0.008*\"start\" + 0.008*\"love\"\n",
      "INFO : topic #7 (0.075): 0.036*\"pedometer\" + 0.024*\"step\" + 0.019*\"day\" + 0.016*\"great\" + 0.014*\"easy\" + 0.012*\"good\" + 0.012*\"try\" + 0.010*\"omron\" + 0.010*\"track\" + 0.010*\"find\"\n",
      "INFO : topic #4 (0.090): 0.041*\"pedometer\" + 0.032*\"use\" + 0.021*\"day\" + 0.021*\"step\" + 0.018*\"easy\" + 0.017*\"omron\" + 0.017*\"love\" + 0.016*\"product\" + 0.015*\"good\" + 0.015*\"great\"\n",
      "INFO : topic #2 (0.093): 0.038*\"pedometer\" + 0.028*\"product\" + 0.025*\"good\" + 0.024*\"use\" + 0.017*\"great\" + 0.015*\"work\" + 0.015*\"love\" + 0.014*\"walk\" + 0.014*\"day\" + 0.013*\"step\"\n",
      "INFO : topic #3 (0.097): 0.031*\"pedometer\" + 0.028*\"product\" + 0.027*\"day\" + 0.026*\"use\" + 0.020*\"good\" + 0.019*\"great\" + 0.018*\"step\" + 0.013*\"work\" + 0.012*\"walk\" + 0.011*\"time\"\n",
      "INFO : topic diff=9.644226, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4995/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.075089633, 0.076340787, 0.095470667, 0.088975593, 0.082638264, 0.084786847, 0.082443051, 0.068022683]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #7 (0.068): 0.027*\"pedometer\" + 0.019*\"day\" + 0.018*\"step\" + 0.015*\"great\" + 0.013*\"good\" + 0.011*\"try\" + 0.011*\"easy\" + 0.011*\"find\" + 0.009*\"product\" + 0.008*\"purchase\"\n",
      "INFO : topic #0 (0.075): 0.021*\"day\" + 0.018*\"like\" + 0.017*\"good\" + 0.013*\"use\" + 0.012*\"product\" + 0.011*\"work\" + 0.011*\"help\" + 0.011*\"great\" + 0.009*\"step\" + 0.008*\"recommend\"\n",
      "INFO : topic #5 (0.085): 0.019*\"good\" + 0.019*\"work\" + 0.017*\"product\" + 0.017*\"use\" + 0.014*\"great\" + 0.013*\"try\" + 0.011*\"help\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.009*\"feel\"\n",
      "INFO : topic #3 (0.089): 0.032*\"product\" + 0.026*\"day\" + 0.025*\"use\" + 0.022*\"good\" + 0.022*\"pedometer\" + 0.019*\"great\" + 0.013*\"step\" + 0.012*\"work\" + 0.011*\"time\" + 0.010*\"like\"\n",
      "INFO : topic #2 (0.095): 0.038*\"product\" + 0.028*\"good\" + 0.026*\"use\" + 0.019*\"great\" + 0.016*\"pedometer\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"work\" + 0.012*\"day\" + 0.012*\"skin\"\n",
      "INFO : topic diff=1.452417, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.078879811, 0.083236851, 0.0984945, 0.084958054, 0.072278053, 0.10392271, 0.079965256, 0.063122556]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #7 (0.063): 0.017*\"day\" + 0.016*\"pedometer\" + 0.014*\"great\" + 0.013*\"good\" + 0.011*\"find\" + 0.011*\"step\" + 0.011*\"try\" + 0.010*\"product\" + 0.009*\"purchase\" + 0.009*\"help\"\n",
      "INFO : topic #4 (0.072): 0.039*\"pedometer\" + 0.031*\"use\" + 0.021*\"day\" + 0.019*\"step\" + 0.017*\"easy\" + 0.016*\"love\" + 0.015*\"good\" + 0.015*\"product\" + 0.014*\"great\" + 0.014*\"omron\"\n",
      "INFO : topic #3 (0.085): 0.037*\"product\" + 0.025*\"day\" + 0.024*\"use\" + 0.021*\"good\" + 0.019*\"great\" + 0.012*\"pedometer\" + 0.011*\"work\" + 0.011*\"time\" + 0.009*\"pain\" + 0.009*\"like\"\n",
      "INFO : topic #2 (0.098): 0.045*\"product\" + 0.028*\"good\" + 0.026*\"use\" + 0.020*\"great\" + 0.013*\"love\" + 0.012*\"skin\" + 0.012*\"recommend\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.104): 0.019*\"good\" + 0.019*\"product\" + 0.018*\"work\" + 0.016*\"use\" + 0.013*\"help\" + 0.013*\"great\" + 0.013*\"try\" + 0.012*\"year\" + 0.012*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.982055, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.081142664, 0.087126352, 0.10413397, 0.083612338, 0.066541672, 0.1224435, 0.079554886, 0.06066677]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #7 (0.061): 0.015*\"day\" + 0.013*\"good\" + 0.012*\"great\" + 0.012*\"find\" + 0.010*\"try\" + 0.010*\"product\" + 0.009*\"pedometer\" + 0.009*\"help\" + 0.008*\"purchase\" + 0.008*\"work\"\n",
      "INFO : topic #4 (0.067): 0.032*\"use\" + 0.030*\"pedometer\" + 0.020*\"day\" + 0.016*\"love\" + 0.015*\"easy\" + 0.015*\"step\" + 0.014*\"product\" + 0.014*\"good\" + 0.013*\"great\" + 0.012*\"work\"\n",
      "INFO : topic #1 (0.087): 0.039*\"product\" + 0.032*\"good\" + 0.028*\"great\" + 0.019*\"taste\" + 0.015*\"price\" + 0.013*\"use\" + 0.013*\"vitamin\" + 0.012*\"find\" + 0.012*\"recommend\" + 0.012*\"work\"\n",
      "INFO : topic #2 (0.104): 0.047*\"product\" + 0.029*\"use\" + 0.028*\"good\" + 0.021*\"great\" + 0.016*\"skin\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.122): 0.019*\"work\" + 0.019*\"good\" + 0.018*\"product\" + 0.016*\"use\" + 0.014*\"help\" + 0.012*\"great\" + 0.012*\"try\" + 0.012*\"year\" + 0.012*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.625968, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082564302, 0.094330817, 0.11127749, 0.083100311, 0.062607236, 0.13478649, 0.080268241, 0.059414789]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #7 (0.059): 0.013*\"day\" + 0.013*\"good\" + 0.012*\"find\" + 0.012*\"great\" + 0.010*\"try\" + 0.010*\"purchase\" + 0.009*\"product\" + 0.009*\"buy\" + 0.009*\"help\" + 0.008*\"calm\"\n",
      "INFO : topic #4 (0.063): 0.032*\"use\" + 0.022*\"pedometer\" + 0.019*\"day\" + 0.018*\"love\" + 0.014*\"product\" + 0.014*\"easy\" + 0.013*\"great\" + 0.013*\"good\" + 0.012*\"work\" + 0.011*\"step\"\n",
      "INFO : topic #1 (0.094): 0.042*\"product\" + 0.035*\"good\" + 0.032*\"great\" + 0.018*\"taste\" + 0.017*\"price\" + 0.015*\"use\" + 0.012*\"find\" + 0.012*\"recommend\" + 0.011*\"like\" + 0.011*\"work\"\n",
      "INFO : topic #2 (0.111): 0.045*\"product\" + 0.033*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.017*\"skin\" + 0.015*\"oil\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"mouse\" + 0.010*\"recommend\"\n",
      "INFO : topic #5 (0.135): 0.020*\"work\" + 0.018*\"good\" + 0.017*\"product\" + 0.017*\"use\" + 0.015*\"help\" + 0.012*\"try\" + 0.012*\"great\" + 0.012*\"feel\" + 0.012*\"year\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.456830, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.086687185, 0.10077164, 0.11088746, 0.084610708, 0.06008203, 0.14830749, 0.08100085, 0.059913281]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #7 (0.060): 0.022*\"swanson\" + 0.014*\"find\" + 0.013*\"good\" + 0.012*\"day\" + 0.011*\"great\" + 0.010*\"product\" + 0.010*\"buy\" + 0.009*\"try\" + 0.009*\"purchase\" + 0.009*\"tooth\"\n",
      "INFO : topic #4 (0.060): 0.032*\"use\" + 0.019*\"day\" + 0.018*\"love\" + 0.015*\"pedometer\" + 0.013*\"product\" + 0.013*\"work\" + 0.012*\"great\" + 0.012*\"good\" + 0.011*\"easy\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #1 (0.101): 0.048*\"product\" + 0.039*\"good\" + 0.033*\"great\" + 0.019*\"taste\" + 0.019*\"price\" + 0.015*\"use\" + 0.012*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.010*\"work\"\n",
      "INFO : topic #2 (0.111): 0.049*\"product\" + 0.033*\"use\" + 0.026*\"good\" + 0.024*\"great\" + 0.016*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.012*\"oil\" + 0.011*\"recommend\" + 0.009*\"year\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.148): 0.020*\"work\" + 0.018*\"good\" + 0.018*\"product\" + 0.017*\"use\" + 0.015*\"help\" + 0.013*\"feel\" + 0.013*\"try\" + 0.012*\"year\" + 0.012*\"great\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.425750, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089173198, 0.10862322, 0.11195778, 0.086337395, 0.05848914, 0.15853426, 0.083004758, 0.05961604]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.031*\"use\" + 0.019*\"day\" + 0.017*\"love\" + 0.013*\"product\" + 0.012*\"work\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"easy\" + 0.010*\"pedometer\" + 0.009*\"tablespoon\"\n",
      "INFO : topic #7 (0.060): 0.014*\"find\" + 0.014*\"swanson\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"buy\" + 0.010*\"product\" + 0.010*\"great\" + 0.010*\"try\" + 0.010*\"purchase\" + 0.009*\"amazon\"\n",
      "INFO : topic #1 (0.109): 0.050*\"product\" + 0.041*\"good\" + 0.035*\"great\" + 0.020*\"price\" + 0.020*\"taste\" + 0.014*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.112): 0.047*\"product\" + 0.033*\"use\" + 0.025*\"good\" + 0.023*\"great\" + 0.020*\"skin\" + 0.015*\"love\" + 0.013*\"oil\" + 0.013*\"work\" + 0.010*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #5 (0.159): 0.020*\"work\" + 0.017*\"good\" + 0.017*\"product\" + 0.016*\"help\" + 0.016*\"use\" + 0.014*\"feel\" + 0.014*\"try\" + 0.012*\"year\" + 0.011*\"great\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.370463, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.09162768, 0.11643097, 0.11321839, 0.089301668, 0.057560459, 0.16384046, 0.08567103, 0.059229404]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.031*\"use\" + 0.017*\"day\" + 0.016*\"love\" + 0.014*\"dry_eye\" + 0.012*\"eye\" + 0.012*\"product\" + 0.011*\"work\" + 0.011*\"good\" + 0.010*\"great\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #7 (0.059): 0.015*\"find\" + 0.013*\"good\" + 0.011*\"buy\" + 0.010*\"amazon\" + 0.010*\"day\" + 0.010*\"swanson\" + 0.010*\"brand\" + 0.010*\"product\" + 0.010*\"purchase\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.113): 0.047*\"product\" + 0.034*\"use\" + 0.025*\"good\" + 0.023*\"great\" + 0.023*\"skin\" + 0.015*\"love\" + 0.013*\"oil\" + 0.012*\"work\" + 0.010*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.116): 0.051*\"product\" + 0.044*\"good\" + 0.036*\"great\" + 0.021*\"price\" + 0.020*\"taste\" + 0.015*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.010*\"vitamin\"\n",
      "INFO : topic #5 (0.164): 0.020*\"work\" + 0.018*\"good\" + 0.017*\"product\" + 0.016*\"help\" + 0.016*\"use\" + 0.014*\"feel\" + 0.014*\"try\" + 0.012*\"year\" + 0.011*\"great\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.341721, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4998/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095237195, 0.1229631, 0.11538646, 0.092521742, 0.056636866, 0.17220931, 0.087655276, 0.0595745]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.032*\"use\" + 0.017*\"day\" + 0.016*\"love\" + 0.011*\"product\" + 0.011*\"work\" + 0.011*\"eye\" + 0.011*\"dry_eye\" + 0.010*\"good\" + 0.010*\"seed\" + 0.010*\"tablespoon\"\n",
      "INFO : topic #7 (0.060): 0.016*\"find\" + 0.012*\"good\" + 0.012*\"amazon\" + 0.011*\"buy\" + 0.010*\"try\" + 0.010*\"product\" + 0.010*\"day\" + 0.009*\"purchase\" + 0.009*\"brand\" + 0.008*\"great\"\n",
      "INFO : topic #2 (0.115): 0.047*\"product\" + 0.036*\"use\" + 0.024*\"great\" + 0.024*\"good\" + 0.023*\"skin\" + 0.019*\"oil\" + 0.016*\"love\" + 0.012*\"work\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.123): 0.054*\"product\" + 0.045*\"good\" + 0.037*\"great\" + 0.022*\"taste\" + 0.021*\"price\" + 0.016*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.009*\"flavor\"\n",
      "INFO : topic #5 (0.172): 0.021*\"work\" + 0.017*\"product\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"help\" + 0.015*\"try\" + 0.014*\"feel\" + 0.013*\"year\" + 0.011*\"start\" + 0.010*\"great\"\n",
      "INFO : topic diff=0.296920, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.097631425, 0.1297154, 0.11508606, 0.096771196, 0.055762328, 0.17723252, 0.091104165, 0.059861239]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.031*\"use\" + 0.016*\"day\" + 0.015*\"seed\" + 0.015*\"love\" + 0.011*\"work\" + 0.011*\"product\" + 0.010*\"eye\" + 0.009*\"hair\" + 0.009*\"good\" + 0.008*\"dry_eye\"\n",
      "INFO : topic #7 (0.060): 0.017*\"find\" + 0.013*\"amazon\" + 0.012*\"good\" + 0.010*\"buy\" + 0.010*\"try\" + 0.010*\"product\" + 0.009*\"calm\" + 0.009*\"day\" + 0.008*\"purchase\" + 0.008*\"brand\"\n",
      "INFO : topic #2 (0.115): 0.047*\"product\" + 0.037*\"use\" + 0.024*\"skin\" + 0.023*\"good\" + 0.023*\"great\" + 0.017*\"oil\" + 0.016*\"love\" + 0.012*\"work\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.130): 0.058*\"product\" + 0.047*\"good\" + 0.039*\"great\" + 0.022*\"taste\" + 0.021*\"price\" + 0.016*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.012*\"like\" + 0.010*\"vitamin\"\n",
      "INFO : topic #5 (0.177): 0.021*\"work\" + 0.018*\"product\" + 0.017*\"good\" + 0.016*\"help\" + 0.016*\"use\" + 0.015*\"try\" + 0.015*\"feel\" + 0.013*\"year\" + 0.011*\"start\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.297636, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10431402, 0.13597722, 0.11374053, 0.10163353, 0.055210933, 0.18259118, 0.093290456, 0.060577329]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.029*\"use\" + 0.016*\"love\" + 0.015*\"day\" + 0.014*\"bladder\" + 0.014*\"seed\" + 0.011*\"tablespoon\" + 0.011*\"product\" + 0.010*\"work\" + 0.009*\"hair\" + 0.009*\"eye\"\n",
      "INFO : topic #7 (0.061): 0.017*\"find\" + 0.014*\"amazon\" + 0.011*\"good\" + 0.011*\"buy\" + 0.010*\"calm\" + 0.010*\"try\" + 0.010*\"product\" + 0.009*\"brand\" + 0.009*\"save\" + 0.009*\"purchase\"\n",
      "INFO : topic #2 (0.114): 0.048*\"product\" + 0.037*\"use\" + 0.023*\"good\" + 0.023*\"great\" + 0.023*\"skin\" + 0.016*\"oil\" + 0.016*\"love\" + 0.012*\"work\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.136): 0.059*\"product\" + 0.049*\"good\" + 0.039*\"great\" + 0.021*\"price\" + 0.021*\"taste\" + 0.016*\"use\" + 0.012*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.010*\"buy\"\n",
      "INFO : topic #5 (0.183): 0.022*\"work\" + 0.017*\"product\" + 0.017*\"good\" + 0.016*\"help\" + 0.016*\"use\" + 0.015*\"try\" + 0.015*\"feel\" + 0.013*\"year\" + 0.011*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.282695, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10737542, 0.14259967, 0.11495999, 0.10239565, 0.054760549, 0.18835141, 0.095049717, 0.061582107]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.029*\"use\" + 0.015*\"love\" + 0.014*\"seed\" + 0.014*\"day\" + 0.014*\"bladder\" + 0.012*\"tablespoon\" + 0.011*\"product\" + 0.010*\"work\" + 0.009*\"eye\" + 0.008*\"good\"\n",
      "INFO : topic #7 (0.062): 0.018*\"find\" + 0.015*\"amazon\" + 0.011*\"good\" + 0.011*\"buy\" + 0.010*\"coq10\" + 0.010*\"brand\" + 0.010*\"try\" + 0.010*\"product\" + 0.009*\"calm\" + 0.009*\"purchase\"\n",
      "INFO : topic #2 (0.115): 0.047*\"product\" + 0.038*\"use\" + 0.025*\"skin\" + 0.023*\"great\" + 0.023*\"good\" + 0.016*\"love\" + 0.016*\"oil\" + 0.013*\"work\" + 0.010*\"cream\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.143): 0.062*\"product\" + 0.052*\"good\" + 0.041*\"great\" + 0.023*\"price\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.010*\"buy\"\n",
      "INFO : topic #5 (0.188): 0.024*\"work\" + 0.018*\"product\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"help\" + 0.015*\"try\" + 0.015*\"feel\" + 0.013*\"year\" + 0.012*\"day\" + 0.011*\"start\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.238265, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10970945, 0.14724971, 0.11536144, 0.10666095, 0.054656278, 0.1943728, 0.097723678, 0.062685505]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.028*\"use\" + 0.020*\"hair\" + 0.014*\"love\" + 0.013*\"day\" + 0.011*\"seed\" + 0.011*\"tablespoon\" + 0.010*\"bladder\" + 0.010*\"product\" + 0.010*\"eye\" + 0.009*\"work\"\n",
      "INFO : topic #7 (0.063): 0.018*\"find\" + 0.016*\"amazon\" + 0.015*\"calm\" + 0.011*\"good\" + 0.010*\"buy\" + 0.010*\"anxiety\" + 0.010*\"try\" + 0.010*\"brand\" + 0.009*\"product\" + 0.009*\"'s\"\n",
      "INFO : topic #2 (0.115): 0.046*\"product\" + 0.037*\"use\" + 0.025*\"skin\" + 0.023*\"great\" + 0.022*\"good\" + 0.016*\"love\" + 0.015*\"oil\" + 0.013*\"work\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.147): 0.064*\"product\" + 0.053*\"good\" + 0.042*\"great\" + 0.023*\"price\" + 0.019*\"taste\" + 0.017*\"use\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.011*\"like\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.194): 0.023*\"work\" + 0.017*\"product\" + 0.017*\"good\" + 0.017*\"help\" + 0.016*\"use\" + 0.016*\"feel\" + 0.015*\"try\" + 0.013*\"year\" + 0.012*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.237505, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11347219, 0.15376598, 0.11706478, 0.11068194, 0.054934707, 0.19883829, 0.099056341, 0.063613951]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.027*\"camera\" + 0.026*\"use\" + 0.021*\"hair\" + 0.014*\"love\" + 0.011*\"day\" + 0.011*\"seed\" + 0.009*\"eye\" + 0.009*\"tablespoon\" + 0.008*\"bladder\" + 0.008*\"product\"\n",
      "INFO : topic #7 (0.064): 0.018*\"find\" + 0.016*\"amazon\" + 0.014*\"calm\" + 0.011*\"buy\" + 0.011*\"good\" + 0.011*\"anxiety\" + 0.010*\"brand\" + 0.009*\"product\" + 0.009*\"try\" + 0.009*\"'s\"\n",
      "INFO : topic #2 (0.117): 0.045*\"product\" + 0.037*\"use\" + 0.025*\"skin\" + 0.023*\"great\" + 0.022*\"good\" + 0.016*\"love\" + 0.014*\"oil\" + 0.013*\"work\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.154): 0.066*\"product\" + 0.054*\"good\" + 0.042*\"great\" + 0.023*\"price\" + 0.017*\"taste\" + 0.017*\"use\" + 0.014*\"recommend\" + 0.012*\"find\" + 0.011*\"buy\" + 0.011*\"like\"\n",
      "INFO : topic #5 (0.199): 0.024*\"work\" + 0.018*\"product\" + 0.017*\"good\" + 0.017*\"feel\" + 0.016*\"use\" + 0.016*\"help\" + 0.015*\"try\" + 0.013*\"year\" + 0.013*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.243803, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11619628, 0.16176531, 0.11754523, 0.11335474, 0.054882552, 0.20525695, 0.10098454, 0.063962832]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.027*\"use\" + 0.022*\"hair\" + 0.021*\"camera\" + 0.013*\"love\" + 0.012*\"seed\" + 0.011*\"day\" + 0.010*\"eye\" + 0.009*\"tablespoon\" + 0.008*\"bladder\" + 0.008*\"product\"\n",
      "INFO : topic #7 (0.064): 0.018*\"find\" + 0.018*\"amazon\" + 0.012*\"calm\" + 0.011*\"buy\" + 0.010*\"good\" + 0.010*\"brand\" + 0.010*\"anxiety\" + 0.009*\"try\" + 0.009*\"product\" + 0.009*\"save\"\n",
      "INFO : topic #2 (0.118): 0.044*\"product\" + 0.040*\"use\" + 0.023*\"skin\" + 0.022*\"great\" + 0.021*\"good\" + 0.016*\"love\" + 0.015*\"oil\" + 0.013*\"work\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.162): 0.064*\"product\" + 0.053*\"good\" + 0.041*\"great\" + 0.022*\"price\" + 0.020*\"use\" + 0.018*\"taste\" + 0.013*\"recommend\" + 0.013*\"find\" + 0.012*\"brand\" + 0.011*\"buy\"\n",
      "INFO : topic #5 (0.205): 0.024*\"work\" + 0.017*\"product\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"feel\" + 0.016*\"help\" + 0.016*\"try\" + 0.013*\"year\" + 0.013*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.207813, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11794152, 0.16931243, 0.11958621, 0.11670748, 0.05488833, 0.20839164, 0.10428714, 0.064077318]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.026*\"use\" + 0.023*\"hair\" + 0.016*\"camera\" + 0.012*\"love\" + 0.012*\"seed\" + 0.010*\"day\" + 0.010*\"tablespoon\" + 0.010*\"eye\" + 0.008*\"grow\" + 0.008*\"product\"\n",
      "INFO : topic #7 (0.064): 0.018*\"find\" + 0.018*\"amazon\" + 0.011*\"calm\" + 0.011*\"buy\" + 0.010*\"good\" + 0.010*\"anxiety\" + 0.010*\"brand\" + 0.010*\"save\" + 0.009*\"try\" + 0.009*\"'s\"\n",
      "INFO : topic #2 (0.120): 0.043*\"product\" + 0.041*\"use\" + 0.025*\"skin\" + 0.023*\"great\" + 0.020*\"good\" + 0.016*\"oil\" + 0.015*\"love\" + 0.012*\"work\" + 0.009*\"look\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.169): 0.066*\"product\" + 0.054*\"good\" + 0.042*\"great\" + 0.022*\"price\" + 0.020*\"use\" + 0.018*\"taste\" + 0.013*\"recommend\" + 0.012*\"find\" + 0.012*\"buy\" + 0.012*\"brand\"\n",
      "INFO : topic #5 (0.208): 0.024*\"work\" + 0.017*\"product\" + 0.017*\"feel\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"help\" + 0.016*\"try\" + 0.013*\"year\" + 0.013*\"day\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.203099, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12309369, 0.17603613, 0.12024043, 0.11949762, 0.055100769, 0.21249102, 0.10774064, 0.065377064]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.041*\"hair\" + 0.024*\"use\" + 0.015*\"grow\" + 0.012*\"camera\" + 0.011*\"love\" + 0.011*\"eye\" + 0.010*\"seed\" + 0.010*\"day\" + 0.008*\"tablespoon\" + 0.008*\"product\"\n",
      "INFO : topic #7 (0.065): 0.019*\"find\" + 0.018*\"amazon\" + 0.011*\"buy\" + 0.011*\"brand\" + 0.011*\"calm\" + 0.010*\"good\" + 0.010*\"anxiety\" + 0.010*\"coq10\" + 0.009*\"save\" + 0.009*\"try\"\n",
      "INFO : topic #0 (0.123): 0.019*\"help\" + 0.013*\"supplement\" + 0.013*\"day\" + 0.011*\"good\" + 0.010*\"product\" + 0.009*\"doctor\" + 0.009*\"work\" + 0.009*\"body\" + 0.008*\"recommend\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.176): 0.067*\"product\" + 0.055*\"good\" + 0.043*\"great\" + 0.023*\"price\" + 0.020*\"use\" + 0.017*\"taste\" + 0.013*\"recommend\" + 0.013*\"brand\" + 0.012*\"find\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.212): 0.024*\"work\" + 0.017*\"feel\" + 0.017*\"product\" + 0.017*\"good\" + 0.017*\"help\" + 0.016*\"try\" + 0.016*\"use\" + 0.013*\"day\" + 0.013*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.202334, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12741466, 0.18321578, 0.12024942, 0.1222071, 0.055466123, 0.21425028, 0.11049076, 0.066448726]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.041*\"hair\" + 0.028*\"eye\" + 0.022*\"use\" + 0.020*\"lutein\" + 0.017*\"grow\" + 0.010*\"love\" + 0.009*\"day\" + 0.009*\"seed\" + 0.009*\"camera\" + 0.008*\"product\"\n",
      "INFO : topic #7 (0.066): 0.019*\"find\" + 0.018*\"amazon\" + 0.013*\"coq10\" + 0.011*\"brand\" + 0.011*\"buy\" + 0.010*\"good\" + 0.009*\"calm\" + 0.009*\"save\" + 0.009*\"anxiety\" + 0.009*\"'s\"\n",
      "INFO : topic #0 (0.127): 0.019*\"help\" + 0.016*\"supplement\" + 0.012*\"day\" + 0.010*\"good\" + 0.010*\"product\" + 0.009*\"health\" + 0.009*\"doctor\" + 0.009*\"body\" + 0.009*\"work\" + 0.008*\"recommend\"\n",
      "INFO : topic #1 (0.183): 0.067*\"product\" + 0.056*\"good\" + 0.043*\"great\" + 0.025*\"price\" + 0.020*\"use\" + 0.016*\"taste\" + 0.014*\"recommend\" + 0.013*\"brand\" + 0.012*\"find\" + 0.012*\"buy\"\n",
      "INFO : topic #5 (0.214): 0.025*\"work\" + 0.018*\"feel\" + 0.017*\"help\" + 0.017*\"good\" + 0.017*\"product\" + 0.016*\"use\" + 0.016*\"try\" + 0.014*\"day\" + 0.013*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.192683, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13035817, 0.19299202, 0.11960226, 0.1247451, 0.055376563, 0.21573696, 0.11428943, 0.067874826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.055): 0.038*\"hair\" + 0.028*\"eye\" + 0.022*\"use\" + 0.018*\"lutein\" + 0.016*\"grow\" + 0.012*\"seed\" + 0.010*\"tablespoon\" + 0.009*\"love\" + 0.009*\"day\" + 0.007*\"product\"\n",
      "INFO : topic #7 (0.068): 0.020*\"find\" + 0.018*\"amazon\" + 0.012*\"brand\" + 0.011*\"buy\" + 0.011*\"coq10\" + 0.010*\"good\" + 0.009*\"save\" + 0.009*\"'s\" + 0.008*\"calm\" + 0.008*\"anxiety\"\n",
      "INFO : topic #0 (0.130): 0.018*\"help\" + 0.015*\"supplement\" + 0.012*\"day\" + 0.010*\"good\" + 0.010*\"product\" + 0.009*\"body\" + 0.009*\"doctor\" + 0.009*\"health\" + 0.009*\"recommend\" + 0.008*\"work\"\n",
      "INFO : topic #1 (0.193): 0.068*\"product\" + 0.057*\"good\" + 0.046*\"great\" + 0.025*\"price\" + 0.020*\"use\" + 0.016*\"taste\" + 0.014*\"recommend\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"find\"\n",
      "INFO : topic #5 (0.216): 0.025*\"work\" + 0.019*\"feel\" + 0.017*\"help\" + 0.017*\"good\" + 0.017*\"product\" + 0.016*\"use\" + 0.015*\"try\" + 0.015*\"day\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.171622, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13164917, 0.20076545, 0.12118687, 0.12635918, 0.05597081, 0.2170932, 0.11696818, 0.068765834]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.038*\"eye\" + 0.036*\"hair\" + 0.021*\"use\" + 0.021*\"seed\" + 0.016*\"lutein\" + 0.016*\"grow\" + 0.010*\"tablespoon\" + 0.009*\"love\" + 0.009*\"day\" + 0.007*\"product\"\n",
      "INFO : topic #7 (0.069): 0.020*\"find\" + 0.018*\"amazon\" + 0.012*\"brand\" + 0.012*\"coq10\" + 0.011*\"buy\" + 0.010*\"save\" + 0.010*\"good\" + 0.009*\"'s\" + 0.008*\"order\" + 0.008*\"try\"\n",
      "INFO : topic #0 (0.132): 0.018*\"help\" + 0.015*\"supplement\" + 0.012*\"day\" + 0.010*\"good\" + 0.010*\"doctor\" + 0.010*\"product\" + 0.010*\"body\" + 0.009*\"health\" + 0.009*\"recommend\" + 0.009*\"'s\"\n",
      "INFO : topic #1 (0.201): 0.067*\"product\" + 0.056*\"good\" + 0.046*\"great\" + 0.024*\"price\" + 0.022*\"use\" + 0.016*\"taste\" + 0.014*\"recommend\" + 0.013*\"brand\" + 0.013*\"buy\" + 0.012*\"find\"\n",
      "INFO : topic #5 (0.217): 0.025*\"work\" + 0.019*\"feel\" + 0.017*\"help\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"product\" + 0.015*\"try\" + 0.015*\"day\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.163988, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13407022, 0.20561694, 0.12240783, 0.12961733, 0.056053974, 0.2227291, 0.11935931, 0.0697226]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.038*\"hair\" + 0.036*\"eye\" + 0.021*\"use\" + 0.020*\"seed\" + 0.017*\"grow\" + 0.013*\"lutein\" + 0.010*\"tablespoon\" + 0.009*\"love\" + 0.008*\"day\" + 0.007*\"product\"\n",
      "INFO : topic #7 (0.070): 0.020*\"find\" + 0.019*\"amazon\" + 0.012*\"buy\" + 0.012*\"brand\" + 0.011*\"coq10\" + 0.010*\"save\" + 0.010*\"anxiety\" + 0.009*\"calm\" + 0.009*\"'s\" + 0.009*\"good\"\n",
      "INFO : topic #0 (0.134): 0.019*\"help\" + 0.015*\"supplement\" + 0.012*\"day\" + 0.010*\"good\" + 0.010*\"doctor\" + 0.010*\"product\" + 0.009*\"body\" + 0.009*\"health\" + 0.009*\"'s\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.206): 0.069*\"product\" + 0.057*\"good\" + 0.047*\"great\" + 0.025*\"price\" + 0.022*\"use\" + 0.016*\"taste\" + 0.014*\"recommend\" + 0.013*\"buy\" + 0.013*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic #5 (0.223): 0.026*\"work\" + 0.019*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"product\" + 0.016*\"try\" + 0.015*\"day\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.152047, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13702601, 0.21640685, 0.12196226, 0.13168715, 0.056235, 0.22423016, 0.12517069, 0.071062677]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.039*\"hair\" + 0.039*\"eye\" + 0.020*\"use\" + 0.018*\"seed\" + 0.017*\"grow\" + 0.014*\"lutein\" + 0.009*\"tablespoon\" + 0.008*\"day\" + 0.008*\"love\" + 0.006*\"product\"\n",
      "INFO : topic #7 (0.071): 0.022*\"find\" + 0.020*\"amazon\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.011*\"anxiety\" + 0.010*\"save\" + 0.010*\"'s\" + 0.009*\"coq10\" + 0.009*\"good\" + 0.008*\"order\"\n",
      "INFO : topic #0 (0.137): 0.019*\"help\" + 0.016*\"supplement\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.010*\"good\" + 0.009*\"product\" + 0.009*\"body\" + 0.009*\"health\" + 0.009*\"level\" + 0.009*\"'s\"\n",
      "INFO : topic #1 (0.216): 0.069*\"product\" + 0.057*\"good\" + 0.047*\"great\" + 0.027*\"price\" + 0.022*\"use\" + 0.015*\"taste\" + 0.014*\"recommend\" + 0.013*\"buy\" + 0.013*\"brand\" + 0.013*\"find\"\n",
      "INFO : topic #5 (0.224): 0.026*\"work\" + 0.019*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"try\" + 0.016*\"day\" + 0.016*\"product\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.157072, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13858634, 0.22623228, 0.12327709, 0.13278459, 0.056340583, 0.2238687, 0.13098788, 0.071765311]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.038*\"eye\" + 0.037*\"hair\" + 0.020*\"use\" + 0.018*\"grow\" + 0.017*\"seed\" + 0.013*\"lutein\" + 0.010*\"tablespoon\" + 0.008*\"day\" + 0.008*\"love\" + 0.006*\"product\"\n",
      "INFO : topic #7 (0.072): 0.023*\"find\" + 0.021*\"amazon\" + 0.013*\"brand\" + 0.013*\"buy\" + 0.011*\"anxiety\" + 0.010*\"save\" + 0.010*\"'s\" + 0.009*\"order\" + 0.008*\"purchase\" + 0.008*\"good\"\n",
      "INFO : topic #0 (0.139): 0.018*\"help\" + 0.016*\"supplement\" + 0.011*\"doctor\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"level\" + 0.010*\"'s\" + 0.009*\"product\" + 0.009*\"health\" + 0.009*\"recommend\"\n",
      "INFO : topic #5 (0.224): 0.026*\"work\" + 0.019*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"day\" + 0.016*\"product\" + 0.013*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.226): 0.071*\"product\" + 0.059*\"good\" + 0.048*\"great\" + 0.027*\"price\" + 0.021*\"use\" + 0.016*\"taste\" + 0.015*\"recommend\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.013*\"find\"\n",
      "INFO : topic diff=0.138986, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14031251, 0.23287109, 0.12391954, 0.13472995, 0.056502543, 0.22636519, 0.13606805, 0.072716117]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.048*\"hair\" + 0.035*\"eye\" + 0.026*\"grow\" + 0.019*\"use\" + 0.015*\"seed\" + 0.011*\"lutein\" + 0.008*\"tablespoon\" + 0.008*\"love\" + 0.008*\"day\" + 0.006*\"product\"\n",
      "INFO : topic #7 (0.073): 0.023*\"find\" + 0.021*\"amazon\" + 0.013*\"buy\" + 0.013*\"brand\" + 0.011*\"save\" + 0.011*\"'s\" + 0.010*\"anxiety\" + 0.009*\"order\" + 0.009*\"purchase\" + 0.008*\"coq10\"\n",
      "INFO : topic #0 (0.140): 0.018*\"help\" + 0.016*\"supplement\" + 0.011*\"doctor\" + 0.011*\"day\" + 0.010*\"'s\" + 0.010*\"good\" + 0.010*\"level\" + 0.010*\"health\" + 0.009*\"product\" + 0.009*\"recommend\"\n",
      "INFO : topic #5 (0.226): 0.027*\"work\" + 0.019*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"day\" + 0.016*\"use\" + 0.016*\"product\" + 0.013*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.233): 0.073*\"product\" + 0.060*\"good\" + 0.048*\"great\" + 0.027*\"price\" + 0.021*\"use\" + 0.016*\"taste\" + 0.015*\"recommend\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.013*\"find\"\n",
      "INFO : topic diff=0.132055, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14207441, 0.24407728, 0.12471583, 0.13647564, 0.056507632, 0.22337303, 0.14364292, 0.073522702]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.057): 0.049*\"hair\" + 0.036*\"eye\" + 0.025*\"grow\" + 0.018*\"use\" + 0.015*\"seed\" + 0.010*\"lutein\" + 0.008*\"day\" + 0.008*\"love\" + 0.008*\"tablespoon\" + 0.006*\"nail\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #7 (0.074): 0.023*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.013*\"brand\" + 0.011*\"save\" + 0.011*\"'s\" + 0.010*\"anxiety\" + 0.009*\"order\" + 0.009*\"purchase\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.144): 0.025*\"taste\" + 0.021*\"like\" + 0.019*\"pill\" + 0.019*\"easy\" + 0.017*\"vitamin\" + 0.017*\"good\" + 0.016*\"fish_oil\" + 0.014*\"great\" + 0.014*\"swallow\" + 0.013*\"supplement\"\n",
      "INFO : topic #5 (0.223): 0.027*\"work\" + 0.020*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.017*\"day\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"product\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.244): 0.074*\"product\" + 0.061*\"good\" + 0.049*\"great\" + 0.028*\"price\" + 0.020*\"use\" + 0.016*\"taste\" + 0.016*\"recommend\" + 0.015*\"buy\" + 0.014*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.127501, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14270869, 0.25511378, 0.1239526, 0.13807669, 0.056478348, 0.22652264, 0.14873998, 0.074497618]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.056): 0.045*\"hair\" + 0.035*\"eye\" + 0.028*\"grow\" + 0.018*\"use\" + 0.014*\"seed\" + 0.010*\"lutein\" + 0.008*\"tablespoon\" + 0.008*\"love\" + 0.007*\"day\" + 0.006*\"second_bottle\"\n",
      "INFO : topic #7 (0.074): 0.024*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.012*\"save\" + 0.010*\"'s\" + 0.009*\"order\" + 0.009*\"anxiety\" + 0.009*\"purchase\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.149): 0.027*\"taste\" + 0.022*\"like\" + 0.019*\"easy\" + 0.019*\"pill\" + 0.017*\"vitamin\" + 0.017*\"good\" + 0.016*\"fish_oil\" + 0.014*\"great\" + 0.014*\"swallow\" + 0.013*\"supplement\"\n",
      "INFO : topic #5 (0.227): 0.027*\"work\" + 0.020*\"feel\" + 0.018*\"help\" + 0.017*\"good\" + 0.017*\"day\" + 0.016*\"try\" + 0.016*\"product\" + 0.016*\"use\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.255): 0.075*\"product\" + 0.061*\"good\" + 0.050*\"great\" + 0.028*\"price\" + 0.020*\"use\" + 0.016*\"taste\" + 0.016*\"recommend\" + 0.015*\"buy\" + 0.014*\"brand\" + 0.012*\"love\"\n",
      "INFO : topic diff=0.118990, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14418268, 0.26562199, 0.1274004, 0.13997304, 0.057707272, 0.22721736, 0.15245421, 0.07496208]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.058): 0.045*\"hair\" + 0.032*\"eye\" + 0.024*\"grow\" + 0.019*\"seed\" + 0.018*\"use\" + 0.017*\"coconut_oil\" + 0.013*\"flax\" + 0.012*\"tablespoon\" + 0.008*\"cooking\" + 0.008*\"love\"\n",
      "INFO : topic #7 (0.075): 0.023*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.012*\"save\" + 0.012*\"'s\" + 0.009*\"order\" + 0.009*\"purchase\" + 0.008*\"anxiety\" + 0.008*\"good\"\n",
      "INFO : topic #6 (0.152): 0.030*\"taste\" + 0.022*\"like\" + 0.019*\"easy\" + 0.018*\"fish_oil\" + 0.018*\"pill\" + 0.017*\"good\" + 0.016*\"vitamin\" + 0.014*\"great\" + 0.014*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.227): 0.027*\"work\" + 0.021*\"feel\" + 0.017*\"help\" + 0.017*\"day\" + 0.017*\"good\" + 0.017*\"try\" + 0.016*\"product\" + 0.016*\"use\" + 0.013*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.266): 0.076*\"product\" + 0.061*\"good\" + 0.051*\"great\" + 0.027*\"price\" + 0.021*\"use\" + 0.017*\"taste\" + 0.016*\"recommend\" + 0.015*\"buy\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.132989, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.146961, 0.27438328, 0.12840964, 0.14323123, 0.058546446, 0.23541008, 0.15404926, 0.076443896]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.042*\"hair\" + 0.028*\"eye\" + 0.024*\"gaia\" + 0.023*\"grow\" + 0.021*\"seed\" + 0.018*\"use\" + 0.014*\"coconut_oil\" + 0.012*\"flax\" + 0.011*\"tablespoon\" + 0.008*\"love\"\n",
      "INFO : topic #7 (0.076): 0.024*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.012*\"'s\" + 0.012*\"save\" + 0.010*\"anxiety\" + 0.009*\"purchase\" + 0.009*\"order\" + 0.009*\"stress\"\n",
      "INFO : topic #6 (0.154): 0.030*\"taste\" + 0.022*\"like\" + 0.020*\"easy\" + 0.018*\"pill\" + 0.016*\"good\" + 0.016*\"fish_oil\" + 0.016*\"vitamin\" + 0.014*\"great\" + 0.013*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.235): 0.027*\"work\" + 0.022*\"feel\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"product\" + 0.016*\"try\" + 0.016*\"use\" + 0.012*\"year\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.274): 0.077*\"product\" + 0.061*\"good\" + 0.051*\"great\" + 0.027*\"price\" + 0.021*\"use\" + 0.017*\"taste\" + 0.016*\"buy\" + 0.016*\"recommend\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.125426, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14949463, 0.28390855, 0.1298885, 0.14469428, 0.058813922, 0.23643215, 0.15995954, 0.077135548]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.039*\"hair\" + 0.029*\"eye\" + 0.022*\"grow\" + 0.021*\"gaia\" + 0.019*\"seed\" + 0.017*\"use\" + 0.015*\"coconut_oil\" + 0.011*\"flax\" + 0.010*\"tablespoon\" + 0.007*\"love\"\n",
      "INFO : topic #7 (0.077): 0.024*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.012*\"'s\" + 0.012*\"save\" + 0.009*\"purchase\" + 0.009*\"anxiety\" + 0.009*\"order\" + 0.008*\"stress\"\n",
      "INFO : topic #6 (0.160): 0.031*\"taste\" + 0.022*\"like\" + 0.021*\"easy\" + 0.017*\"pill\" + 0.016*\"vitamin\" + 0.016*\"good\" + 0.016*\"fish_oil\" + 0.014*\"great\" + 0.013*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.236): 0.027*\"work\" + 0.022*\"feel\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"product\" + 0.016*\"try\" + 0.016*\"use\" + 0.012*\"year\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.284): 0.078*\"product\" + 0.062*\"good\" + 0.051*\"great\" + 0.026*\"price\" + 0.021*\"use\" + 0.016*\"taste\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.014*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.111596, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15186317, 0.29212689, 0.13056825, 0.1480265, 0.059094496, 0.2386312, 0.16332141, 0.078230582]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.059): 0.038*\"hair\" + 0.031*\"eye\" + 0.023*\"grow\" + 0.018*\"gaia\" + 0.017*\"seed\" + 0.017*\"use\" + 0.013*\"coconut_oil\" + 0.010*\"tablespoon\" + 0.009*\"flax\" + 0.007*\"love\"\n",
      "INFO : topic #7 (0.078): 0.024*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.013*\"save\" + 0.012*\"'s\" + 0.010*\"order\" + 0.010*\"purchase\" + 0.008*\"anxiety\" + 0.008*\"coq10\"\n",
      "INFO : topic #6 (0.163): 0.031*\"taste\" + 0.022*\"like\" + 0.021*\"easy\" + 0.017*\"pill\" + 0.016*\"good\" + 0.016*\"vitamin\" + 0.015*\"fish_oil\" + 0.014*\"great\" + 0.012*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.239): 0.027*\"work\" + 0.022*\"feel\" + 0.018*\"day\" + 0.017*\"help\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"product\" + 0.016*\"try\" + 0.012*\"year\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.292): 0.079*\"product\" + 0.061*\"good\" + 0.052*\"great\" + 0.026*\"price\" + 0.022*\"use\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.016*\"taste\" + 0.013*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.106499, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1538312, 0.30088958, 0.1313795, 0.15068036, 0.059685577, 0.23898597, 0.16627853, 0.07920976]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.060): 0.048*\"hair\" + 0.036*\"eye\" + 0.029*\"grow\" + 0.017*\"use\" + 0.016*\"seed\" + 0.014*\"gaia\" + 0.011*\"coconut_oil\" + 0.010*\"tablespoon\" + 0.009*\"nail\" + 0.008*\"flax\"\n",
      "INFO : topic #7 (0.079): 0.024*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.014*\"brand\" + 0.013*\"save\" + 0.011*\"'s\" + 0.010*\"order\" + 0.010*\"purchase\" + 0.009*\"coq10\" + 0.008*\"anxiety\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #6 (0.166): 0.031*\"taste\" + 0.022*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.016*\"good\" + 0.016*\"vitamin\" + 0.014*\"great\" + 0.013*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.239): 0.028*\"work\" + 0.022*\"feel\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"try\" + 0.016*\"product\" + 0.012*\"year\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.301): 0.080*\"product\" + 0.061*\"good\" + 0.052*\"great\" + 0.026*\"price\" + 0.022*\"use\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.015*\"taste\" + 0.013*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.106016, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15506718, 0.30802262, 0.13283591, 0.15217704, 0.061667215, 0.23966146, 0.16949026, 0.079659179]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.094*\"hair\" + 0.046*\"grow\" + 0.027*\"nail\" + 0.027*\"eye\" + 0.016*\"use\" + 0.011*\"seed\" + 0.010*\"nutrigold\" + 0.009*\"coconut_oil\" + 0.009*\"thin\" + 0.009*\"gaia\"\n",
      "INFO : topic #7 (0.080): 0.026*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.013*\"brand\" + 0.013*\"save\" + 0.011*\"'s\" + 0.010*\"order\" + 0.010*\"purchase\" + 0.009*\"coq10\" + 0.007*\"anxiety\"\n",
      "INFO : topic #6 (0.169): 0.029*\"taste\" + 0.022*\"easy\" + 0.022*\"like\" + 0.019*\"pill\" + 0.017*\"vitamin\" + 0.016*\"good\" + 0.014*\"fish_oil\" + 0.014*\"great\" + 0.013*\"swallow\" + 0.012*\"supplement\"\n",
      "INFO : topic #5 (0.240): 0.028*\"work\" + 0.022*\"feel\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"product\" + 0.016*\"use\" + 0.016*\"try\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.308): 0.082*\"product\" + 0.061*\"good\" + 0.052*\"great\" + 0.027*\"price\" + 0.022*\"use\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.014*\"taste\" + 0.014*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.123806, rho=0.176777\n",
      "INFO : PROGRESS: pass 0, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15847841, 0.31872311, 0.13458373, 0.15474361, 0.061928488, 0.24146961, 0.17404352, 0.080480911]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.062): 0.090*\"hair\" + 0.043*\"grow\" + 0.030*\"eye\" + 0.026*\"nail\" + 0.015*\"use\" + 0.012*\"nutrigold\" + 0.010*\"seed\" + 0.009*\"thin\" + 0.008*\"long\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.080): 0.026*\"find\" + 0.023*\"amazon\" + 0.014*\"buy\" + 0.013*\"brand\" + 0.013*\"save\" + 0.011*\"'s\" + 0.011*\"order\" + 0.010*\"purchase\" + 0.008*\"coq10\" + 0.008*\"anxiety\"\n",
      "INFO : topic #6 (0.174): 0.029*\"taste\" + 0.022*\"easy\" + 0.022*\"like\" + 0.019*\"pill\" + 0.017*\"vitamin\" + 0.016*\"good\" + 0.015*\"fish_oil\" + 0.014*\"swallow\" + 0.013*\"great\" + 0.013*\"supplement\"\n",
      "INFO : topic #5 (0.241): 0.028*\"work\" + 0.023*\"feel\" + 0.019*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"product\" + 0.016*\"try\" + 0.016*\"use\" + 0.012*\"year\" + 0.011*\"start\"\n",
      "INFO : topic #1 (0.319): 0.083*\"product\" + 0.062*\"good\" + 0.052*\"great\" + 0.026*\"price\" + 0.021*\"use\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.014*\"taste\" + 0.013*\"brand\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.108229, rho=0.174078\n",
      "INFO : PROGRESS: pass 0, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16168487, 0.32733655, 0.13545331, 0.15820347, 0.062503688, 0.24436966, 0.17734094, 0.081603467]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.090*\"hair\" + 0.043*\"grow\" + 0.029*\"nail\" + 0.026*\"eye\" + 0.019*\"nutrigold\" + 0.015*\"use\" + 0.010*\"seed\" + 0.009*\"gold\" + 0.009*\"thin\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.082): 0.026*\"find\" + 0.023*\"amazon\" + 0.014*\"buy\" + 0.013*\"brand\" + 0.013*\"save\" + 0.011*\"'s\" + 0.011*\"coq10\" + 0.011*\"order\" + 0.010*\"purchase\" + 0.008*\"anxiety\"\n",
      "INFO : topic #6 (0.177): 0.029*\"taste\" + 0.022*\"easy\" + 0.022*\"like\" + 0.019*\"pill\" + 0.018*\"vitamin\" + 0.016*\"good\" + 0.014*\"fish_oil\" + 0.014*\"swallow\" + 0.013*\"great\" + 0.013*\"supplement\"\n",
      "INFO : topic #5 (0.244): 0.028*\"work\" + 0.023*\"feel\" + 0.019*\"help\" + 0.019*\"day\" + 0.016*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"product\" + 0.011*\"year\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.327): 0.085*\"product\" + 0.062*\"good\" + 0.054*\"great\" + 0.026*\"price\" + 0.021*\"use\" + 0.016*\"recommend\" + 0.016*\"buy\" + 0.013*\"taste\" + 0.013*\"love\" + 0.013*\"brand\"\n",
      "INFO : topic diff=0.096478, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16347976, 0.33742645, 0.1377569, 0.16047499, 0.062922329, 0.24999064, 0.17971689, 0.082305335]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.082*\"hair\" + 0.041*\"grow\" + 0.028*\"nail\" + 0.027*\"eye\" + 0.024*\"nutrigold\" + 0.015*\"use\" + 0.012*\"gold\" + 0.010*\"seed\" + 0.009*\"thin\" + 0.009*\"gaia\"\n",
      "INFO : topic #7 (0.082): 0.026*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.013*\"brand\" + 0.012*\"save\" + 0.011*\"'s\" + 0.011*\"order\" + 0.010*\"anxiety\" + 0.010*\"coq10\" + 0.010*\"purchase\"\n",
      "INFO : topic #6 (0.180): 0.029*\"taste\" + 0.023*\"easy\" + 0.022*\"like\" + 0.019*\"pill\" + 0.018*\"vitamin\" + 0.016*\"good\" + 0.014*\"swallow\" + 0.014*\"fish_oil\" + 0.013*\"great\" + 0.013*\"supplement\"\n",
      "INFO : topic #5 (0.250): 0.028*\"work\" + 0.023*\"feel\" + 0.019*\"help\" + 0.019*\"day\" + 0.017*\"good\" + 0.016*\"use\" + 0.016*\"try\" + 0.016*\"product\" + 0.011*\"year\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.337): 0.087*\"product\" + 0.062*\"good\" + 0.055*\"great\" + 0.026*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.013*\"love\" + 0.013*\"brand\" + 0.013*\"taste\"\n",
      "INFO : topic diff=0.094816, rho=0.169031\n",
      "INFO : PROGRESS: pass 0, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1656104, 0.34860256, 0.13828148, 0.16307947, 0.0633839, 0.25325063, 0.18411151, 0.083179943]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.063): 0.079*\"hair\" + 0.039*\"grow\" + 0.029*\"nail\" + 0.026*\"eye\" + 0.024*\"nutrigold\" + 0.014*\"use\" + 0.012*\"gold\" + 0.011*\"seed\" + 0.008*\"thin\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.083): 0.026*\"find\" + 0.022*\"amazon\" + 0.013*\"buy\" + 0.013*\"coq10\" + 0.013*\"save\" + 0.013*\"brand\" + 0.011*\"'s\" + 0.011*\"order\" + 0.010*\"anxiety\" + 0.010*\"purchase\"\n",
      "INFO : topic #6 (0.184): 0.030*\"taste\" + 0.023*\"easy\" + 0.022*\"like\" + 0.018*\"pill\" + 0.018*\"vitamin\" + 0.016*\"good\" + 0.014*\"swallow\" + 0.013*\"great\" + 0.013*\"supplement\" + 0.012*\"fish_oil\"\n",
      "INFO : topic #5 (0.253): 0.029*\"work\" + 0.023*\"feel\" + 0.019*\"help\" + 0.019*\"day\" + 0.017*\"good\" + 0.017*\"use\" + 0.016*\"try\" + 0.016*\"product\" + 0.011*\"year\" + 0.011*\"like\"\n",
      "INFO : topic #1 (0.349): 0.088*\"product\" + 0.062*\"good\" + 0.056*\"great\" + 0.025*\"price\" + 0.022*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.014*\"love\" + 0.013*\"taste\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.092292, rho=0.166667\n",
      "INFO : PROGRESS: pass 0, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16805969, 0.35917568, 0.13918227, 0.16779713, 0.064036004, 0.2566632, 0.18733017, 0.084265187]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.064): 0.076*\"hair\" + 0.037*\"grow\" + 0.028*\"nutrigold\" + 0.027*\"nail\" + 0.026*\"eye\" + 0.014*\"use\" + 0.012*\"gold\" + 0.012*\"seed\" + 0.008*\"long\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.084): 0.026*\"find\" + 0.022*\"amazon\" + 0.014*\"coq10\" + 0.013*\"buy\" + 0.013*\"brand\" + 0.012*\"save\" + 0.011*\"order\" + 0.011*\"'s\" + 0.010*\"anxiety\" + 0.010*\"purchase\"\n",
      "INFO : topic #6 (0.187): 0.031*\"taste\" + 0.023*\"like\" + 0.023*\"easy\" + 0.018*\"pill\" + 0.017*\"vitamin\" + 0.016*\"good\" + 0.013*\"swallow\" + 0.013*\"great\" + 0.013*\"supplement\" + 0.012*\"fish_oil\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.257): 0.029*\"work\" + 0.024*\"feel\" + 0.019*\"day\" + 0.019*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"product\" + 0.011*\"like\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.359): 0.089*\"product\" + 0.062*\"good\" + 0.057*\"great\" + 0.025*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.014*\"love\" + 0.013*\"taste\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.096496, rho=0.164399\n",
      "INFO : PROGRESS: pass 0, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16941729, 0.36966136, 0.13990796, 0.17311123, 0.064697951, 0.26091745, 0.19013225, 0.085031033]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.079*\"hair\" + 0.037*\"grow\" + 0.029*\"nail\" + 0.025*\"eye\" + 0.025*\"nutrigold\" + 0.014*\"use\" + 0.011*\"gold\" + 0.011*\"seed\" + 0.009*\"thick\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.085): 0.026*\"find\" + 0.022*\"amazon\" + 0.015*\"coq10\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.011*\"save\" + 0.011*\"'s\" + 0.011*\"order\" + 0.010*\"anxiety\" + 0.010*\"purchase\"\n",
      "INFO : topic #6 (0.190): 0.032*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.017*\"vitamin\" + 0.016*\"good\" + 0.013*\"swallow\" + 0.013*\"supplement\" + 0.013*\"great\" + 0.013*\"fish_oil\"\n",
      "INFO : topic #5 (0.261): 0.029*\"work\" + 0.025*\"feel\" + 0.019*\"day\" + 0.019*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"product\" + 0.012*\"like\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.370): 0.090*\"product\" + 0.062*\"good\" + 0.058*\"great\" + 0.025*\"price\" + 0.022*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.014*\"love\" + 0.013*\"taste\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.086394, rho=0.162221\n",
      "INFO : PROGRESS: pass 0, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17216943, 0.38046288, 0.14084069, 0.17802046, 0.06507232, 0.26425722, 0.19296208, 0.086076081]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.077*\"hair\" + 0.036*\"grow\" + 0.028*\"nail\" + 0.026*\"eye\" + 0.025*\"nutrigold\" + 0.013*\"use\" + 0.011*\"gold\" + 0.010*\"seed\" + 0.008*\"long\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.086): 0.025*\"find\" + 0.021*\"amazon\" + 0.013*\"buy\" + 0.013*\"coq10\" + 0.013*\"brand\" + 0.011*\"'s\" + 0.011*\"order\" + 0.010*\"save\" + 0.010*\"purchase\" + 0.010*\"anxiety\"\n",
      "INFO : topic #6 (0.193): 0.031*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.017*\"vitamin\" + 0.016*\"good\" + 0.013*\"fish_oil\" + 0.013*\"supplement\" + 0.013*\"swallow\" + 0.012*\"great\"\n",
      "INFO : topic #5 (0.264): 0.029*\"work\" + 0.026*\"feel\" + 0.019*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.012*\"like\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.380): 0.091*\"product\" + 0.062*\"good\" + 0.058*\"great\" + 0.024*\"price\" + 0.022*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.014*\"love\" + 0.012*\"quality\" + 0.012*\"taste\"\n",
      "INFO : topic diff=0.091596, rho=0.160128\n",
      "INFO : PROGRESS: pass 0, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17496879, 0.39467785, 0.141739, 0.182, 0.065438017, 0.26659971, 0.19746444, 0.087160245]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.065): 0.077*\"hair\" + 0.036*\"grow\" + 0.029*\"nail\" + 0.027*\"eye\" + 0.022*\"nutrigold\" + 0.013*\"use\" + 0.011*\"seed\" + 0.010*\"gold\" + 0.008*\"thick\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.087): 0.025*\"find\" + 0.020*\"amazon\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"'s\" + 0.011*\"coq10\" + 0.011*\"order\" + 0.010*\"purchase\" + 0.010*\"save\" + 0.010*\"anxiety\"\n",
      "INFO : topic #6 (0.197): 0.031*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.019*\"pill\" + 0.017*\"vitamin\" + 0.017*\"fish_oil\" + 0.016*\"good\" + 0.013*\"supplement\" + 0.013*\"swallow\" + 0.012*\"great\"\n",
      "INFO : topic #5 (0.267): 0.029*\"work\" + 0.026*\"feel\" + 0.019*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.012*\"like\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.395): 0.092*\"product\" + 0.062*\"good\" + 0.058*\"great\" + 0.024*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.016*\"buy\" + 0.014*\"love\" + 0.013*\"quality\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.084523, rho=0.158114\n",
      "INFO : PROGRESS: pass 0, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17585278, 0.40484509, 0.14554718, 0.1866709, 0.066088058, 0.26740587, 0.19814314, 0.088011831]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.066): 0.073*\"hair\" + 0.033*\"grow\" + 0.028*\"nail\" + 0.028*\"eye\" + 0.018*\"nutrigold\" + 0.013*\"use\" + 0.012*\"naturewise\" + 0.009*\"seed\" + 0.008*\"gold\" + 0.008*\"second_bottle\"\n",
      "INFO : topic #7 (0.088): 0.025*\"find\" + 0.020*\"amazon\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"order\" + 0.011*\"'s\" + 0.011*\"coq10\" + 0.010*\"purchase\" + 0.010*\"anxiety\" + 0.010*\"save\"\n",
      "INFO : topic #6 (0.198): 0.030*\"taste\" + 0.023*\"like\" + 0.023*\"easy\" + 0.019*\"pill\" + 0.018*\"vitamin\" + 0.017*\"fish_oil\" + 0.015*\"good\" + 0.014*\"swallow\" + 0.014*\"supplement\" + 0.012*\"great\"\n",
      "INFO : topic #5 (0.267): 0.029*\"work\" + 0.028*\"feel\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.015*\"product\" + 0.012*\"like\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (0.405): 0.094*\"product\" + 0.061*\"good\" + 0.059*\"great\" + 0.023*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.015*\"buy\" + 0.014*\"love\" + 0.013*\"quality\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.091034, rho=0.156174\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17889053, 0.42287377, 0.14766616, 0.19140467, 0.066970646, 0.26842421, 0.20470807, 0.088774063]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.067): 0.067*\"hair\" + 0.034*\"naturewise\" + 0.030*\"grow\" + 0.027*\"nail\" + 0.026*\"eye\" + 0.015*\"nutrigold\" + 0.012*\"use\" + 0.009*\"seed\" + 0.008*\"notice\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.089): 0.024*\"find\" + 0.018*\"amazon\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.011*\"order\" + 0.011*\"'s\" + 0.010*\"coq10\" + 0.010*\"review\" + 0.010*\"purchase\" + 0.010*\"anxiety\"\n",
      "INFO : topic #6 (0.205): 0.029*\"taste\" + 0.024*\"easy\" + 0.023*\"like\" + 0.019*\"pill\" + 0.018*\"vitamin\" + 0.017*\"fish_oil\" + 0.015*\"supplement\" + 0.015*\"good\" + 0.014*\"swallow\" + 0.011*\"capsule\"\n",
      "INFO : topic #5 (0.268): 0.029*\"feel\" + 0.028*\"work\" + 0.019*\"day\" + 0.018*\"help\" + 0.017*\"good\" + 0.017*\"try\" + 0.015*\"use\" + 0.015*\"product\" + 0.012*\"like\" + 0.012*\"energy\"\n",
      "INFO : topic #1 (0.423): 0.096*\"product\" + 0.059*\"great\" + 0.058*\"good\" + 0.022*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.015*\"buy\" + 0.014*\"quality\" + 0.014*\"love\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.091495, rho=0.154303\n",
      "INFO : PROGRESS: pass 0, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18011709, 0.43432325, 0.15078549, 0.19887142, 0.067587011, 0.27023479, 0.20491165, 0.089657478]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.068): 0.072*\"hair\" + 0.030*\"grow\" + 0.028*\"naturewise\" + 0.025*\"eye\" + 0.025*\"nail\" + 0.013*\"nutrigold\" + 0.012*\"use\" + 0.009*\"seed\" + 0.009*\"thick\" + 0.009*\"notice\"\n",
      "INFO : topic #7 (0.090): 0.024*\"find\" + 0.018*\"amazon\" + 0.012*\"brand\" + 0.012*\"buy\" + 0.012*\"coq10\" + 0.011*\"order\" + 0.011*\"review\" + 0.010*\"'s\" + 0.010*\"purchase\" + 0.010*\"save\"\n",
      "INFO : topic #6 (0.205): 0.028*\"taste\" + 0.024*\"easy\" + 0.023*\"like\" + 0.019*\"pill\" + 0.017*\"vitamin\" + 0.016*\"fish_oil\" + 0.015*\"supplement\" + 0.014*\"good\" + 0.014*\"swallow\" + 0.012*\"capsule\"\n",
      "INFO : topic #5 (0.270): 0.030*\"feel\" + 0.028*\"work\" + 0.020*\"day\" + 0.018*\"help\" + 0.017*\"try\" + 0.017*\"good\" + 0.015*\"use\" + 0.015*\"product\" + 0.013*\"like\" + 0.012*\"energy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.434): 0.097*\"product\" + 0.059*\"great\" + 0.057*\"good\" + 0.021*\"price\" + 0.021*\"use\" + 0.017*\"recommend\" + 0.015*\"buy\" + 0.014*\"quality\" + 0.014*\"love\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.086438, rho=0.152499\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18098997, 0.44388598, 0.15259108, 0.20839767, 0.067775927, 0.27386859, 0.20515646, 0.090125233]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.068): 0.069*\"hair\" + 0.030*\"grow\" + 0.025*\"eye\" + 0.025*\"naturewise\" + 0.024*\"nail\" + 0.012*\"use\" + 0.011*\"nutrigold\" + 0.009*\"inch\" + 0.009*\"second_bottle\" + 0.009*\"notice\"\n",
      "INFO : topic #7 (0.090): 0.024*\"find\" + 0.017*\"amazon\" + 0.013*\"brand\" + 0.012*\"buy\" + 0.012*\"review\" + 0.011*\"order\" + 0.010*\"coq10\" + 0.010*\"'s\" + 0.010*\"save\" + 0.010*\"purchase\"\n",
      "INFO : topic #3 (0.208): 0.027*\"product\" + 0.023*\"work\" + 0.022*\"day\" + 0.021*\"week\" + 0.016*\"feel\" + 0.016*\"try\" + 0.016*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.015*\"pain\"\n",
      "INFO : topic #5 (0.274): 0.031*\"feel\" + 0.027*\"work\" + 0.019*\"day\" + 0.019*\"help\" + 0.018*\"try\" + 0.017*\"good\" + 0.014*\"use\" + 0.014*\"product\" + 0.013*\"probiotic\" + 0.013*\"like\"\n",
      "INFO : topic #1 (0.444): 0.099*\"product\" + 0.059*\"great\" + 0.056*\"good\" + 0.020*\"use\" + 0.020*\"price\" + 0.018*\"recommend\" + 0.015*\"buy\" + 0.014*\"love\" + 0.014*\"quality\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.088876, rho=0.150756\n",
      "INFO : PROGRESS: pass 1, at document #5000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17551942, 0.44328037, 0.14932096, 0.21585497, 0.072202906, 0.25866148, 0.20285207, 0.091986746]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.072): 0.093*\"pedometer\" + 0.034*\"hair\" + 0.024*\"omron\" + 0.021*\"clip\" + 0.019*\"step\" + 0.017*\"use\" + 0.014*\"grow\" + 0.014*\"eye\" + 0.012*\"accurate\" + 0.011*\"naturewise\"\n",
      "INFO : topic #7 (0.092): 0.024*\"find\" + 0.016*\"amazon\" + 0.014*\"buy\" + 0.012*\"review\" + 0.011*\"purchase\" + 0.011*\"brand\" + 0.010*\"order\" + 0.010*\"'s\" + 0.010*\"save\" + 0.008*\"coq10\"\n",
      "INFO : topic #3 (0.216): 0.026*\"day\" + 0.025*\"product\" + 0.024*\"work\" + 0.019*\"week\" + 0.017*\"use\" + 0.015*\"try\" + 0.015*\"help\" + 0.014*\"feel\" + 0.014*\"start\" + 0.013*\"pain\"\n",
      "INFO : topic #5 (0.259): 0.030*\"feel\" + 0.029*\"work\" + 0.020*\"day\" + 0.018*\"help\" + 0.018*\"try\" + 0.017*\"good\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"like\" + 0.013*\"probiotic\"\n",
      "INFO : topic #1 (0.443): 0.095*\"product\" + 0.061*\"great\" + 0.057*\"good\" + 0.022*\"use\" + 0.021*\"price\" + 0.018*\"recommend\" + 0.016*\"buy\" + 0.016*\"love\" + 0.013*\"quality\" + 0.011*\"order\"\n",
      "INFO : topic diff=0.201343, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #10000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17620718, 0.44719166, 0.14970545, 0.21708368, 0.073925562, 0.2626203, 0.2042702, 0.092857413]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.097*\"pedometer\" + 0.028*\"hair\" + 0.026*\"step\" + 0.025*\"omron\" + 0.021*\"clip\" + 0.019*\"use\" + 0.016*\"accurate\" + 0.011*\"grow\" + 0.011*\"eye\" + 0.011*\"hj-112\"\n",
      "INFO : topic #7 (0.093): 0.025*\"find\" + 0.018*\"amazon\" + 0.014*\"buy\" + 0.012*\"purchase\" + 0.012*\"review\" + 0.011*\"brand\" + 0.010*\"save\" + 0.010*\"order\" + 0.010*\"'s\" + 0.008*\"store\"\n",
      "INFO : topic #3 (0.217): 0.027*\"day\" + 0.024*\"work\" + 0.024*\"product\" + 0.019*\"week\" + 0.017*\"use\" + 0.015*\"try\" + 0.015*\"help\" + 0.014*\"start\" + 0.014*\"feel\" + 0.013*\"pain\"\n",
      "INFO : topic #5 (0.263): 0.029*\"work\" + 0.028*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.017*\"try\" + 0.017*\"good\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"like\" + 0.012*\"probiotic\"\n",
      "INFO : topic #1 (0.447): 0.094*\"product\" + 0.062*\"great\" + 0.059*\"good\" + 0.023*\"use\" + 0.021*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"quality\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.090184, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #15000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17905186, 0.45511478, 0.14985679, 0.21896936, 0.073982328, 0.27369711, 0.20706551, 0.093314171]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.090*\"pedometer\" + 0.037*\"hair\" + 0.024*\"step\" + 0.023*\"omron\" + 0.020*\"clip\" + 0.018*\"use\" + 0.015*\"grow\" + 0.015*\"accurate\" + 0.011*\"eye\" + 0.010*\"nail\"\n",
      "INFO : topic #7 (0.093): 0.025*\"find\" + 0.019*\"amazon\" + 0.014*\"buy\" + 0.012*\"purchase\" + 0.011*\"review\" + 0.011*\"brand\" + 0.011*\"save\" + 0.010*\"'s\" + 0.010*\"order\" + 0.008*\"store\"\n",
      "INFO : topic #3 (0.219): 0.027*\"day\" + 0.024*\"work\" + 0.024*\"product\" + 0.019*\"week\" + 0.017*\"use\" + 0.015*\"help\" + 0.015*\"try\" + 0.014*\"pain\" + 0.014*\"start\" + 0.013*\"feel\"\n",
      "INFO : topic #5 (0.274): 0.029*\"work\" + 0.028*\"feel\" + 0.020*\"day\" + 0.019*\"help\" + 0.017*\"good\" + 0.017*\"try\" + 0.015*\"use\" + 0.014*\"product\" + 0.012*\"like\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.455): 0.095*\"product\" + 0.063*\"great\" + 0.059*\"good\" + 0.024*\"use\" + 0.022*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.012*\"quality\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.083683, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #20000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18161513, 0.46013209, 0.15199921, 0.22084995, 0.074041508, 0.28293109, 0.20732708, 0.093981147]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.083*\"pedometer\" + 0.044*\"hair\" + 0.022*\"step\" + 0.021*\"omron\" + 0.018*\"use\" + 0.018*\"clip\" + 0.018*\"grow\" + 0.013*\"accurate\" + 0.013*\"nail\" + 0.012*\"eye\"\n",
      "INFO : topic #7 (0.094): 0.025*\"find\" + 0.020*\"amazon\" + 0.014*\"buy\" + 0.011*\"purchase\" + 0.011*\"review\" + 0.011*\"brand\" + 0.011*\"'s\" + 0.011*\"save\" + 0.010*\"order\" + 0.009*\"anxiety\"\n",
      "INFO : topic #3 (0.221): 0.028*\"day\" + 0.025*\"work\" + 0.023*\"product\" + 0.018*\"week\" + 0.017*\"use\" + 0.015*\"pain\" + 0.015*\"help\" + 0.014*\"try\" + 0.014*\"start\" + 0.013*\"feel\"\n",
      "INFO : topic #5 (0.283): 0.029*\"work\" + 0.026*\"feel\" + 0.020*\"day\" + 0.020*\"help\" + 0.017*\"good\" + 0.016*\"try\" + 0.016*\"use\" + 0.013*\"product\" + 0.012*\"like\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.460): 0.095*\"product\" + 0.063*\"great\" + 0.060*\"good\" + 0.024*\"use\" + 0.022*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.012*\"quality\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.077916, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #25000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18227725, 0.4689267, 0.15576416, 0.22041799, 0.074032806, 0.29013562, 0.20807853, 0.094745547]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.077*\"pedometer\" + 0.048*\"hair\" + 0.020*\"step\" + 0.019*\"omron\" + 0.019*\"grow\" + 0.018*\"use\" + 0.017*\"clip\" + 0.015*\"nail\" + 0.012*\"accurate\" + 0.012*\"eye\"\n",
      "INFO : topic #7 (0.095): 0.025*\"find\" + 0.020*\"amazon\" + 0.014*\"buy\" + 0.011*\"purchase\" + 0.011*\"brand\" + 0.011*\"review\" + 0.011*\"save\" + 0.011*\"'s\" + 0.010*\"order\" + 0.009*\"store\"\n",
      "INFO : topic #3 (0.220): 0.027*\"day\" + 0.025*\"work\" + 0.023*\"product\" + 0.018*\"week\" + 0.017*\"use\" + 0.016*\"pain\" + 0.016*\"help\" + 0.014*\"start\" + 0.014*\"try\" + 0.013*\"feel\"\n",
      "INFO : topic #5 (0.290): 0.029*\"work\" + 0.025*\"feel\" + 0.021*\"help\" + 0.020*\"day\" + 0.016*\"good\" + 0.016*\"use\" + 0.016*\"try\" + 0.013*\"product\" + 0.012*\"like\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.469): 0.094*\"product\" + 0.064*\"great\" + 0.061*\"good\" + 0.025*\"use\" + 0.023*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.012*\"quality\" + 0.012*\"brand\"\n",
      "INFO : topic diff=0.079442, rho=0.148240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, at document #30000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18574314, 0.47603461, 0.15556122, 0.22287267, 0.074068651, 0.30143756, 0.20831725, 0.096219242]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.071*\"pedometer\" + 0.051*\"hair\" + 0.021*\"grow\" + 0.019*\"step\" + 0.018*\"use\" + 0.018*\"omron\" + 0.016*\"nail\" + 0.016*\"clip\" + 0.012*\"eye\" + 0.011*\"accurate\"\n",
      "INFO : topic #7 (0.096): 0.026*\"find\" + 0.020*\"amazon\" + 0.014*\"buy\" + 0.011*\"save\" + 0.011*\"brand\" + 0.011*\"'s\" + 0.011*\"purchase\" + 0.011*\"review\" + 0.010*\"order\" + 0.009*\"anxiety\"\n",
      "INFO : topic #3 (0.223): 0.028*\"day\" + 0.025*\"work\" + 0.022*\"product\" + 0.018*\"week\" + 0.017*\"use\" + 0.016*\"pain\" + 0.016*\"help\" + 0.014*\"start\" + 0.014*\"try\" + 0.013*\"feel\"\n",
      "INFO : topic #5 (0.301): 0.029*\"work\" + 0.025*\"feel\" + 0.021*\"help\" + 0.020*\"day\" + 0.016*\"good\" + 0.016*\"use\" + 0.015*\"try\" + 0.013*\"product\" + 0.012*\"like\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.476): 0.096*\"product\" + 0.064*\"great\" + 0.061*\"good\" + 0.025*\"use\" + 0.023*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.012*\"quality\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.078266, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18781018, 0.48413348, 0.15624914, 0.22628307, 0.07409431, 0.30945253, 0.210862, 0.0972201]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.065*\"pedometer\" + 0.050*\"hair\" + 0.021*\"grow\" + 0.018*\"use\" + 0.017*\"step\" + 0.016*\"omron\" + 0.016*\"nail\" + 0.015*\"eye\" + 0.014*\"clip\" + 0.011*\"accurate\"\n",
      "INFO : topic #7 (0.097): 0.027*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.012*\"'s\" + 0.011*\"save\" + 0.011*\"brand\" + 0.011*\"review\" + 0.011*\"purchase\" + 0.010*\"order\" + 0.009*\"anxiety\"\n",
      "INFO : topic #3 (0.226): 0.027*\"day\" + 0.024*\"work\" + 0.022*\"product\" + 0.018*\"week\" + 0.017*\"use\" + 0.016*\"help\" + 0.016*\"pain\" + 0.015*\"start\" + 0.014*\"try\" + 0.012*\"feel\"\n",
      "INFO : topic #5 (0.309): 0.029*\"work\" + 0.025*\"feel\" + 0.021*\"help\" + 0.019*\"day\" + 0.016*\"good\" + 0.016*\"use\" + 0.015*\"try\" + 0.012*\"product\" + 0.012*\"like\" + 0.012*\"year\"\n",
      "INFO : topic #1 (0.484): 0.096*\"product\" + 0.064*\"great\" + 0.062*\"good\" + 0.025*\"use\" + 0.024*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.012*\"brand\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.071801, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #40000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19048476, 0.49383408, 0.15726653, 0.22927916, 0.074495286, 0.31400537, 0.2146374, 0.097939298]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.057*\"pedometer\" + 0.055*\"hair\" + 0.022*\"grow\" + 0.020*\"eye\" + 0.019*\"nail\" + 0.018*\"use\" + 0.015*\"step\" + 0.014*\"omron\" + 0.013*\"clip\" + 0.009*\"accurate\"\n",
      "INFO : topic #7 (0.098): 0.027*\"find\" + 0.021*\"amazon\" + 0.014*\"buy\" + 0.013*\"save\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.011*\"review\" + 0.011*\"purchase\" + 0.010*\"order\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.229): 0.027*\"day\" + 0.024*\"work\" + 0.022*\"product\" + 0.018*\"pain\" + 0.018*\"week\" + 0.017*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.014*\"try\" + 0.012*\"month\"\n",
      "INFO : topic #5 (0.314): 0.028*\"work\" + 0.025*\"feel\" + 0.022*\"help\" + 0.019*\"day\" + 0.016*\"good\" + 0.016*\"use\" + 0.015*\"try\" + 0.012*\"product\" + 0.012*\"year\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.494): 0.095*\"product\" + 0.063*\"great\" + 0.063*\"good\" + 0.026*\"use\" + 0.025*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.063741, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #45000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19395693, 0.50186241, 0.15885469, 0.2327594, 0.074596405, 0.32232866, 0.21781002, 0.09889821]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.058*\"hair\" + 0.051*\"pedometer\" + 0.023*\"grow\" + 0.019*\"eye\" + 0.019*\"nail\" + 0.018*\"use\" + 0.014*\"step\" + 0.013*\"omron\" + 0.011*\"clip\" + 0.009*\"seed\"\n",
      "INFO : topic #7 (0.099): 0.027*\"find\" + 0.022*\"amazon\" + 0.014*\"buy\" + 0.013*\"save\" + 0.012*\"'s\" + 0.011*\"brand\" + 0.011*\"review\" + 0.010*\"purchase\" + 0.010*\"order\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.233): 0.028*\"day\" + 0.024*\"work\" + 0.021*\"product\" + 0.018*\"pain\" + 0.018*\"week\" + 0.017*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.014*\"try\" + 0.013*\"month\"\n",
      "INFO : topic #5 (0.322): 0.029*\"work\" + 0.024*\"feel\" + 0.021*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"good\" + 0.016*\"try\" + 0.012*\"year\" + 0.012*\"product\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.502): 0.097*\"product\" + 0.064*\"great\" + 0.063*\"good\" + 0.027*\"use\" + 0.024*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.012*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.065376, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #50000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19635974, 0.51116478, 0.15888803, 0.23903279, 0.074596465, 0.32868823, 0.22094946, 0.099695571]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.060*\"hair\" + 0.047*\"pedometer\" + 0.025*\"grow\" + 0.021*\"nail\" + 0.019*\"eye\" + 0.017*\"use\" + 0.013*\"step\" + 0.012*\"seed\" + 0.012*\"omron\" + 0.011*\"clip\"\n",
      "INFO : topic #7 (0.100): 0.028*\"find\" + 0.023*\"amazon\" + 0.014*\"buy\" + 0.013*\"save\" + 0.012*\"'s\" + 0.011*\"brand\" + 0.011*\"review\" + 0.010*\"store\" + 0.010*\"purchase\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.239): 0.028*\"day\" + 0.025*\"work\" + 0.021*\"product\" + 0.017*\"week\" + 0.017*\"pain\" + 0.017*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.014*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.329): 0.029*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.019*\"day\" + 0.016*\"use\" + 0.016*\"good\" + 0.015*\"try\" + 0.013*\"year\" + 0.012*\"product\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.511): 0.098*\"product\" + 0.064*\"great\" + 0.064*\"good\" + 0.027*\"use\" + 0.025*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.012*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.069281, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #55000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20345385, 0.51724291, 0.15807015, 0.24451473, 0.074507013, 0.33597693, 0.22354338, 0.10073319]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.058*\"hair\" + 0.043*\"pedometer\" + 0.024*\"grow\" + 0.021*\"nail\" + 0.019*\"eye\" + 0.017*\"use\" + 0.012*\"seed\" + 0.012*\"step\" + 0.011*\"omron\" + 0.010*\"clip\"\n",
      "INFO : topic #7 (0.101): 0.028*\"find\" + 0.023*\"amazon\" + 0.014*\"buy\" + 0.014*\"save\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.011*\"review\" + 0.010*\"calm\" + 0.010*\"store\" + 0.010*\"purchase\"\n",
      "INFO : topic #3 (0.245): 0.028*\"day\" + 0.025*\"work\" + 0.021*\"product\" + 0.019*\"pain\" + 0.017*\"week\" + 0.017*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.014*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.336): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"good\" + 0.016*\"try\" + 0.013*\"year\" + 0.012*\"product\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.517): 0.098*\"product\" + 0.064*\"good\" + 0.064*\"great\" + 0.027*\"use\" + 0.025*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.012*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.077572, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #60000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20677924, 0.52960473, 0.15844204, 0.24506973, 0.074405007, 0.34530097, 0.22486874, 0.10195173]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.074): 0.056*\"hair\" + 0.039*\"pedometer\" + 0.024*\"grow\" + 0.020*\"nail\" + 0.019*\"eye\" + 0.017*\"use\" + 0.013*\"seed\" + 0.011*\"step\" + 0.010*\"omron\" + 0.009*\"clip\"\n",
      "INFO : topic #7 (0.102): 0.028*\"find\" + 0.024*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.012*\"brand\" + 0.010*\"review\" + 0.010*\"store\" + 0.010*\"calm\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.245): 0.028*\"day\" + 0.025*\"work\" + 0.021*\"product\" + 0.018*\"pain\" + 0.017*\"week\" + 0.017*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.014*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.345): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"good\" + 0.015*\"try\" + 0.013*\"year\" + 0.012*\"product\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.530): 0.100*\"product\" + 0.066*\"good\" + 0.064*\"great\" + 0.028*\"use\" + 0.025*\"price\" + 0.018*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.063813, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #65000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21005455, 0.53610253, 0.15842211, 0.25093415, 0.074871093, 0.35359147, 0.22731, 0.10361486]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.067*\"hair\" + 0.033*\"pedometer\" + 0.028*\"grow\" + 0.024*\"nail\" + 0.018*\"eye\" + 0.017*\"use\" + 0.011*\"seed\" + 0.009*\"step\" + 0.008*\"omron\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #7 (0.104): 0.028*\"find\" + 0.024*\"amazon\" + 0.014*\"calm\" + 0.013*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.011*\"brand\" + 0.011*\"anxiety\" + 0.010*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.251): 0.028*\"day\" + 0.025*\"work\" + 0.020*\"product\" + 0.019*\"pain\" + 0.017*\"week\" + 0.016*\"use\" + 0.016*\"help\" + 0.015*\"start\" + 0.015*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.354): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"good\" + 0.015*\"try\" + 0.013*\"year\" + 0.012*\"product\" + 0.012*\"like\"\n",
      "INFO : topic #1 (0.536): 0.101*\"product\" + 0.067*\"good\" + 0.064*\"great\" + 0.028*\"use\" + 0.025*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.059249, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21491428, 0.54755247, 0.15928577, 0.25650212, 0.075394034, 0.36133143, 0.22803937, 0.10488981]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.075): 0.067*\"hair\" + 0.027*\"pedometer\" + 0.026*\"grow\" + 0.022*\"nail\" + 0.018*\"eye\" + 0.017*\"use\" + 0.014*\"camera\" + 0.011*\"seed\" + 0.008*\"step\" + 0.008*\"love\"\n",
      "INFO : topic #7 (0.105): 0.028*\"find\" + 0.024*\"amazon\" + 0.014*\"calm\" + 0.013*\"buy\" + 0.013*\"save\" + 0.013*\"'s\" + 0.012*\"anxiety\" + 0.011*\"brand\" + 0.011*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.257): 0.028*\"day\" + 0.025*\"work\" + 0.020*\"product\" + 0.019*\"pain\" + 0.018*\"week\" + 0.016*\"help\" + 0.016*\"use\" + 0.016*\"start\" + 0.014*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.361): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.021*\"day\" + 0.016*\"use\" + 0.015*\"good\" + 0.015*\"try\" + 0.013*\"year\" + 0.012*\"like\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.548): 0.101*\"product\" + 0.067*\"good\" + 0.064*\"great\" + 0.028*\"use\" + 0.025*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.069201, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #75000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21841218, 0.5599131, 0.15964811, 0.25958467, 0.075543322, 0.37195048, 0.22965056, 0.10576849]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.067*\"hair\" + 0.026*\"grow\" + 0.024*\"pedometer\" + 0.022*\"nail\" + 0.018*\"eye\" + 0.016*\"use\" + 0.013*\"camera\" + 0.012*\"seed\" + 0.008*\"tablespoon\" + 0.008*\"love\"\n",
      "INFO : topic #7 (0.106): 0.028*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"calm\" + 0.013*\"'s\" + 0.011*\"brand\" + 0.011*\"anxiety\" + 0.011*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.260): 0.028*\"day\" + 0.025*\"work\" + 0.020*\"product\" + 0.019*\"pain\" + 0.018*\"week\" + 0.016*\"help\" + 0.016*\"start\" + 0.016*\"use\" + 0.015*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.372): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"year\" + 0.011*\"like\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.560): 0.099*\"product\" + 0.066*\"good\" + 0.063*\"great\" + 0.030*\"use\" + 0.024*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.062269, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #80000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21981154, 0.57407016, 0.16136134, 0.26438612, 0.075890876, 0.38131854, 0.23389223, 0.10618097]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.069*\"hair\" + 0.027*\"grow\" + 0.025*\"nail\" + 0.021*\"pedometer\" + 0.018*\"eye\" + 0.016*\"use\" + 0.012*\"seed\" + 0.011*\"camera\" + 0.009*\"tablespoon\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.106): 0.028*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"calm\" + 0.013*\"'s\" + 0.011*\"brand\" + 0.011*\"anxiety\" + 0.011*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.264): 0.028*\"day\" + 0.025*\"work\" + 0.020*\"pain\" + 0.020*\"product\" + 0.017*\"week\" + 0.016*\"use\" + 0.016*\"help\" + 0.016*\"start\" + 0.015*\"try\" + 0.014*\"month\"\n",
      "INFO : topic #5 (0.381): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"year\" + 0.011*\"like\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.574): 0.100*\"product\" + 0.067*\"good\" + 0.063*\"great\" + 0.031*\"use\" + 0.024*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.013*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.062463, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #85000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22690682, 0.58299094, 0.16167077, 0.26820225, 0.076323316, 0.38922495, 0.23711933, 0.10780349]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.076): 0.079*\"hair\" + 0.030*\"grow\" + 0.025*\"nail\" + 0.018*\"eye\" + 0.018*\"pedometer\" + 0.016*\"use\" + 0.011*\"seed\" + 0.009*\"camera\" + 0.008*\"thick\" + 0.008*\"tablespoon\"\n",
      "INFO : topic #7 (0.108): 0.029*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.012*\"calm\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.011*\"anxiety\" + 0.011*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.268): 0.028*\"day\" + 0.024*\"work\" + 0.020*\"pain\" + 0.019*\"product\" + 0.017*\"week\" + 0.016*\"help\" + 0.016*\"start\" + 0.016*\"use\" + 0.015*\"month\" + 0.015*\"try\"\n",
      "INFO : topic #5 (0.389): 0.030*\"work\" + 0.024*\"feel\" + 0.022*\"help\" + 0.020*\"day\" + 0.016*\"use\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"year\" + 0.011*\"like\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.583): 0.101*\"product\" + 0.068*\"good\" + 0.063*\"great\" + 0.031*\"use\" + 0.025*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.014*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.064779, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #90000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23277269, 0.59487331, 0.16165945, 0.27154145, 0.07690943, 0.39483175, 0.24067129, 0.10913749]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.077): 0.077*\"hair\" + 0.029*\"grow\" + 0.029*\"eye\" + 0.028*\"nail\" + 0.016*\"lutein\" + 0.015*\"use\" + 0.015*\"pedometer\" + 0.010*\"seed\" + 0.008*\"thin\" + 0.008*\"thick\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #7 (0.109): 0.029*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.012*\"calm\" + 0.010*\"anxiety\" + 0.010*\"review\" + 0.010*\"coq10\"\n",
      "INFO : topic #3 (0.272): 0.028*\"day\" + 0.024*\"work\" + 0.021*\"pain\" + 0.019*\"product\" + 0.018*\"week\" + 0.016*\"help\" + 0.016*\"start\" + 0.015*\"use\" + 0.015*\"month\" + 0.014*\"try\"\n",
      "INFO : topic #5 (0.395): 0.030*\"work\" + 0.024*\"feel\" + 0.023*\"help\" + 0.021*\"day\" + 0.016*\"use\" + 0.016*\"try\" + 0.015*\"good\" + 0.013*\"year\" + 0.011*\"like\" + 0.010*\"product\"\n",
      "INFO : topic #1 (0.595): 0.100*\"product\" + 0.068*\"good\" + 0.063*\"great\" + 0.031*\"use\" + 0.025*\"price\" + 0.019*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.014*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.063136, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #95000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23658279, 0.61346495, 0.1610799, 0.27544683, 0.076959476, 0.40227526, 0.24655725, 0.11080863]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.077): 0.074*\"hair\" + 0.030*\"eye\" + 0.029*\"grow\" + 0.026*\"nail\" + 0.015*\"lutein\" + 0.015*\"use\" + 0.013*\"pedometer\" + 0.012*\"seed\" + 0.009*\"tablespoon\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.111): 0.028*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"brand\" + 0.012*\"'s\" + 0.011*\"calm\" + 0.010*\"review\" + 0.010*\"anxiety\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.275): 0.029*\"day\" + 0.024*\"work\" + 0.021*\"pain\" + 0.019*\"product\" + 0.017*\"week\" + 0.016*\"help\" + 0.016*\"start\" + 0.015*\"use\" + 0.015*\"month\" + 0.014*\"try\"\n",
      "INFO : topic #5 (0.402): 0.030*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.021*\"day\" + 0.016*\"use\" + 0.015*\"try\" + 0.015*\"good\" + 0.012*\"year\" + 0.011*\"like\" + 0.010*\"start\"\n",
      "INFO : topic #1 (0.613): 0.099*\"product\" + 0.069*\"good\" + 0.064*\"great\" + 0.031*\"use\" + 0.025*\"price\" + 0.020*\"recommend\" + 0.017*\"buy\" + 0.015*\"love\" + 0.014*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.063754, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #100000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23834482, 0.62805915, 0.16255957, 0.27650461, 0.077719286, 0.40743557, 0.25116101, 0.11210718]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.073*\"hair\" + 0.037*\"eye\" + 0.028*\"grow\" + 0.024*\"nail\" + 0.018*\"seed\" + 0.015*\"use\" + 0.014*\"lutein\" + 0.011*\"pedometer\" + 0.009*\"tablespoon\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.112): 0.029*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.012*\"'s\" + 0.010*\"coq10\" + 0.010*\"calm\" + 0.010*\"review\" + 0.010*\"store\"\n",
      "INFO : topic #3 (0.277): 0.028*\"day\" + 0.024*\"work\" + 0.021*\"pain\" + 0.018*\"product\" + 0.017*\"week\" + 0.016*\"help\" + 0.016*\"start\" + 0.015*\"use\" + 0.014*\"month\" + 0.014*\"try\"\n",
      "INFO : topic #5 (0.407): 0.030*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.021*\"day\" + 0.016*\"use\" + 0.015*\"try\" + 0.015*\"good\" + 0.012*\"year\" + 0.011*\"like\" + 0.010*\"time\"\n",
      "INFO : topic #1 (0.628): 0.098*\"product\" + 0.068*\"good\" + 0.064*\"great\" + 0.032*\"use\" + 0.025*\"price\" + 0.020*\"recommend\" + 0.017*\"buy\" + 0.016*\"love\" + 0.014*\"brand\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.059565, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24121237, 0.6365431, 0.1637601, 0.28094926, 0.077855855, 0.41938952, 0.25313273, 0.11344779]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.074*\"hair\" + 0.036*\"eye\" + 0.028*\"grow\" + 0.027*\"nail\" + 0.018*\"seed\" + 0.014*\"use\" + 0.013*\"lutein\" + 0.010*\"pedometer\" + 0.009*\"tablespoon\" + 0.008*\"long\"\n",
      "INFO : topic #7 (0.113): 0.029*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.011*\"calm\" + 0.010*\"anxiety\" + 0.010*\"review\" + 0.009*\"coq10\"\n",
      "INFO : topic #3 (0.281): 0.028*\"day\" + 0.024*\"work\" + 0.021*\"pain\" + 0.018*\"product\" + 0.017*\"week\" + 0.016*\"start\" + 0.016*\"help\" + 0.015*\"use\" + 0.015*\"month\" + 0.014*\"try\"\n",
      "INFO : topic #5 (0.419): 0.031*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.021*\"day\" + 0.016*\"use\" + 0.015*\"try\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.010*\"time\"\n",
      "INFO : topic #1 (0.637): 0.099*\"product\" + 0.068*\"good\" + 0.065*\"great\" + 0.032*\"use\" + 0.025*\"price\" + 0.020*\"recommend\" + 0.018*\"buy\" + 0.016*\"love\" + 0.014*\"brand\" + 0.011*\"excellent\"\n",
      "INFO : topic diff=0.056588, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #110000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24563441, 0.6538049, 0.16339406, 0.28334457, 0.07810349, 0.42475951, 0.26197648, 0.1151745]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.074*\"hair\" + 0.039*\"eye\" + 0.028*\"grow\" + 0.027*\"nail\" + 0.017*\"seed\" + 0.014*\"use\" + 0.014*\"lutein\" + 0.009*\"tablespoon\" + 0.009*\"pedometer\" + 0.008*\"thin\"\n",
      "INFO : topic #7 (0.115): 0.030*\"find\" + 0.026*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.012*\"'s\" + 0.012*\"brand\" + 0.011*\"anxiety\" + 0.010*\"calm\" + 0.010*\"store\" + 0.010*\"review\"\n",
      "INFO : topic #3 (0.283): 0.028*\"day\" + 0.024*\"work\" + 0.021*\"pain\" + 0.017*\"product\" + 0.017*\"week\" + 0.016*\"start\" + 0.016*\"help\" + 0.015*\"use\" + 0.015*\"month\" + 0.015*\"try\"\n",
      "INFO : topic #5 (0.425): 0.031*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.022*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.654): 0.097*\"product\" + 0.068*\"good\" + 0.064*\"great\" + 0.031*\"use\" + 0.026*\"price\" + 0.019*\"recommend\" + 0.018*\"buy\" + 0.016*\"love\" + 0.015*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.065535, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #115000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24817598, 0.67212379, 0.16455255, 0.28455767, 0.07836318, 0.4277539, 0.27141064, 0.1161862]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.078): 0.072*\"hair\" + 0.039*\"eye\" + 0.028*\"nail\" + 0.028*\"grow\" + 0.016*\"seed\" + 0.014*\"use\" + 0.013*\"lutein\" + 0.010*\"tablespoon\" + 0.008*\"thin\" + 0.008*\"pedometer\"\n",
      "INFO : topic #7 (0.116): 0.030*\"find\" + 0.027*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.012*\"brand\" + 0.011*\"anxiety\" + 0.010*\"store\" + 0.010*\"review\" + 0.009*\"calm\"\n",
      "INFO : topic #3 (0.285): 0.028*\"day\" + 0.024*\"work\" + 0.020*\"pain\" + 0.017*\"week\" + 0.017*\"product\" + 0.016*\"start\" + 0.016*\"help\" + 0.015*\"month\" + 0.015*\"use\" + 0.015*\"try\"\n",
      "INFO : topic #5 (0.428): 0.031*\"work\" + 0.024*\"feel\" + 0.023*\"help\" + 0.022*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.672): 0.098*\"product\" + 0.069*\"good\" + 0.064*\"great\" + 0.031*\"use\" + 0.026*\"price\" + 0.020*\"recommend\" + 0.018*\"buy\" + 0.016*\"love\" + 0.015*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.060940, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #120000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25084567, 0.68494505, 0.16520795, 0.2876482, 0.07862515, 0.43526256, 0.27890348, 0.11743549]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.080*\"hair\" + 0.036*\"eye\" + 0.036*\"nail\" + 0.032*\"grow\" + 0.015*\"seed\" + 0.013*\"use\" + 0.011*\"lutein\" + 0.009*\"strong\" + 0.009*\"tablespoon\" + 0.008*\"thick\"\n",
      "INFO : topic #7 (0.117): 0.030*\"find\" + 0.027*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.012*\"brand\" + 0.010*\"store\" + 0.010*\"anxiety\" + 0.010*\"review\" + 0.009*\"order\"\n",
      "INFO : topic #3 (0.288): 0.028*\"day\" + 0.024*\"work\" + 0.020*\"pain\" + 0.018*\"week\" + 0.017*\"product\" + 0.016*\"start\" + 0.016*\"help\" + 0.015*\"month\" + 0.015*\"try\" + 0.015*\"use\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.435): 0.031*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.022*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.685): 0.098*\"product\" + 0.070*\"good\" + 0.064*\"great\" + 0.031*\"use\" + 0.026*\"price\" + 0.020*\"recommend\" + 0.018*\"buy\" + 0.017*\"love\" + 0.015*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.053086, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #125000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25377238, 0.70561898, 0.1660351, 0.29031247, 0.078786157, 0.43509942, 0.29157323, 0.1185807]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.081*\"hair\" + 0.038*\"nail\" + 0.037*\"eye\" + 0.032*\"grow\" + 0.015*\"seed\" + 0.013*\"use\" + 0.011*\"lutein\" + 0.010*\"strong\" + 0.008*\"thick\" + 0.008*\"thin\"\n",
      "INFO : topic #7 (0.119): 0.031*\"find\" + 0.026*\"amazon\" + 0.014*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.012*\"brand\" + 0.011*\"anxiety\" + 0.011*\"store\" + 0.009*\"review\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.292): 0.038*\"taste\" + 0.023*\"like\" + 0.019*\"easy\" + 0.018*\"vitamin\" + 0.018*\"pill\" + 0.015*\"fish_oil\" + 0.013*\"good\" + 0.013*\"swallow\" + 0.012*\"supplement\" + 0.010*\"water\"\n",
      "INFO : topic #5 (0.435): 0.031*\"work\" + 0.025*\"feel\" + 0.023*\"help\" + 0.022*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.706): 0.098*\"product\" + 0.071*\"good\" + 0.064*\"great\" + 0.030*\"use\" + 0.026*\"price\" + 0.021*\"recommend\" + 0.018*\"buy\" + 0.017*\"love\" + 0.015*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.060622, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #130000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2547515, 0.72488111, 0.16548015, 0.29246208, 0.078818507, 0.4437795, 0.30144545, 0.11983899]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.079): 0.077*\"hair\" + 0.037*\"nail\" + 0.036*\"eye\" + 0.034*\"grow\" + 0.014*\"seed\" + 0.013*\"use\" + 0.011*\"lutein\" + 0.010*\"strong\" + 0.008*\"tablespoon\" + 0.008*\"thin\"\n",
      "INFO : topic #7 (0.120): 0.030*\"find\" + 0.027*\"amazon\" + 0.015*\"save\" + 0.013*\"buy\" + 0.013*\"'s\" + 0.012*\"brand\" + 0.011*\"store\" + 0.010*\"anxiety\" + 0.009*\"review\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.301): 0.039*\"taste\" + 0.023*\"like\" + 0.020*\"easy\" + 0.018*\"vitamin\" + 0.017*\"pill\" + 0.015*\"fish_oil\" + 0.013*\"good\" + 0.013*\"swallow\" + 0.012*\"supplement\" + 0.011*\"water\"\n",
      "INFO : topic #5 (0.444): 0.031*\"work\" + 0.026*\"feel\" + 0.023*\"help\" + 0.023*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.010*\"start\"\n",
      "INFO : topic #1 (0.725): 0.099*\"product\" + 0.071*\"good\" + 0.065*\"great\" + 0.030*\"use\" + 0.026*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.017*\"love\" + 0.015*\"brand\" + 0.012*\"find\"\n",
      "INFO : topic diff=0.058985, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #135000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 4999/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25644982, 0.74342823, 0.16920644, 0.29487729, 0.080333725, 0.44828498, 0.30841285, 0.12046464]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.080): 0.080*\"hair\" + 0.034*\"eye\" + 0.034*\"nail\" + 0.031*\"grow\" + 0.018*\"seed\" + 0.018*\"coconut_oil\" + 0.013*\"use\" + 0.011*\"tablespoon\" + 0.010*\"flax\" + 0.010*\"strong\"\n",
      "INFO : topic #7 (0.120): 0.030*\"find\" + 0.027*\"amazon\" + 0.015*\"save\" + 0.014*\"'s\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.010*\"store\" + 0.010*\"review\" + 0.009*\"order\" + 0.009*\"mg\"\n",
      "INFO : topic #6 (0.308): 0.042*\"taste\" + 0.023*\"like\" + 0.019*\"easy\" + 0.017*\"fish_oil\" + 0.017*\"pill\" + 0.017*\"vitamin\" + 0.013*\"good\" + 0.013*\"swallow\" + 0.011*\"supplement\" + 0.010*\"flavor\"\n",
      "INFO : topic #5 (0.448): 0.031*\"work\" + 0.026*\"feel\" + 0.023*\"help\" + 0.023*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.014*\"good\" + 0.013*\"year\" + 0.012*\"like\" + 0.010*\"time\"\n",
      "INFO : topic #1 (0.743): 0.098*\"product\" + 0.071*\"good\" + 0.065*\"great\" + 0.030*\"use\" + 0.026*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.017*\"love\" + 0.015*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.066026, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26054516, 0.75907165, 0.17030115, 0.2995927, 0.08122582, 0.46483791, 0.31160223, 0.12241386]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.081): 0.076*\"hair\" + 0.031*\"nail\" + 0.031*\"eye\" + 0.029*\"grow\" + 0.021*\"seed\" + 0.020*\"gaia\" + 0.016*\"coconut_oil\" + 0.013*\"use\" + 0.011*\"tablespoon\" + 0.010*\"flax\"\n",
      "INFO : topic #7 (0.122): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.014*\"'s\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.010*\"anxiety\" + 0.010*\"store\" + 0.010*\"review\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.312): 0.042*\"taste\" + 0.023*\"like\" + 0.020*\"easy\" + 0.017*\"pill\" + 0.017*\"vitamin\" + 0.016*\"fish_oil\" + 0.013*\"good\" + 0.013*\"swallow\" + 0.011*\"supplement\" + 0.011*\"water\"\n",
      "INFO : topic #5 (0.465): 0.031*\"work\" + 0.027*\"feel\" + 0.023*\"help\" + 0.023*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.759): 0.099*\"product\" + 0.071*\"good\" + 0.065*\"great\" + 0.030*\"use\" + 0.025*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.015*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.060009, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #145000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26419836, 0.77533185, 0.17212649, 0.30121925, 0.08148019, 0.46977326, 0.3222374, 0.12326286]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.081): 0.072*\"hair\" + 0.031*\"eye\" + 0.031*\"nail\" + 0.029*\"grow\" + 0.019*\"seed\" + 0.018*\"gaia\" + 0.017*\"coconut_oil\" + 0.012*\"use\" + 0.010*\"tablespoon\" + 0.009*\"strong\"\n",
      "INFO : topic #7 (0.123): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.015*\"'s\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.010*\"store\" + 0.010*\"review\" + 0.010*\"anxiety\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.322): 0.042*\"taste\" + 0.023*\"like\" + 0.020*\"easy\" + 0.017*\"vitamin\" + 0.016*\"pill\" + 0.015*\"fish_oil\" + 0.013*\"good\" + 0.012*\"swallow\" + 0.011*\"flavor\" + 0.011*\"supplement\"\n",
      "INFO : topic #5 (0.470): 0.030*\"work\" + 0.027*\"feel\" + 0.023*\"day\" + 0.023*\"help\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.010*\"time\"\n",
      "INFO : topic #1 (0.775): 0.099*\"product\" + 0.071*\"good\" + 0.065*\"great\" + 0.030*\"use\" + 0.025*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.015*\"brand\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.058693, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #150000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26771489, 0.79034334, 0.17294762, 0.30666023, 0.081855111, 0.47751039, 0.32864368, 0.12468366]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.082): 0.069*\"hair\" + 0.034*\"eye\" + 0.031*\"nail\" + 0.030*\"grow\" + 0.018*\"seed\" + 0.016*\"gaia\" + 0.015*\"coconut_oil\" + 0.012*\"use\" + 0.011*\"tablespoon\" + 0.010*\"strong\"\n",
      "INFO : topic #7 (0.125): 0.030*\"find\" + 0.026*\"amazon\" + 0.016*\"save\" + 0.014*\"'s\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.010*\"store\" + 0.010*\"review\" + 0.009*\"order\" + 0.009*\"anxiety\"\n",
      "INFO : topic #6 (0.329): 0.042*\"taste\" + 0.023*\"like\" + 0.021*\"easy\" + 0.017*\"vitamin\" + 0.016*\"pill\" + 0.015*\"fish_oil\" + 0.012*\"good\" + 0.012*\"swallow\" + 0.011*\"flavor\" + 0.011*\"supplement\"\n",
      "INFO : topic #5 (0.478): 0.031*\"work\" + 0.027*\"feel\" + 0.023*\"day\" + 0.023*\"help\" + 0.016*\"try\" + 0.015*\"use\" + 0.013*\"good\" + 0.012*\"year\" + 0.012*\"like\" + 0.011*\"time\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.790): 0.100*\"product\" + 0.070*\"good\" + 0.065*\"great\" + 0.031*\"use\" + 0.024*\"price\" + 0.022*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.015*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.051735, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #155000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27095923, 0.80621457, 0.17409664, 0.31141302, 0.082571074, 0.48273855, 0.33417466, 0.12590799]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.083): 0.078*\"hair\" + 0.037*\"eye\" + 0.034*\"grow\" + 0.033*\"nail\" + 0.017*\"seed\" + 0.013*\"gaia\" + 0.013*\"coconut_oil\" + 0.012*\"use\" + 0.010*\"strong\" + 0.010*\"tablespoon\"\n",
      "INFO : topic #7 (0.126): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.013*\"buy\" + 0.012*\"brand\" + 0.011*\"store\" + 0.010*\"review\" + 0.010*\"order\" + 0.009*\"coq10\"\n",
      "INFO : topic #6 (0.334): 0.042*\"taste\" + 0.024*\"like\" + 0.021*\"easy\" + 0.017*\"pill\" + 0.016*\"vitamin\" + 0.014*\"fish_oil\" + 0.012*\"swallow\" + 0.012*\"good\" + 0.011*\"calcium\" + 0.011*\"flavor\"\n",
      "INFO : topic #5 (0.483): 0.031*\"work\" + 0.028*\"feel\" + 0.023*\"help\" + 0.023*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.013*\"good\" + 0.012*\"like\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.806): 0.100*\"product\" + 0.070*\"good\" + 0.065*\"great\" + 0.031*\"use\" + 0.024*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.055912, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #160000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27285704, 0.81928849, 0.17569526, 0.31461385, 0.085247688, 0.48641121, 0.33951309, 0.12639232]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.085): 0.117*\"hair\" + 0.054*\"nail\" + 0.046*\"grow\" + 0.028*\"eye\" + 0.015*\"strong\" + 0.012*\"seed\" + 0.012*\"use\" + 0.011*\"coconut_oil\" + 0.010*\"month\" + 0.010*\"thick\"\n",
      "INFO : topic #7 (0.126): 0.031*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.013*\"buy\" + 0.011*\"brand\" + 0.010*\"store\" + 0.010*\"review\" + 0.010*\"order\" + 0.009*\"coq10\"\n",
      "INFO : topic #6 (0.340): 0.040*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.017*\"vitamin\" + 0.014*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"calcium\" + 0.012*\"good\" + 0.011*\"supplement\"\n",
      "INFO : topic #5 (0.486): 0.032*\"work\" + 0.028*\"feel\" + 0.024*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.012*\"good\" + 0.012*\"like\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.819): 0.101*\"product\" + 0.070*\"good\" + 0.064*\"great\" + 0.031*\"use\" + 0.024*\"price\" + 0.021*\"recommend\" + 0.019*\"buy\" + 0.019*\"love\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.080800, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #165000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27851871, 0.83722311, 0.17793125, 0.31906399, 0.085544765, 0.49269214, 0.34845808, 0.12735885]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.113*\"hair\" + 0.052*\"nail\" + 0.044*\"grow\" + 0.031*\"eye\" + 0.014*\"strong\" + 0.012*\"nutrigold\" + 0.012*\"use\" + 0.011*\"seed\" + 0.010*\"month\" + 0.010*\"notice\"\n",
      "INFO : topic #7 (0.127): 0.031*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.013*\"buy\" + 0.011*\"brand\" + 0.011*\"review\" + 0.010*\"store\" + 0.010*\"order\" + 0.009*\"bottle\"\n",
      "INFO : topic #6 (0.348): 0.039*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.017*\"vitamin\" + 0.015*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"good\" + 0.011*\"supplement\" + 0.011*\"calcium\"\n",
      "INFO : topic #5 (0.493): 0.032*\"work\" + 0.028*\"feel\" + 0.024*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.012*\"like\" + 0.012*\"good\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.837): 0.102*\"product\" + 0.070*\"good\" + 0.064*\"great\" + 0.030*\"use\" + 0.024*\"price\" + 0.022*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.059909, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #170000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28394049, 0.85083175, 0.17921761, 0.32557368, 0.086347505, 0.50069875, 0.35449904, 0.12870304]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.086): 0.114*\"hair\" + 0.054*\"nail\" + 0.045*\"grow\" + 0.028*\"eye\" + 0.018*\"nutrigold\" + 0.015*\"strong\" + 0.011*\"use\" + 0.011*\"seed\" + 0.011*\"month\" + 0.010*\"notice\"\n",
      "INFO : topic #7 (0.129): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.012*\"buy\" + 0.011*\"brand\" + 0.011*\"coq10\" + 0.010*\"review\" + 0.010*\"store\" + 0.010*\"order\"\n",
      "INFO : topic #6 (0.354): 0.040*\"taste\" + 0.023*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.018*\"vitamin\" + 0.015*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"good\" + 0.012*\"supplement\" + 0.011*\"capsule\"\n",
      "INFO : topic #5 (0.501): 0.032*\"work\" + 0.029*\"feel\" + 0.025*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.012*\"like\" + 0.012*\"good\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.851): 0.103*\"product\" + 0.070*\"good\" + 0.065*\"great\" + 0.031*\"use\" + 0.024*\"price\" + 0.022*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.014*\"brand\" + 0.012*\"order\"\n",
      "INFO : topic diff=0.052977, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28716108, 0.87021387, 0.18228263, 0.32993519, 0.086903721, 0.51388305, 0.35969684, 0.12956402]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.087): 0.106*\"hair\" + 0.052*\"nail\" + 0.043*\"grow\" + 0.028*\"eye\" + 0.023*\"nutrigold\" + 0.016*\"strong\" + 0.011*\"use\" + 0.011*\"seed\" + 0.010*\"month\" + 0.010*\"thin\"\n",
      "INFO : topic #7 (0.130): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.012*\"buy\" + 0.011*\"review\" + 0.011*\"brand\" + 0.010*\"anxiety\" + 0.010*\"store\" + 0.010*\"coq10\"\n",
      "INFO : topic #6 (0.360): 0.040*\"taste\" + 0.023*\"like\" + 0.023*\"easy\" + 0.018*\"pill\" + 0.018*\"vitamin\" + 0.015*\"fish_oil\" + 0.014*\"swallow\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"capsule\"\n",
      "INFO : topic #5 (0.514): 0.032*\"work\" + 0.028*\"feel\" + 0.025*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.012*\"like\" + 0.012*\"good\" + 0.012*\"year\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.870): 0.104*\"product\" + 0.070*\"good\" + 0.065*\"great\" + 0.030*\"use\" + 0.023*\"price\" + 0.022*\"recommend\" + 0.019*\"buy\" + 0.018*\"love\" + 0.014*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.058104, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #180000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2905601, 0.8894524, 0.18330941, 0.33516556, 0.087500639, 0.52399868, 0.36937943, 0.13071667]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.088): 0.102*\"hair\" + 0.051*\"nail\" + 0.042*\"grow\" + 0.027*\"eye\" + 0.024*\"nutrigold\" + 0.017*\"strong\" + 0.012*\"seed\" + 0.011*\"use\" + 0.010*\"month\" + 0.010*\"notice\"\n",
      "INFO : topic #7 (0.131): 0.030*\"find\" + 0.026*\"amazon\" + 0.015*\"save\" + 0.013*\"'s\" + 0.013*\"coq10\" + 0.012*\"buy\" + 0.011*\"review\" + 0.011*\"anxiety\" + 0.010*\"brand\" + 0.010*\"store\"\n",
      "INFO : topic #6 (0.369): 0.041*\"taste\" + 0.024*\"like\" + 0.023*\"easy\" + 0.017*\"pill\" + 0.017*\"vitamin\" + 0.013*\"swallow\" + 0.013*\"fish_oil\" + 0.012*\"good\" + 0.011*\"supplement\" + 0.010*\"flavor\"\n",
      "INFO : topic #5 (0.524): 0.032*\"work\" + 0.028*\"feel\" + 0.025*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.015*\"use\" + 0.012*\"like\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"energy\"\n",
      "INFO : topic #1 (0.889): 0.105*\"product\" + 0.070*\"good\" + 0.066*\"great\" + 0.031*\"use\" + 0.023*\"price\" + 0.022*\"recommend\" + 0.019*\"buy\" + 0.019*\"love\" + 0.013*\"brand\" + 0.013*\"order\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.055286, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #185000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29461232, 0.90645528, 0.1845849, 0.34457445, 0.088353604, 0.53386462, 0.37577271, 0.13206159]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.088): 0.098*\"hair\" + 0.048*\"nail\" + 0.040*\"grow\" + 0.027*\"nutrigold\" + 0.027*\"eye\" + 0.016*\"strong\" + 0.012*\"seed\" + 0.010*\"use\" + 0.010*\"notice\" + 0.010*\"month\"\n",
      "INFO : topic #7 (0.132): 0.029*\"find\" + 0.025*\"amazon\" + 0.014*\"save\" + 0.014*\"coq10\" + 0.013*\"'s\" + 0.012*\"buy\" + 0.011*\"review\" + 0.011*\"anxiety\" + 0.010*\"brand\" + 0.010*\"store\"\n",
      "INFO : topic #6 (0.376): 0.041*\"taste\" + 0.024*\"like\" + 0.022*\"easy\" + 0.017*\"pill\" + 0.017*\"vitamin\" + 0.013*\"fish_oil\" + 0.013*\"swallow\" + 0.011*\"good\" + 0.011*\"supplement\" + 0.011*\"flavor\"\n",
      "INFO : topic #5 (0.534): 0.033*\"work\" + 0.029*\"feel\" + 0.025*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"like\" + 0.012*\"energy\" + 0.011*\"good\" + 0.011*\"sleep\"\n",
      "INFO : topic #1 (0.906): 0.105*\"product\" + 0.070*\"good\" + 0.066*\"great\" + 0.030*\"use\" + 0.023*\"price\" + 0.022*\"recommend\" + 0.019*\"love\" + 0.019*\"buy\" + 0.013*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.058292, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #190000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29700747, 0.92316777, 0.18581189, 0.35546407, 0.089269564, 0.5448966, 0.38286123, 0.13295636]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.089): 0.102*\"hair\" + 0.048*\"nail\" + 0.040*\"grow\" + 0.026*\"eye\" + 0.025*\"nutrigold\" + 0.017*\"strong\" + 0.011*\"seed\" + 0.010*\"notice\" + 0.010*\"use\" + 0.010*\"month\"\n",
      "INFO : topic #7 (0.133): 0.029*\"find\" + 0.025*\"amazon\" + 0.015*\"coq10\" + 0.013*\"save\" + 0.013*\"'s\" + 0.012*\"review\" + 0.011*\"buy\" + 0.011*\"anxiety\" + 0.010*\"brand\" + 0.010*\"store\"\n",
      "INFO : topic #6 (0.383): 0.042*\"taste\" + 0.024*\"like\" + 0.022*\"easy\" + 0.017*\"pill\" + 0.016*\"vitamin\" + 0.013*\"fish_oil\" + 0.013*\"swallow\" + 0.011*\"flavor\" + 0.011*\"supplement\" + 0.011*\"good\"\n",
      "INFO : topic #5 (0.545): 0.033*\"work\" + 0.030*\"feel\" + 0.024*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"like\" + 0.013*\"energy\" + 0.012*\"good\" + 0.011*\"year\"\n",
      "INFO : topic #1 (0.923): 0.106*\"product\" + 0.070*\"good\" + 0.067*\"great\" + 0.030*\"use\" + 0.022*\"price\" + 0.022*\"recommend\" + 0.019*\"love\" + 0.018*\"buy\" + 0.013*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.052077, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #195000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30181101, 0.9381004, 0.18726379, 0.36577076, 0.089802191, 0.55462188, 0.38904703, 0.13434193]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.090): 0.100*\"hair\" + 0.046*\"nail\" + 0.039*\"grow\" + 0.028*\"eye\" + 0.026*\"nutrigold\" + 0.017*\"strong\" + 0.011*\"seed\" + 0.011*\"notice\" + 0.010*\"month\" + 0.010*\"use\"\n",
      "INFO : topic #7 (0.134): 0.029*\"find\" + 0.024*\"amazon\" + 0.013*\"'s\" + 0.013*\"coq10\" + 0.012*\"save\" + 0.012*\"review\" + 0.011*\"buy\" + 0.010*\"anxiety\" + 0.010*\"brand\" + 0.010*\"order\"\n",
      "INFO : topic #6 (0.389): 0.041*\"taste\" + 0.025*\"like\" + 0.022*\"easy\" + 0.017*\"pill\" + 0.017*\"vitamin\" + 0.014*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"supplement\" + 0.012*\"flavor\" + 0.011*\"good\"\n",
      "INFO : topic #5 (0.555): 0.033*\"work\" + 0.031*\"feel\" + 0.024*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"like\" + 0.013*\"energy\" + 0.011*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.938): 0.106*\"product\" + 0.070*\"good\" + 0.067*\"great\" + 0.030*\"use\" + 0.022*\"price\" + 0.021*\"recommend\" + 0.019*\"love\" + 0.018*\"buy\" + 0.013*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.058365, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #200000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3069199, 0.96140087, 0.18871872, 0.37428856, 0.090345331, 0.56124038, 0.39871612, 0.13575071]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.090): 0.098*\"hair\" + 0.046*\"nail\" + 0.039*\"grow\" + 0.028*\"eye\" + 0.022*\"nutrigold\" + 0.016*\"strong\" + 0.012*\"seed\" + 0.011*\"notice\" + 0.010*\"month\" + 0.010*\"use\"\n",
      "INFO : topic #7 (0.136): 0.028*\"find\" + 0.023*\"amazon\" + 0.013*\"'s\" + 0.012*\"review\" + 0.012*\"save\" + 0.012*\"coq10\" + 0.011*\"anxiety\" + 0.010*\"buy\" + 0.010*\"brand\" + 0.010*\"order\"\n",
      "INFO : topic #6 (0.399): 0.041*\"taste\" + 0.024*\"like\" + 0.022*\"easy\" + 0.018*\"pill\" + 0.018*\"fish_oil\" + 0.016*\"vitamin\" + 0.013*\"swallow\" + 0.012*\"supplement\" + 0.011*\"flavor\" + 0.011*\"omega\"\n",
      "INFO : topic #5 (0.561): 0.033*\"work\" + 0.032*\"feel\" + 0.024*\"help\" + 0.024*\"day\" + 0.016*\"try\" + 0.014*\"use\" + 0.013*\"like\" + 0.013*\"energy\" + 0.011*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.961): 0.106*\"product\" + 0.070*\"good\" + 0.066*\"great\" + 0.030*\"use\" + 0.022*\"price\" + 0.022*\"recommend\" + 0.018*\"love\" + 0.018*\"buy\" + 0.013*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.054640, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #205000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30843642, 0.97915429, 0.19393295, 0.38514391, 0.091252968, 0.56634849, 0.39974934, 0.13670141]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.091): 0.093*\"hair\" + 0.045*\"nail\" + 0.036*\"grow\" + 0.029*\"eye\" + 0.019*\"nutrigold\" + 0.016*\"strong\" + 0.012*\"naturewise\" + 0.011*\"notice\" + 0.010*\"month\" + 0.010*\"seed\"\n",
      "INFO : topic #7 (0.137): 0.028*\"find\" + 0.022*\"amazon\" + 0.013*\"'s\" + 0.013*\"review\" + 0.012*\"coq10\" + 0.012*\"save\" + 0.011*\"anxiety\" + 0.010*\"buy\" + 0.010*\"brand\" + 0.010*\"order\"\n",
      "INFO : topic #6 (0.400): 0.040*\"taste\" + 0.025*\"like\" + 0.023*\"easy\" + 0.018*\"pill\" + 0.018*\"fish_oil\" + 0.017*\"vitamin\" + 0.014*\"swallow\" + 0.012*\"supplement\" + 0.011*\"omega\" + 0.011*\"capsule\"\n",
      "INFO : topic #5 (0.566): 0.034*\"feel\" + 0.032*\"work\" + 0.024*\"day\" + 0.024*\"help\" + 0.016*\"try\" + 0.014*\"energy\" + 0.014*\"like\" + 0.014*\"use\" + 0.011*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.979): 0.108*\"product\" + 0.069*\"good\" + 0.066*\"great\" + 0.030*\"use\" + 0.022*\"recommend\" + 0.021*\"price\" + 0.019*\"love\" + 0.018*\"buy\" + 0.013*\"brand\" + 0.013*\"order\"\n",
      "INFO : topic diff=0.064677, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n",
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31406251, 1.0082567, 0.19677734, 0.39564696, 0.092413321, 0.57069612, 0.41424417, 0.13759716]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.092): 0.086*\"hair\" + 0.042*\"nail\" + 0.034*\"naturewise\" + 0.033*\"grow\" + 0.027*\"eye\" + 0.016*\"strong\" + 0.016*\"nutrigold\" + 0.012*\"notice\" + 0.010*\"month\" + 0.010*\"seed\"\n",
      "INFO : topic #7 (0.138): 0.027*\"find\" + 0.021*\"amazon\" + 0.014*\"review\" + 0.013*\"'s\" + 0.012*\"save\" + 0.011*\"coq10\" + 0.011*\"anxiety\" + 0.010*\"brand\" + 0.010*\"buy\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.414): 0.038*\"taste\" + 0.024*\"like\" + 0.024*\"easy\" + 0.018*\"pill\" + 0.018*\"fish_oil\" + 0.018*\"vitamin\" + 0.013*\"swallow\" + 0.013*\"supplement\" + 0.011*\"capsule\" + 0.011*\"omega\"\n",
      "INFO : topic #5 (0.571): 0.036*\"feel\" + 0.031*\"work\" + 0.024*\"help\" + 0.024*\"day\" + 0.017*\"try\" + 0.015*\"energy\" + 0.014*\"like\" + 0.013*\"use\" + 0.011*\"probiotic\" + 0.011*\"good\"\n",
      "INFO : topic #1 (1.008): 0.109*\"product\" + 0.066*\"good\" + 0.065*\"great\" + 0.029*\"use\" + 0.021*\"recommend\" + 0.019*\"price\" + 0.018*\"love\" + 0.017*\"buy\" + 0.013*\"order\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.064218, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #215000/217530\n",
      "DEBUG : performing inference on a chunk of 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 5000/5000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31630573, 1.0289456, 0.20094344, 0.41309544, 0.093222007, 0.57830602, 0.41426778, 0.13849539]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5000 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.093): 0.091*\"hair\" + 0.039*\"nail\" + 0.033*\"grow\" + 0.029*\"naturewise\" + 0.026*\"eye\" + 0.016*\"strong\" + 0.014*\"nutrigold\" + 0.012*\"notice\" + 0.010*\"thick\" + 0.010*\"month\"\n",
      "INFO : topic #7 (0.138): 0.026*\"find\" + 0.020*\"amazon\" + 0.015*\"review\" + 0.013*\"coq10\" + 0.012*\"'s\" + 0.012*\"save\" + 0.010*\"anxiety\" + 0.010*\"buy\" + 0.010*\"brand\" + 0.009*\"order\"\n",
      "INFO : topic #6 (0.414): 0.037*\"taste\" + 0.024*\"like\" + 0.024*\"easy\" + 0.019*\"pill\" + 0.017*\"fish_oil\" + 0.017*\"vitamin\" + 0.014*\"swallow\" + 0.014*\"supplement\" + 0.012*\"capsule\" + 0.012*\"powder\"\n",
      "INFO : topic #5 (0.578): 0.037*\"feel\" + 0.031*\"work\" + 0.024*\"day\" + 0.024*\"help\" + 0.017*\"try\" + 0.015*\"energy\" + 0.014*\"like\" + 0.013*\"use\" + 0.011*\"probiotic\" + 0.011*\"good\"\n",
      "INFO : topic #1 (1.029): 0.110*\"product\" + 0.065*\"great\" + 0.064*\"good\" + 0.029*\"use\" + 0.022*\"recommend\" + 0.019*\"price\" + 0.018*\"love\" + 0.017*\"buy\" + 0.013*\"order\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.062288, rho=0.148240\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 2530 documents\n",
      "DEBUG : 2530/2530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31777734, 1.0467129, 0.2036119, 0.43549675, 0.093478926, 0.59087712, 0.41434026, 0.13876355]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 2530 documents into a model of 217530 documents\n",
      "INFO : topic #4 (0.093): 0.087*\"hair\" + 0.038*\"nail\" + 0.033*\"grow\" + 0.026*\"eye\" + 0.026*\"naturewise\" + 0.016*\"strong\" + 0.013*\"notice\" + 0.012*\"nutrigold\" + 0.010*\"inch\" + 0.010*\"month\"\n",
      "INFO : topic #7 (0.139): 0.026*\"find\" + 0.019*\"amazon\" + 0.015*\"review\" + 0.012*\"save\" + 0.012*\"'s\" + 0.011*\"coq10\" + 0.010*\"bottle\" + 0.010*\"anxiety\" + 0.010*\"brand\" + 0.009*\"buy\"\n",
      "INFO : topic #3 (0.435): 0.025*\"week\" + 0.023*\"day\" + 0.021*\"work\" + 0.018*\"start\" + 0.017*\"try\" + 0.016*\"notice\" + 0.015*\"pain\" + 0.015*\"month\" + 0.015*\"help\" + 0.014*\"feel\"\n",
      "INFO : topic #5 (0.591): 0.039*\"feel\" + 0.031*\"work\" + 0.025*\"help\" + 0.024*\"day\" + 0.018*\"try\" + 0.016*\"energy\" + 0.015*\"like\" + 0.013*\"probiotic\" + 0.013*\"use\" + 0.011*\"good\"\n",
      "INFO : topic #1 (1.047): 0.112*\"product\" + 0.065*\"great\" + 0.063*\"good\" + 0.028*\"use\" + 0.023*\"recommend\" + 0.019*\"love\" + 0.017*\"price\" + 0.017*\"buy\" + 0.012*\"order\" + 0.012*\"quality\"\n",
      "INFO : topic diff=0.069865, rho=0.148240\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=8, decay=0.5, chunksize=5000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 8 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.69024937681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217530/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 1/1 [07:26<00:00, 446.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_8.html\n",
      "CPU times: user 7min 26s, sys: 1.44 s, total: 7min 27s\n",
      "Wall time: 7min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [8]\n",
    "chunksize = 5000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Order Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with exploring the 4-topic model because it showed clear separation of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 0.25\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 4 topics, 2 passes over the supplied corpus of 217530 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6713/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072370008, 0.082538292, 0.11915907, 0.15756291]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.072): 0.023*\"pedometer\" + 0.019*\"step\" + 0.019*\"day\" + 0.016*\"use\" + 0.015*\"great\" + 0.015*\"good\" + 0.015*\"work\" + 0.015*\"like\" + 0.010*\"product\" + 0.009*\"walk\"\n",
      "INFO : topic #1 (0.083): 0.028*\"pedometer\" + 0.022*\"great\" + 0.018*\"work\" + 0.018*\"use\" + 0.015*\"good\" + 0.015*\"easy\" + 0.014*\"product\" + 0.012*\"day\" + 0.010*\"accurate\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.119): 0.036*\"pedometer\" + 0.024*\"good\" + 0.024*\"product\" + 0.023*\"use\" + 0.018*\"great\" + 0.016*\"love\" + 0.016*\"work\" + 0.013*\"step\" + 0.012*\"day\" + 0.012*\"walk\"\n",
      "INFO : topic #3 (0.158): 0.033*\"pedometer\" + 0.026*\"use\" + 0.024*\"day\" + 0.021*\"product\" + 0.020*\"step\" + 0.020*\"great\" + 0.018*\"good\" + 0.014*\"easy\" + 0.014*\"work\" + 0.012*\"pocket\"\n",
      "INFO : topic diff=5.779627, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6988/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082452402, 0.091588706, 0.11311321, 0.097086042]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.082): 0.017*\"good\" + 0.015*\"day\" + 0.014*\"product\" + 0.013*\"work\" + 0.013*\"like\" + 0.013*\"use\" + 0.012*\"help\" + 0.012*\"great\" + 0.009*\"feel\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.092): 0.024*\"product\" + 0.022*\"great\" + 0.019*\"good\" + 0.016*\"use\" + 0.015*\"taste\" + 0.014*\"work\" + 0.009*\"recommend\" + 0.009*\"like\" + 0.009*\"day\" + 0.008*\"help\"\n",
      "INFO : topic #2 (0.113): 0.036*\"product\" + 0.026*\"good\" + 0.024*\"use\" + 0.018*\"great\" + 0.014*\"work\" + 0.013*\"love\" + 0.011*\"pedometer\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"skin\"\n",
      "INFO : topic #3 (0.097): 0.027*\"product\" + 0.025*\"use\" + 0.021*\"day\" + 0.020*\"pedometer\" + 0.019*\"great\" + 0.019*\"good\" + 0.014*\"work\" + 0.012*\"step\" + 0.010*\"time\" + 0.010*\"easy\"\n",
      "INFO : topic diff=1.545039, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090033136, 0.099368781, 0.11270516, 0.084802181]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.090): 0.017*\"good\" + 0.016*\"day\" + 0.015*\"help\" + 0.014*\"work\" + 0.014*\"product\" + 0.012*\"use\" + 0.011*\"like\" + 0.010*\"great\" + 0.010*\"feel\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.099): 0.027*\"product\" + 0.023*\"great\" + 0.022*\"good\" + 0.017*\"use\" + 0.014*\"work\" + 0.014*\"taste\" + 0.009*\"like\" + 0.009*\"help\" + 0.008*\"recommend\" + 0.008*\"price\"\n",
      "INFO : topic #2 (0.113): 0.040*\"product\" + 0.027*\"good\" + 0.026*\"use\" + 0.019*\"great\" + 0.015*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic #3 (0.085): 0.028*\"product\" + 0.025*\"use\" + 0.021*\"day\" + 0.018*\"good\" + 0.018*\"great\" + 0.015*\"work\" + 0.012*\"pedometer\" + 0.011*\"time\" + 0.008*\"year\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.771673, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.094821341, 0.10767336, 0.11680388, 0.079674095]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.095): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"good\" + 0.015*\"work\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"like\" + 0.009*\"great\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.108): 0.028*\"product\" + 0.024*\"great\" + 0.023*\"good\" + 0.017*\"use\" + 0.013*\"taste\" + 0.013*\"work\" + 0.010*\"like\" + 0.009*\"price\" + 0.008*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.042*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.016*\"work\" + 0.013*\"love\" + 0.012*\"skin\" + 0.011*\"oil\" + 0.010*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.080): 0.027*\"product\" + 0.025*\"use\" + 0.020*\"day\" + 0.017*\"great\" + 0.016*\"good\" + 0.016*\"work\" + 0.011*\"time\" + 0.008*\"year\" + 0.008*\"help\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.567345, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10244222, 0.11420026, 0.11670072, 0.077437147]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.102): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.015*\"good\" + 0.014*\"product\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"like\" + 0.009*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.114): 0.029*\"product\" + 0.026*\"good\" + 0.025*\"great\" + 0.016*\"use\" + 0.015*\"taste\" + 0.012*\"work\" + 0.011*\"like\" + 0.010*\"price\" + 0.009*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.045*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.016*\"work\" + 0.013*\"skin\" + 0.013*\"love\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.077): 0.027*\"product\" + 0.024*\"use\" + 0.019*\"day\" + 0.016*\"work\" + 0.016*\"great\" + 0.016*\"good\" + 0.011*\"time\" + 0.009*\"year\" + 0.008*\"help\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.496468, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1093009, 0.11959437, 0.11728799, 0.076797262]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.109): 0.017*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.015*\"good\" + 0.014*\"product\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.120): 0.029*\"product\" + 0.028*\"good\" + 0.025*\"great\" + 0.017*\"use\" + 0.016*\"taste\" + 0.012*\"like\" + 0.010*\"price\" + 0.010*\"work\" + 0.008*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.117): 0.047*\"product\" + 0.028*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.015*\"work\" + 0.014*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.009*\"oil\"\n",
      "INFO : topic #3 (0.077): 0.027*\"product\" + 0.024*\"use\" + 0.020*\"day\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"great\" + 0.011*\"time\" + 0.011*\"pain\" + 0.009*\"year\" + 0.008*\"help\"\n",
      "INFO : topic diff=0.446620, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11482716, 0.1257468, 0.11888508, 0.077609047]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.115): 0.017*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.126): 0.029*\"product\" + 0.029*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.016*\"use\" + 0.012*\"like\" + 0.010*\"price\" + 0.010*\"work\" + 0.009*\"supplement\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.119): 0.049*\"product\" + 0.029*\"use\" + 0.026*\"good\" + 0.024*\"great\" + 0.015*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.011*\"oil\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #3 (0.078): 0.027*\"product\" + 0.023*\"use\" + 0.019*\"day\" + 0.019*\"work\" + 0.014*\"good\" + 0.013*\"great\" + 0.011*\"time\" + 0.011*\"pain\" + 0.010*\"year\" + 0.008*\"help\"\n",
      "INFO : topic diff=0.416753, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12343414, 0.12966594, 0.11762695, 0.079292007]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.123): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"use\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.130): 0.030*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.010*\"price\" + 0.009*\"work\" + 0.009*\"find\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.118): 0.051*\"product\" + 0.029*\"use\" + 0.027*\"good\" + 0.025*\"great\" + 0.016*\"work\" + 0.016*\"skin\" + 0.013*\"love\" + 0.011*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.079): 0.026*\"product\" + 0.023*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.013*\"good\" + 0.012*\"pain\" + 0.012*\"great\" + 0.011*\"time\" + 0.011*\"uti\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.383540, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13068466, 0.13434196, 0.11937566, 0.078368261]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.131): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.014*\"good\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.134): 0.032*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"supplement\" + 0.009*\"find\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.119): 0.053*\"product\" + 0.029*\"use\" + 0.028*\"good\" + 0.026*\"great\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.011*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.078): 0.026*\"product\" + 0.022*\"use\" + 0.020*\"work\" + 0.020*\"day\" + 0.013*\"pain\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"time\" + 0.010*\"year\" + 0.009*\"uti\"\n",
      "INFO : topic diff=0.332496, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13711213, 0.13563071, 0.12254435, 0.080282904]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.137): 0.017*\"help\" + 0.017*\"day\" + 0.016*\"work\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.136): 0.032*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"supplement\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.123): 0.054*\"product\" + 0.029*\"use\" + 0.028*\"good\" + 0.027*\"great\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic #3 (0.080): 0.025*\"product\" + 0.021*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.014*\"pain\" + 0.012*\"good\" + 0.011*\"great\" + 0.011*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.339808, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14204738, 0.1419716, 0.12403292, 0.080710754]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.142): 0.018*\"help\" + 0.017*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.142): 0.033*\"good\" + 0.028*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.018*\"use\" + 0.013*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"love\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.124): 0.054*\"product\" + 0.031*\"use\" + 0.028*\"great\" + 0.026*\"good\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.081): 0.024*\"product\" + 0.022*\"use\" + 0.020*\"day\" + 0.019*\"work\" + 0.015*\"pain\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.309580, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14905578, 0.14514583, 0.12714738, 0.081536062]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.149): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.145): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.127): 0.056*\"product\" + 0.030*\"use\" + 0.028*\"great\" + 0.026*\"good\" + 0.016*\"work\" + 0.013*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"hair\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.082): 0.025*\"product\" + 0.021*\"use\" + 0.020*\"work\" + 0.019*\"day\" + 0.017*\"pain\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"week\"\n",
      "INFO : topic diff=0.289438, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15631747, 0.14985779, 0.12782843, 0.082744226]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.156): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.150): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.010*\"vitamin\" + 0.010*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.128): 0.057*\"product\" + 0.030*\"use\" + 0.029*\"great\" + 0.027*\"good\" + 0.015*\"work\" + 0.014*\"skin\" + 0.012*\"love\" + 0.012*\"recommend\" + 0.011*\"hair\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.083): 0.024*\"product\" + 0.020*\"use\" + 0.020*\"day\" + 0.020*\"work\" + 0.018*\"pain\" + 0.011*\"good\" + 0.010*\"great\" + 0.010*\"year\" + 0.010*\"time\" + 0.010*\"week\"\n",
      "INFO : topic diff=0.282694, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15795387, 0.15619229, 0.12993996, 0.083497956]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.158): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.156): 0.035*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.011*\"price\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.130): 0.057*\"product\" + 0.031*\"use\" + 0.030*\"great\" + 0.027*\"good\" + 0.015*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"hair\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.083): 0.023*\"product\" + 0.021*\"day\" + 0.020*\"use\" + 0.020*\"work\" + 0.018*\"pain\" + 0.011*\"good\" + 0.010*\"great\" + 0.010*\"time\" + 0.010*\"year\" + 0.009*\"help\"\n",
      "INFO : topic diff=0.249285, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16313532, 0.15855511, 0.13291879, 0.084954567]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.163): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.159): 0.035*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.009*\"supplement\" + 0.009*\"brand\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.133): 0.058*\"product\" + 0.031*\"use\" + 0.031*\"great\" + 0.027*\"good\" + 0.016*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.010*\"hair\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.085): 0.022*\"product\" + 0.020*\"day\" + 0.020*\"use\" + 0.020*\"work\" + 0.019*\"pain\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"great\" + 0.009*\"help\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.253378, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16724987, 0.16620769, 0.13625547, 0.085355461]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.167): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.166): 0.034*\"good\" + 0.026*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.012*\"price\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.136): 0.058*\"product\" + 0.031*\"great\" + 0.030*\"use\" + 0.027*\"good\" + 0.015*\"work\" + 0.013*\"love\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.010*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.085): 0.021*\"product\" + 0.021*\"work\" + 0.020*\"day\" + 0.020*\"use\" + 0.018*\"pain\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"year\" + 0.010*\"great\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.238933, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16898209, 0.17096286, 0.1390024, 0.085833132]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.169): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.171): 0.034*\"good\" + 0.025*\"product\" + 0.024*\"great\" + 0.022*\"taste\" + 0.015*\"like\" + 0.015*\"use\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.139): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.015*\"work\" + 0.013*\"love\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.086): 0.021*\"product\" + 0.021*\"work\" + 0.020*\"day\" + 0.020*\"use\" + 0.017*\"pain\" + 0.010*\"week\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.228935, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16906099, 0.17793798, 0.14387201, 0.085871525]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.169): 0.018*\"help\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.178): 0.035*\"good\" + 0.025*\"product\" + 0.025*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.144): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"skin\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.086): 0.021*\"work\" + 0.021*\"product\" + 0.021*\"day\" + 0.019*\"use\" + 0.017*\"pain\" + 0.011*\"week\" + 0.010*\"good\" + 0.010*\"help\" + 0.010*\"time\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.211883, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.167919, 0.18397315, 0.14783899, 0.086044557]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.168): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.184): 0.035*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.024*\"product\" + 0.015*\"like\" + 0.013*\"use\" + 0.013*\"vitamin\" + 0.011*\"price\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.148): 0.059*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.029*\"good\" + 0.014*\"oil\" + 0.014*\"love\" + 0.014*\"skin\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.086): 0.022*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.019*\"use\" + 0.016*\"pain\" + 0.011*\"week\" + 0.010*\"help\" + 0.010*\"good\" + 0.010*\"time\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.211599, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17378229, 0.18530473, 0.15064622, 0.087697856]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.174): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.185): 0.035*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.024*\"product\" + 0.015*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"price\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.151): 0.062*\"product\" + 0.032*\"great\" + 0.029*\"good\" + 0.029*\"use\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.088): 0.022*\"work\" + 0.021*\"product\" + 0.021*\"day\" + 0.019*\"use\" + 0.015*\"pain\" + 0.011*\"week\" + 0.010*\"help\" + 0.010*\"time\" + 0.010*\"good\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.216266, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.175501, 0.18868011, 0.15290572, 0.088813968]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.176): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.189): 0.034*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"love\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.153): 0.063*\"product\" + 0.033*\"great\" + 0.030*\"good\" + 0.029*\"use\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"oil\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.089): 0.022*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.016*\"pain\" + 0.011*\"week\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.202595, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17733867, 0.19178689, 0.15487689, 0.090235047]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.177): 0.018*\"help\" + 0.017*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.192): 0.034*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"price\"\n",
      "INFO : topic #2 (0.155): 0.064*\"product\" + 0.033*\"great\" + 0.030*\"use\" + 0.030*\"good\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.012*\"skin\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.090): 0.023*\"work\" + 0.020*\"product\" + 0.020*\"day\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"time\" + 0.010*\"help\" + 0.010*\"start\" + 0.009*\"good\"\n",
      "INFO : topic diff=0.201018, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17716767, 0.18951592, 0.16166346, 0.090444118]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.177): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"supplement\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.190): 0.034*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"price\"\n",
      "INFO : topic #2 (0.162): 0.062*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.017*\"hair\" + 0.013*\"love\" + 0.013*\"work\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.012*\"oil\"\n",
      "INFO : topic #3 (0.090): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"start\" + 0.010*\"time\" + 0.010*\"help\" + 0.009*\"feel\"\n",
      "INFO : topic diff=0.197441, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18067282, 0.19118102, 0.16493705, 0.091609389]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.181): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.191): 0.034*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.165): 0.063*\"product\" + 0.031*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.016*\"hair\" + 0.014*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.092): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"time\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.192417, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18454447, 0.19268326, 0.16975927, 0.092190467]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.185): 0.018*\"help\" + 0.016*\"work\" + 0.015*\"day\" + 0.015*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.193): 0.033*\"good\" + 0.024*\"great\" + 0.024*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.170): 0.064*\"product\" + 0.032*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"oil\" + 0.013*\"hair\" + 0.013*\"love\" + 0.013*\"skin\" + 0.013*\"work\" + 0.012*\"recommend\"\n",
      "INFO : topic #3 (0.092): 0.023*\"work\" + 0.021*\"day\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"pain\" + 0.012*\"week\" + 0.011*\"help\" + 0.010*\"start\" + 0.010*\"feel\" + 0.010*\"time\"\n",
      "INFO : topic diff=0.182581, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18603575, 0.19468854, 0.17148259, 0.094140485]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.186): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.195): 0.033*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.171): 0.065*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"hair\" + 0.013*\"skin\" + 0.012*\"recommend\"\n",
      "INFO : topic #3 (0.094): 0.023*\"work\" + 0.020*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.016*\"pain\" + 0.013*\"week\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.195012, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1871279, 0.19577834, 0.1724543, 0.096336752]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.187): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.033*\"good\" + 0.025*\"taste\" + 0.025*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.172): 0.067*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.014*\"oil\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"hair\" + 0.013*\"recommend\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.096): 0.024*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.183644, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18892746, 0.19620804, 0.17386544, 0.099184059]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.013*\"feel\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.033*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.174): 0.068*\"product\" + 0.034*\"great\" + 0.030*\"good\" + 0.027*\"use\" + 0.013*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"oil\" + 0.013*\"skin\" + 0.012*\"hair\"\n",
      "INFO : topic #3 (0.099): 0.025*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"good\"\n",
      "INFO : topic diff=0.184489, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1879535, 0.19437112, 0.17978567, 0.10033237]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.188): 0.018*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.015*\"day\" + 0.013*\"good\" + 0.013*\"feel\" + 0.012*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.194): 0.032*\"good\" + 0.025*\"taste\" + 0.024*\"great\" + 0.022*\"product\" + 0.017*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.180): 0.067*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.015*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"work\" + 0.012*\"oil\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.100): 0.024*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.018*\"use\" + 0.016*\"pain\" + 0.013*\"week\" + 0.012*\"feel\" + 0.011*\"try\" + 0.011*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.178492, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.189362, 0.19849733, 0.18487963, 0.10260747]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"day\" + 0.014*\"feel\" + 0.013*\"good\" + 0.013*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.198): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.185): 0.069*\"product\" + 0.033*\"great\" + 0.028*\"good\" + 0.027*\"use\" + 0.016*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"oil\" + 0.012*\"work\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.103): 0.023*\"work\" + 0.021*\"product\" + 0.020*\"day\" + 0.017*\"use\" + 0.015*\"pain\" + 0.014*\"week\" + 0.014*\"feel\" + 0.011*\"try\" + 0.011*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.186617, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18876565, 0.19359982, 0.19071725, 0.10616958]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.189): 0.018*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.014*\"day\" + 0.014*\"feel\" + 0.014*\"supplement\" + 0.013*\"good\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.194): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.191): 0.069*\"product\" + 0.032*\"great\" + 0.027*\"good\" + 0.026*\"use\" + 0.019*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"work\" + 0.010*\"oil\" + 0.010*\"look\"\n",
      "INFO : topic #3 (0.106): 0.023*\"work\" + 0.022*\"product\" + 0.020*\"day\" + 0.016*\"use\" + 0.015*\"week\" + 0.014*\"feel\" + 0.014*\"pain\" + 0.012*\"try\" + 0.011*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.182206, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18824422, 0.18603107, 0.19514026, 0.10940558]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.188): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"feel\" + 0.015*\"supplement\" + 0.015*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"try\" + 0.010*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.186): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.017*\"like\" + 0.015*\"easy\" + 0.014*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.195): 0.072*\"product\" + 0.034*\"great\" + 0.027*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.014*\"recommend\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"look\" + 0.010*\"feel\"\n",
      "INFO : topic #3 (0.109): 0.023*\"product\" + 0.022*\"work\" + 0.019*\"day\" + 0.017*\"pain\" + 0.016*\"use\" + 0.016*\"feel\" + 0.014*\"week\" + 0.013*\"try\" + 0.011*\"help\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.172691, rho=0.176777\n",
      "INFO : PROGRESS: pass 1, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17799795, 0.18255495, 0.1903078, 0.11632661]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.178): 0.019*\"help\" + 0.016*\"product\" + 0.015*\"work\" + 0.015*\"feel\" + 0.014*\"good\" + 0.014*\"supplement\" + 0.014*\"day\" + 0.011*\"try\" + 0.010*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.183): 0.030*\"good\" + 0.025*\"great\" + 0.021*\"taste\" + 0.021*\"product\" + 0.019*\"easy\" + 0.017*\"like\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"love\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.190): 0.069*\"product\" + 0.034*\"great\" + 0.027*\"good\" + 0.027*\"use\" + 0.018*\"skin\" + 0.014*\"love\" + 0.014*\"recommend\" + 0.013*\"work\" + 0.010*\"look\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.116): 0.022*\"day\" + 0.021*\"work\" + 0.019*\"pedometer\" + 0.018*\"product\" + 0.018*\"use\" + 0.012*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"step\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.218292, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18480211, 0.18610239, 0.18875685, 0.11519255]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.185): 0.019*\"help\" + 0.016*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"feel\" + 0.014*\"day\" + 0.013*\"supplement\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.186): 0.030*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"easy\" + 0.017*\"like\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"vitamin\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.189): 0.068*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.027*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.010*\"buy\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.115): 0.022*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.018*\"product\" + 0.017*\"pedometer\" + 0.013*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"great\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.175803, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19179255, 0.18646948, 0.18922505, 0.11470156]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.192): 0.019*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.014*\"feel\" + 0.012*\"supplement\" + 0.010*\"use\" + 0.010*\"try\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.186): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"like\" + 0.016*\"easy\" + 0.012*\"use\" + 0.011*\"supplement\" + 0.011*\"vitamin\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.189): 0.067*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.027*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.011*\"oil\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.115): 0.022*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.018*\"product\" + 0.015*\"pedometer\" + 0.014*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"great\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.157960, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19529314, 0.18715534, 0.1924337, 0.11437142]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.195): 0.020*\"help\" + 0.016*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.187): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.021*\"product\" + 0.017*\"like\" + 0.015*\"easy\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.192): 0.066*\"product\" + 0.035*\"great\" + 0.029*\"use\" + 0.027*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.011*\"oil\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.114): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.014*\"pain\" + 0.013*\"pedometer\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"great\"\n",
      "INFO : topic diff=0.155190, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20020992, 0.18785389, 0.19151044, 0.11461343]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.200): 0.020*\"help\" + 0.017*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.188): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.192): 0.067*\"product\" + 0.035*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.013*\"recommend\" + 0.012*\"price\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.115): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.014*\"pain\" + 0.012*\"week\" + 0.012*\"pedometer\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.151011, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #42000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2048969, 0.1874655, 0.19132914, 0.11475474]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.205): 0.020*\"help\" + 0.017*\"work\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.187): 0.031*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.020*\"product\" + 0.016*\"like\" + 0.014*\"easy\" + 0.014*\"use\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.191): 0.066*\"product\" + 0.034*\"great\" + 0.030*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.014*\"work\" + 0.012*\"recommend\" + 0.012*\"price\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.115): 0.023*\"day\" + 0.022*\"work\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"help\" + 0.010*\"try\" + 0.010*\"pedometer\"\n",
      "INFO : topic diff=0.149110, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20697893, 0.18844217, 0.19041695, 0.11595067]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.207): 0.020*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.024*\"taste\" + 0.024*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.013*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.190): 0.067*\"product\" + 0.034*\"great\" + 0.031*\"use\" + 0.028*\"good\" + 0.017*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"price\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.116): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.149458, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21328223, 0.1881234, 0.18772557, 0.11684494]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.020*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.068*\"product\" + 0.035*\"great\" + 0.031*\"use\" + 0.029*\"good\" + 0.017*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.024*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.016*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.148822, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21984485, 0.1883132, 0.18766758, 0.11601531]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.220): 0.020*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.012*\"feel\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.188): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.069*\"product\" + 0.036*\"great\" + 0.030*\"use\" + 0.030*\"good\" + 0.017*\"skin\" + 0.015*\"work\" + 0.014*\"love\" + 0.013*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.116): 0.024*\"work\" + 0.023*\"day\" + 0.018*\"use\" + 0.017*\"product\" + 0.017*\"pain\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.136532, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22446886, 0.18594456, 0.18906766, 0.11715781]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.224): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.015*\"day\" + 0.014*\"good\" + 0.012*\"feel\" + 0.011*\"year\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.186): 0.032*\"good\" + 0.024*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.069*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.030*\"good\" + 0.016*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.014*\"price\" + 0.013*\"recommend\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.023*\"day\" + 0.023*\"work\" + 0.018*\"use\" + 0.017*\"pain\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.010*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.146625, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22763981, 0.18724807, 0.18839441, 0.11712882]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.228): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.032*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.188): 0.069*\"product\" + 0.036*\"great\" + 0.032*\"use\" + 0.030*\"good\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.014*\"price\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.023*\"day\" + 0.023*\"work\" + 0.018*\"use\" + 0.017*\"pain\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"start\" + 0.011*\"try\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.141023, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23272769, 0.18646647, 0.18943864, 0.11743854]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.012*\"year\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.186): 0.033*\"good\" + 0.024*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.069*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.030*\"good\" + 0.015*\"skin\" + 0.014*\"price\" + 0.014*\"work\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"hair\"\n",
      "INFO : topic #3 (0.117): 0.023*\"work\" + 0.023*\"day\" + 0.018*\"pain\" + 0.018*\"use\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"try\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.137312, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23787811, 0.18779679, 0.18831362, 0.11788799]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.238): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.188): 0.033*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.188): 0.070*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.031*\"good\" + 0.015*\"skin\" + 0.015*\"price\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.023*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"week\" + 0.011*\"feel\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.142588, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23718999, 0.19094783, 0.18879527, 0.11786384]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"feel\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.033*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.189): 0.070*\"product\" + 0.037*\"great\" + 0.031*\"use\" + 0.031*\"good\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.018*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.130692, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24076603, 0.19114582, 0.18952838, 0.11855568]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.241): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.191): 0.032*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.016*\"like\" + 0.015*\"use\" + 0.011*\"easy\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.190): 0.070*\"product\" + 0.038*\"great\" + 0.032*\"good\" + 0.031*\"use\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"work\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.119): 0.024*\"day\" + 0.023*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.136184, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24356957, 0.19598214, 0.19123375, 0.11787803]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.244): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.196): 0.032*\"good\" + 0.023*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"easy\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.191): 0.070*\"product\" + 0.037*\"great\" + 0.032*\"good\" + 0.031*\"use\" + 0.017*\"price\" + 0.014*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.019*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.135668, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24256288, 0.19891818, 0.19216248, 0.11760422]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.243): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.199): 0.032*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.017*\"like\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.192): 0.070*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.030*\"use\" + 0.017*\"price\" + 0.014*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.133012, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23895404, 0.20425718, 0.19474481, 0.11707886]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"work\" + 0.017*\"product\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.010*\"feel\" + 0.010*\"supplement\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.204): 0.032*\"good\" + 0.026*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.010*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.195): 0.070*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.029*\"use\" + 0.018*\"price\" + 0.013*\"love\" + 0.013*\"work\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"buy\"\n",
      "INFO : topic #3 (0.117): 0.024*\"day\" + 0.024*\"work\" + 0.018*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.013*\"feel\" + 0.013*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.128591, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23528239, 0.2083039, 0.1967998, 0.11642897]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.235): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.010*\"feel\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.208): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"product\" + 0.017*\"like\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.197): 0.070*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"skin\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"work\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.116): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.013*\"feel\" + 0.012*\"week\" + 0.011*\"help\" + 0.011*\"start\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.130732, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23970246, 0.20816387, 0.19843741, 0.1173287]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.208): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"product\" + 0.017*\"like\" + 0.013*\"use\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.198): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.117): 0.024*\"day\" + 0.024*\"work\" + 0.016*\"use\" + 0.016*\"pain\" + 0.015*\"product\" + 0.014*\"feel\" + 0.012*\"week\" + 0.011*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.136096, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2392199, 0.21064433, 0.19962873, 0.11766813]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.012*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.211): 0.032*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"like\" + 0.017*\"product\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.200): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.118): 0.024*\"day\" + 0.024*\"work\" + 0.017*\"use\" + 0.017*\"pain\" + 0.015*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.132017, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23943159, 0.21259424, 0.20091364, 0.11853683]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.017*\"product\" + 0.017*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"year\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.213): 0.031*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.013*\"easy\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.201): 0.073*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.030*\"use\" + 0.018*\"price\" + 0.014*\"love\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.016*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.134634, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23712933, 0.20974047, 0.20717795, 0.11827373]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"year\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.210): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.207): 0.070*\"product\" + 0.036*\"great\" + 0.033*\"good\" + 0.029*\"use\" + 0.017*\"price\" + 0.016*\"hair\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.118): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.014*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.138794, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2391582, 0.21078502, 0.20954332, 0.11889697]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"day\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.211): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.210): 0.071*\"product\" + 0.036*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.015*\"hair\" + 0.013*\"buy\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.134244, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24239142, 0.21119587, 0.21391612, 0.11897726]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.242): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.014*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.211): 0.031*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.014*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.214): 0.071*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.013*\"love\" + 0.013*\"buy\" + 0.013*\"hair\" + 0.013*\"skin\" + 0.013*\"recommend\"\n",
      "INFO : topic #3 (0.119): 0.024*\"work\" + 0.024*\"day\" + 0.017*\"pain\" + 0.017*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"week\" + 0.012*\"start\" + 0.012*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.132644, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24177086, 0.21242128, 0.21542174, 0.12076033]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.242): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.215): 0.072*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.013*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"skin\" + 0.013*\"hair\"\n",
      "INFO : topic #3 (0.121): 0.025*\"work\" + 0.024*\"day\" + 0.016*\"use\" + 0.016*\"product\" + 0.016*\"pain\" + 0.015*\"feel\" + 0.014*\"week\" + 0.012*\"start\" + 0.011*\"help\" + 0.011*\"try\"\n",
      "INFO : topic diff=0.145183, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24062435, 0.21239218, 0.21611519, 0.12310937]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.241): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"year\" + 0.010*\"supplement\" + 0.010*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.027*\"taste\" + 0.022*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.216): 0.074*\"product\" + 0.038*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"hair\"\n",
      "INFO : topic #3 (0.123): 0.025*\"work\" + 0.023*\"day\" + 0.016*\"product\" + 0.016*\"use\" + 0.016*\"feel\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"try\" + 0.012*\"start\" + 0.011*\"help\"\n",
      "INFO : topic diff=0.139377, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2403713, 0.21211062, 0.21730782, 0.12612621]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.030*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.011*\"use\" + 0.011*\"love\" + 0.010*\"supplement\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.217): 0.075*\"product\" + 0.038*\"great\" + 0.034*\"good\" + 0.027*\"use\" + 0.017*\"price\" + 0.014*\"love\" + 0.013*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"skin\"\n",
      "INFO : topic #3 (0.126): 0.025*\"work\" + 0.023*\"day\" + 0.016*\"feel\" + 0.016*\"product\" + 0.016*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.012*\"try\" + 0.011*\"energy\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.143215, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23713008, 0.21013279, 0.22332871, 0.12718305]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.016*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.210): 0.030*\"good\" + 0.027*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.011*\"love\" + 0.011*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.223): 0.074*\"product\" + 0.037*\"great\" + 0.033*\"good\" + 0.028*\"use\" + 0.016*\"price\" + 0.015*\"skin\" + 0.014*\"love\" + 0.013*\"recommend\" + 0.013*\"buy\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.127): 0.025*\"work\" + 0.023*\"day\" + 0.017*\"feel\" + 0.016*\"product\" + 0.016*\"use\" + 0.015*\"pain\" + 0.014*\"week\" + 0.012*\"try\" + 0.011*\"energy\" + 0.011*\"start\"\n",
      "INFO : topic diff=0.143772, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23668925, 0.21377808, 0.22846378, 0.12976378]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.019*\"help\" + 0.018*\"product\" + 0.015*\"work\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.214): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.228): 0.076*\"product\" + 0.037*\"great\" + 0.032*\"good\" + 0.027*\"use\" + 0.016*\"skin\" + 0.015*\"price\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\" + 0.011*\"work\"\n",
      "INFO : topic #3 (0.130): 0.024*\"work\" + 0.023*\"day\" + 0.019*\"feel\" + 0.017*\"product\" + 0.015*\"use\" + 0.015*\"pain\" + 0.015*\"week\" + 0.013*\"energy\" + 0.012*\"try\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.153638, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23348764, 0.20784597, 0.23483755, 0.13426413]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.019*\"help\" + 0.018*\"product\" + 0.014*\"work\" + 0.014*\"good\" + 0.013*\"supplement\" + 0.012*\"day\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.208): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.017*\"like\" + 0.016*\"product\" + 0.015*\"easy\" + 0.013*\"vitamin\" + 0.013*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.235): 0.075*\"product\" + 0.036*\"great\" + 0.030*\"good\" + 0.027*\"use\" + 0.019*\"skin\" + 0.013*\"price\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"buy\" + 0.011*\"work\"\n",
      "INFO : topic #3 (0.134): 0.024*\"work\" + 0.022*\"day\" + 0.019*\"feel\" + 0.017*\"product\" + 0.015*\"week\" + 0.015*\"use\" + 0.013*\"pain\" + 0.013*\"try\" + 0.013*\"energy\" + 0.012*\"start\"\n",
      "INFO : topic diff=0.154417, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22841106, 0.19824417, 0.2395906, 0.1390768]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.228): 0.020*\"help\" + 0.019*\"product\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.015*\"work\" + 0.013*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.198): 0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.016*\"easy\" + 0.013*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"fish_oil\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.240): 0.078*\"product\" + 0.038*\"great\" + 0.030*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"price\" + 0.012*\"work\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.139): 0.023*\"work\" + 0.021*\"feel\" + 0.021*\"day\" + 0.019*\"product\" + 0.015*\"pain\" + 0.015*\"week\" + 0.015*\"use\" + 0.014*\"try\" + 0.012*\"start\" + 0.012*\"energy\"\n",
      "INFO : topic diff=0.150953, rho=0.173878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LdaModel' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/ShopSmart.ml/Notebooks/S3_read_write.py\u001b[0m in \u001b[0;36msave_df_s3\u001b[0;34m(df, bucket_name, filepath, filetype)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'feather'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mwrite_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0ms3_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mwrite_feather\u001b[0;34m(df, dest)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatherWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Try to make sure the resource is closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot serialize duplicate column names\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LdaModel' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [4]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                   passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    save_df_s3(lda, bucket_name, filepath='amazon_reviews/kk/lda_tier2_topics_4.pkl', filetype='pickle')\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# error in the previous cell, so redoing some steps above:\n",
    "\n",
    "# save_df_s3(lda, bucket_name, filepath='amazon_reviews/kk/lda_tier1_topics_4.pkl', filetype='pickle')\n",
    "lda = load_df_s3(bucket_name, filepath='amazon_reviews/kk/lda_tier1_topics_4.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# performance metric\n",
    "cm = CoherenceModel(model=lda, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "coherenceList_umass.append(cm.get_coherence())\n",
    "print('Calculated coherence score...: ', cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217516/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "# visualization\n",
    "vis = pyLDAvis.gensim.prepare(lda, bow_corpus, vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_4.html\n"
     ]
    }
   ],
   "source": [
    "plot_fname = 'pyLDAvis_{}.html'.format(4)\n",
    "pyLDAvis.save_html(vis, plot_fname)\n",
    "print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el350631396733018616245428103129\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el350631396733018616245428103129_data = {\"mdsDat\": {\"Freq\": [33.95744082843555, 25.396042574645833, 21.87007002857469, 18.77644656834393], \"cluster\": [1, 1, 1, 1], \"topics\": [1, 2, 3, 4], \"x\": [0.07665428024706823, -0.18357166445425746, -0.04592232010553853, 0.15283970431272778], \"y\": [0.10417334936204002, 0.06255164137125487, -0.14580035685704815, -0.0209246338762466]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [173091.0, 35787.0, 22924.0, 97745.0, 17277.0, 25237.0, 23218.0, 68698.0, 27984.0, 56950.0, 10365.0, 59351.0, 19141.0, 11370.0, 55490.0, 37438.0, 9089.0, 9588.0, 24278.0, 30442.0, 21990.0, 7826.0, 9815.0, 14904.0, 14258.0, 114800.0, 7231.0, 8865.0, 80390.0, 7823.0, 2223.5352053125175, 1521.5311818532614, 1559.7396738423888, 2998.7195806961417, 1985.8312632617585, 1092.166335448853, 1798.2852194161983, 2043.3079885497973, 1151.0604345740005, 1026.7619073456679, 896.2724469352903, 780.6474293033127, 782.4301175813274, 2117.491621621221, 903.1183394717218, 728.0397301477176, 2898.1327817921747, 742.2355204668405, 632.7714101724497, 664.3360516627623, 628.5782405022574, 578.7813673316136, 777.775946713165, 623.6125377497455, 552.1981014247112, 532.0028614224499, 596.872158587135, 5518.482288466693, 530.0608052007733, 481.5775902213631, 4084.0735689659246, 4221.5844576187965, 3653.863269571227, 2377.5572288435164, 2574.730240327947, 1781.8198382058163, 2279.923081296034, 1978.1805021182065, 1493.4233619428608, 1413.3599426270177, 1604.1592262279778, 3071.6940411156547, 1359.8212223236671, 1101.9494814728166, 2282.3902598413915, 2736.7169215595795, 13826.922050062522, 5189.081837106222, 2227.659414526757, 3271.791327883786, 4431.605230987918, 9062.560072221528, 8316.269357988791, 3403.085436931727, 2460.6887835329903, 1918.1446510102064, 12342.538447027537, 8815.008093127648, 2974.5438265924045, 3718.002432785602, 37532.666322613906, 7509.093710160082, 4815.499620450706, 3608.6771232813435, 12063.525542089219, 4888.17316251697, 11660.09697613375, 27640.190149815895, 9703.280357278934, 16806.055981389767, 7613.966395902293, 6985.114348294621, 27409.97177177178, 24001.645560157176, 22355.95476185049, 6325.227235903835, 19439.584304131462, 19885.327093847558, 10038.665300392704, 13258.351779526982, 28267.682345625475, 35791.57384778834, 10530.27086275438, 9538.560994247286, 19863.25446867322, 11619.023380671611, 11589.885333343487, 10129.09126554425, 9083.59444108933, 11144.999545019396, 12223.81647801003, 7822.875277299285, 35780.35882546343, 3399.5588809012706, 2522.1565037010605, 2142.947747688958, 1717.8134125858164, 1716.6363398209771, 2798.503576991891, 1479.4705723762725, 1889.1572052164956, 1235.0311368358855, 1220.263438804059, 1181.621645205633, 997.3473257662363, 936.0491434642912, 11359.434241312554, 1051.6473950921504, 1589.0023340983012, 1031.1128630652067, 601.8442552318086, 586.229615919537, 563.484144769874, 617.3687839071911, 501.0970673413779, 473.31761543198127, 464.18826867978055, 1412.4606671031408, 3938.2623948885216, 453.61155837970324, 446.29672152659225, 8838.176294067336, 9750.635862907975, 5539.989782157066, 5181.139696426888, 3826.4936191264806, 1947.6186503807037, 2944.728751978261, 1635.9756806860926, 1641.3502393481892, 2684.169095172487, 3308.551661682662, 1317.2881237937777, 2997.5142552092075, 2416.7608654526293, 4857.146757545797, 1638.6473315178173, 22684.42392815816, 1797.831658143245, 15309.202830934604, 2874.344571445923, 3138.577090103242, 8154.288298638141, 7694.707763962861, 3846.2263268828683, 17012.04463470005, 3106.2037230729584, 8066.108028181377, 8293.812302135628, 8965.049438638735, 6665.737239133869, 25122.98046327788, 10980.259867998962, 4210.544084229008, 38971.81025447799, 14871.08368055141, 30102.066365903385, 15297.947679253366, 10307.973154383391, 18809.343114250587, 7931.253491260719, 22714.499083235292, 9722.167646582338, 14628.266154219766, 8375.093659459433, 10972.404819728446, 7945.64223931158, 7235.577330068097, 7774.945466552895, 22923.735623113385, 9586.90996716225, 4152.231484679938, 3040.1584921554295, 4830.94085749268, 4452.827959179291, 1655.5442125180125, 1831.4575136006456, 1532.6083965042606, 1377.5378646643176, 1151.2781731850835, 1226.9755545916212, 905.146860103612, 897.756575313237, 896.5030794237077, 822.1116826303737, 678.2912338979647, 574.806067172285, 539.3320484966257, 531.9161214417836, 534.1167164944543, 699.1297530415718, 2727.388033842896, 2196.7642966972508, 436.4396643405394, 432.52135632600505, 461.3015751985066, 415.07721423248256, 457.06897235678866, 392.2490112031958, 2496.42716609805, 2370.5036720989588, 2586.4989492829777, 1119.1238360681261, 4285.333821040028, 1865.668261158885, 7181.180482174076, 1219.5550232627727, 3331.600819910001, 2313.64853829677, 3107.5423964157926, 2275.0232128210782, 95096.9258031969, 15914.039272097638, 9265.917550503795, 12646.435027478894, 10004.755908442316, 5107.655741905039, 45657.896426651925, 6337.171439048565, 12936.802930185298, 8380.118133115524, 30625.230235417643, 13469.125006924487, 7266.197764189629, 16568.819737053596, 36965.46457920531, 8668.137147728754, 5197.849644603616, 17122.20757156736, 7248.33488108396, 8584.405095675884, 14550.107674637453, 8444.316163103313, 10751.799791010306, 9534.883087588254, 7772.977878800089, 6332.852771700417, 8354.12339376449, 6147.707688729755, 6312.525397806518, 6401.637508618518, 7230.707500348908, 2831.589146779727, 2748.1265782220657, 2177.29539674723, 1432.879344145404, 1247.0665134331973, 1642.797434475215, 2325.753437120917, 1029.8654565045879, 1124.937374003799, 992.979002832101, 789.401732414922, 1236.3016422146084, 1064.4004964396206, 781.5747144183407, 713.134921680694, 755.0805789952034, 676.1363858791982, 630.1033493510772, 620.1313238817495, 636.6562921427014, 2054.03886800282, 549.7955612830174, 478.78253192915366, 481.19825757731144, 462.621693753628, 520.3780466187209, 475.86274077568396, 469.3951214930122, 4505.717641328497, 7811.086887240407, 10279.68197364536, 9004.901314159591, 4237.373442036592, 3310.973230732262, 887.2150705389089, 3280.0013443179178, 659.8156953563584, 1085.5766602017811, 1320.4837743916037, 761.7537531319446, 15967.878603916797, 1019.9952354215656, 1747.9506620806658, 1505.0858991763505, 3597.0243386960187, 2670.968097289505, 1406.3214137394505, 12785.9916321985, 15296.132152102984, 2770.1447871053642, 21622.037296435792, 3250.4364747981745, 21734.48772136071, 24208.13824087486, 13021.934548244688, 9193.409467692023, 14546.92971215362, 6563.764464095398, 9467.15598911313, 15273.965311248283, 12506.83403128163, 7085.857251739368, 19488.15508486803, 6606.134549613028, 10164.244313681424, 7506.836998516126, 10595.489153632172, 9761.682768349652, 5075.201891173769, 6752.9075267037115, 5741.111558172738, 4646.359488783717, 4522.500161307492, 4583.053945454342], \"Term\": [\"product\", \"taste\", \"skin\", \"great\", \"pain\", \"easy\", \"price\", \"work\", \"week\", \"help\", \"lose\", \"feel\", \"fish_oil\", \"swallow\", \"day\", \"love\", \"joint\", \"hair\", \"vitamin\", \"start\", \"energy\", \"weight\", \"powder\", \"oil\", \"probiotic\", \"good\", \"knee\", \"mix\", \"use\", \"flavor\", \"blood_pressure\", \"migraine\", \"triphala\", \"blood\", \"disease\", \"statin\", \"ibs\", \"thyroid\", \"niacin\", \"hangover\", \"flush\", \"substance\", \"candida\", \"bloating\", \"diarrhea\", \"k2\", \"lower\", \"rhodiola\", \"prescription_medication\", \"stressed\", \"pharmaceutical\", \"insomnia\", \"acid_reflux\", \"ala\", \"reflux\", \"ldl\", \"intestinal\", \"cholesterol\", \"bio\", \"lyme\", \"symptom\", \"stress\", \"medication\", \"cancer\", \"anxiety\", \"bacteria\", \"calm\", \"headache\", \"gut\", \"constipation\", \"colon\", \"brain\", \"flu\", \"asleep\", \"drug\", \"digestive\", \"probiotic\", \"liver\", \"study\", \"allergy\", \"digestion\", \"level\", \"sleep\", \"immune_system\", \"prescription\", \"enzyme\", \"doctor\", \"stomach\", \"detox\", \"sick\", \"help\", \"issue\", \"side_effect\", \"mood\", \"health\", \"suffer\", \"body\", \"supplement\", \"problem\", \"year\", \"effect\", \"life\", \"work\", \"feel\", \"day\", \"improve\", \"recommend\", \"try\", \"natural\", \"start\", \"good\", \"product\", \"month\", \"'s\", \"use\", \"find\", \"time\", \"need\", \"know\", \"like\", \"great\", \"flavor\", \"taste\", \"smoothie\", \"tasting\", \"delicious\", \"tasty\", \"gummie\", \"aftertaste\", \"stevia\", \"gmo\", \"yummy\", \"candy\", \"chocolate\", \"gummy\", \"gelatin\", \"swallow\", \"fishy_taste\", \"vegetarian\", \"chewable\", \"sweetener\", \"taste!.\", \"powdered\", \"liposomal\", \"vanilla\", \"stir\", \"chalky\", \"lemon\", \"fiber\", \"benefiber\", \"blender\", \"mix\", \"powder\", \"fishy\", \"tea\", \"green\", \"filler\", \"juice\", \"chew\", \"vegan\", \"sweet\", \"dissolve\", \"bitter\", \"protein\", \"shake\", \"calcium\", \"fruit\", \"easy\", \"burp\", \"fish_oil\", \"bulk\", \"liquid\", \"water\", \"fish\", \"organic\", \"vitamin\", \"omega-3\", \"drink\", \"omega\", \"add\", \"small\", \"like\", \"capsule\", \"size\", \"good\", \"pill\", \"great\", \"love\", \"brand\", \"supplement\", \"way\", \"product\", \"find\", \"use\", \"need\", \"try\", \"buy\", \"price\", \"recommend\", \"skin\", \"hair\", \"nail\", \"serum\", \"face\", \"krill\", \"wrinkle\", \"acne\", \"biotin\", \"phytoceramide\", \"lotion\", \"fishy_aftertaste\", \"moisturizer\", \"glow\", \"resveratrol\", \"fine_line\", \"phytoceramides\", \"facial\", \"moisturize\", \"breakout\", \"complexion\", \"oily\", \"dha\", \"service\", \"dry_eye\", \"hair_growth\", \"bath\", \"radiant\", \"skin_tone\", \"vitamin_c_serum\", \"customer_service\", \"cream\", \"seller\", \"spray\", \"arrive\", \"epa\", \"star\", \"apply\", \"dry\", \"ship\", \"grow\", \"shipping\", \"product\", \"price\", \"purchase\", \"order\", \"oil\", \"receive\", \"great\", \"company\", \"look\", \"excellent\", \"use\", \"buy\", \"thank\", \"love\", \"good\", \"quality\", \"amazon\", \"recommend\", \"happy\", \"result\", \"work\", \"find\", \"feel\", \"try\", \"time\", \"highly\", \"like\", \"definitely\", \"notice\", \"supplement\", \"knee\", \"curcumin\", \"glucosamine\", \"gym\", \"shoulder\", \"garcinia\", \"walk\", \"appetite\", \"bee\", \"forskolin\", \"second_bottle\", \"chondroitin\", \"i&#8217;ve\", \"hip\", \"cambogia\", \"ankle\", \"bedroom\", \"sex\", \"uti\", \"soreness\", \"mile\", \"gain\", \"athlete\", \"stiff\", \"instant\", \"hunger\", \"stiffness\", \"wrist\", \"shed\", \"workout\", \"weight\", \"lose\", \"joint\", \"pound\", \"arthritis\", \"silver\", \"joint_pain\", \"training\", \"sore\", \"lift\", \"creatine\", \"pain\", \"pump\", \"lb\", \"recovery\", \"weight_loss\", \"muscle\", \"msm\", \"energy\", \"week\", \"exercise\", \"day\", \"turmeric\", \"feel\", \"work\", \"start\", \"notice\", \"try\", \"stuff\", \"time\", \"use\", \"help\", \"month\", \"product\", \"result\", \"like\", \"pill\", \"good\", \"great\", \"lot\", \"recommend\", \"year\", \"thing\", \"definitely\", \"love\"], \"Total\": [173091.0, 35787.0, 22924.0, 97745.0, 17277.0, 25237.0, 23218.0, 68698.0, 27984.0, 56950.0, 10365.0, 59351.0, 19141.0, 11370.0, 55490.0, 37438.0, 9089.0, 9588.0, 24278.0, 30442.0, 21990.0, 7826.0, 9815.0, 14904.0, 14258.0, 114800.0, 7231.0, 8865.0, 80390.0, 7823.0, 2224.340148061269, 1522.2249758036273, 1560.5269717555916, 3000.267598521713, 1987.0705405313192, 1092.8523024402934, 1799.4354895837218, 2044.6757865537934, 1151.8341230663546, 1027.5199065322251, 897.0916643056655, 781.3910613973087, 783.176463369186, 2119.554031151744, 904.0169956245518, 728.7769665980055, 2901.18437280765, 743.0275294086547, 633.4815921521667, 665.095653932614, 629.301273093315, 579.4711286364451, 778.713165397095, 624.4008865160575, 552.9050844872005, 532.6871809092413, 597.6658617081468, 5525.904756554212, 530.786516198639, 482.2617333715964, 4093.672040915456, 4246.57229729649, 3673.1575375503367, 2385.580603898287, 2587.77218452257, 1785.0649266915052, 2292.036293075567, 1986.0959841678998, 1496.9084364419498, 1417.7823793176285, 1612.1238638948441, 3107.2812308708126, 1364.646614957889, 1103.7238275957516, 2304.6878691280135, 2769.71481115632, 14258.183834031766, 5299.7040957353, 2250.444982007402, 3331.3928463061475, 4548.994700307326, 9730.312861761702, 8919.51950295059, 3593.7610633134695, 2558.9284745685222, 1954.1138364064682, 14958.429994231736, 10370.143588497585, 3156.417063568587, 4071.648109625586, 56950.19392038736, 9638.00591520713, 5706.635218818995, 4062.2371481164405, 18682.204950702013, 6085.236370412493, 18450.26025843674, 57023.01089464243, 14860.115641596265, 31726.979482641207, 11289.129004118708, 10120.02330324721, 68698.5954396714, 59351.428504254254, 55490.71679364664, 9124.283556228864, 51089.64486895542, 54939.54471331788, 19424.819581479893, 30442.121644185703, 114800.44633294095, 173091.15381908853, 22909.24409305855, 19218.482014930585, 80390.71616955892, 32069.508179563483, 32655.900183209702, 26177.611004593782, 20122.989712993607, 54786.34771574319, 97745.462038915, 7823.637004280957, 35787.018219184894, 3400.2830135422078, 2522.888497614664, 2143.6685013105225, 1718.5267495779815, 1717.3493362873633, 2799.7266401205093, 1480.1792879796703, 1890.109312805185, 1235.745734313266, 1220.9824229265291, 1182.3473162976072, 998.0577613015654, 936.8014266671407, 11370.071331985746, 1052.6443299872255, 1590.717679803271, 1032.3121272244405, 602.5580597299273, 586.9564006202637, 564.2098809447632, 618.2005710217404, 501.8078859227968, 474.0388921953245, 464.9007498318213, 1414.647572008379, 3944.438843270391, 454.334710800817, 447.0152298874154, 8865.741900197634, 9815.195830497805, 5554.552417671334, 5198.531304237281, 3838.078420015794, 1950.826575138413, 2968.915107992724, 1640.9939223956874, 1647.2851860438725, 2707.8024531679052, 3345.8478704454783, 1319.9658484135152, 3049.90097487806, 2449.7576740111435, 4999.212923382906, 1653.3781219055447, 25237.723779970787, 1819.573904058653, 19141.65151188485, 3056.747046541076, 3378.684463660115, 10147.306998917384, 9552.501874427882, 4357.741754806503, 24278.04278550375, 3423.017854154549, 11013.391464631839, 11434.568126390246, 12645.503623556277, 8986.11212095719, 54786.34771574319, 18874.861830330698, 5131.146752541581, 114800.44633294095, 31806.34101899905, 97745.462038915, 37438.01930631459, 21571.547102195582, 57023.01089464243, 16534.579269001733, 173091.15381908853, 32069.508179563483, 80390.71616955892, 26177.611004593782, 54939.54471331788, 27105.234596138533, 23218.100879604095, 51089.64486895542, 22924.796928075884, 9588.06125395997, 4153.14321659255, 3041.052455262355, 4832.942000369573, 4454.686189639169, 1656.3275820245342, 1832.3954904145053, 1533.508455638439, 1378.3796698460799, 1152.0513720328436, 1227.8922036419094, 905.9060336695068, 898.5266651719389, 897.3061099372027, 822.8807264598804, 679.1399672090049, 575.5818653173782, 540.0943095605077, 532.6891216874343, 534.9070284515122, 700.1805148630767, 2731.5230080923548, 2200.589598594929, 437.2106817307579, 433.30147491690917, 462.1526669573018, 415.8641631400304, 457.9575753478083, 393.0237615332596, 2505.3652628386303, 2408.003868920024, 2634.073719841357, 1136.5304055097477, 4524.198983773052, 1936.493341435854, 7784.881208455448, 1264.8720115564647, 3675.121122735705, 2507.958718455821, 3458.283160724491, 2571.381733910003, 173091.15381908853, 23218.100879604095, 13331.239227772305, 19461.780660173205, 14904.224433442132, 6807.577746537564, 97745.462038915, 9308.522671788267, 22587.77121199864, 14095.363118022002, 80390.71616955892, 27105.234596138533, 11919.444701841685, 37438.01930631459, 114800.44633294095, 16078.755195412952, 7983.299110397747, 51089.64486895542, 15381.347096497238, 21038.058055707676, 68698.5954396714, 32069.508179563483, 59351.428504254254, 54939.54471331788, 32655.900183209702, 17865.683447895513, 54786.34771574319, 16372.428923119616, 21986.534008710725, 57023.01089464243, 7231.4837087725755, 2832.4147677650217, 2749.017153784541, 2178.25206360112, 1433.6726350259128, 1247.88344010431, 1643.9712367298225, 2327.4839615580013, 1030.6914778569005, 1125.8596996193664, 993.7991314202925, 790.160961846768, 1237.53344529491, 1065.5002609003418, 782.383359271018, 713.8884366684456, 755.9020306200597, 676.9085245754933, 630.856116081701, 620.9071645051196, 637.5153321154457, 2056.813615394, 550.6136213190523, 479.5577991377785, 481.9869519735027, 463.3890977706189, 521.2416839241814, 476.65554963792357, 470.1910091071021, 4513.4555993863005, 7826.078305003755, 10365.828495233329, 9089.875682837663, 4269.914933359086, 3334.029183627184, 890.6902641221245, 3338.753299288536, 660.9946631084104, 1094.043203758477, 1337.2943270330632, 764.888848603626, 17277.820913574084, 1031.3392243214462, 1799.0875399721867, 1574.513442303912, 4067.002592416719, 3122.924031406478, 1533.2166495167085, 21990.813739739922, 27984.45554751714, 3525.083577938358, 55490.71679364664, 4680.898585153096, 59351.428504254254, 68698.5954396714, 30442.121644185703, 21986.534008710725, 54939.54471331788, 16195.161251265064, 32655.900183209702, 80390.71616955892, 56950.19392038736, 22909.24409305855, 173091.15381908853, 21038.058055707676, 54786.34771574319, 31806.34101899905, 114800.44633294095, 97745.462038915, 16655.370565905432, 51089.64486895542, 31726.979482641207, 17535.14256143688, 16372.428923119616, 37438.01930631459], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0797, 1.0796, 1.0796, 1.0795, 1.0794, 1.0794, 1.0794, 1.0794, 1.0794, 1.0793, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.079, 1.079, 1.0789, 1.0789, 1.0789, 1.0789, 1.0789, 1.0788, 1.0788, 1.0788, 1.0787, 1.0787, 1.0787, 1.0786, 1.0777, 1.0742, 1.0748, 1.0767, 1.075, 1.0782, 1.0748, 1.0761, 1.0777, 1.0769, 1.0751, 1.0685, 1.0765, 1.0785, 1.0703, 1.0681, 1.0493, 1.059, 1.0699, 1.062, 1.0539, 1.009, 1.01, 1.0255, 1.0409, 1.0615, 0.8878, 0.9176, 1.0207, 0.9892, 0.6631, 0.8305, 0.9103, 0.9617, 0.6427, 0.861, 0.6212, 0.3559, 0.6538, 0.4446, 0.6862, 0.7093, 0.1612, 0.1747, 0.1709, 0.7137, 0.1138, 0.0638, 0.42, 0.2489, -0.3214, -0.496, 0.3028, 0.3795, -0.318, 0.0648, 0.0442, 0.1306, 0.2847, -0.5124, -0.9989, 1.3705, 1.3704, 1.3704, 1.3703, 1.3702, 1.3702, 1.3702, 1.3701, 1.3701, 1.3701, 1.37, 1.37, 1.37, 1.3699, 1.3698, 1.3696, 1.3696, 1.3695, 1.3694, 1.3694, 1.3693, 1.3693, 1.3692, 1.3692, 1.3691, 1.369, 1.369, 1.369, 1.369, 1.369, 1.3675, 1.364, 1.368, 1.3672, 1.3676, 1.3689, 1.3624, 1.3675, 1.367, 1.3618, 1.3594, 1.3685, 1.3533, 1.357, 1.3417, 1.3616, 1.2639, 1.3586, 1.1472, 1.3091, 1.2969, 1.1519, 1.1543, 1.2457, 1.0149, 1.2735, 1.0591, 1.0494, 1.0266, 1.0719, 0.5909, 0.8288, 1.1728, 0.2902, 0.6103, 0.1928, 0.4756, 0.6321, 0.2615, 0.6359, -0.6602, 0.1771, -0.3334, 0.2309, -0.2403, 0.1435, 0.2047, -0.5121, 1.52, 1.5199, 1.5198, 1.5198, 1.5196, 1.5196, 1.5196, 1.5195, 1.5195, 1.5194, 1.5194, 1.5193, 1.5192, 1.5192, 1.5192, 1.5191, 1.5188, 1.5187, 1.5186, 1.5186, 1.5186, 1.5185, 1.5185, 1.5183, 1.5183, 1.5182, 1.5182, 1.5182, 1.5181, 1.5181, 1.5165, 1.5044, 1.5018, 1.5046, 1.4658, 1.4828, 1.4393, 1.4836, 1.4219, 1.4394, 1.4131, 1.3976, 0.9211, 1.1423, 1.1563, 1.089, 1.1215, 1.2328, 0.7589, 1.1356, 0.9627, 1.0001, 0.555, 0.8207, 1.0251, 0.7049, 0.3868, 0.9022, 1.0909, 0.4268, 0.7677, 0.6237, -0.0321, 0.1856, -0.1884, -0.2312, 0.0847, 0.4829, -0.3606, 0.5405, 0.2722, -0.6668, 1.6725, 1.6723, 1.6722, 1.6721, 1.672, 1.6719, 1.6719, 1.6718, 1.6718, 1.6717, 1.6717, 1.6716, 1.6716, 1.6715, 1.6715, 1.6715, 1.6715, 1.6714, 1.6714, 1.6713, 1.6712, 1.6712, 1.6711, 1.6709, 1.6709, 1.6709, 1.6709, 1.6709, 1.6709, 1.6709, 1.6706, 1.6642, 1.6632, 1.6649, 1.6656, 1.6687, 1.6548, 1.6708, 1.6648, 1.6599, 1.6685, 1.5937, 1.6615, 1.6437, 1.6275, 1.5498, 1.5162, 1.5862, 1.1303, 1.0685, 1.4316, 0.7301, 1.3079, 0.668, 0.6295, 0.8234, 0.8006, 0.3437, 0.7694, 0.4344, 0.0118, 0.1567, 0.4991, -0.5114, 0.5142, -0.012, 0.2287, -0.7102, -0.6313, 0.4842, -0.351, -0.0369, 0.3444, 0.386, -0.4278], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.744999885559082, -7.1244001388549805, -7.099599838256836, -6.445899963378906, -6.857999801635742, -7.455900192260742, -6.957200050354004, -6.829500198364258, -7.40339994430542, -7.5177001953125, -7.653600215911865, -7.7916998863220215, -7.789400100708008, -6.793799877166748, -7.645999908447266, -7.861499786376953, -6.480000019073486, -7.842199802398682, -8.001700401306152, -7.953000068664551, -8.008399963378906, -8.090900421142578, -7.795400142669678, -8.016300201416016, -8.137900352478027, -8.175200462341309, -8.060099601745605, -5.835999965667725, -8.178799629211426, -8.274800300598145, -6.13700008392334, -6.103899955749512, -6.248300075531006, -6.677999973297119, -6.598299980163574, -6.966400146484375, -6.719900131225586, -6.8618998527526855, -7.14300012588501, -7.1981000900268555, -7.071499824523926, -6.421899795532227, -7.236700057983398, -7.447000026702881, -6.718900203704834, -6.537300109863281, -4.917500019073486, -5.897500038146973, -6.743100166320801, -6.358699798583984, -6.055300235748291, -5.339900016784668, -5.425899982452393, -6.319399833679199, -6.643599987030029, -6.8927001953125, -5.031000137329102, -5.367599964141846, -6.453999996185303, -6.230899810791016, -3.9189000129699707, -5.5279998779296875, -5.9721999168396, -6.260700225830078, -5.053899765014648, -5.957300186157227, -5.087900161743164, -4.224800109863281, -5.271599769592285, -4.722300052642822, -5.514100074768066, -5.600299835205078, -4.2332000732421875, -4.366000175476074, -4.436999797821045, -5.69950008392334, -4.5767998695373535, -4.554100036621094, -5.237599849700928, -4.959499835968018, -4.202400207519531, -3.966399908065796, -5.189799785614014, -5.288700103759766, -4.555200099945068, -5.091400146484375, -5.093900203704834, -5.228700160980225, -5.337600231170654, -5.1331000328063965, -5.0406999588012695, -5.196499824523926, -3.6761999130249023, -6.029900074005127, -6.328499794006348, -6.491399765014648, -6.712500095367432, -6.713200092315674, -6.2245001792907715, -6.8618998527526855, -6.617400169372559, -7.042500019073486, -7.054500102996826, -7.086699962615967, -7.256199836730957, -7.319699764251709, -4.823500156402588, -7.203199863433838, -6.790500164031982, -7.222899913787842, -7.761300086975098, -7.787600040435791, -7.827199935913086, -7.735899925231934, -7.944499969482422, -8.00160026550293, -8.020999908447266, -6.908199787139893, -5.882800102233887, -8.044099807739258, -8.06029987335205, -5.07450008392334, -4.976200103759766, -5.541600227355957, -5.608500003814697, -5.911600112915039, -6.586999893188477, -6.173500061035156, -6.761300086975098, -6.757999897003174, -6.266200065612793, -6.05709981918335, -6.978000164031982, -6.155799865722656, -6.371099948883057, -5.673099994659424, -6.759699821472168, -4.131899833679199, -6.666999816894531, -4.525100231170654, -6.197700023651123, -6.109799861907959, -5.15500020980835, -5.2129998207092285, -5.906499862670898, -4.419600009918213, -6.120200157165527, -5.165900230407715, -5.1381001472473145, -5.060200214385986, -5.356599807739258, -4.029799938201904, -4.857500076293945, -5.815999984741211, -3.5906999111175537, -4.554100036621094, -3.8489999771118164, -4.5258002281188965, -4.920599937438965, -4.319200038909912, -5.182799816131592, -4.1305999755859375, -4.9791998863220215, -4.5706000328063965, -5.128300189971924, -4.8582000732421875, -5.1809000968933105, -5.274600028991699, -5.202700138092041, -3.97189998626709, -4.843699932098389, -5.6803998947143555, -5.992199897766113, -5.5289998054504395, -5.610599994659424, -6.599999904632568, -6.499000072479248, -6.67710018157959, -6.78380012512207, -6.963200092315674, -6.899499893188477, -7.203700065612793, -7.211900234222412, -7.2133002281188965, -7.300000190734863, -7.492300033569336, -7.657800197601318, -7.721499919891357, -7.735400199890137, -7.731200218200684, -7.461999893188477, -6.1006999015808105, -6.3171000480651855, -7.933199882507324, -7.942200183868408, -7.877799987792969, -7.983399868011475, -7.88700008392334, -8.039899826049805, -6.189199924468994, -6.241000175476074, -6.153800010681152, -6.991499900817871, -5.648900032043457, -6.480500221252441, -5.132599830627441, -6.905600070953369, -5.900599956512451, -6.2652997970581055, -5.970300197601318, -6.282100200653076, -2.5492000579833984, -4.336900234222412, -4.877699851989746, -4.566699981689453, -4.801000118255615, -5.473400115966797, -3.282900094985962, -5.257699966430664, -4.544000148773193, -4.9781999588012695, -3.682300090789795, -4.503699779510498, -5.1209001541137695, -4.296599864959717, -3.4941000938415527, -4.944399833679199, -5.4558000564575195, -4.263700008392334, -5.123300075531006, -4.954100131988525, -4.426499843597412, -4.970600128173828, -4.729000091552734, -4.849100112915039, -5.053400039672852, -5.258299827575684, -4.981299877166748, -5.288000106811523, -5.261600017547607, -5.247499942779541, -4.973199844360352, -5.910699844360352, -5.940700054168701, -6.173500061035156, -6.591899871826172, -6.730800151824951, -6.4552001953125, -6.107500076293945, -6.922100067138672, -6.833799839019775, -6.958600044250488, -7.1880998611450195, -6.739500045776367, -6.889200210571289, -7.197999954223633, -7.289700031280518, -7.232500076293945, -7.342899799346924, -7.413400173187256, -7.4293999671936035, -7.40310001373291, -6.231800079345703, -7.549799919128418, -7.6880998611450195, -7.68310022354126, -7.722400188446045, -7.604800224304199, -7.694200038909912, -7.707900047302246, -5.446199893951416, -4.895999908447266, -4.621399879455566, -4.753799915313721, -5.507599830627441, -5.754300117492676, -7.071199893951416, -5.763700008392334, -7.367400169372559, -6.869500160217285, -6.673600196838379, -7.223700046539307, -4.181000232696533, -6.93179988861084, -6.393099784851074, -6.542699813842773, -5.671500205993652, -5.969099998474121, -6.610599994659424, -4.403200149536133, -4.223999977111816, -5.932700157165527, -3.8778998851776123, -5.772799968719482, -3.8726999759674072, -3.764899969100952, -4.384900093078613, -4.733099937438965, -4.274199962615967, -5.070000171661377, -4.703700065612793, -4.225399971008301, -4.425300121307373, -4.993500232696533, -3.981800079345703, -5.063600063323975, -4.632699966430664, -4.935800075531006, -4.591100215911865, -4.673099994659424, -5.327199935913086, -5.041600227355957, -5.20389986038208, -5.415500164031982, -5.442500114440918, -5.429200172424316]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 2, 3, 1, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 3, 2, 4, 3, 4, 1, 2, 3, 4, 1, 4, 1, 4, 4, 1, 2, 4, 3, 4, 4, 2, 1, 3, 1, 2, 2, 1, 2, 1, 2, 3, 1, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 3, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 2, 1, 2, 1, 2, 2, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 3, 1, 2, 1, 2, 3, 4, 2, 4, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 3, 4, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 1, 2, 4, 1, 2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 2, 3, 1, 2, 3, 4, 1, 2, 4, 3, 4, 3, 1, 2, 3, 4, 1, 2, 2, 3, 1, 2, 3, 4, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 2, 1, 2, 4, 1, 4, 1, 2, 1, 2, 4, 4, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 2, 2, 1, 2, 4, 3, 3, 1, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 4, 1, 4, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 4, 1, 4, 1, 2, 3, 4, 1, 3, 1, 2, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 1, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 1, 3, 3, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 2, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 1, 1, 2, 3, 4, 3, 1, 4, 2, 3, 4, 3, 2, 3, 4, 1, 2, 4, 4, 2, 3, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 3, 3, 1, 3, 4, 1, 2, 3, 4, 2, 1, 3, 4, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 2, 1, 4, 1, 2, 3, 4, 2, 2, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 2, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 2, 1, 2, 2, 3, 4, 1, 2, 3, 4, 3, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 3, 4, 1, 2, 3, 4, 2], \"Freq\": [0.4963451323881499, 0.3017407928209332, 0.13580677173006303, 0.06613425550532954, 0.9990841744704144, 0.0012841698900648, 0.9992384338305758, 0.16306191207433357, 0.7089476439119303, 0.042228448616728476, 0.08580124859391461, 0.9997404603327709, 0.0003571777278788035, 0.9993579661324724, 0.9821717674719743, 0.0039022716922786265, 0.013507863550195245, 0.0006003494911197886, 0.06551176309037766, 0.24588831920919954, 0.651109262990025, 0.037578449191421226, 0.9987554964854288, 0.9950644092246758, 0.00463719336337713, 0.0004296485030687847, 0.9993624181379931, 0.9645244648102789, 0.03557672206267422, 0.00044206720508390584, 0.05039566137956526, 0.9471289868922682, 0.0019893024228775764, 0.00689855989052191, 0.9930926868486106, 0.9984381712592845, 0.0009060237488741239, 0.9988855682182683, 0.9982830166871376, 0.0005602037130679785, 0.001120407426135957, 0.9975058740547995, 0.9988066831632668, 0.99932911266683, 0.999263294674917, 0.9985182061438338, 0.999668436364619, 0.0015151907167930345, 0.9977530870082132, 0.9977288695786247, 0.9987950148407606, 0.0009435947235151258, 0.9995775048457886, 0.0003333036028162016, 0.0003333036028162016, 0.9998470791162201, 0.6319694051290272, 0.1389140296179843, 0.09203122211913278, 0.1370170374070481, 0.9886456267555399, 0.004183721727806647, 0.007401969210734836, 0.23327024140460637, 0.4778516789345553, 0.28092560868678745, 0.007973466121143143, 0.9987063342212597, 0.9402151883166561, 0.0006542903189399137, 0.059213273864062194, 0.006045371378136351, 0.9881434307171963, 0.006045371378136351, 0.07227386256524351, 0.2931537069644844, 0.4969150867234634, 0.1376855819772786, 0.028404471299036074, 0.9715529373198466, 0.9947486463840344, 0.0013088797978737295, 0.003926639393621189, 0.9995100109601319, 0.9968223233011287, 0.0008383703307831192, 0.002095925826957798, 0.0004191851653915596, 0.9984978310454774, 0.9991953832355961, 0.3048499136959874, 0.5817261126836881, 0.0415367279001832, 0.07184158550082706, 0.9980624900429884, 0.003046933892784014, 0.9969567697189294, 0.0009686992660724451, 0.9987289433206908, 0.9997062485000644, 0.9985695090844922, 0.0005428975221553963, 0.0007238633628738616, 0.998530727405142, 0.994960769406876, 0.0031014986577521074, 0.0018608991946512644, 0.08186046560421727, 0.2340865545296449, 0.6807739770786415, 0.0032228529765439867, 0.9983043250447877, 0.9966268593915448, 0.002821307457583991, 0.0004152817247957721, 0.0004152817247957721, 0.9846329694907757, 0.014950142092647797, 0.003922138498262554, 0.9962231785586888, 0.9998535639025251, 0.003193147170459223, 0.9962619171832776, 0.0003991433963074029, 0.40287819822431326, 0.12623733130082815, 0.08123881363370926, 0.3896507605119923, 0.20620031492289626, 0.1420681079711661, 0.3755093412754639, 0.2762571162311196, 0.9996881507984495, 0.9425243686385727, 0.056076239747572226, 0.0012672596553123666, 0.0014643845166779415, 0.9983441442451867, 0.9988750259901373, 0.9742811966126447, 0.020883735035695225, 0.0048362333766873156, 0.988188382780586, 0.011553536079276124, 0.00036104800247737886, 0.9994612468407724, 0.01016184875000701, 0.9889869856992116, 0.0008966337132359127, 0.825153442223529, 0.04646209530465468, 0.1095034706604667, 0.018919097800312625, 0.06964248955130005, 0.7323811221913771, 0.1979408438355073, 0.9901557736160612, 0.00043389823558986034, 0.009111862947387066, 0.0013604993775764526, 0.07918106377494954, 0.906636785216948, 0.013060794024733945, 0.9972308962672978, 0.02246636839927631, 0.8988132288698127, 0.04889505926756079, 0.029836288191631505, 0.6744541582634161, 0.04083574559488243, 0.11205470320504615, 0.1726439656495138, 0.2601086102449817, 0.09972345843832953, 0.05875180497141894, 0.5814245962574014, 0.9815190723622939, 0.018422672891054526, 0.036147813422429335, 0.9635974263750449, 0.17161672102701087, 0.22198789586338033, 0.5945217537025015, 0.011918813200718407, 0.2136119565256922, 0.00028368121716559387, 0.7857969715486951, 0.9995981742860922, 0.00020691330455104373, 0.9989890833043231, 0.40440475663158737, 0.04823809758504436, 0.18115823445141352, 0.3661916915519923, 0.0015211289205907205, 0.998367614881043, 0.9985510884594072, 0.00153780968448574, 0.3623067723690346, 0.30315400989515057, 0.26330307133868047, 0.07122030020577287, 0.9989297033803801, 0.00010468461698782885, 0.805548127721343, 0.19439933374639817, 0.0007836314432266545, 0.7997742509571236, 0.19940808125307602, 0.9973800917559015, 0.0025204551055203286, 0.9992733860193401, 0.000949988492325927, 0.9993878939268752, 0.9999185795199077, 0.9965950049580914, 0.0021983713344663784, 0.0014655808896442521, 0.998783107290925, 0.9992364060818083, 0.00846751255173546, 0.9913037908781727, 0.0004861889247113145, 0.0004861889247113145, 0.9986320513570399, 0.9992920491803016, 0.999144507422462, 0.9994138569367463, 0.9996299936567727, 0.0005290699290380517, 0.9994130959528797, 0.2462359764527218, 0.3394760320615351, 0.3219935216348826, 0.09229058194837228, 0.12505951422208542, 0.30796314603347635, 0.46711119930889855, 0.09987164412925376, 0.0028660175213290023, 0.9968530033277058, 0.024000348190867042, 0.054940556099575155, 0.8987118334604189, 0.022554544082983487, 0.9997965840263353, 0.9989401802755524, 0.9973889943120102, 0.0020041305980817354, 0.9994251980190713, 0.99988931506257, 0.9993042375012294, 0.9994940180439134, 0.24048608855867804, 0.1397146808090292, 0.4712201053996481, 0.14862157297786915, 0.9959236692322845, 0.0005035003383378587, 0.003524502368365011, 0.6457481882804565, 0.1650768743913236, 0.18621998897776096, 0.002943977980390012, 0.6590495556954323, 0.012993810012911835, 0.10835783998605261, 0.21961294842093015, 0.3479855678699116, 0.19226804337029857, 0.35447846249318804, 0.10528564471019844, 0.0009385262835646849, 0.9985919657128248, 0.9991603216983506, 0.0008080589690743229, 0.9987608857758631, 0.9992022556006973, 0.000555729841824637, 0.9469188240529306, 0.03728684173467314, 0.015582560724938029, 0.6932051115051241, 0.005589479950476099, 0.2206200615746743, 0.08066386752059626, 0.9991869678864697, 0.9979523263659699, 0.9988858963665018, 0.7791030702888528, 0.052292974753706464, 0.10022820161127072, 0.06847889551080609, 0.0005500625282962184, 0.0005500625282962184, 0.008250937924443277, 0.9906626134614894, 0.008086850863091175, 0.009584415837737687, 0.982402623368113, 0.005389173963555178, 0.991944832666875, 0.002694586981777589, 0.9989338760229588, 0.9999331107153033, 0.4514239747453816, 0.22307818390929304, 0.14769177157015348, 0.1778065809818335, 0.00022448270370331076, 0.9996214795908428, 0.0005558373218545328, 0.02779186609272664, 0.9716036386017234, 0.9987099728811413, 0.9981284582388105, 0.0014137796858906665, 0.9314191772410406, 0.06217682911076129, 0.004213636352960682, 0.0022609756040276835, 0.690215801949658, 0.0453556267852388, 0.11373491596908467, 0.15069135260890887, 0.011964456646950554, 0.0007477785404344096, 0.9870676733734207, 0.20342659192807291, 0.45856314661363623, 0.15248324351432221, 0.1855206711850097, 0.9980579587305198, 0.055642958678757584, 0.9290598260245747, 0.01272684693184349, 0.002367785475691812, 0.9791112685282969, 0.014906492621648766, 0.005849383180646984, 0.10510111766755144, 0.1823553090449218, 0.5727435380223728, 0.139810164108731, 0.007910607438441342, 0.00019294164484003274, 0.00028941246726004913, 0.9917200544777683, 0.28213121896063614, 0.22683374020712566, 0.18630627206530198, 0.3047065197329705, 0.9990873913626018, 0.0263902850179191, 0.4086220447410186, 0.4425714903460542, 0.12241566420761461, 0.9989023886804655, 0.0006893736291790653, 0.0003446868145895326, 0.9994572794118112, 0.9947844497943551, 0.00027224533382439934, 0.0010889813352975973, 0.0038114346735415904, 0.9998522059437971, 0.999191655338334, 0.0006767623135821547, 0.996870887906514, 0.0015791120650250278, 0.0009023497514428731, 0.9979738546747546, 0.9989998591069796, 0.4596397837146694, 0.019206220781999482, 0.21183588512509882, 0.3093074555937462, 0.8884267137563361, 0.0034463768336349975, 0.10068343749690814, 0.007631262988763208, 0.0013044470920860586, 0.01826225928920482, 0.06326568396617384, 0.9170263057364992, 0.14377551150284718, 0.0009606381614889566, 0.8552881764456678, 0.9997247346087218, 0.5168130369443139, 0.14800652268302644, 0.268779845192376, 0.06646136375088248, 0.38693370446304326, 0.3199298820098714, 0.15975483779883967, 0.1333964355795189, 0.9992758305647917, 0.27225755535767665, 0.022468298086650892, 0.28713029518426536, 0.4181195633817443, 0.00026838028492276266, 0.3284303736742308, 0.67128618766306, 6.709507123069066e-05, 0.0014282031258689828, 0.9983139849824191, 0.725344403769652, 0.2746059112414646, 0.0023371189812201316, 0.9073864444587161, 0.08997908077697507, 0.0868882467399543, 0.13118018564583284, 0.6497863798187239, 0.1321564580811132, 0.017440225758255063, 0.8825672140295918, 0.09982234480053885, 0.00022947665471388242, 0.07576186873030974, 0.924190618705566, 0.9995212577723955, 0.9997245535070013, 0.9983214546867419, 0.23410426227752013, 0.4675482788515984, 0.06231461829627248, 0.23602211884466068, 0.0004683934062420832, 0.007025901093631249, 0.9922914311238533, 0.0015282425597044082, 0.9934595466451789, 0.004992259028367733, 0.9978556190069956, 0.9617306714346384, 0.00039078857027006846, 0.037124914175656505, 0.0011723657108102053, 0.9992397693032712, 0.0018089334790036525, 0.3116533965254864, 0.6854135091634316, 0.0011198159631927373, 0.9697588529471329, 0.025318792645831707, 0.004909461177862104, 0.6529558876944048, 0.10282557934628987, 0.13095456636641367, 0.11325618458102478, 0.2067812202431159, 0.13122565480001494, 0.5494041601882989, 0.11258807610912615, 0.007541228449530096, 0.9829827344213578, 0.009508505436364034, 0.008726517704125344, 0.0009696130782361494, 0.0009696130782361494, 0.9890053398008724, 0.03938118512690806, 0.1869293587357236, 0.6950591645446288, 0.0786123466914279, 0.06051463488153508, 0.3995334167307105, 0.5390964595613011, 0.0008085203016032436, 0.9979220062303387, 0.013220561461200403, 0.14160690276219098, 0.7503403104867962, 0.09504114739329622, 0.3805076361337696, 0.1521834810154351, 0.3351364066811936, 0.1321794273051104, 0.043823061871758634, 0.9558508422753151, 0.998363038227366, 0.2739796603249801, 0.00394523105603285, 0.4080224504215179, 0.3140023657367832, 0.9996588567337137, 0.9986171045244145, 0.9991958823518486, 0.01784308451429016, 0.9817492883820076, 0.00037964009604872685, 0.9996539174256814, 0.001363271007876931, 0.9983688014352058, 0.9986578325689383, 0.0004082036401431643, 0.9866281982260281, 0.013062516484581257, 0.99746696750037, 0.0773537453277732, 0.9226627148890061, 0.11355762396118034, 0.8847383373687853, 0.0015555838898791829, 0.9995308308120838, 0.913143744227419, 0.07146983043845587, 0.007368023756541842, 0.007859225340311297, 0.8437546496964421, 0.0028037537684616975, 0.01752346105288561, 0.13598205777039232, 0.0011227247453811707, 0.0033681742361435126, 0.9958568491530985, 0.007015975518955549, 0.8206742475089394, 0.0354696540124975, 0.13700641082849307, 0.9999652372896307, 0.9979090304444008, 0.9323372180809801, 0.00044845465035160177, 0.06715608389015237, 0.08023492143153896, 0.7418113540397208, 0.07378051721097133, 0.10427201301158391, 0.9999167676510806, 0.0018280813711279394, 0.006398284798947788, 0.992648184522471, 0.998538969177715, 0.014957804839700082, 0.9845755068014348, 0.005138164466344652, 0.03905004994421936, 0.9224289758205236, 0.03326961491958162, 0.4355149800320246, 0.011891418220817084, 0.12482704209697491, 0.42776256373337035, 0.9992201119598776, 0.9992032803125627, 0.9988368469060843, 0.9976178345622067, 0.9978084241347513, 0.8500364459540822, 0.14589961914106936, 0.0010607374821888717, 0.0029893510861686385, 0.9942136161647044, 0.00023548403983057896, 0.0035322605974586847, 0.0021193563584752106, 0.9983526370588417, 0.9900264249129161, 0.007109705026304604, 0.001777426256576151, 0.0013330696924321133, 0.18431431176808014, 0.21969525000697793, 0.1906742360937459, 0.4053062453754365, 0.9994995317752812, 0.8032555684716423, 0.010845922160214483, 0.18585966610913, 0.48471660065562944, 0.32984929601055474, 0.1122704658971541, 0.07316344637971367, 0.0007915517622727331, 0.9990262741839974, 8.795019580808147e-05, 0.0007386063180717504, 0.9912096788522892, 0.008124669498789256, 0.9990738490326103, 0.9976373190576124, 0.002198515149735189, 8.382928081981818e-05, 0.9998038892443648, 8.382928081981818e-05, 2.7943093606606058e-05, 0.9983705763847995, 0.9996478252544636, 0.9996934877050294, 0.0023083442798966887, 0.9966276428453953, 0.00019236202332472405, 0.0009618101166236203, 0.16913539602130173, 0.08372873275260868, 0.6095921564934417, 0.13750640579311185, 0.41088916013977367, 0.18294690155841692, 0.14120215968162103, 0.26495364857867987, 0.9991804145357354, 0.0004890750927732429, 0.35491289276904064, 0.11716106365266174, 0.2380274301547673, 0.2899016700469808, 0.0015128715189580723, 0.9984952025123277, 0.9996623116645021, 0.36194329792433977, 0.19971042820346271, 0.1735544051148393, 0.2647819539806573, 0.22004322060447123, 0.08160826239893981, 0.004059049700470828, 0.6943111329752734, 0.24708076935283488, 0.18196130967594365, 0.38095194892164164, 0.18999706343931932, 0.9986429297269583, 0.9983900493685723, 0.0030352971315234264, 0.9961845185659886, 0.9989201856337678, 0.0006286470645901623, 0.0006286470645901623, 0.10692789459742008, 0.7007154633633708, 0.1922313112812633, 8.237896348029283e-05, 0.9973951663144598, 0.0006082831485477774, 0.9994092130639983, 0.036364327997504026, 0.8035629552619183, 0.0011825797722765537, 0.15885988274248372, 0.257702353998709, 0.4796614338333164, 0.14775096240761468, 0.11491069528222181, 0.2561064655279983, 0.0005360118575303439, 0.19675208583747159, 0.5465891581856094, 0.0016611129474245355, 0.00025555583806531314, 0.9980733255640805, 0.09884429401386763, 0.011310541106064454, 0.005409389224639522, 0.8844351382285618, 0.39898923441703354, 0.03682753604797865, 0.21179472312177444, 0.35237983899188424, 0.0015509181038474817, 0.998348139419536, 0.9998022238909203, 0.9986246889637149, 0.5297069016354069, 0.0966685152514455, 0.19267513326771646, 0.18095009653033864, 0.9993965309427668], \"Term\": [\"'s\", \"'s\", \"'s\", \"'s\", \"acid_reflux\", \"acid_reflux\", \"acne\", \"add\", \"add\", \"add\", \"add\", \"aftertaste\", \"aftertaste\", \"ala\", \"allergy\", \"allergy\", \"allergy\", \"allergy\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"ankle\", \"anxiety\", \"anxiety\", \"appetite\", \"appetite\", \"apply\", \"apply\", \"arrive\", \"arrive\", \"arrive\", \"arrive\", \"arthritis\", \"arthritis\", \"asleep\", \"asleep\", \"athlete\", \"bacteria\", \"bacteria\", \"bacteria\", \"bath\", \"bedroom\", \"bee\", \"benefiber\", \"bio\", \"biotin\", \"bitter\", \"bitter\", \"blender\", \"bloating\", \"bloating\", \"blood\", \"blood\", \"blood\", \"blood_pressure\", \"body\", \"body\", \"body\", \"body\", \"brain\", \"brain\", \"brain\", \"brand\", \"brand\", \"brand\", \"brand\", \"breakout\", \"bulk\", \"bulk\", \"bulk\", \"burp\", \"burp\", \"burp\", \"buy\", \"buy\", \"buy\", \"buy\", \"calcium\", \"calcium\", \"calm\", \"calm\", \"calm\", \"cambogia\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"candida\", \"candy\", \"capsule\", \"capsule\", \"capsule\", \"capsule\", \"chalky\", \"chew\", \"chew\", \"chewable\", \"chewable\", \"chocolate\", \"cholesterol\", \"cholesterol\", \"cholesterol\", \"chondroitin\", \"colon\", \"colon\", \"colon\", \"company\", \"company\", \"company\", \"company\", \"complexion\", \"constipation\", \"constipation\", \"cream\", \"cream\", \"cream\", \"cream\", \"creatine\", \"creatine\", \"curcumin\", \"customer_service\", \"customer_service\", \"customer_service\", \"day\", \"day\", \"day\", \"day\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"detox\", \"detox\", \"detox\", \"dha\", \"dha\", \"diarrhea\", \"digestion\", \"digestion\", \"digestion\", \"digestive\", \"digestive\", \"digestive\", \"disease\", \"dissolve\", \"dissolve\", \"dissolve\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"drink\", \"drink\", \"drink\", \"drug\", \"drug\", \"drug\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry_eye\", \"easy\", \"easy\", \"easy\", \"easy\", \"effect\", \"effect\", \"effect\", \"effect\", \"energy\", \"energy\", \"energy\", \"energy\", \"enzyme\", \"enzyme\", \"epa\", \"epa\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"exercise\", \"exercise\", \"exercise\", \"face\", \"face\", \"facial\", \"feel\", \"feel\", \"feel\", \"feel\", \"fiber\", \"fiber\", \"filler\", \"filler\", \"find\", \"find\", \"find\", \"find\", \"fine_line\", \"fish\", \"fish\", \"fish\", \"fish_oil\", \"fish_oil\", \"fish_oil\", \"fishy\", \"fishy\", \"fishy_aftertaste\", \"fishy_taste\", \"fishy_taste\", \"flavor\", \"flu\", \"flu\", \"flu\", \"flush\", \"forskolin\", \"fruit\", \"fruit\", \"gain\", \"gain\", \"gain\", \"garcinia\", \"gelatin\", \"glow\", \"glucosamine\", \"gmo\", \"gmo\", \"good\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"great\", \"green\", \"green\", \"grow\", \"grow\", \"grow\", \"grow\", \"gummie\", \"gummy\", \"gut\", \"gut\", \"gym\", \"hair\", \"hair_growth\", \"hangover\", \"happy\", \"happy\", \"happy\", \"happy\", \"headache\", \"headache\", \"headache\", \"health\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"help\", \"highly\", \"highly\", \"highly\", \"highly\", \"hip\", \"hip\", \"hunger\", \"i&#8217;ve\", \"i&#8217;ve\", \"ibs\", \"ibs\", \"immune_system\", \"immune_system\", \"immune_system\", \"improve\", \"improve\", \"improve\", \"improve\", \"insomnia\", \"instant\", \"intestinal\", \"issue\", \"issue\", \"issue\", \"issue\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint_pain\", \"joint_pain\", \"joint_pain\", \"juice\", \"juice\", \"juice\", \"k2\", \"knee\", \"know\", \"know\", \"know\", \"know\", \"krill\", \"krill\", \"lb\", \"lb\", \"lb\", \"ldl\", \"lemon\", \"lemon\", \"level\", \"level\", \"level\", \"level\", \"life\", \"life\", \"life\", \"life\", \"lift\", \"lift\", \"lift\", \"like\", \"like\", \"like\", \"like\", \"liposomal\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liver\", \"liver\", \"liver\", \"look\", \"look\", \"look\", \"look\", \"lose\", \"lose\", \"lose\", \"lose\", \"lot\", \"lot\", \"lot\", \"lot\", \"lotion\", \"love\", \"love\", \"love\", \"love\", \"lower\", \"lower\", \"lower\", \"lyme\", \"medication\", \"medication\", \"medication\", \"medication\", \"migraine\", \"mile\", \"mix\", \"mix\", \"mix\", \"mix\", \"moisturize\", \"moisturizer\", \"month\", \"month\", \"month\", \"month\", \"mood\", \"mood\", \"mood\", \"mood\", \"msm\", \"msm\", \"msm\", \"msm\", \"muscle\", \"muscle\", \"muscle\", \"nail\", \"natural\", \"natural\", \"natural\", \"natural\", \"need\", \"need\", \"need\", \"need\", \"niacin\", \"notice\", \"notice\", \"notice\", \"notice\", \"oil\", \"oil\", \"oil\", \"oil\", \"oily\", \"oily\", \"omega\", \"omega\", \"omega-3\", \"omega-3\", \"omega-3\", \"order\", \"order\", \"order\", \"order\", \"organic\", \"organic\", \"organic\", \"organic\", \"pain\", \"pain\", \"pharmaceutical\", \"phytoceramide\", \"phytoceramides\", \"pill\", \"pill\", \"pill\", \"pill\", \"pound\", \"pound\", \"pound\", \"powder\", \"powder\", \"powder\", \"powdered\", \"prescription\", \"prescription\", \"prescription\", \"prescription\", \"prescription_medication\", \"price\", \"price\", \"price\", \"price\", \"probiotic\", \"probiotic\", \"probiotic\", \"problem\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"product\", \"protein\", \"protein\", \"protein\", \"pump\", \"pump\", \"pump\", \"pump\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"quality\", \"quality\", \"quality\", \"quality\", \"radiant\", \"receive\", \"receive\", \"receive\", \"receive\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recovery\", \"recovery\", \"reflux\", \"result\", \"result\", \"result\", \"result\", \"resveratrol\", \"rhodiola\", \"second_bottle\", \"seller\", \"seller\", \"seller\", \"serum\", \"service\", \"service\", \"sex\", \"shake\", \"shake\", \"shake\", \"shed\", \"ship\", \"ship\", \"shipping\", \"shipping\", \"shipping\", \"shoulder\", \"sick\", \"sick\", \"sick\", \"sick\", \"side_effect\", \"side_effect\", \"side_effect\", \"side_effect\", \"silver\", \"silver\", \"silver\", \"size\", \"size\", \"size\", \"size\", \"skin\", \"skin_tone\", \"sleep\", \"sleep\", \"sleep\", \"small\", \"small\", \"small\", \"small\", \"smoothie\", \"sore\", \"sore\", \"sore\", \"soreness\", \"spray\", \"spray\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"statin\", \"stevia\", \"stiff\", \"stiffness\", \"stir\", \"stomach\", \"stomach\", \"stomach\", \"stomach\", \"stress\", \"stress\", \"stress\", \"stress\", \"stressed\", \"study\", \"study\", \"study\", \"study\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"substance\", \"suffer\", \"suffer\", \"suffer\", \"supplement\", \"supplement\", \"supplement\", \"supplement\", \"swallow\", \"swallow\", \"swallow\", \"sweet\", \"sweet\", \"sweet\", \"sweetener\", \"symptom\", \"symptom\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste!.\", \"tasting\", \"tasty\", \"tea\", \"tea\", \"tea\", \"tea\", \"thank\", \"thank\", \"thank\", \"thank\", \"thing\", \"thing\", \"thing\", \"thing\", \"thyroid\", \"thyroid\", \"time\", \"time\", \"time\", \"time\", \"training\", \"training\", \"triphala\", \"try\", \"try\", \"try\", \"try\", \"turmeric\", \"turmeric\", \"turmeric\", \"turmeric\", \"use\", \"use\", \"use\", \"use\", \"uti\", \"vanilla\", \"vegan\", \"vegan\", \"vegetarian\", \"vegetarian\", \"vegetarian\", \"vitamin\", \"vitamin\", \"vitamin\", \"vitamin\", \"vitamin_c_serum\", \"walk\", \"walk\", \"water\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"week\", \"week\", \"week\", \"week\", \"weight\", \"weight\", \"weight\", \"weight_loss\", \"weight_loss\", \"weight_loss\", \"weight_loss\", \"work\", \"work\", \"work\", \"work\", \"workout\", \"workout\", \"wrinkle\", \"wrist\", \"year\", \"year\", \"year\", \"year\", \"yummy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el350631396733018616245428103129\", ldavis_el350631396733018616245428103129_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el350631396733018616245428103129\", ldavis_el350631396733018616245428103129_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el350631396733018616245428103129\", ldavis_el350631396733018616245428103129_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"help\" + 0.019*\"product\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.015*\"work\" + 0.013*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"recommend\"'),\n",
       " (1,\n",
       "  '0.028*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"product\" + 0.016*\"easy\" + 0.013*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"fish_oil\" + 0.011*\"love\"'),\n",
       " (2,\n",
       "  '0.078*\"product\" + 0.038*\"great\" + 0.030*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.014*\"recommend\" + 0.014*\"love\" + 0.013*\"price\" + 0.012*\"work\" + 0.011*\"buy\"'),\n",
       " (3,\n",
       "  '0.023*\"work\" + 0.021*\"feel\" + 0.021*\"day\" + 0.019*\"product\" + 0.015*\"pain\" + 0.015*\"week\" + 0.015*\"use\" + 0.014*\"try\" + 0.012*\"start\" + 0.012*\"energy\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217530/217530 [03:09<00:00, 1147.45it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_lda = []\n",
    "\n",
    "for doc_bow in tqdm(bow_corpus):\n",
    "    reviews_lda.append(lda[doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.48668307), (1, 0.12851799), (2, 0.38285732)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_lda[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(reviews_lda, bucket_name, filepath='amazon_reviews/kk/doc2topic_tier1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews_lda = load_df_s3(bucket_name, filepath='amazon_reviews/kk/doc2topic_tier1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.48668307), (1, 0.12851799), (2, 0.38285732)],\n",
       " [(0, 0.31406406), (2, 0.67967421)],\n",
       " [(0, 0.30542588), (1, 0.24088043), (2, 0.44061708), (3, 0.013076626)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews_lda is a list with one element (sublist) per document\n",
    "# each sublist contains is a list of tuples of the format (topic_num, probability)\n",
    "reviews_lda[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each document, get the topic number with the highest probability\n",
    "def get_first_order_topic(reviews_lda):\n",
    "    first_order_topics = []\n",
    "    for doc in reviews_lda:\n",
    "        topic_max_prob = sorted(doc, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        first_order_topics.append(topic_max_prob)\n",
    "    return first_order_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_order_topics = get_first_order_topic(reviews_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217530"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_order_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 68797), (2, 59465), (1, 56018), (3, 33250)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(first_order_topics).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews_temp = pd.DataFrame(unigram_sents_pos_df.groupby(('review_number'))['preprocessed_review'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bioavailability, key, -PRON-, start, -PRON-, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-PRON-, find, longer, -PRON-, wish, -PRON-, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[resveratrol, product, -PRON-, need, resveratr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kid, love, bracelet, -PRON-, buy, bracelet, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[pleasant, surprise, -PRON-, buy, week, -PRON-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             preprocessed_review\n",
       "review_number                                                   \n",
       "1              [bioavailability, key, -PRON-, start, -PRON-, ...\n",
       "2              [-PRON-, find, longer, -PRON-, wish, -PRON-, p...\n",
       "3              [resveratrol, product, -PRON-, need, resveratr...\n",
       "4              [kid, love, bracelet, -PRON-, buy, bracelet, -...\n",
       "5              [pleasant, surprise, -PRON-, buy, week, -PRON-..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reviews_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct a df with the first order topics as a column\n",
    "docs_first_order_lda = pd.DataFrame({'review_num': tokenized_reviews_temp.index.tolist(),\n",
    "                                     'tokenized_reviews': tokenized_reviews, \n",
    "                                     'first_order_topics': first_order_topics,\n",
    "                                    })\n",
    "\n",
    "del tokenized_reviews_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_order_topics</th>\n",
       "      <th>review_num</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[bioavailability, key, start, parent, die, can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[find, longer, wish, product, like, stay, mout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[resveratrol, product, need, resveratrol, pola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[kid, love, bracelet, buy, bracelet, ymca, kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[pleasant, surprise, buy, week, day, wear, bra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_order_topics  review_num  \\\n",
       "0                   0           1   \n",
       "1                   2           2   \n",
       "2                   2           3   \n",
       "3                   3           4   \n",
       "4                   3           5   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [bioavailability, key, start, parent, die, can...  \n",
       "1  [find, longer, wish, product, like, stay, mout...  \n",
       "2  [resveratrol, product, need, resveratrol, pola...  \n",
       "3  [kid, love, bracelet, buy, bracelet, ymca, kid...  \n",
       "4  [pleasant, surprise, buy, week, day, wear, bra...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_first_order_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217530, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_first_order_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_df_s3(docs_first_order_lda, bucket_name, filepath='amazon_reviews/kk/docs_first_order_lda_v1.pkl', filetype='pickle')\n",
    "docs_first_order_lda = load_df_s3(bucket_name, filepath='amazon_reviews/kk/docs_first_order_lda_v1.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LDA for Topic 0 Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that topic # 1 in the LDA vis plot has index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_topic_0 = docs_first_order_lda[docs_first_order_lda.first_order_topics == 0].tokenized_reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bioavailability', 'key', 'start', 'parent', 'die', 'cancer', 'suppose', 'enhance', 'immune_system', 'story', 'resveratrol', 'minutes', 'incredibly', 'inspiring', 'research', 'internet', 'indicate', 'resveratrol', 'lozenge', 'form', 'preferable', 'stomach', 'acid', 'break', 'ez', 'melt', 'formula', 'recommend', 'review', 'ok', 'dissolve', 'mouth', 'quickly', 'lozenge', 'formula', 'dissolve', 'slowly', 'preferable', 'accord', 'research', 'great', 'side_effect', 'cold', 'sore_throat', 'product', 'soon', 'start', 'start', 'come', 'cold', 'usual', 'symptom', 'anticipate', 'sick', 'day', 'usual', 'pattern', 'day', 'sick', 'anticipate', 'taking', 'reason', 'come', 'product', 'cold', 'sore_throat', 'great', 'recommend', 'know', 'gift', 'family', 'product']\n"
     ]
    }
   ],
   "source": [
    "print(docs_topic_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_topic_1 = docs_first_order_lda[docs_first_order_lda.first_order_topics == 1].tokenized_reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_topic_2 = docs_first_order_lda[docs_first_order_lda.first_order_topics == 2].tokenized_reviews.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic 0\n",
    "vocab_dictionary_0 = Dictionary(docs_topic_0)\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(vocab_dictionary_0.dfs) if docfreq == 1]\n",
    "vocab_dictionary_0.filter_tokens(bad_ids=once_ids)  # remove words that appear only once\n",
    "vocab_dictionary_0.filter_extremes(no_below=10, no_above=1.0)\n",
    "vocab_dictionary_0.compactify() \n",
    "save_df_s3(vocab_dictionary_0, bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic_0.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dictionary_0 = load_df_s3(bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic_0.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8233 unique tokens: ['accord', 'acid', 'anticipate', 'bioavailability', 'break']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus_0 = [vocab_dictionary_0.doc2bow(review) for review in docs_topic_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.33333334, 0.33333334, 0.33333334]\n",
      "INFO : using symmetric eta at 0.3333333333333333\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 3 topics, 2 passes over the supplied corpus of 68797 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6431/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13363323, 0.064715683, 0.21139389]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.134): 0.021*\"good\" + 0.020*\"product\" + 0.018*\"work\" + 0.013*\"day\" + 0.013*\"help\" + 0.012*\"feel\" + 0.012*\"great\" + 0.011*\"use\" + 0.011*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #1 (0.065): 0.018*\"product\" + 0.016*\"work\" + 0.015*\"use\" + 0.013*\"year\" + 0.012*\"help\" + 0.011*\"day\" + 0.011*\"recommend\" + 0.010*\"time\" + 0.010*\"good\" + 0.008*\"month\"\n",
      "INFO : topic #2 (0.211): 0.024*\"product\" + 0.020*\"help\" + 0.019*\"use\" + 0.018*\"good\" + 0.015*\"day\" + 0.013*\"great\" + 0.013*\"work\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.010*\"year\"\n",
      "INFO : topic diff=2.924262, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6958/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11993096, 0.066454343, 0.16335139]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.120): 0.021*\"good\" + 0.021*\"product\" + 0.018*\"work\" + 0.014*\"feel\" + 0.013*\"help\" + 0.013*\"day\" + 0.010*\"great\" + 0.010*\"use\" + 0.010*\"recommend\" + 0.010*\"like\"\n",
      "INFO : topic #1 (0.066): 0.018*\"product\" + 0.017*\"work\" + 0.015*\"use\" + 0.013*\"year\" + 0.013*\"help\" + 0.012*\"day\" + 0.011*\"recommend\" + 0.010*\"time\" + 0.010*\"month\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.163): 0.026*\"product\" + 0.021*\"help\" + 0.019*\"use\" + 0.018*\"good\" + 0.014*\"day\" + 0.014*\"great\" + 0.014*\"work\" + 0.010*\"year\" + 0.010*\"try\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.793205, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6971/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11453811, 0.067449234, 0.15180151]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.115): 0.022*\"product\" + 0.021*\"good\" + 0.017*\"work\" + 0.013*\"feel\" + 0.013*\"day\" + 0.013*\"help\" + 0.010*\"use\" + 0.010*\"recommend\" + 0.010*\"great\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.067): 0.019*\"product\" + 0.018*\"work\" + 0.015*\"use\" + 0.013*\"year\" + 0.013*\"help\" + 0.012*\"day\" + 0.011*\"recommend\" + 0.011*\"month\" + 0.010*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #2 (0.152): 0.027*\"product\" + 0.020*\"help\" + 0.019*\"use\" + 0.018*\"good\" + 0.014*\"work\" + 0.014*\"great\" + 0.013*\"day\" + 0.011*\"supplement\" + 0.010*\"year\" + 0.010*\"try\"\n",
      "INFO : topic diff=0.525663, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6981/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11202593, 0.06854111, 0.14450575]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.112): 0.022*\"product\" + 0.021*\"good\" + 0.017*\"work\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"help\" + 0.010*\"use\" + 0.010*\"recommend\" + 0.010*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.069): 0.019*\"product\" + 0.019*\"work\" + 0.015*\"use\" + 0.014*\"year\" + 0.014*\"day\" + 0.013*\"help\" + 0.011*\"recommend\" + 0.010*\"month\" + 0.010*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #2 (0.145): 0.028*\"product\" + 0.019*\"help\" + 0.019*\"use\" + 0.018*\"good\" + 0.014*\"great\" + 0.014*\"work\" + 0.012*\"day\" + 0.011*\"supplement\" + 0.010*\"year\" + 0.009*\"try\"\n",
      "INFO : topic diff=0.418021, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6974/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10982275, 0.069112621, 0.14700489]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.110): 0.022*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"help\" + 0.010*\"use\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"great\"\n",
      "INFO : topic #1 (0.069): 0.018*\"work\" + 0.018*\"product\" + 0.015*\"use\" + 0.014*\"day\" + 0.014*\"year\" + 0.014*\"help\" + 0.011*\"recommend\" + 0.010*\"time\" + 0.010*\"try\" + 0.010*\"month\"\n",
      "INFO : topic #2 (0.147): 0.026*\"product\" + 0.018*\"help\" + 0.018*\"good\" + 0.018*\"use\" + 0.014*\"great\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.385841, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6987/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10675147, 0.07161323, 0.1437054]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.022*\"product\" + 0.021*\"good\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"help\" + 0.013*\"day\" + 0.010*\"try\" + 0.010*\"use\" + 0.009*\"like\" + 0.009*\"great\"\n",
      "INFO : topic #1 (0.072): 0.019*\"work\" + 0.017*\"product\" + 0.015*\"day\" + 0.015*\"use\" + 0.014*\"help\" + 0.014*\"year\" + 0.011*\"try\" + 0.010*\"sleep\" + 0.010*\"time\" + 0.010*\"month\"\n",
      "INFO : topic #2 (0.144): 0.025*\"product\" + 0.018*\"good\" + 0.017*\"use\" + 0.017*\"help\" + 0.014*\"great\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.334460, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6991/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10628523, 0.07319662, 0.14035244]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.023*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.014*\"feel\" + 0.014*\"help\" + 0.013*\"day\" + 0.011*\"try\" + 0.010*\"use\" + 0.009*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.073): 0.020*\"work\" + 0.018*\"product\" + 0.016*\"day\" + 0.015*\"help\" + 0.015*\"use\" + 0.014*\"year\" + 0.011*\"sleep\" + 0.011*\"try\" + 0.011*\"time\" + 0.010*\"recommend\"\n",
      "INFO : topic #2 (0.140): 0.026*\"product\" + 0.019*\"good\" + 0.017*\"use\" + 0.016*\"help\" + 0.014*\"great\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.298900, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10616525, 0.073713012, 0.13959855]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.024*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.014*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.011*\"use\" + 0.011*\"try\" + 0.010*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.074): 0.020*\"work\" + 0.018*\"product\" + 0.016*\"day\" + 0.015*\"help\" + 0.015*\"use\" + 0.014*\"year\" + 0.011*\"time\" + 0.011*\"sleep\" + 0.011*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #2 (0.140): 0.026*\"product\" + 0.019*\"good\" + 0.016*\"use\" + 0.015*\"help\" + 0.013*\"great\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.010*\"day\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.275595, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10686876, 0.075266093, 0.13496087]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.025*\"product\" + 0.021*\"good\" + 0.015*\"work\" + 0.015*\"feel\" + 0.014*\"help\" + 0.013*\"day\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"great\" + 0.010*\"probiotic\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.075): 0.020*\"work\" + 0.019*\"product\" + 0.016*\"day\" + 0.016*\"help\" + 0.015*\"use\" + 0.013*\"year\" + 0.013*\"sleep\" + 0.011*\"try\" + 0.011*\"time\" + 0.010*\"good\"\n",
      "INFO : topic #2 (0.135): 0.026*\"product\" + 0.019*\"good\" + 0.016*\"use\" + 0.015*\"help\" + 0.014*\"supplement\" + 0.013*\"great\" + 0.011*\"work\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.277873, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n",
      "DEBUG : 5792/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11213784, 0.074389808, 0.13245399]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.112): 0.026*\"product\" + 0.020*\"good\" + 0.016*\"feel\" + 0.014*\"work\" + 0.014*\"help\" + 0.013*\"day\" + 0.012*\"probiotic\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.074): 0.020*\"work\" + 0.019*\"product\" + 0.016*\"help\" + 0.016*\"day\" + 0.015*\"use\" + 0.013*\"sleep\" + 0.012*\"year\" + 0.011*\"try\" + 0.011*\"time\" + 0.011*\"good\"\n",
      "INFO : topic #2 (0.132): 0.026*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.008*\"year\"\n",
      "INFO : topic diff=0.288407, rho=0.316228\n",
      "INFO : PROGRESS: pass 1, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6990/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10917873, 0.076286651, 0.1260964]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.025*\"product\" + 0.021*\"good\" + 0.015*\"feel\" + 0.015*\"help\" + 0.015*\"work\" + 0.013*\"day\" + 0.011*\"try\" + 0.011*\"use\" + 0.011*\"great\" + 0.010*\"probiotic\"\n",
      "INFO : topic #1 (0.076): 0.020*\"work\" + 0.019*\"product\" + 0.017*\"day\" + 0.016*\"help\" + 0.016*\"use\" + 0.013*\"year\" + 0.012*\"sleep\" + 0.011*\"time\" + 0.011*\"try\" + 0.010*\"good\"\n",
      "INFO : topic #2 (0.126): 0.025*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.015*\"use\" + 0.014*\"help\" + 0.013*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.240327, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10857721, 0.079629183, 0.1203275]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.026*\"product\" + 0.020*\"good\" + 0.015*\"help\" + 0.015*\"work\" + 0.015*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"try\" + 0.011*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.080): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"day\" + 0.017*\"help\" + 0.016*\"use\" + 0.013*\"year\" + 0.012*\"sleep\" + 0.011*\"try\" + 0.011*\"time\" + 0.011*\"good\"\n",
      "INFO : topic #2 (0.120): 0.025*\"product\" + 0.019*\"good\" + 0.015*\"use\" + 0.015*\"supplement\" + 0.014*\"help\" + 0.013*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.009*\"day\"\n",
      "INFO : topic diff=0.239249, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10921736, 0.081292383, 0.11717601]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.026*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"try\" + 0.011*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.081): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.017*\"use\" + 0.013*\"year\" + 0.011*\"sleep\" + 0.011*\"try\" + 0.011*\"time\" + 0.011*\"good\"\n",
      "INFO : topic #2 (0.117): 0.025*\"product\" + 0.019*\"good\" + 0.015*\"use\" + 0.015*\"supplement\" + 0.014*\"help\" + 0.012*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.008*\"day\"\n",
      "INFO : topic diff=0.232657, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11030541, 0.083079338, 0.1146269]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.110): 0.026*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"try\" + 0.011*\"great\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.083): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"day\" + 0.017*\"help\" + 0.017*\"use\" + 0.014*\"year\" + 0.011*\"try\" + 0.011*\"sleep\" + 0.011*\"time\" + 0.011*\"good\"\n",
      "INFO : topic #2 (0.115): 0.025*\"product\" + 0.019*\"good\" + 0.015*\"use\" + 0.015*\"supplement\" + 0.014*\"help\" + 0.012*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.008*\"day\"\n",
      "INFO : topic diff=0.219699, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10923833, 0.083987057, 0.11714657]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.026*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"try\" + 0.011*\"great\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.084): 0.021*\"work\" + 0.018*\"product\" + 0.017*\"day\" + 0.017*\"help\" + 0.017*\"use\" + 0.014*\"year\" + 0.011*\"try\" + 0.011*\"good\" + 0.011*\"sleep\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.117): 0.024*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.014*\"use\" + 0.014*\"help\" + 0.011*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.008*\"day\"\n",
      "INFO : topic diff=0.229556, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10733436, 0.08606296, 0.11628811]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.026*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"try\" + 0.011*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.086): 0.021*\"work\" + 0.018*\"product\" + 0.017*\"day\" + 0.017*\"help\" + 0.016*\"use\" + 0.014*\"year\" + 0.012*\"try\" + 0.011*\"good\" + 0.011*\"sleep\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.116): 0.023*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.014*\"use\" + 0.013*\"help\" + 0.011*\"great\" + 0.009*\"work\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.008*\"day\"\n",
      "INFO : topic diff=0.215540, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6991/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10651221, 0.087229207, 0.11586469]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.027*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"use\" + 0.012*\"try\" + 0.011*\"great\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.087): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.016*\"use\" + 0.014*\"year\" + 0.012*\"good\" + 0.011*\"sleep\" + 0.011*\"try\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.116): 0.024*\"product\" + 0.019*\"good\" + 0.015*\"supplement\" + 0.014*\"use\" + 0.013*\"help\" + 0.011*\"great\" + 0.010*\"recommend\" + 0.010*\"work\" + 0.009*\"year\" + 0.008*\"doctor\"\n",
      "INFO : topic diff=0.212706, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10611512, 0.086654462, 0.11748929]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.027*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"try\" + 0.012*\"great\" + 0.010*\"probiotic\"\n",
      "INFO : topic #1 (0.087): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.016*\"use\" + 0.014*\"year\" + 0.012*\"good\" + 0.011*\"sleep\" + 0.011*\"try\" + 0.011*\"time\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.117): 0.024*\"product\" + 0.019*\"good\" + 0.015*\"supplement\" + 0.014*\"use\" + 0.012*\"help\" + 0.011*\"great\" + 0.010*\"work\" + 0.010*\"recommend\" + 0.009*\"year\" + 0.009*\"doctor\"\n",
      "INFO : topic diff=0.206205, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10642471, 0.087443143, 0.11678594]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.027*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"use\" + 0.012*\"try\" + 0.012*\"great\" + 0.011*\"probiotic\"\n",
      "INFO : topic #1 (0.087): 0.021*\"work\" + 0.019*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.016*\"use\" + 0.013*\"year\" + 0.013*\"sleep\" + 0.012*\"good\" + 0.011*\"try\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.117): 0.024*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.014*\"use\" + 0.012*\"help\" + 0.010*\"great\" + 0.010*\"recommend\" + 0.010*\"work\" + 0.009*\"doctor\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.214812, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n",
      "DEBUG : 5797/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11065116, 0.085775122, 0.11763726]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.111): 0.028*\"product\" + 0.020*\"good\" + 0.016*\"help\" + 0.016*\"feel\" + 0.015*\"work\" + 0.013*\"day\" + 0.013*\"probiotic\" + 0.012*\"try\" + 0.012*\"use\" + 0.012*\"great\"\n",
      "INFO : topic #1 (0.086): 0.020*\"work\" + 0.020*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.016*\"use\" + 0.013*\"sleep\" + 0.012*\"year\" + 0.012*\"good\" + 0.011*\"try\" + 0.011*\"feel\"\n",
      "INFO : topic #2 (0.118): 0.024*\"product\" + 0.018*\"good\" + 0.017*\"supplement\" + 0.013*\"use\" + 0.012*\"help\" + 0.010*\"great\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.008*\"doctor\" + 0.008*\"year\"\n",
      "INFO : topic diff=0.241278, rho=0.290765\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=8233, num_topics=3, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 3 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -1.81119897786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 68797 documents\n",
      "DEBUG : 68770/68797 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 33%|███▎      | 1/3 [03:57<07:55, 237.68s/it]INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 0.25\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 4 topics, 2 passes over the supplied corpus of 68797 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic0_3.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6627/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093036473, 0.057495415, 0.13608372, 0.12716214]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.093): 0.024*\"good\" + 0.019*\"work\" + 0.017*\"product\" + 0.015*\"help\" + 0.012*\"feel\" + 0.012*\"day\" + 0.011*\"like\" + 0.010*\"year\" + 0.010*\"use\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.057): 0.017*\"work\" + 0.014*\"product\" + 0.013*\"year\" + 0.013*\"use\" + 0.013*\"help\" + 0.011*\"time\" + 0.011*\"recommend\" + 0.011*\"good\" + 0.011*\"day\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.136): 0.023*\"help\" + 0.021*\"product\" + 0.019*\"good\" + 0.016*\"use\" + 0.014*\"work\" + 0.013*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.011*\"supplement\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.127): 0.029*\"product\" + 0.019*\"use\" + 0.016*\"day\" + 0.016*\"great\" + 0.014*\"work\" + 0.013*\"good\" + 0.011*\"help\" + 0.010*\"recommend\" + 0.009*\"year\" + 0.009*\"start\"\n",
      "INFO : topic diff=3.620476, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6961/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089832604, 0.057608414, 0.11621235, 0.11744214]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.090): 0.023*\"good\" + 0.019*\"product\" + 0.018*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.011*\"day\" + 0.011*\"like\" + 0.010*\"year\" + 0.009*\"recommend\" + 0.009*\"great\"\n",
      "INFO : topic #1 (0.058): 0.018*\"work\" + 0.015*\"product\" + 0.014*\"year\" + 0.014*\"help\" + 0.013*\"use\" + 0.012*\"recommend\" + 0.011*\"time\" + 0.011*\"try\" + 0.011*\"day\" + 0.010*\"night\"\n",
      "INFO : topic #2 (0.116): 0.023*\"help\" + 0.022*\"product\" + 0.019*\"good\" + 0.016*\"use\" + 0.013*\"work\" + 0.012*\"day\" + 0.011*\"try\" + 0.011*\"supplement\" + 0.011*\"great\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.117): 0.030*\"product\" + 0.020*\"use\" + 0.016*\"day\" + 0.015*\"great\" + 0.015*\"work\" + 0.013*\"good\" + 0.012*\"help\" + 0.011*\"year\" + 0.009*\"recommend\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.777402, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6977/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089145027, 0.057832863, 0.11106977, 0.1146443]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.089): 0.023*\"good\" + 0.020*\"product\" + 0.018*\"work\" + 0.015*\"help\" + 0.013*\"feel\" + 0.012*\"day\" + 0.010*\"like\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.058): 0.019*\"work\" + 0.016*\"product\" + 0.014*\"help\" + 0.014*\"year\" + 0.013*\"use\" + 0.012*\"sleep\" + 0.012*\"try\" + 0.012*\"day\" + 0.011*\"time\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.111): 0.023*\"product\" + 0.022*\"help\" + 0.020*\"good\" + 0.015*\"use\" + 0.013*\"work\" + 0.013*\"supplement\" + 0.012*\"day\" + 0.011*\"try\" + 0.011*\"great\" + 0.009*\"year\"\n",
      "INFO : topic #3 (0.115): 0.031*\"product\" + 0.020*\"use\" + 0.016*\"day\" + 0.015*\"work\" + 0.014*\"great\" + 0.013*\"good\" + 0.012*\"help\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.495666, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6986/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.088990971, 0.059118629, 0.10798698, 0.11268558]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.089): 0.023*\"good\" + 0.021*\"product\" + 0.017*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.010*\"try\" + 0.010*\"recommend\" + 0.009*\"like\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.059): 0.021*\"work\" + 0.016*\"product\" + 0.015*\"help\" + 0.014*\"year\" + 0.013*\"sleep\" + 0.013*\"try\" + 0.013*\"day\" + 0.013*\"use\" + 0.011*\"night\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.108): 0.024*\"product\" + 0.020*\"help\" + 0.019*\"good\" + 0.015*\"use\" + 0.013*\"supplement\" + 0.013*\"work\" + 0.011*\"day\" + 0.011*\"great\" + 0.010*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #3 (0.113): 0.030*\"product\" + 0.021*\"use\" + 0.016*\"day\" + 0.016*\"work\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"year\" + 0.012*\"help\" + 0.010*\"recommend\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.391595, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6985/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090310633, 0.059087142, 0.11118986, 0.11062572]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.090): 0.022*\"good\" + 0.021*\"product\" + 0.016*\"work\" + 0.014*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.010*\"try\" + 0.009*\"like\" + 0.009*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.059): 0.020*\"work\" + 0.016*\"help\" + 0.016*\"product\" + 0.014*\"sleep\" + 0.013*\"day\" + 0.013*\"year\" + 0.013*\"try\" + 0.012*\"use\" + 0.012*\"night\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.111): 0.022*\"product\" + 0.019*\"good\" + 0.019*\"help\" + 0.016*\"supplement\" + 0.014*\"use\" + 0.011*\"work\" + 0.011*\"great\" + 0.010*\"day\" + 0.009*\"try\" + 0.008*\"recommend\"\n",
      "INFO : topic #3 (0.111): 0.029*\"product\" + 0.021*\"use\" + 0.016*\"day\" + 0.015*\"work\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"year\" + 0.012*\"help\" + 0.010*\"recommend\" + 0.010*\"cold\"\n",
      "INFO : topic diff=0.363865, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089741416, 0.060854126, 0.11225694, 0.10883509]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.090): 0.023*\"good\" + 0.021*\"product\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.010*\"try\" + 0.009*\"like\" + 0.009*\"recommend\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.061): 0.022*\"work\" + 0.016*\"help\" + 0.016*\"sleep\" + 0.015*\"product\" + 0.014*\"try\" + 0.014*\"day\" + 0.013*\"night\" + 0.013*\"year\" + 0.012*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.112): 0.022*\"product\" + 0.019*\"good\" + 0.017*\"help\" + 0.016*\"supplement\" + 0.013*\"use\" + 0.011*\"work\" + 0.011*\"great\" + 0.010*\"day\" + 0.009*\"try\" + 0.008*\"recommend\"\n",
      "INFO : topic #3 (0.109): 0.027*\"product\" + 0.021*\"use\" + 0.016*\"day\" + 0.015*\"work\" + 0.014*\"great\" + 0.013*\"year\" + 0.013*\"good\" + 0.012*\"help\" + 0.010*\"recommend\" + 0.010*\"cold\"\n",
      "INFO : topic diff=0.309946, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.091185972, 0.061281063, 0.11024572, 0.11085322]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.091): 0.023*\"product\" + 0.023*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.009*\"probiotic\" + 0.009*\"great\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.061): 0.023*\"work\" + 0.017*\"help\" + 0.017*\"sleep\" + 0.016*\"product\" + 0.015*\"day\" + 0.014*\"try\" + 0.013*\"night\" + 0.012*\"year\" + 0.012*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.110): 0.022*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.016*\"help\" + 0.013*\"use\" + 0.011*\"work\" + 0.011*\"great\" + 0.010*\"day\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #3 (0.111): 0.028*\"product\" + 0.021*\"use\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"great\" + 0.014*\"year\" + 0.013*\"good\" + 0.012*\"help\" + 0.011*\"cold\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.276322, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093073465, 0.061840929, 0.11196373, 0.10995103]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.093): 0.024*\"product\" + 0.022*\"good\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.011*\"probiotic\" + 0.011*\"try\" + 0.010*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.062): 0.022*\"work\" + 0.018*\"help\" + 0.018*\"sleep\" + 0.016*\"product\" + 0.015*\"day\" + 0.014*\"try\" + 0.013*\"night\" + 0.012*\"use\" + 0.012*\"time\" + 0.012*\"good\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.112): 0.023*\"product\" + 0.019*\"good\" + 0.016*\"supplement\" + 0.015*\"help\" + 0.013*\"use\" + 0.011*\"work\" + 0.010*\"great\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"doctor\"\n",
      "INFO : topic #3 (0.110): 0.028*\"product\" + 0.021*\"use\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"year\" + 0.014*\"great\" + 0.013*\"good\" + 0.012*\"help\" + 0.010*\"cold\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.255920, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095225215, 0.063222915, 0.11180746, 0.10944496]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.095): 0.025*\"product\" + 0.023*\"good\" + 0.016*\"help\" + 0.015*\"work\" + 0.015*\"feel\" + 0.012*\"probiotic\" + 0.012*\"day\" + 0.011*\"try\" + 0.010*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.063): 0.023*\"work\" + 0.020*\"sleep\" + 0.018*\"help\" + 0.017*\"product\" + 0.015*\"try\" + 0.015*\"day\" + 0.014*\"night\" + 0.012*\"good\" + 0.012*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.112): 0.023*\"product\" + 0.019*\"good\" + 0.017*\"supplement\" + 0.014*\"help\" + 0.013*\"use\" + 0.010*\"work\" + 0.010*\"great\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.008*\"doctor\"\n",
      "INFO : topic #3 (0.109): 0.029*\"product\" + 0.021*\"use\" + 0.014*\"day\" + 0.014*\"work\" + 0.014*\"year\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"help\" + 0.010*\"recommend\" + 0.009*\"cold\"\n",
      "INFO : topic diff=0.257936, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n",
      "DEBUG : 5793/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10116811, 0.063714169, 0.11283152, 0.10752237]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.101): 0.026*\"product\" + 0.023*\"good\" + 0.017*\"feel\" + 0.016*\"help\" + 0.015*\"probiotic\" + 0.014*\"work\" + 0.012*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.064): 0.022*\"work\" + 0.020*\"sleep\" + 0.018*\"help\" + 0.018*\"product\" + 0.015*\"day\" + 0.015*\"try\" + 0.014*\"night\" + 0.012*\"good\" + 0.012*\"feel\" + 0.012*\"use\"\n",
      "INFO : topic #2 (0.113): 0.023*\"product\" + 0.019*\"supplement\" + 0.019*\"good\" + 0.013*\"help\" + 0.012*\"use\" + 0.010*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.008*\"day\" + 0.008*\"doctor\"\n",
      "INFO : topic #3 (0.108): 0.029*\"product\" + 0.021*\"use\" + 0.014*\"work\" + 0.014*\"day\" + 0.014*\"great\" + 0.013*\"year\" + 0.013*\"good\" + 0.013*\"help\" + 0.009*\"recommend\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.273985, rho=0.316228\n",
      "INFO : PROGRESS: pass 1, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.099912666, 0.063943841, 0.10872206, 0.10808215]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.100): 0.026*\"product\" + 0.023*\"good\" + 0.016*\"help\" + 0.016*\"feel\" + 0.015*\"work\" + 0.013*\"probiotic\" + 0.012*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.064): 0.023*\"work\" + 0.020*\"sleep\" + 0.019*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.014*\"night\" + 0.014*\"try\" + 0.012*\"good\" + 0.012*\"use\" + 0.012*\"feel\"\n",
      "INFO : topic #2 (0.109): 0.022*\"product\" + 0.020*\"supplement\" + 0.019*\"good\" + 0.013*\"help\" + 0.012*\"use\" + 0.010*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.008*\"day\" + 0.008*\"iron\"\n",
      "INFO : topic #3 (0.108): 0.027*\"product\" + 0.021*\"use\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"year\" + 0.013*\"great\" + 0.013*\"good\" + 0.013*\"help\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.215922, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10096389, 0.065083474, 0.10532059, 0.11053814]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.101): 0.026*\"product\" + 0.023*\"good\" + 0.017*\"help\" + 0.015*\"work\" + 0.015*\"feel\" + 0.012*\"day\" + 0.011*\"probiotic\" + 0.011*\"try\" + 0.011*\"great\" + 0.011*\"use\"\n",
      "INFO : topic #1 (0.065): 0.023*\"work\" + 0.021*\"sleep\" + 0.019*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"night\" + 0.015*\"try\" + 0.012*\"feel\" + 0.012*\"good\" + 0.012*\"use\"\n",
      "INFO : topic #2 (0.105): 0.022*\"product\" + 0.019*\"good\" + 0.019*\"supplement\" + 0.013*\"help\" + 0.012*\"use\" + 0.010*\"great\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.008*\"day\" + 0.008*\"doctor\"\n",
      "INFO : topic #3 (0.111): 0.027*\"product\" + 0.021*\"use\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"year\" + 0.014*\"help\" + 0.013*\"good\" + 0.013*\"great\" + 0.011*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.213596, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10266617, 0.065804966, 0.10457966, 0.11248863]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.103): 0.027*\"product\" + 0.023*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.012*\"day\" + 0.012*\"try\" + 0.011*\"probiotic\" + 0.011*\"great\" + 0.011*\"use\"\n",
      "INFO : topic #1 (0.066): 0.024*\"work\" + 0.020*\"sleep\" + 0.020*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"try\" + 0.015*\"night\" + 0.012*\"feel\" + 0.012*\"good\" + 0.012*\"use\"\n",
      "INFO : topic #2 (0.105): 0.022*\"product\" + 0.019*\"good\" + 0.019*\"supplement\" + 0.013*\"help\" + 0.012*\"use\" + 0.010*\"great\" + 0.009*\"work\" + 0.009*\"recommend\" + 0.008*\"doctor\" + 0.008*\"day\"\n",
      "INFO : topic #3 (0.112): 0.026*\"product\" + 0.022*\"use\" + 0.015*\"work\" + 0.015*\"day\" + 0.015*\"year\" + 0.013*\"help\" + 0.013*\"good\" + 0.013*\"great\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.210790, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10488383, 0.066905998, 0.10440911, 0.11311837]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.105): 0.027*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.012*\"probiotic\" + 0.012*\"day\" + 0.012*\"try\" + 0.011*\"use\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.067): 0.024*\"work\" + 0.020*\"help\" + 0.019*\"sleep\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"try\" + 0.015*\"night\" + 0.012*\"feel\" + 0.012*\"good\" + 0.012*\"use\"\n",
      "INFO : topic #2 (0.104): 0.022*\"product\" + 0.019*\"good\" + 0.019*\"supplement\" + 0.013*\"help\" + 0.012*\"use\" + 0.010*\"great\" + 0.009*\"work\" + 0.009*\"recommend\" + 0.009*\"doctor\" + 0.008*\"day\"\n",
      "INFO : topic #3 (0.113): 0.026*\"product\" + 0.022*\"use\" + 0.015*\"year\" + 0.015*\"work\" + 0.015*\"day\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.203082, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10602061, 0.067675821, 0.10851682, 0.11274192]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.027*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.012*\"day\" + 0.012*\"try\" + 0.012*\"use\" + 0.011*\"great\" + 0.010*\"probiotic\"\n",
      "INFO : topic #1 (0.068): 0.024*\"work\" + 0.020*\"help\" + 0.019*\"sleep\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"try\" + 0.015*\"night\" + 0.013*\"feel\" + 0.012*\"good\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.109): 0.021*\"product\" + 0.020*\"supplement\" + 0.019*\"good\" + 0.012*\"help\" + 0.012*\"use\" + 0.009*\"great\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.008*\"doctor\" + 0.008*\"day\"\n",
      "INFO : topic #3 (0.113): 0.025*\"product\" + 0.022*\"use\" + 0.015*\"year\" + 0.015*\"day\" + 0.015*\"work\" + 0.014*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.212239, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10548508, 0.069488592, 0.11056466, 0.11232814]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.105): 0.027*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.012*\"day\" + 0.012*\"try\" + 0.012*\"great\" + 0.012*\"use\" + 0.010*\"probiotic\"\n",
      "INFO : topic #1 (0.069): 0.024*\"work\" + 0.020*\"help\" + 0.019*\"sleep\" + 0.017*\"day\" + 0.016*\"product\" + 0.015*\"try\" + 0.015*\"night\" + 0.013*\"good\" + 0.013*\"feel\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.111): 0.021*\"product\" + 0.019*\"supplement\" + 0.019*\"good\" + 0.012*\"help\" + 0.011*\"use\" + 0.009*\"recommend\" + 0.009*\"great\" + 0.009*\"work\" + 0.009*\"doctor\" + 0.008*\"mg\"\n",
      "INFO : topic #3 (0.112): 0.024*\"product\" + 0.022*\"use\" + 0.016*\"year\" + 0.015*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.200361, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10632741, 0.070184208, 0.1113117, 0.11402119]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.106): 0.028*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.012*\"try\" + 0.012*\"day\" + 0.012*\"great\" + 0.012*\"probiotic\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.070): 0.024*\"work\" + 0.021*\"help\" + 0.020*\"sleep\" + 0.017*\"day\" + 0.017*\"product\" + 0.015*\"try\" + 0.015*\"night\" + 0.013*\"feel\" + 0.013*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.111): 0.021*\"product\" + 0.019*\"good\" + 0.018*\"supplement\" + 0.012*\"use\" + 0.012*\"help\" + 0.010*\"recommend\" + 0.010*\"doctor\" + 0.009*\"great\" + 0.009*\"work\" + 0.009*\"level\"\n",
      "INFO : topic #3 (0.114): 0.025*\"product\" + 0.021*\"use\" + 0.016*\"year\" + 0.015*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic diff=0.193573, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10748788, 0.070589751, 0.11457422, 0.113414]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.029*\"product\" + 0.022*\"good\" + 0.018*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"probiotic\" + 0.012*\"great\" + 0.012*\"try\" + 0.012*\"day\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.071): 0.024*\"work\" + 0.020*\"help\" + 0.020*\"sleep\" + 0.017*\"day\" + 0.017*\"product\" + 0.015*\"try\" + 0.015*\"night\" + 0.013*\"feel\" + 0.013*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.115): 0.022*\"product\" + 0.019*\"good\" + 0.018*\"supplement\" + 0.011*\"use\" + 0.011*\"help\" + 0.010*\"doctor\" + 0.010*\"recommend\" + 0.009*\"work\" + 0.009*\"level\" + 0.009*\"great\"\n",
      "INFO : topic #3 (0.113): 0.025*\"product\" + 0.021*\"use\" + 0.016*\"year\" + 0.015*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.192567, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10884365, 0.071741335, 0.11610953, 0.11288378]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.029*\"product\" + 0.022*\"good\" + 0.018*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.014*\"probiotic\" + 0.013*\"great\" + 0.013*\"try\" + 0.012*\"day\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.072): 0.024*\"work\" + 0.021*\"sleep\" + 0.020*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.015*\"try\" + 0.015*\"night\" + 0.014*\"feel\" + 0.013*\"good\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.116): 0.022*\"product\" + 0.019*\"good\" + 0.019*\"supplement\" + 0.011*\"use\" + 0.011*\"help\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"level\" + 0.009*\"work\" + 0.009*\"great\"\n",
      "INFO : topic #3 (0.113): 0.025*\"product\" + 0.022*\"use\" + 0.015*\"year\" + 0.015*\"work\" + 0.014*\"day\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.201855, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n",
      "DEBUG : 5795/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11364952, 0.072311662, 0.11904, 0.11071594]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.114): 0.030*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"probiotic\" + 0.016*\"feel\" + 0.015*\"work\" + 0.013*\"great\" + 0.013*\"try\" + 0.012*\"day\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.072): 0.023*\"work\" + 0.021*\"sleep\" + 0.020*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.015*\"try\" + 0.015*\"feel\" + 0.015*\"night\" + 0.014*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.119): 0.022*\"product\" + 0.021*\"supplement\" + 0.019*\"good\" + 0.011*\"help\" + 0.011*\"use\" + 0.009*\"great\" + 0.009*\"recommend\" + 0.009*\"level\" + 0.009*\"doctor\" + 0.009*\"work\"\n",
      "INFO : topic #3 (0.111): 0.025*\"product\" + 0.021*\"use\" + 0.015*\"year\" + 0.014*\"work\" + 0.014*\"day\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.228398, rho=0.290765\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=8233, num_topics=4, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -1.85580957035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 68797 documents\n",
      "DEBUG : 68778/68797 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 67%|██████▋   | 2/3 [07:46<03:53, 233.06s/it]INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "INFO : using symmetric eta at 0.2\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 5 topics, 2 passes over the supplied corpus of 68797 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic0_4.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6695/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.098054171, 0.07440649, 0.12991381, 0.12280817, 0.061158299]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.098): 0.023*\"good\" + 0.019*\"work\" + 0.018*\"product\" + 0.015*\"help\" + 0.012*\"day\" + 0.012*\"feel\" + 0.012*\"like\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.074): 0.017*\"work\" + 0.015*\"product\" + 0.014*\"help\" + 0.014*\"use\" + 0.014*\"year\" + 0.012*\"time\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.010*\"good\" + 0.009*\"try\"\n",
      "INFO : topic #2 (0.130): 0.024*\"help\" + 0.021*\"product\" + 0.018*\"good\" + 0.017*\"use\" + 0.014*\"work\" + 0.013*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.011*\"year\" + 0.011*\"supplement\"\n",
      "INFO : topic #3 (0.123): 0.029*\"product\" + 0.020*\"use\" + 0.017*\"day\" + 0.016*\"great\" + 0.014*\"work\" + 0.012*\"good\" + 0.011*\"help\" + 0.010*\"year\" + 0.009*\"recommend\" + 0.008*\"start\"\n",
      "INFO : topic #4 (0.061): 0.028*\"good\" + 0.019*\"work\" + 0.016*\"product\" + 0.011*\"day\" + 0.011*\"feel\" + 0.010*\"start\" + 0.010*\"iron\" + 0.009*\"recommend\" + 0.009*\"stomach\" + 0.009*\"supplement\"\n",
      "INFO : topic diff=4.360672, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6965/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.094449073, 0.071858317, 0.11601413, 0.11668021, 0.056705691]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.094): 0.022*\"good\" + 0.019*\"product\" + 0.018*\"work\" + 0.015*\"help\" + 0.013*\"feel\" + 0.012*\"day\" + 0.012*\"like\" + 0.010*\"year\" + 0.009*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.072): 0.018*\"work\" + 0.016*\"product\" + 0.015*\"help\" + 0.014*\"year\" + 0.014*\"use\" + 0.012*\"time\" + 0.012*\"sleep\" + 0.011*\"recommend\" + 0.011*\"night\" + 0.011*\"try\"\n",
      "INFO : topic #2 (0.116): 0.024*\"help\" + 0.023*\"product\" + 0.018*\"good\" + 0.016*\"use\" + 0.013*\"work\" + 0.012*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.011*\"supplement\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.117): 0.031*\"product\" + 0.021*\"use\" + 0.017*\"day\" + 0.016*\"great\" + 0.014*\"work\" + 0.013*\"help\" + 0.012*\"good\" + 0.011*\"year\" + 0.009*\"cold\" + 0.009*\"recommend\"\n",
      "INFO : topic #4 (0.057): 0.029*\"good\" + 0.019*\"work\" + 0.018*\"product\" + 0.011*\"feel\" + 0.011*\"recommend\" + 0.010*\"day\" + 0.010*\"stomach\" + 0.010*\"great\" + 0.009*\"start\" + 0.009*\"supplement\"\n",
      "INFO : topic diff=0.756106, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6980/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093208976, 0.070476718, 0.11160333, 0.11436146, 0.055582169]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.093): 0.022*\"good\" + 0.021*\"product\" + 0.017*\"work\" + 0.015*\"help\" + 0.013*\"feel\" + 0.012*\"day\" + 0.011*\"like\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.070): 0.019*\"work\" + 0.016*\"product\" + 0.016*\"help\" + 0.014*\"year\" + 0.014*\"use\" + 0.014*\"sleep\" + 0.012*\"try\" + 0.012*\"time\" + 0.012*\"day\" + 0.012*\"night\"\n",
      "INFO : topic #2 (0.112): 0.023*\"product\" + 0.023*\"help\" + 0.018*\"good\" + 0.016*\"use\" + 0.013*\"work\" + 0.012*\"supplement\" + 0.012*\"try\" + 0.012*\"day\" + 0.011*\"great\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.114): 0.031*\"product\" + 0.022*\"use\" + 0.016*\"day\" + 0.015*\"work\" + 0.015*\"great\" + 0.012*\"help\" + 0.012*\"good\" + 0.012*\"year\" + 0.010*\"recommend\" + 0.009*\"cold\"\n",
      "INFO : topic #4 (0.056): 0.030*\"good\" + 0.019*\"product\" + 0.019*\"work\" + 0.012*\"recommend\" + 0.011*\"supplement\" + 0.011*\"doctor\" + 0.010*\"day\" + 0.009*\"great\" + 0.009*\"feel\" + 0.009*\"start\"\n",
      "INFO : topic diff=0.463873, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6989/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.092791013, 0.07055977, 0.10777529, 0.11328571, 0.055152148]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.093): 0.022*\"product\" + 0.021*\"good\" + 0.017*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.010*\"like\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.071): 0.020*\"work\" + 0.017*\"product\" + 0.016*\"help\" + 0.014*\"sleep\" + 0.014*\"year\" + 0.013*\"use\" + 0.013*\"try\" + 0.013*\"day\" + 0.012*\"night\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.108): 0.024*\"product\" + 0.022*\"help\" + 0.018*\"good\" + 0.016*\"use\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.011*\"great\" + 0.011*\"day\" + 0.011*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #3 (0.113): 0.031*\"product\" + 0.022*\"use\" + 0.017*\"day\" + 0.015*\"work\" + 0.015*\"great\" + 0.013*\"year\" + 0.012*\"help\" + 0.012*\"good\" + 0.010*\"cold\" + 0.010*\"recommend\"\n",
      "INFO : topic #4 (0.055): 0.030*\"good\" + 0.021*\"product\" + 0.019*\"work\" + 0.013*\"recommend\" + 0.013*\"supplement\" + 0.012*\"doctor\" + 0.010*\"great\" + 0.010*\"day\" + 0.009*\"iron\" + 0.009*\"stomach\"\n",
      "INFO : topic diff=0.351042, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6988/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.094032913, 0.069879197, 0.10862006, 0.11183323, 0.056881484]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.094): 0.021*\"product\" + 0.020*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.010*\"try\" + 0.010*\"like\" + 0.009*\"use\" + 0.009*\"eat\"\n",
      "INFO : topic #1 (0.070): 0.020*\"work\" + 0.017*\"help\" + 0.016*\"product\" + 0.015*\"sleep\" + 0.013*\"day\" + 0.013*\"year\" + 0.013*\"try\" + 0.013*\"night\" + 0.013*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.109): 0.022*\"product\" + 0.021*\"help\" + 0.017*\"good\" + 0.015*\"supplement\" + 0.015*\"use\" + 0.011*\"work\" + 0.011*\"great\" + 0.010*\"day\" + 0.010*\"try\" + 0.008*\"year\"\n",
      "INFO : topic #3 (0.112): 0.029*\"product\" + 0.022*\"use\" + 0.017*\"day\" + 0.015*\"work\" + 0.014*\"great\" + 0.013*\"year\" + 0.013*\"help\" + 0.012*\"good\" + 0.011*\"cold\" + 0.010*\"recommend\"\n",
      "INFO : topic #4 (0.057): 0.029*\"good\" + 0.021*\"product\" + 0.017*\"work\" + 0.016*\"supplement\" + 0.014*\"recommend\" + 0.013*\"doctor\" + 0.011*\"iron\" + 0.011*\"cholesterol\" + 0.011*\"great\" + 0.010*\"level\"\n",
      "INFO : topic diff=0.338535, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093252711, 0.071153805, 0.10863324, 0.11079783, 0.057764769]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.093): 0.021*\"product\" + 0.021*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.014*\"feel\" + 0.012*\"day\" + 0.011*\"try\" + 0.010*\"like\" + 0.009*\"eat\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.071): 0.022*\"work\" + 0.018*\"help\" + 0.018*\"sleep\" + 0.016*\"product\" + 0.014*\"day\" + 0.014*\"try\" + 0.014*\"night\" + 0.013*\"year\" + 0.012*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.109): 0.022*\"product\" + 0.020*\"help\" + 0.016*\"good\" + 0.015*\"supplement\" + 0.015*\"use\" + 0.010*\"great\" + 0.010*\"work\" + 0.010*\"day\" + 0.009*\"try\" + 0.008*\"year\"\n",
      "INFO : topic #3 (0.111): 0.028*\"product\" + 0.022*\"use\" + 0.017*\"day\" + 0.015*\"work\" + 0.014*\"great\" + 0.014*\"year\" + 0.013*\"help\" + 0.012*\"good\" + 0.011*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic #4 (0.058): 0.029*\"good\" + 0.021*\"product\" + 0.017*\"work\" + 0.015*\"supplement\" + 0.015*\"doctor\" + 0.014*\"recommend\" + 0.014*\"iron\" + 0.014*\"level\" + 0.012*\"cholesterol\" + 0.011*\"great\"\n",
      "INFO : topic diff=0.272472, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095109977, 0.071119748, 0.10582571, 0.11309727, 0.058708932]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.095): 0.023*\"product\" + 0.021*\"good\" + 0.016*\"work\" + 0.015*\"help\" + 0.015*\"feel\" + 0.012*\"day\" + 0.012*\"try\" + 0.010*\"like\" + 0.009*\"probiotic\" + 0.009*\"use\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.071): 0.022*\"work\" + 0.019*\"help\" + 0.019*\"sleep\" + 0.017*\"product\" + 0.015*\"day\" + 0.014*\"try\" + 0.014*\"night\" + 0.012*\"use\" + 0.012*\"year\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.106): 0.022*\"product\" + 0.019*\"help\" + 0.016*\"good\" + 0.015*\"supplement\" + 0.015*\"use\" + 0.010*\"great\" + 0.010*\"work\" + 0.010*\"day\" + 0.009*\"try\" + 0.008*\"year\"\n",
      "INFO : topic #3 (0.113): 0.028*\"product\" + 0.022*\"use\" + 0.016*\"day\" + 0.015*\"year\" + 0.014*\"work\" + 0.014*\"great\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic #4 (0.059): 0.029*\"good\" + 0.023*\"product\" + 0.017*\"work\" + 0.016*\"doctor\" + 0.016*\"level\" + 0.015*\"recommend\" + 0.015*\"supplement\" + 0.013*\"cholesterol\" + 0.012*\"great\" + 0.011*\"iron\"\n",
      "INFO : topic diff=0.241193, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.097931325, 0.071409419, 0.10602817, 0.11263612, 0.06003103]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.098): 0.024*\"product\" + 0.020*\"good\" + 0.016*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"day\" + 0.012*\"try\" + 0.011*\"probiotic\" + 0.010*\"use\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.071): 0.022*\"work\" + 0.019*\"help\" + 0.019*\"sleep\" + 0.017*\"product\" + 0.015*\"day\" + 0.014*\"night\" + 0.014*\"try\" + 0.012*\"time\" + 0.012*\"use\" + 0.011*\"year\"\n",
      "INFO : topic #2 (0.106): 0.022*\"product\" + 0.018*\"help\" + 0.016*\"good\" + 0.015*\"supplement\" + 0.014*\"use\" + 0.010*\"great\" + 0.009*\"work\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"year\"\n",
      "INFO : topic #3 (0.113): 0.028*\"product\" + 0.022*\"use\" + 0.016*\"day\" + 0.015*\"year\" + 0.014*\"work\" + 0.014*\"great\" + 0.013*\"help\" + 0.013*\"good\" + 0.011*\"cold\" + 0.009*\"recommend\"\n",
      "INFO : topic #4 (0.060): 0.027*\"good\" + 0.023*\"product\" + 0.018*\"level\" + 0.018*\"doctor\" + 0.017*\"work\" + 0.016*\"cholesterol\" + 0.016*\"recommend\" + 0.015*\"supplement\" + 0.012*\"great\" + 0.011*\"iron\"\n",
      "INFO : topic diff=0.225835, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1009272, 0.072784252, 0.10544063, 0.11252346, 0.060838543]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.101): 0.025*\"product\" + 0.021*\"good\" + 0.016*\"help\" + 0.015*\"work\" + 0.015*\"feel\" + 0.012*\"day\" + 0.012*\"probiotic\" + 0.012*\"try\" + 0.010*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.073): 0.022*\"work\" + 0.021*\"sleep\" + 0.019*\"help\" + 0.018*\"product\" + 0.015*\"day\" + 0.015*\"night\" + 0.015*\"try\" + 0.012*\"use\" + 0.012*\"time\" + 0.012*\"good\"\n",
      "INFO : topic #2 (0.105): 0.022*\"product\" + 0.017*\"help\" + 0.016*\"good\" + 0.016*\"supplement\" + 0.014*\"use\" + 0.009*\"great\" + 0.008*\"work\" + 0.008*\"day\" + 0.008*\"try\" + 0.007*\"find\"\n",
      "INFO : topic #3 (0.113): 0.029*\"product\" + 0.023*\"use\" + 0.015*\"day\" + 0.015*\"year\" + 0.015*\"work\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"help\" + 0.011*\"cold\" + 0.009*\"recommend\"\n",
      "INFO : topic #4 (0.061): 0.027*\"good\" + 0.025*\"product\" + 0.019*\"level\" + 0.018*\"doctor\" + 0.017*\"work\" + 0.016*\"recommend\" + 0.016*\"supplement\" + 0.016*\"cholesterol\" + 0.014*\"iron\" + 0.013*\"great\"\n",
      "INFO : topic diff=0.221905, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n",
      "DEBUG : 5796/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10786436, 0.073149815, 0.107799, 0.11075813, 0.060850348]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.108): 0.026*\"product\" + 0.021*\"good\" + 0.017*\"feel\" + 0.016*\"help\" + 0.015*\"probiotic\" + 0.014*\"work\" + 0.012*\"try\" + 0.012*\"day\" + 0.010*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.073): 0.022*\"work\" + 0.021*\"sleep\" + 0.019*\"help\" + 0.018*\"product\" + 0.015*\"day\" + 0.015*\"night\" + 0.014*\"try\" + 0.012*\"good\" + 0.012*\"use\" + 0.012*\"feel\"\n",
      "INFO : topic #2 (0.108): 0.022*\"product\" + 0.018*\"supplement\" + 0.017*\"help\" + 0.016*\"good\" + 0.013*\"use\" + 0.009*\"great\" + 0.008*\"health\" + 0.008*\"day\" + 0.007*\"work\" + 0.007*\"find\"\n",
      "INFO : topic #3 (0.111): 0.029*\"product\" + 0.022*\"use\" + 0.015*\"day\" + 0.014*\"work\" + 0.014*\"year\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"help\" + 0.010*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic #4 (0.061): 0.027*\"good\" + 0.025*\"product\" + 0.020*\"level\" + 0.018*\"doctor\" + 0.017*\"supplement\" + 0.017*\"work\" + 0.016*\"recommend\" + 0.016*\"cholesterol\" + 0.013*\"great\" + 0.012*\"iron\"\n",
      "INFO : topic diff=0.235869, rho=0.316228\n",
      "INFO : PROGRESS: pass 1, at document #7000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10702856, 0.073010221, 0.10397872, 0.11186735, 0.059870739]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.107): 0.026*\"product\" + 0.022*\"good\" + 0.016*\"help\" + 0.016*\"feel\" + 0.015*\"work\" + 0.014*\"probiotic\" + 0.013*\"day\" + 0.012*\"try\" + 0.011*\"great\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.073): 0.023*\"work\" + 0.021*\"sleep\" + 0.020*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"night\" + 0.014*\"try\" + 0.012*\"time\" + 0.012*\"use\" + 0.011*\"good\"\n",
      "INFO : topic #2 (0.104): 0.021*\"product\" + 0.018*\"supplement\" + 0.017*\"help\" + 0.017*\"good\" + 0.013*\"use\" + 0.009*\"great\" + 0.008*\"health\" + 0.008*\"day\" + 0.007*\"find\" + 0.007*\"work\"\n",
      "INFO : topic #3 (0.112): 0.027*\"product\" + 0.023*\"use\" + 0.016*\"day\" + 0.015*\"work\" + 0.014*\"year\" + 0.014*\"great\" + 0.013*\"good\" + 0.013*\"help\" + 0.011*\"cold\" + 0.009*\"start\"\n",
      "INFO : topic #4 (0.060): 0.026*\"good\" + 0.024*\"product\" + 0.021*\"iron\" + 0.020*\"level\" + 0.018*\"supplement\" + 0.018*\"doctor\" + 0.017*\"work\" + 0.016*\"recommend\" + 0.014*\"cholesterol\" + 0.013*\"great\"\n",
      "INFO : topic diff=0.188476, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #14000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10872367, 0.074479446, 0.10225646, 0.11470339, 0.058751024]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.109): 0.026*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.015*\"feel\" + 0.015*\"work\" + 0.012*\"day\" + 0.012*\"try\" + 0.012*\"probiotic\" + 0.011*\"great\" + 0.011*\"use\"\n",
      "INFO : topic #1 (0.074): 0.023*\"work\" + 0.022*\"sleep\" + 0.020*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.015*\"night\" + 0.014*\"try\" + 0.012*\"use\" + 0.012*\"time\" + 0.012*\"feel\"\n",
      "INFO : topic #2 (0.102): 0.021*\"product\" + 0.018*\"supplement\" + 0.017*\"help\" + 0.017*\"good\" + 0.013*\"use\" + 0.009*\"great\" + 0.008*\"magnesium\" + 0.008*\"day\" + 0.007*\"find\" + 0.007*\"health\"\n",
      "INFO : topic #3 (0.115): 0.027*\"product\" + 0.023*\"use\" + 0.016*\"day\" + 0.015*\"year\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"great\" + 0.013*\"good\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.059): 0.026*\"good\" + 0.024*\"product\" + 0.020*\"level\" + 0.019*\"doctor\" + 0.018*\"iron\" + 0.017*\"work\" + 0.017*\"supplement\" + 0.016*\"recommend\" + 0.014*\"cholesterol\" + 0.014*\"great\"\n",
      "INFO : topic diff=0.183850, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #21000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11145754, 0.075115345, 0.10233514, 0.11664488, 0.058757123]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.111): 0.027*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.015*\"feel\" + 0.013*\"day\" + 0.012*\"try\" + 0.012*\"probiotic\" + 0.011*\"use\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.075): 0.024*\"work\" + 0.021*\"sleep\" + 0.021*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.012*\"use\" + 0.012*\"time\" + 0.012*\"feel\"\n",
      "INFO : topic #2 (0.102): 0.021*\"product\" + 0.018*\"supplement\" + 0.017*\"help\" + 0.017*\"good\" + 0.013*\"use\" + 0.008*\"great\" + 0.008*\"magnesium\" + 0.008*\"health\" + 0.007*\"find\" + 0.007*\"day\"\n",
      "INFO : topic #3 (0.117): 0.026*\"product\" + 0.023*\"use\" + 0.016*\"day\" + 0.016*\"year\" + 0.016*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.013*\"great\" + 0.011*\"cold\" + 0.009*\"start\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.059): 0.026*\"good\" + 0.025*\"product\" + 0.020*\"doctor\" + 0.019*\"level\" + 0.018*\"work\" + 0.017*\"recommend\" + 0.016*\"supplement\" + 0.016*\"iron\" + 0.016*\"cholesterol\" + 0.014*\"great\"\n",
      "INFO : topic diff=0.177313, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #28000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11432887, 0.076072134, 0.10175167, 0.11803995, 0.058899045]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.114): 0.027*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"try\" + 0.013*\"probiotic\" + 0.011*\"use\" + 0.011*\"great\"\n",
      "INFO : topic #1 (0.076): 0.024*\"work\" + 0.021*\"sleep\" + 0.020*\"help\" + 0.017*\"product\" + 0.016*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.012*\"feel\" + 0.012*\"good\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.102): 0.021*\"product\" + 0.017*\"supplement\" + 0.017*\"good\" + 0.016*\"help\" + 0.013*\"use\" + 0.010*\"magnesium\" + 0.008*\"great\" + 0.008*\"health\" + 0.007*\"find\" + 0.007*\"body\"\n",
      "INFO : topic #3 (0.118): 0.026*\"product\" + 0.023*\"use\" + 0.016*\"year\" + 0.016*\"day\" + 0.016*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.059): 0.026*\"product\" + 0.025*\"good\" + 0.020*\"doctor\" + 0.019*\"level\" + 0.018*\"work\" + 0.017*\"recommend\" + 0.016*\"supplement\" + 0.015*\"cholesterol\" + 0.015*\"iron\" + 0.014*\"great\"\n",
      "INFO : topic diff=0.168075, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #35000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11575421, 0.076885596, 0.10454313, 0.11827265, 0.060361899]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.116): 0.027*\"product\" + 0.021*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.015*\"feel\" + 0.013*\"day\" + 0.012*\"try\" + 0.011*\"use\" + 0.011*\"great\" + 0.011*\"eat\"\n",
      "INFO : topic #1 (0.077): 0.023*\"work\" + 0.021*\"help\" + 0.020*\"sleep\" + 0.017*\"product\" + 0.016*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.012*\"feel\" + 0.012*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.105): 0.020*\"product\" + 0.019*\"supplement\" + 0.017*\"good\" + 0.016*\"help\" + 0.012*\"use\" + 0.008*\"magnesium\" + 0.008*\"health\" + 0.008*\"great\" + 0.007*\"body\" + 0.007*\"find\"\n",
      "INFO : topic #3 (0.118): 0.025*\"product\" + 0.023*\"use\" + 0.016*\"day\" + 0.016*\"year\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.060): 0.025*\"product\" + 0.025*\"good\" + 0.020*\"doctor\" + 0.019*\"level\" + 0.018*\"work\" + 0.017*\"supplement\" + 0.017*\"recommend\" + 0.016*\"cholesterol\" + 0.015*\"iron\" + 0.014*\"great\"\n",
      "INFO : topic diff=0.188663, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #42000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11586643, 0.078620084, 0.10572638, 0.11841002, 0.061718933]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.116): 0.027*\"product\" + 0.021*\"good\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"feel\" + 0.013*\"day\" + 0.013*\"try\" + 0.011*\"use\" + 0.011*\"great\" + 0.011*\"probiotic\"\n",
      "INFO : topic #1 (0.079): 0.024*\"work\" + 0.021*\"help\" + 0.021*\"sleep\" + 0.017*\"day\" + 0.017*\"product\" + 0.016*\"night\" + 0.016*\"try\" + 0.012*\"feel\" + 0.012*\"good\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.106): 0.020*\"product\" + 0.018*\"supplement\" + 0.016*\"good\" + 0.015*\"help\" + 0.012*\"use\" + 0.008*\"health\" + 0.008*\"magnesium\" + 0.008*\"mg\" + 0.008*\"great\" + 0.007*\"find\"\n",
      "INFO : topic #3 (0.118): 0.024*\"product\" + 0.023*\"use\" + 0.017*\"year\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.012*\"good\" + 0.012*\"great\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.062): 0.024*\"good\" + 0.024*\"product\" + 0.021*\"doctor\" + 0.020*\"level\" + 0.018*\"work\" + 0.017*\"recommend\" + 0.016*\"supplement\" + 0.016*\"iron\" + 0.016*\"cholesterol\" + 0.013*\"great\"\n",
      "INFO : topic diff=0.173084, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #49000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11777294, 0.079383403, 0.10601512, 0.12036461, 0.062949948]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.118): 0.028*\"product\" + 0.021*\"good\" + 0.017*\"help\" + 0.017*\"work\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"day\" + 0.012*\"probiotic\" + 0.012*\"great\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.079): 0.024*\"work\" + 0.022*\"help\" + 0.021*\"sleep\" + 0.017*\"product\" + 0.017*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.013*\"feel\" + 0.012*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.106): 0.020*\"product\" + 0.018*\"supplement\" + 0.016*\"good\" + 0.015*\"help\" + 0.012*\"use\" + 0.009*\"health\" + 0.008*\"great\" + 0.007*\"magnesium\" + 0.007*\"mg\" + 0.007*\"body\"\n",
      "INFO : topic #3 (0.120): 0.025*\"product\" + 0.022*\"use\" + 0.017*\"year\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"cold\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.063): 0.025*\"product\" + 0.024*\"good\" + 0.022*\"doctor\" + 0.021*\"level\" + 0.018*\"work\" + 0.017*\"recommend\" + 0.016*\"cholesterol\" + 0.015*\"supplement\" + 0.014*\"great\" + 0.013*\"iron\"\n",
      "INFO : topic diff=0.164210, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #56000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1199478, 0.079939365, 0.10801113, 0.12006515, 0.064586036]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.120): 0.029*\"product\" + 0.021*\"good\" + 0.018*\"help\" + 0.017*\"work\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"day\" + 0.013*\"probiotic\" + 0.012*\"great\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.080): 0.023*\"work\" + 0.022*\"help\" + 0.021*\"sleep\" + 0.017*\"product\" + 0.017*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.013*\"feel\" + 0.013*\"good\" + 0.012*\"time\"\n",
      "INFO : topic #2 (0.108): 0.020*\"product\" + 0.018*\"supplement\" + 0.017*\"good\" + 0.014*\"help\" + 0.012*\"use\" + 0.009*\"health\" + 0.007*\"body\" + 0.007*\"mg\" + 0.007*\"find\" + 0.007*\"great\"\n",
      "INFO : topic #3 (0.120): 0.025*\"product\" + 0.022*\"use\" + 0.017*\"year\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.065): 0.026*\"product\" + 0.023*\"good\" + 0.022*\"doctor\" + 0.021*\"level\" + 0.018*\"work\" + 0.018*\"cholesterol\" + 0.017*\"recommend\" + 0.015*\"supplement\" + 0.014*\"great\" + 0.012*\"low\"\n",
      "INFO : topic diff=0.164957, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #63000/68797\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12227311, 0.081599981, 0.10937938, 0.12009909, 0.065821134]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.122): 0.029*\"product\" + 0.022*\"good\" + 0.018*\"help\" + 0.017*\"work\" + 0.014*\"feel\" + 0.014*\"probiotic\" + 0.013*\"try\" + 0.013*\"day\" + 0.012*\"great\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.082): 0.023*\"work\" + 0.022*\"sleep\" + 0.021*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.016*\"night\" + 0.015*\"try\" + 0.014*\"feel\" + 0.013*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.109): 0.020*\"product\" + 0.019*\"supplement\" + 0.017*\"good\" + 0.014*\"help\" + 0.012*\"use\" + 0.009*\"health\" + 0.008*\"body\" + 0.008*\"magnesium\" + 0.007*\"find\" + 0.007*\"great\"\n",
      "INFO : topic #3 (0.120): 0.025*\"product\" + 0.023*\"use\" + 0.016*\"year\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.012*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.066): 0.026*\"product\" + 0.023*\"good\" + 0.022*\"doctor\" + 0.022*\"level\" + 0.018*\"work\" + 0.017*\"cholesterol\" + 0.017*\"recommend\" + 0.015*\"supplement\" + 0.015*\"iron\" + 0.014*\"great\"\n",
      "INFO : topic diff=0.166800, rho=0.290765\n",
      "INFO : PROGRESS: pass 1, at document #68797/68797\n",
      "DEBUG : performing inference on a chunk of 5797 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 5796/5797 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12841012, 0.082446873, 0.11396834, 0.11821312, 0.066026196]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 5797 documents into a model of 68797 documents\n",
      "INFO : topic #0 (0.128): 0.030*\"product\" + 0.022*\"good\" + 0.017*\"help\" + 0.017*\"probiotic\" + 0.016*\"feel\" + 0.016*\"work\" + 0.014*\"try\" + 0.013*\"great\" + 0.013*\"day\" + 0.012*\"use\"\n",
      "INFO : topic #1 (0.082): 0.022*\"work\" + 0.022*\"sleep\" + 0.021*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.015*\"night\" + 0.015*\"feel\" + 0.015*\"try\" + 0.013*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #2 (0.114): 0.021*\"supplement\" + 0.020*\"product\" + 0.017*\"good\" + 0.013*\"help\" + 0.011*\"use\" + 0.010*\"health\" + 0.008*\"body\" + 0.007*\"find\" + 0.007*\"great\" + 0.007*\"magnesium\"\n",
      "INFO : topic #3 (0.118): 0.025*\"product\" + 0.022*\"use\" + 0.016*\"year\" + 0.015*\"day\" + 0.015*\"work\" + 0.013*\"help\" + 0.013*\"good\" + 0.012*\"great\" + 0.011*\"cold\" + 0.010*\"start\"\n",
      "INFO : topic #4 (0.066): 0.027*\"product\" + 0.023*\"good\" + 0.023*\"level\" + 0.022*\"doctor\" + 0.018*\"work\" + 0.017*\"cholesterol\" + 0.017*\"recommend\" + 0.016*\"supplement\" + 0.014*\"great\" + 0.013*\"iron\"\n",
      "INFO : topic diff=0.190018, rho=0.290765\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=8233, num_topics=5, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 5 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -1.90241640371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 68797 documents\n",
      "DEBUG : 68783/68797 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 3/3 [11:34<00:00, 231.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic0_5.html\n",
      "CPU times: user 11min 32s, sys: 2.83 s, total: 11min 34s\n",
      "Wall time: 11min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [3, 4, 5]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_0 = LdaModel(bow_corpus_0, num_topics=num_topics, id2word=vocab_dictionary_0, chunksize=chunksize, \n",
    "                     passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    save_df_s3(lda_0, bucket_name, filepath='amazon_reviews/kk/lda_tier2_topic0_{}.pkl'.format(num_topics), filetype='pickle')\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda_0, corpus=bow_corpus_0, dictionary=vocab_dictionary_0, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_0, bow_corpus_0, vocab_dictionary_0)\n",
    "    plot_fname = 'pyLDAvis_tier2_topic0_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.8111989778566919, -1.8558095703462949, -1.9024164037097104]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherenceList_umass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(16478 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...)\n",
      "INFO : adding document #20000 to Dictionary(25318 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...)\n",
      "INFO : adding document #30000 to Dictionary(31718 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...)\n",
      "INFO : adding document #40000 to Dictionary(37994 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...)\n",
      "INFO : adding document #50000 to Dictionary(44538 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...)\n",
      "INFO : built Dictionary(48406 unique tokens: ['bracelet', 'color', 'elegant', 'forget', 'light']...) from 56018 documents (total 1467096 corpus positions)\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n",
      "INFO : discarding 11177 tokens: [('bracelet', 3), ('elegant', 6), ('mon', 5), ('om', 7), ('survival', 8), ('brave', 8), ('motrin', 5), ('undesired', 3), ('blunt', 2), ('cheese_ice_cream', 4)]...\n",
      "INFO : keeping 5974 tokens which were in no less than 10 and no more than 56018 (=100.0%) documents\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n",
      "INFO : resulting dictionary: Dictionary(5974 unique tokens: ['color', 'forget', 'light', 'love', 'nice']...)\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n"
     ]
    }
   ],
   "source": [
    "# Topic 1\n",
    "vocab_dictionary_1 = Dictionary(docs_topic_1)\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(vocab_dictionary_1.dfs) if docfreq == 1]\n",
    "vocab_dictionary_1.filter_tokens(bad_ids=once_ids)  # remove words that appear only once\n",
    "vocab_dictionary_1.filter_extremes(no_below=10, no_above=1.0)\n",
    "vocab_dictionary_1.compactify()\n",
    "save_df_s3(vocab_dictionary_1, bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic1.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_dictionary_1 = load_df_s3(bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic1.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5974 unique tokens: ['color', 'forget', 'light', 'love', 'nice']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus_1 = [vocab_dictionary_1.doc2bow(review) for review in docs_topic_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.33333334, 0.33333334, 0.33333334]\n",
      "INFO : using symmetric eta at 0.3333333333333333\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 3 topics, 2 passes over the supplied corpus of 56018 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6599/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11540811, 0.16973124, 0.15536271]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.115): 0.023*\"good\" + 0.017*\"product\" + 0.016*\"use\" + 0.014*\"taste\" + 0.014*\"day\" + 0.013*\"supplement\" + 0.011*\"like\" + 0.011*\"great\" + 0.009*\"vitamin\" + 0.008*\"pill\"\n",
      "INFO : topic #1 (0.170): 0.030*\"good\" + 0.026*\"taste\" + 0.024*\"great\" + 0.019*\"like\" + 0.018*\"product\" + 0.017*\"use\" + 0.012*\"love\" + 0.011*\"try\" + 0.010*\"work\" + 0.010*\"drink\"\n",
      "INFO : topic #2 (0.155): 0.028*\"product\" + 0.028*\"good\" + 0.023*\"great\" + 0.017*\"taste\" + 0.017*\"use\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.009*\"love\" + 0.009*\"brand\"\n",
      "INFO : topic diff=2.658933, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6962/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10271702, 0.14945708, 0.13157979]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.103): 0.024*\"good\" + 0.017*\"product\" + 0.015*\"supplement\" + 0.015*\"use\" + 0.013*\"day\" + 0.012*\"taste\" + 0.011*\"great\" + 0.010*\"like\" + 0.009*\"pill\" + 0.009*\"need\"\n",
      "INFO : topic #1 (0.149): 0.029*\"taste\" + 0.028*\"good\" + 0.024*\"great\" + 0.019*\"like\" + 0.018*\"use\" + 0.018*\"product\" + 0.013*\"love\" + 0.012*\"water\" + 0.011*\"try\" + 0.010*\"drink\"\n",
      "INFO : topic #2 (0.132): 0.031*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.013*\"easy\" + 0.012*\"vitamin\" + 0.010*\"brand\" + 0.010*\"capsule\"\n",
      "INFO : topic diff=0.731335, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6980/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095251806, 0.14103508, 0.12413172]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.095): 0.025*\"good\" + 0.016*\"product\" + 0.016*\"supplement\" + 0.015*\"day\" + 0.014*\"use\" + 0.011*\"vitamin\" + 0.011*\"great\" + 0.011*\"pill\" + 0.010*\"need\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.141): 0.029*\"taste\" + 0.028*\"good\" + 0.023*\"great\" + 0.021*\"use\" + 0.018*\"like\" + 0.018*\"product\" + 0.012*\"love\" + 0.011*\"try\" + 0.011*\"drink\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.124): 0.033*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.016*\"taste\" + 0.015*\"easy\" + 0.014*\"like\" + 0.014*\"vitamin\" + 0.013*\"use\" + 0.012*\"brand\" + 0.011*\"capsule\"\n",
      "INFO : topic diff=0.507860, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089821704, 0.12590206, 0.12880334]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.090): 0.023*\"good\" + 0.016*\"product\" + 0.016*\"supplement\" + 0.015*\"day\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"need\" + 0.011*\"calcium\" + 0.010*\"pill\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.126): 0.030*\"taste\" + 0.027*\"good\" + 0.023*\"great\" + 0.022*\"use\" + 0.018*\"like\" + 0.018*\"product\" + 0.013*\"love\" + 0.011*\"drink\" + 0.011*\"try\" + 0.010*\"stevia\"\n",
      "INFO : topic #2 (0.129): 0.033*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.016*\"easy\" + 0.016*\"vitamin\" + 0.014*\"like\" + 0.011*\"swallow\" + 0.011*\"use\" + 0.011*\"capsule\"\n",
      "INFO : topic diff=0.417905, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.086220078, 0.11990914, 0.13236478]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.086): 0.023*\"good\" + 0.016*\"product\" + 0.015*\"supplement\" + 0.014*\"vitamin\" + 0.014*\"day\" + 0.011*\"need\" + 0.011*\"use\" + 0.011*\"calcium\" + 0.011*\"pill\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.120): 0.031*\"taste\" + 0.027*\"good\" + 0.023*\"great\" + 0.021*\"use\" + 0.018*\"product\" + 0.018*\"like\" + 0.013*\"love\" + 0.012*\"drink\" + 0.011*\"mix\" + 0.011*\"water\"\n",
      "INFO : topic #2 (0.132): 0.033*\"good\" + 0.026*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.016*\"easy\" + 0.015*\"fish_oil\" + 0.015*\"like\" + 0.014*\"vitamin\" + 0.011*\"swallow\" + 0.011*\"brand\"\n",
      "INFO : topic diff=0.361968, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.086589389, 0.11780756, 0.13193867]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.087): 0.022*\"good\" + 0.017*\"product\" + 0.016*\"vitamin\" + 0.015*\"supplement\" + 0.014*\"day\" + 0.013*\"calcium\" + 0.012*\"need\" + 0.011*\"use\" + 0.010*\"pill\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.118): 0.030*\"taste\" + 0.026*\"good\" + 0.022*\"great\" + 0.021*\"use\" + 0.018*\"like\" + 0.017*\"product\" + 0.013*\"love\" + 0.011*\"drink\" + 0.010*\"try\" + 0.010*\"mix\"\n",
      "INFO : topic #2 (0.132): 0.033*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.017*\"easy\" + 0.015*\"like\" + 0.015*\"fish_oil\" + 0.014*\"vitamin\" + 0.012*\"swallow\" + 0.011*\"pill\"\n",
      "INFO : topic diff=0.342267, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.085493535, 0.11783049, 0.12905827]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.085): 0.021*\"good\" + 0.017*\"vitamin\" + 0.017*\"product\" + 0.016*\"supplement\" + 0.013*\"day\" + 0.012*\"calcium\" + 0.012*\"need\" + 0.011*\"use\" + 0.010*\"pill\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.118): 0.029*\"taste\" + 0.026*\"good\" + 0.023*\"great\" + 0.020*\"use\" + 0.018*\"product\" + 0.018*\"like\" + 0.013*\"love\" + 0.011*\"drink\" + 0.010*\"mix\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.129): 0.032*\"good\" + 0.027*\"product\" + 0.025*\"great\" + 0.022*\"taste\" + 0.018*\"easy\" + 0.015*\"like\" + 0.015*\"vitamin\" + 0.014*\"fish_oil\" + 0.013*\"swallow\" + 0.012*\"pill\"\n",
      "INFO : topic diff=0.303424, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.086231031, 0.11766329, 0.12980978]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.086): 0.019*\"good\" + 0.018*\"product\" + 0.018*\"supplement\" + 0.018*\"vitamin\" + 0.012*\"day\" + 0.011*\"need\" + 0.010*\"use\" + 0.010*\"calcium\" + 0.010*\"great\" + 0.009*\"pill\"\n",
      "INFO : topic #1 (0.118): 0.028*\"taste\" + 0.025*\"good\" + 0.024*\"great\" + 0.019*\"use\" + 0.019*\"product\" + 0.018*\"like\" + 0.012*\"love\" + 0.012*\"mix\" + 0.011*\"drink\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.130): 0.030*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.018*\"easy\" + 0.017*\"fish_oil\" + 0.015*\"like\" + 0.013*\"pill\" + 0.013*\"vitamin\" + 0.013*\"swallow\"\n",
      "INFO : topic diff=0.340717, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082676299, 0.1065677, 0.14132391]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.083): 0.035*\"vitamin\" + 0.025*\"supplement\" + 0.018*\"tablet\" + 0.016*\"good\" + 0.015*\"product\" + 0.014*\"great\" + 0.012*\"use\" + 0.012*\"need\" + 0.011*\"melt\" + 0.010*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.107): 0.041*\"taste\" + 0.026*\"great\" + 0.023*\"product\" + 0.023*\"good\" + 0.022*\"like\" + 0.020*\"use\" + 0.016*\"love\" + 0.012*\"flavor\" + 0.011*\"try\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.141): 0.031*\"taste\" + 0.030*\"product\" + 0.030*\"good\" + 0.030*\"fish_oil\" + 0.024*\"fish\" + 0.023*\"great\" + 0.020*\"like\" + 0.020*\"pill\" + 0.019*\"try\" + 0.019*\"fishy\"\n",
      "INFO : topic diff=0.377676, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082386926, 0.11447696, 0.12582971]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.082): 0.032*\"vitamin\" + 0.023*\"supplement\" + 0.017*\"good\" + 0.016*\"product\" + 0.015*\"tablet\" + 0.014*\"great\" + 0.013*\"use\" + 0.012*\"need\" + 0.010*\"calcium\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.114): 0.035*\"taste\" + 0.025*\"great\" + 0.024*\"good\" + 0.022*\"product\" + 0.021*\"use\" + 0.020*\"like\" + 0.014*\"love\" + 0.011*\"drink\" + 0.010*\"flavor\" + 0.010*\"try\"\n",
      "INFO : topic #2 (0.126): 0.032*\"good\" + 0.030*\"product\" + 0.029*\"taste\" + 0.026*\"fish_oil\" + 0.023*\"great\" + 0.020*\"fish\" + 0.020*\"pill\" + 0.020*\"like\" + 0.018*\"try\" + 0.017*\"love\"\n",
      "INFO : topic diff=0.291581, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082516566, 0.12253347, 0.11627772]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.083): 0.029*\"vitamin\" + 0.022*\"supplement\" + 0.019*\"good\" + 0.017*\"product\" + 0.014*\"great\" + 0.013*\"tablet\" + 0.012*\"use\" + 0.012*\"need\" + 0.011*\"calcium\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.123): 0.033*\"taste\" + 0.025*\"good\" + 0.024*\"great\" + 0.021*\"product\" + 0.021*\"use\" + 0.019*\"like\" + 0.013*\"love\" + 0.011*\"water\" + 0.011*\"drink\" + 0.010*\"flavor\"\n",
      "INFO : topic #2 (0.116): 0.033*\"good\" + 0.030*\"product\" + 0.028*\"taste\" + 0.024*\"fish_oil\" + 0.023*\"great\" + 0.020*\"pill\" + 0.020*\"like\" + 0.018*\"fish\" + 0.018*\"easy\" + 0.017*\"love\"\n",
      "INFO : topic diff=0.250542, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.083591618, 0.12811482, 0.10846125]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.084): 0.028*\"vitamin\" + 0.022*\"supplement\" + 0.021*\"good\" + 0.017*\"product\" + 0.014*\"great\" + 0.012*\"need\" + 0.012*\"use\" + 0.011*\"day\" + 0.011*\"tablet\" + 0.010*\"calcium\"\n",
      "INFO : topic #1 (0.128): 0.030*\"taste\" + 0.026*\"good\" + 0.023*\"great\" + 0.022*\"use\" + 0.020*\"product\" + 0.018*\"like\" + 0.012*\"love\" + 0.011*\"drink\" + 0.011*\"water\" + 0.010*\"mix\"\n",
      "INFO : topic #2 (0.108): 0.035*\"good\" + 0.029*\"product\" + 0.027*\"taste\" + 0.023*\"great\" + 0.022*\"fish_oil\" + 0.020*\"pill\" + 0.019*\"like\" + 0.018*\"easy\" + 0.016*\"fish\" + 0.016*\"love\"\n",
      "INFO : topic diff=0.246552, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.085317537, 0.12688003, 0.10814356]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.085): 0.027*\"vitamin\" + 0.021*\"good\" + 0.021*\"supplement\" + 0.017*\"product\" + 0.014*\"great\" + 0.013*\"calcium\" + 0.012*\"need\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"tablet\"\n",
      "INFO : topic #1 (0.127): 0.030*\"taste\" + 0.026*\"good\" + 0.023*\"great\" + 0.022*\"use\" + 0.020*\"product\" + 0.018*\"like\" + 0.012*\"love\" + 0.011*\"drink\" + 0.010*\"water\" + 0.010*\"mix\"\n",
      "INFO : topic #2 (0.108): 0.035*\"good\" + 0.028*\"product\" + 0.028*\"taste\" + 0.024*\"great\" + 0.022*\"fish_oil\" + 0.019*\"pill\" + 0.019*\"easy\" + 0.019*\"like\" + 0.016*\"love\" + 0.015*\"fish\"\n",
      "INFO : topic diff=0.239062, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.085502923, 0.12649074, 0.10980932]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.086): 0.027*\"vitamin\" + 0.022*\"good\" + 0.020*\"supplement\" + 0.018*\"product\" + 0.014*\"great\" + 0.012*\"calcium\" + 0.012*\"need\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"find\"\n",
      "INFO : topic #1 (0.126): 0.029*\"taste\" + 0.026*\"good\" + 0.023*\"great\" + 0.021*\"use\" + 0.020*\"product\" + 0.017*\"like\" + 0.012*\"love\" + 0.011*\"drink\" + 0.010*\"water\" + 0.010*\"mix\"\n",
      "INFO : topic #2 (0.110): 0.035*\"good\" + 0.028*\"taste\" + 0.027*\"product\" + 0.024*\"great\" + 0.023*\"fish_oil\" + 0.018*\"easy\" + 0.018*\"like\" + 0.018*\"pill\" + 0.016*\"oil\" + 0.015*\"fish\"\n",
      "INFO : topic diff=0.237228, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.088531323, 0.12670861, 0.10890535]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.089): 0.026*\"vitamin\" + 0.022*\"good\" + 0.019*\"supplement\" + 0.018*\"product\" + 0.014*\"calcium\" + 0.013*\"great\" + 0.012*\"need\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"find\"\n",
      "INFO : topic #1 (0.127): 0.029*\"taste\" + 0.026*\"good\" + 0.022*\"great\" + 0.021*\"use\" + 0.019*\"product\" + 0.017*\"like\" + 0.012*\"love\" + 0.011*\"drink\" + 0.010*\"flavor\" + 0.010*\"mix\"\n",
      "INFO : topic #2 (0.109): 0.035*\"good\" + 0.028*\"taste\" + 0.027*\"product\" + 0.024*\"great\" + 0.022*\"fish_oil\" + 0.019*\"easy\" + 0.018*\"pill\" + 0.018*\"like\" + 0.016*\"oil\" + 0.015*\"love\"\n",
      "INFO : topic diff=0.236108, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090284228, 0.1279241, 0.10639464]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.090): 0.027*\"vitamin\" + 0.022*\"good\" + 0.019*\"product\" + 0.019*\"supplement\" + 0.014*\"great\" + 0.013*\"calcium\" + 0.012*\"need\" + 0.011*\"use\" + 0.010*\"day\" + 0.010*\"find\"\n",
      "INFO : topic #1 (0.128): 0.028*\"taste\" + 0.026*\"good\" + 0.023*\"great\" + 0.020*\"use\" + 0.019*\"product\" + 0.017*\"like\" + 0.012*\"love\" + 0.011*\"drink\" + 0.010*\"water\" + 0.010*\"flavor\"\n",
      "INFO : topic #2 (0.106): 0.034*\"good\" + 0.028*\"taste\" + 0.027*\"product\" + 0.024*\"great\" + 0.021*\"fish_oil\" + 0.020*\"easy\" + 0.019*\"pill\" + 0.018*\"like\" + 0.016*\"swallow\" + 0.015*\"love\"\n",
      "INFO : topic diff=0.227252, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093239732, 0.12920648, 0.10596403]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.093): 0.025*\"vitamin\" + 0.020*\"supplement\" + 0.020*\"good\" + 0.020*\"product\" + 0.013*\"great\" + 0.011*\"need\" + 0.010*\"use\" + 0.010*\"calcium\" + 0.010*\"easy\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.129): 0.028*\"taste\" + 0.025*\"good\" + 0.023*\"great\" + 0.020*\"product\" + 0.019*\"use\" + 0.017*\"like\" + 0.012*\"love\" + 0.011*\"mix\" + 0.011*\"drink\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.106): 0.031*\"good\" + 0.027*\"product\" + 0.025*\"taste\" + 0.023*\"great\" + 0.023*\"fish_oil\" + 0.019*\"easy\" + 0.019*\"pill\" + 0.018*\"like\" + 0.016*\"swallow\" + 0.015*\"oil\"\n",
      "INFO : topic diff=0.276032, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.08528962, 0.10937542, 0.11640599]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.085): 0.038*\"vitamin\" + 0.025*\"supplement\" + 0.018*\"tablet\" + 0.018*\"great\" + 0.018*\"product\" + 0.017*\"good\" + 0.013*\"use\" + 0.012*\"need\" + 0.012*\"melt\" + 0.011*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.109): 0.038*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.022*\"product\" + 0.021*\"like\" + 0.020*\"use\" + 0.014*\"love\" + 0.013*\"flavor\" + 0.011*\"try\" + 0.010*\"water\"\n",
      "INFO : topic #2 (0.116): 0.037*\"taste\" + 0.034*\"fish_oil\" + 0.031*\"good\" + 0.031*\"product\" + 0.027*\"fish\" + 0.024*\"pill\" + 0.023*\"try\" + 0.023*\"like\" + 0.023*\"great\" + 0.022*\"love\"\n",
      "INFO : topic diff=0.356704, rho=0.316187\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=5974, num_topics=3, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 3 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.31182920371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 56018 documents\n",
      "DEBUG : 56010/56018 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 33%|███▎      | 1/3 [02:44<05:28, 164.44s/it]INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 0.25\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 4 topics, 2 passes over the supplied corpus of 56018 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic1_3.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6769/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.08983542, 0.12099005, 0.10858308, 0.12380172]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.090): 0.017*\"good\" + 0.016*\"use\" + 0.016*\"product\" + 0.015*\"taste\" + 0.014*\"day\" + 0.013*\"supplement\" + 0.013*\"like\" + 0.012*\"great\" + 0.009*\"vitamin\" + 0.009*\"pill\"\n",
      "INFO : topic #1 (0.121): 0.029*\"taste\" + 0.027*\"great\" + 0.022*\"good\" + 0.021*\"like\" + 0.018*\"product\" + 0.017*\"use\" + 0.014*\"love\" + 0.012*\"try\" + 0.011*\"work\" + 0.009*\"drink\"\n",
      "INFO : topic #2 (0.109): 0.027*\"great\" + 0.027*\"product\" + 0.020*\"good\" + 0.019*\"taste\" + 0.018*\"use\" + 0.015*\"like\" + 0.014*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\" + 0.010*\"find\"\n",
      "INFO : topic #3 (0.124): 0.048*\"good\" + 0.025*\"product\" + 0.016*\"use\" + 0.015*\"taste\" + 0.013*\"great\" + 0.010*\"easy\" + 0.010*\"green\" + 0.010*\"like\" + 0.009*\"drink\" + 0.008*\"day\"\n",
      "INFO : topic diff=3.296205, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6969/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.083263293, 0.11612821, 0.10042498, 0.10409746]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.083): 0.018*\"good\" + 0.015*\"supplement\" + 0.015*\"use\" + 0.015*\"product\" + 0.013*\"day\" + 0.013*\"taste\" + 0.012*\"great\" + 0.011*\"like\" + 0.010*\"pill\" + 0.009*\"flavor\"\n",
      "INFO : topic #1 (0.116): 0.032*\"taste\" + 0.025*\"great\" + 0.023*\"good\" + 0.022*\"like\" + 0.019*\"use\" + 0.017*\"product\" + 0.014*\"love\" + 0.013*\"water\" + 0.012*\"try\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.100): 0.029*\"great\" + 0.027*\"product\" + 0.022*\"good\" + 0.020*\"taste\" + 0.016*\"like\" + 0.015*\"use\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.010*\"love\" + 0.010*\"find\"\n",
      "INFO : topic #3 (0.104): 0.052*\"good\" + 0.027*\"product\" + 0.015*\"use\" + 0.014*\"taste\" + 0.013*\"great\" + 0.009*\"easy\" + 0.009*\"like\" + 0.009*\"price\" + 0.008*\"green\" + 0.008*\"drink\"\n",
      "INFO : topic diff=0.710925, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.078064874, 0.11487851, 0.098315462, 0.099172831]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.078): 0.018*\"good\" + 0.016*\"day\" + 0.015*\"supplement\" + 0.014*\"product\" + 0.013*\"use\" + 0.012*\"pill\" + 0.011*\"great\" + 0.011*\"vitamin\" + 0.011*\"need\" + 0.010*\"like\"\n",
      "INFO : topic #1 (0.115): 0.032*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.022*\"use\" + 0.020*\"like\" + 0.016*\"product\" + 0.013*\"love\" + 0.012*\"try\" + 0.012*\"water\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.098): 0.028*\"great\" + 0.026*\"product\" + 0.024*\"good\" + 0.018*\"taste\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.015*\"like\" + 0.012*\"use\" + 0.012*\"brand\" + 0.011*\"swallow\"\n",
      "INFO : topic #3 (0.099): 0.054*\"good\" + 0.028*\"product\" + 0.015*\"use\" + 0.013*\"great\" + 0.011*\"taste\" + 0.010*\"price\" + 0.009*\"supplement\" + 0.008*\"like\" + 0.008*\"quality\" + 0.008*\"recommend\"\n",
      "INFO : topic diff=0.474465, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.075742655, 0.1090961, 0.10560046, 0.092231333]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.076): 0.017*\"good\" + 0.016*\"supplement\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"calcium\" + 0.013*\"vitamin\" + 0.012*\"pill\" + 0.012*\"need\" + 0.012*\"use\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.109): 0.033*\"taste\" + 0.024*\"great\" + 0.023*\"use\" + 0.023*\"good\" + 0.020*\"like\" + 0.016*\"product\" + 0.014*\"love\" + 0.012*\"stevia\" + 0.012*\"try\" + 0.011*\"drink\"\n",
      "INFO : topic #2 (0.106): 0.027*\"great\" + 0.025*\"good\" + 0.025*\"product\" + 0.021*\"taste\" + 0.018*\"vitamin\" + 0.017*\"easy\" + 0.015*\"like\" + 0.013*\"swallow\" + 0.013*\"fish_oil\" + 0.011*\"pill\"\n",
      "INFO : topic #3 (0.092): 0.053*\"good\" + 0.029*\"product\" + 0.015*\"use\" + 0.014*\"great\" + 0.010*\"price\" + 0.010*\"taste\" + 0.009*\"supplement\" + 0.009*\"quality\" + 0.008*\"like\" + 0.008*\"recommend\"\n",
      "INFO : topic diff=0.395889, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.073771901, 0.10682499, 0.11158819, 0.088635921]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.074): 0.017*\"good\" + 0.016*\"day\" + 0.015*\"vitamin\" + 0.015*\"supplement\" + 0.013*\"product\" + 0.013*\"calcium\" + 0.013*\"pill\" + 0.012*\"need\" + 0.010*\"use\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.107): 0.035*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.022*\"use\" + 0.020*\"like\" + 0.016*\"product\" + 0.014*\"love\" + 0.012*\"drink\" + 0.012*\"water\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.112): 0.027*\"great\" + 0.026*\"good\" + 0.024*\"product\" + 0.023*\"taste\" + 0.018*\"fish_oil\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.015*\"like\" + 0.013*\"swallow\" + 0.012*\"pill\"\n",
      "INFO : topic #3 (0.089): 0.052*\"good\" + 0.029*\"product\" + 0.015*\"use\" + 0.014*\"great\" + 0.011*\"price\" + 0.009*\"quality\" + 0.009*\"taste\" + 0.008*\"supplement\" + 0.008*\"recommend\" + 0.008*\"like\"\n",
      "INFO : topic diff=0.334293, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074999124, 0.10732225, 0.11463007, 0.086829945]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.075): 0.017*\"vitamin\" + 0.017*\"calcium\" + 0.016*\"good\" + 0.015*\"day\" + 0.015*\"supplement\" + 0.013*\"product\" + 0.013*\"pill\" + 0.013*\"need\" + 0.010*\"use\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.107): 0.034*\"taste\" + 0.024*\"great\" + 0.022*\"good\" + 0.021*\"use\" + 0.020*\"like\" + 0.015*\"product\" + 0.015*\"love\" + 0.012*\"drink\" + 0.011*\"try\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.115): 0.027*\"good\" + 0.027*\"great\" + 0.024*\"product\" + 0.023*\"taste\" + 0.019*\"easy\" + 0.017*\"fish_oil\" + 0.016*\"vitamin\" + 0.016*\"like\" + 0.013*\"swallow\" + 0.013*\"pill\"\n",
      "INFO : topic #3 (0.087): 0.049*\"good\" + 0.031*\"product\" + 0.015*\"use\" + 0.014*\"great\" + 0.011*\"price\" + 0.010*\"quality\" + 0.008*\"supplement\" + 0.008*\"taste\" + 0.008*\"like\" + 0.007*\"buy\"\n",
      "INFO : topic diff=0.320130, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074778408, 0.10977216, 0.1155184, 0.085762285]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.075): 0.019*\"vitamin\" + 0.016*\"calcium\" + 0.016*\"good\" + 0.015*\"supplement\" + 0.015*\"day\" + 0.013*\"product\" + 0.013*\"need\" + 0.012*\"pill\" + 0.010*\"use\" + 0.010*\"great\"\n",
      "INFO : topic #1 (0.110): 0.034*\"taste\" + 0.025*\"great\" + 0.022*\"good\" + 0.021*\"use\" + 0.020*\"like\" + 0.016*\"product\" + 0.015*\"love\" + 0.012*\"drink\" + 0.011*\"mix\" + 0.011*\"water\"\n",
      "INFO : topic #2 (0.116): 0.027*\"good\" + 0.027*\"great\" + 0.025*\"product\" + 0.023*\"taste\" + 0.020*\"easy\" + 0.017*\"vitamin\" + 0.016*\"fish_oil\" + 0.016*\"like\" + 0.015*\"swallow\" + 0.014*\"pill\"\n",
      "INFO : topic #3 (0.086): 0.046*\"good\" + 0.032*\"product\" + 0.015*\"great\" + 0.014*\"use\" + 0.011*\"price\" + 0.010*\"quality\" + 0.009*\"supplement\" + 0.008*\"like\" + 0.007*\"buy\" + 0.007*\"capsule\"\n",
      "INFO : topic diff=0.283093, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074972041, 0.11034192, 0.11734531, 0.089712478]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.075): 0.021*\"vitamin\" + 0.017*\"supplement\" + 0.014*\"good\" + 0.014*\"day\" + 0.013*\"calcium\" + 0.013*\"product\" + 0.013*\"need\" + 0.012*\"pill\" + 0.010*\"use\" + 0.009*\"vitamin_d\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.110): 0.034*\"taste\" + 0.025*\"great\" + 0.022*\"good\" + 0.020*\"like\" + 0.020*\"use\" + 0.016*\"product\" + 0.014*\"love\" + 0.013*\"mix\" + 0.012*\"drink\" + 0.012*\"water\"\n",
      "INFO : topic #2 (0.117): 0.025*\"good\" + 0.025*\"product\" + 0.025*\"great\" + 0.022*\"taste\" + 0.020*\"fish_oil\" + 0.020*\"easy\" + 0.016*\"like\" + 0.016*\"pill\" + 0.015*\"vitamin\" + 0.015*\"swallow\"\n",
      "INFO : topic #3 (0.090): 0.040*\"good\" + 0.034*\"product\" + 0.015*\"great\" + 0.013*\"use\" + 0.012*\"supplement\" + 0.010*\"quality\" + 0.010*\"price\" + 0.008*\"like\" + 0.008*\"capsule\" + 0.007*\"find\"\n",
      "INFO : topic diff=0.321286, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074446306, 0.10385773, 0.13210332, 0.082372807]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.074): 0.041*\"vitamin\" + 0.026*\"supplement\" + 0.022*\"tablet\" + 0.015*\"great\" + 0.014*\"melt\" + 0.013*\"need\" + 0.012*\"use\" + 0.012*\"good\" + 0.012*\"product\" + 0.012*\"d3\"\n",
      "INFO : topic #1 (0.104): 0.048*\"taste\" + 0.028*\"great\" + 0.025*\"like\" + 0.022*\"use\" + 0.021*\"product\" + 0.020*\"good\" + 0.019*\"love\" + 0.014*\"flavor\" + 0.012*\"try\" + 0.011*\"water\"\n",
      "INFO : topic #2 (0.132): 0.034*\"fish_oil\" + 0.034*\"taste\" + 0.028*\"product\" + 0.027*\"fish\" + 0.027*\"good\" + 0.024*\"great\" + 0.023*\"pill\" + 0.022*\"fishy\" + 0.022*\"like\" + 0.021*\"try\"\n",
      "INFO : topic #3 (0.082): 0.039*\"good\" + 0.035*\"product\" + 0.018*\"quality\" + 0.016*\"great\" + 0.012*\"use\" + 0.011*\"gmo\" + 0.011*\"supplement\" + 0.009*\"price\" + 0.008*\"brand\" + 0.007*\"like\"\n",
      "INFO : topic diff=0.354214, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.073062904, 0.1096414, 0.1188532, 0.086982191]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.040*\"vitamin\" + 0.025*\"supplement\" + 0.021*\"tablet\" + 0.015*\"great\" + 0.014*\"calcium\" + 0.014*\"need\" + 0.012*\"use\" + 0.012*\"good\" + 0.012*\"day\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.110): 0.043*\"taste\" + 0.027*\"great\" + 0.024*\"like\" + 0.022*\"use\" + 0.021*\"good\" + 0.020*\"product\" + 0.017*\"love\" + 0.013*\"flavor\" + 0.012*\"drink\" + 0.012*\"water\"\n",
      "INFO : topic #2 (0.119): 0.033*\"taste\" + 0.031*\"fish_oil\" + 0.028*\"product\" + 0.027*\"good\" + 0.024*\"fish\" + 0.024*\"great\" + 0.024*\"pill\" + 0.021*\"like\" + 0.019*\"fishy\" + 0.019*\"love\"\n",
      "INFO : topic #3 (0.087): 0.039*\"good\" + 0.033*\"product\" + 0.015*\"great\" + 0.013*\"use\" + 0.013*\"quality\" + 0.010*\"supplement\" + 0.010*\"price\" + 0.008*\"brand\" + 0.008*\"'s\" + 0.008*\"capsule\"\n",
      "INFO : topic diff=0.287636, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072813645, 0.11730569, 0.1118769, 0.091190778]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.039*\"vitamin\" + 0.025*\"supplement\" + 0.018*\"tablet\" + 0.017*\"calcium\" + 0.015*\"great\" + 0.014*\"need\" + 0.013*\"good\" + 0.012*\"day\" + 0.012*\"use\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.117): 0.041*\"taste\" + 0.026*\"great\" + 0.023*\"like\" + 0.022*\"use\" + 0.022*\"good\" + 0.018*\"product\" + 0.016*\"love\" + 0.013*\"water\" + 0.013*\"flavor\" + 0.012*\"drink\"\n",
      "INFO : topic #2 (0.112): 0.032*\"taste\" + 0.029*\"fish_oil\" + 0.028*\"good\" + 0.027*\"product\" + 0.024*\"great\" + 0.024*\"pill\" + 0.022*\"fish\" + 0.021*\"like\" + 0.019*\"easy\" + 0.019*\"love\"\n",
      "INFO : topic #3 (0.091): 0.040*\"good\" + 0.032*\"product\" + 0.016*\"great\" + 0.013*\"use\" + 0.011*\"quality\" + 0.011*\"price\" + 0.010*\"supplement\" + 0.009*\"brand\" + 0.008*\"find\" + 0.008*\"food\"\n",
      "INFO : topic diff=0.244931, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072616935, 0.12236328, 0.10580554, 0.097291157]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.039*\"vitamin\" + 0.024*\"supplement\" + 0.017*\"tablet\" + 0.016*\"calcium\" + 0.015*\"great\" + 0.014*\"need\" + 0.013*\"good\" + 0.013*\"day\" + 0.012*\"use\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.122): 0.038*\"taste\" + 0.025*\"great\" + 0.023*\"use\" + 0.023*\"good\" + 0.022*\"like\" + 0.018*\"product\" + 0.015*\"love\" + 0.013*\"water\" + 0.012*\"drink\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.106): 0.031*\"taste\" + 0.029*\"good\" + 0.026*\"product\" + 0.026*\"fish_oil\" + 0.025*\"pill\" + 0.024*\"great\" + 0.021*\"like\" + 0.020*\"easy\" + 0.020*\"fish\" + 0.018*\"love\"\n",
      "INFO : topic #3 (0.097): 0.041*\"good\" + 0.031*\"product\" + 0.016*\"great\" + 0.014*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"quality\" + 0.010*\"brand\" + 0.009*\"find\" + 0.008*\"capsule\"\n",
      "INFO : topic diff=0.236394, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074113548, 0.12295385, 0.10708243, 0.099637985]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.074): 0.039*\"vitamin\" + 0.023*\"supplement\" + 0.021*\"calcium\" + 0.015*\"tablet\" + 0.015*\"great\" + 0.015*\"need\" + 0.014*\"good\" + 0.013*\"day\" + 0.011*\"product\" + 0.011*\"use\"\n",
      "INFO : topic #1 (0.123): 0.038*\"taste\" + 0.024*\"great\" + 0.024*\"use\" + 0.023*\"good\" + 0.021*\"like\" + 0.017*\"product\" + 0.015*\"love\" + 0.013*\"drink\" + 0.012*\"water\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.107): 0.032*\"taste\" + 0.030*\"good\" + 0.026*\"fish_oil\" + 0.026*\"product\" + 0.025*\"great\" + 0.023*\"pill\" + 0.021*\"easy\" + 0.020*\"like\" + 0.019*\"fish\" + 0.018*\"love\"\n",
      "INFO : topic #3 (0.100): 0.040*\"good\" + 0.031*\"product\" + 0.016*\"great\" + 0.014*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"quality\" + 0.009*\"find\" + 0.008*\"capsule\"\n",
      "INFO : topic diff=0.226657, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074430488, 0.12404688, 0.10975026, 0.10218287]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.074): 0.040*\"vitamin\" + 0.021*\"supplement\" + 0.020*\"calcium\" + 0.015*\"need\" + 0.015*\"great\" + 0.014*\"good\" + 0.014*\"tablet\" + 0.013*\"day\" + 0.011*\"easy\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.124): 0.038*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.023*\"use\" + 0.021*\"like\" + 0.017*\"product\" + 0.015*\"love\" + 0.013*\"drink\" + 0.013*\"water\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.110): 0.032*\"taste\" + 0.030*\"good\" + 0.028*\"fish_oil\" + 0.025*\"great\" + 0.025*\"product\" + 0.022*\"pill\" + 0.020*\"easy\" + 0.020*\"like\" + 0.018*\"fish\" + 0.017*\"swallow\"\n",
      "INFO : topic #3 (0.102): 0.040*\"good\" + 0.031*\"product\" + 0.016*\"great\" + 0.014*\"use\" + 0.013*\"price\" + 0.010*\"brand\" + 0.010*\"supplement\" + 0.010*\"quality\" + 0.010*\"find\" + 0.008*\"buy\"\n",
      "INFO : topic diff=0.219607, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.077301733, 0.1260782, 0.11004492, 0.10403794]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.077): 0.038*\"vitamin\" + 0.023*\"calcium\" + 0.020*\"supplement\" + 0.015*\"need\" + 0.014*\"good\" + 0.014*\"great\" + 0.013*\"day\" + 0.013*\"easy\" + 0.012*\"tablet\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.126): 0.037*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.022*\"use\" + 0.021*\"like\" + 0.016*\"product\" + 0.015*\"love\" + 0.013*\"drink\" + 0.012*\"flavor\" + 0.011*\"water\"\n",
      "INFO : topic #2 (0.110): 0.031*\"taste\" + 0.031*\"good\" + 0.027*\"fish_oil\" + 0.025*\"great\" + 0.025*\"product\" + 0.021*\"pill\" + 0.021*\"easy\" + 0.019*\"like\" + 0.018*\"fish\" + 0.017*\"swallow\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #3 (0.104): 0.039*\"good\" + 0.031*\"product\" + 0.015*\"great\" + 0.014*\"use\" + 0.013*\"price\" + 0.010*\"supplement\" + 0.010*\"brand\" + 0.010*\"find\" + 0.010*\"quality\" + 0.008*\"capsule\"\n",
      "INFO : topic diff=0.228245, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.079067096, 0.12962572, 0.10890783, 0.10533924]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.079): 0.039*\"vitamin\" + 0.021*\"calcium\" + 0.019*\"supplement\" + 0.015*\"need\" + 0.015*\"good\" + 0.014*\"great\" + 0.014*\"easy\" + 0.013*\"day\" + 0.011*\"product\" + 0.011*\"pill\"\n",
      "INFO : topic #1 (0.130): 0.037*\"taste\" + 0.024*\"great\" + 0.023*\"good\" + 0.022*\"use\" + 0.021*\"like\" + 0.016*\"product\" + 0.015*\"love\" + 0.013*\"drink\" + 0.013*\"flavor\" + 0.012*\"water\"\n",
      "INFO : topic #2 (0.109): 0.031*\"taste\" + 0.030*\"good\" + 0.025*\"great\" + 0.025*\"product\" + 0.025*\"fish_oil\" + 0.022*\"pill\" + 0.022*\"easy\" + 0.019*\"like\" + 0.019*\"swallow\" + 0.017*\"love\"\n",
      "INFO : topic #3 (0.105): 0.037*\"good\" + 0.032*\"product\" + 0.015*\"great\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"quality\" + 0.010*\"find\" + 0.009*\"capsule\"\n",
      "INFO : topic diff=0.217188, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.079718329, 0.13071243, 0.10919197, 0.11268713]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.080): 0.041*\"vitamin\" + 0.020*\"supplement\" + 0.018*\"calcium\" + 0.015*\"easy\" + 0.015*\"need\" + 0.014*\"great\" + 0.014*\"good\" + 0.013*\"day\" + 0.012*\"pill\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.131): 0.036*\"taste\" + 0.025*\"great\" + 0.023*\"good\" + 0.021*\"like\" + 0.021*\"use\" + 0.016*\"product\" + 0.015*\"love\" + 0.013*\"drink\" + 0.013*\"mix\" + 0.013*\"flavor\"\n",
      "INFO : topic #2 (0.109): 0.028*\"taste\" + 0.028*\"good\" + 0.028*\"fish_oil\" + 0.025*\"product\" + 0.024*\"great\" + 0.023*\"pill\" + 0.021*\"easy\" + 0.019*\"like\" + 0.019*\"swallow\" + 0.017*\"fish\"\n",
      "INFO : topic #3 (0.113): 0.033*\"good\" + 0.033*\"product\" + 0.015*\"great\" + 0.014*\"supplement\" + 0.013*\"use\" + 0.011*\"price\" + 0.010*\"quality\" + 0.009*\"capsule\" + 0.009*\"find\" + 0.009*\"brand\"\n",
      "INFO : topic diff=0.253056, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.078712985, 0.11918753, 0.12079035, 0.09919025]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.079): 0.053*\"vitamin\" + 0.026*\"supplement\" + 0.024*\"tablet\" + 0.021*\"great\" + 0.016*\"melt\" + 0.014*\"need\" + 0.014*\"recommend\" + 0.013*\"calcium\" + 0.013*\"free\" + 0.013*\"easy\"\n",
      "INFO : topic #1 (0.119): 0.051*\"taste\" + 0.026*\"like\" + 0.026*\"great\" + 0.022*\"use\" + 0.022*\"product\" + 0.021*\"good\" + 0.021*\"love\" + 0.015*\"flavor\" + 0.012*\"water\" + 0.012*\"try\"\n",
      "INFO : topic #2 (0.121): 0.041*\"fish_oil\" + 0.039*\"taste\" + 0.032*\"fish\" + 0.029*\"good\" + 0.028*\"pill\" + 0.028*\"product\" + 0.026*\"fishy\" + 0.025*\"try\" + 0.023*\"like\" + 0.023*\"great\"\n",
      "INFO : topic #3 (0.099): 0.034*\"product\" + 0.034*\"good\" + 0.016*\"great\" + 0.016*\"quality\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"price\" + 0.010*\"brand\" + 0.009*\"capsule\" + 0.009*\"find\"\n",
      "INFO : topic diff=0.351006, rho=0.316187\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=5974, num_topics=4, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.27632378672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 56018 documents\n",
      "DEBUG : 56016/56018 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 67%|██████▋   | 2/3 [05:27<02:43, 163.59s/it]INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "INFO : using symmetric eta at 0.2\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 5 topics, 2 passes over the supplied corpus of 56018 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic1_4.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6811/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082991056, 0.098715715, 0.09734308, 0.10524015, 0.1270909]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.083): 0.018*\"use\" + 0.015*\"good\" + 0.015*\"product\" + 0.013*\"taste\" + 0.012*\"like\" + 0.012*\"day\" + 0.011*\"supplement\" + 0.011*\"pill\" + 0.010*\"great\" + 0.009*\"flavor\"\n",
      "INFO : topic #1 (0.099): 0.027*\"taste\" + 0.024*\"great\" + 0.023*\"like\" + 0.021*\"good\" + 0.019*\"use\" + 0.016*\"product\" + 0.014*\"love\" + 0.013*\"work\" + 0.011*\"try\" + 0.010*\"drink\"\n",
      "INFO : topic #2 (0.097): 0.025*\"product\" + 0.022*\"great\" + 0.019*\"use\" + 0.018*\"good\" + 0.017*\"taste\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.011*\"find\" + 0.010*\"love\"\n",
      "INFO : topic #3 (0.105): 0.049*\"good\" + 0.023*\"product\" + 0.017*\"use\" + 0.014*\"taste\" + 0.011*\"drink\" + 0.010*\"easy\" + 0.010*\"great\" + 0.009*\"green\" + 0.009*\"like\" + 0.008*\"recommend\"\n",
      "INFO : topic #4 (0.127): 0.029*\"great\" + 0.028*\"good\" + 0.025*\"product\" + 0.024*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.012*\"vitamin\" + 0.011*\"day\" + 0.010*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic diff=3.974259, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6977/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.076526888, 0.094660856, 0.090477847, 0.091916673, 0.11671007]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.077): 0.016*\"good\" + 0.015*\"use\" + 0.014*\"supplement\" + 0.014*\"product\" + 0.013*\"pill\" + 0.012*\"day\" + 0.011*\"like\" + 0.011*\"taste\" + 0.010*\"flavor\" + 0.009*\"calcium\"\n",
      "INFO : topic #1 (0.095): 0.030*\"taste\" + 0.023*\"like\" + 0.022*\"great\" + 0.021*\"good\" + 0.021*\"use\" + 0.016*\"water\" + 0.015*\"product\" + 0.014*\"love\" + 0.012*\"drink\" + 0.011*\"add\"\n",
      "INFO : topic #2 (0.090): 0.026*\"product\" + 0.025*\"great\" + 0.021*\"good\" + 0.018*\"taste\" + 0.016*\"like\" + 0.016*\"easy\" + 0.015*\"use\" + 0.013*\"vitamin\" + 0.012*\"swallow\" + 0.011*\"find\"\n",
      "INFO : topic #3 (0.092): 0.054*\"good\" + 0.025*\"product\" + 0.016*\"use\" + 0.013*\"taste\" + 0.009*\"great\" + 0.009*\"easy\" + 0.009*\"drink\" + 0.009*\"like\" + 0.008*\"quality\" + 0.008*\"buy\"\n",
      "INFO : topic #4 (0.117): 0.030*\"great\" + 0.029*\"good\" + 0.026*\"taste\" + 0.025*\"product\" + 0.014*\"like\" + 0.014*\"use\" + 0.011*\"vitamin\" + 0.011*\"day\" + 0.010*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic diff=0.689281, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6986/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072219439, 0.093977265, 0.088945098, 0.087710738, 0.11170916]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.072): 0.016*\"good\" + 0.016*\"pill\" + 0.014*\"supplement\" + 0.014*\"day\" + 0.013*\"use\" + 0.013*\"product\" + 0.010*\"like\" + 0.010*\"need\" + 0.010*\"calcium\" + 0.009*\"taste\"\n",
      "INFO : topic #1 (0.094): 0.030*\"taste\" + 0.024*\"use\" + 0.021*\"like\" + 0.021*\"good\" + 0.020*\"great\" + 0.015*\"water\" + 0.014*\"product\" + 0.013*\"love\" + 0.013*\"stevia\" + 0.013*\"drink\"\n",
      "INFO : topic #2 (0.089): 0.026*\"product\" + 0.024*\"great\" + 0.022*\"good\" + 0.018*\"easy\" + 0.016*\"taste\" + 0.015*\"like\" + 0.014*\"vitamin\" + 0.014*\"swallow\" + 0.013*\"use\" + 0.012*\"brand\"\n",
      "INFO : topic #3 (0.088): 0.057*\"good\" + 0.027*\"product\" + 0.017*\"use\" + 0.010*\"taste\" + 0.010*\"brand\" + 0.010*\"quality\" + 0.009*\"price\" + 0.009*\"great\" + 0.008*\"buy\" + 0.008*\"like\"\n",
      "INFO : topic #4 (0.112): 0.031*\"good\" + 0.031*\"great\" + 0.024*\"product\" + 0.024*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"vitamin\" + 0.012*\"day\" + 0.011*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic diff=0.435581, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.071238413, 0.091757968, 0.095398098, 0.083912343, 0.10212166]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.071): 0.017*\"calcium\" + 0.015*\"pill\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.014*\"day\" + 0.014*\"product\" + 0.011*\"use\" + 0.011*\"need\" + 0.010*\"vitamin\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.092): 0.031*\"taste\" + 0.026*\"use\" + 0.021*\"like\" + 0.021*\"great\" + 0.020*\"good\" + 0.014*\"love\" + 0.014*\"stevia\" + 0.014*\"product\" + 0.014*\"water\" + 0.013*\"drink\"\n",
      "INFO : topic #2 (0.095): 0.025*\"product\" + 0.024*\"great\" + 0.024*\"good\" + 0.020*\"easy\" + 0.020*\"taste\" + 0.016*\"vitamin\" + 0.016*\"swallow\" + 0.016*\"like\" + 0.014*\"fish_oil\" + 0.013*\"pill\"\n",
      "INFO : topic #3 (0.084): 0.057*\"good\" + 0.028*\"product\" + 0.016*\"use\" + 0.010*\"price\" + 0.010*\"quality\" + 0.010*\"brand\" + 0.010*\"great\" + 0.009*\"taste\" + 0.008*\"buy\" + 0.008*\"recommend\"\n",
      "INFO : topic #4 (0.102): 0.032*\"great\" + 0.030*\"good\" + 0.026*\"taste\" + 0.024*\"product\" + 0.016*\"vitamin\" + 0.014*\"like\" + 0.012*\"use\" + 0.011*\"love\" + 0.011*\"day\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.367041, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.070579544, 0.089997523, 0.10092239, 0.081622571, 0.099387825]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.071): 0.016*\"calcium\" + 0.016*\"pill\" + 0.015*\"good\" + 0.015*\"day\" + 0.014*\"supplement\" + 0.013*\"product\" + 0.012*\"need\" + 0.012*\"vitamin\" + 0.010*\"use\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.090): 0.033*\"taste\" + 0.024*\"use\" + 0.021*\"like\" + 0.021*\"great\" + 0.020*\"good\" + 0.015*\"love\" + 0.014*\"water\" + 0.014*\"drink\" + 0.014*\"product\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.101): 0.025*\"good\" + 0.024*\"great\" + 0.024*\"product\" + 0.022*\"taste\" + 0.020*\"easy\" + 0.020*\"fish_oil\" + 0.016*\"swallow\" + 0.015*\"like\" + 0.015*\"vitamin\" + 0.013*\"pill\"\n",
      "INFO : topic #3 (0.082): 0.057*\"good\" + 0.028*\"product\" + 0.016*\"use\" + 0.011*\"price\" + 0.011*\"quality\" + 0.010*\"great\" + 0.010*\"brand\" + 0.009*\"fiber\" + 0.008*\"buy\" + 0.008*\"taste\"\n",
      "INFO : topic #4 (0.099): 0.033*\"great\" + 0.030*\"good\" + 0.027*\"taste\" + 0.024*\"product\" + 0.015*\"vitamin\" + 0.015*\"like\" + 0.012*\"love\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.298331, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072371781, 0.090577319, 0.10330685, 0.080168225, 0.09839204]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.072): 0.020*\"calcium\" + 0.016*\"pill\" + 0.015*\"vitamin\" + 0.015*\"day\" + 0.015*\"good\" + 0.015*\"supplement\" + 0.014*\"product\" + 0.012*\"need\" + 0.010*\"vitamin_d\" + 0.010*\"use\"\n",
      "INFO : topic #1 (0.091): 0.032*\"taste\" + 0.024*\"use\" + 0.021*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.015*\"love\" + 0.013*\"drink\" + 0.013*\"product\" + 0.013*\"water\" + 0.011*\"try\"\n",
      "INFO : topic #2 (0.103): 0.026*\"good\" + 0.024*\"product\" + 0.024*\"great\" + 0.022*\"easy\" + 0.022*\"taste\" + 0.020*\"fish_oil\" + 0.017*\"swallow\" + 0.015*\"like\" + 0.014*\"pill\" + 0.014*\"vitamin\"\n",
      "INFO : topic #3 (0.080): 0.056*\"good\" + 0.030*\"product\" + 0.016*\"use\" + 0.012*\"quality\" + 0.011*\"price\" + 0.010*\"brand\" + 0.010*\"fiber\" + 0.010*\"great\" + 0.009*\"buy\" + 0.008*\"find\"\n",
      "INFO : topic #4 (0.098): 0.032*\"great\" + 0.030*\"good\" + 0.027*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"love\" + 0.011*\"use\" + 0.011*\"day\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.289757, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072677583, 0.09193147, 0.10392523, 0.079286486, 0.099990413]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.019*\"calcium\" + 0.017*\"vitamin\" + 0.015*\"supplement\" + 0.015*\"pill\" + 0.014*\"day\" + 0.014*\"good\" + 0.014*\"product\" + 0.012*\"need\" + 0.011*\"vitamin_d\" + 0.010*\"easy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.092): 0.031*\"taste\" + 0.024*\"use\" + 0.021*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.014*\"water\" + 0.014*\"love\" + 0.014*\"drink\" + 0.014*\"product\" + 0.011*\"mix\"\n",
      "INFO : topic #2 (0.104): 0.026*\"good\" + 0.025*\"product\" + 0.024*\"great\" + 0.024*\"easy\" + 0.022*\"taste\" + 0.019*\"fish_oil\" + 0.018*\"swallow\" + 0.016*\"pill\" + 0.015*\"like\" + 0.014*\"vitamin\"\n",
      "INFO : topic #3 (0.079): 0.053*\"good\" + 0.031*\"product\" + 0.016*\"use\" + 0.012*\"quality\" + 0.011*\"price\" + 0.010*\"great\" + 0.010*\"brand\" + 0.009*\"buy\" + 0.008*\"fiber\" + 0.008*\"capsule\"\n",
      "INFO : topic #4 (0.100): 0.034*\"great\" + 0.030*\"good\" + 0.028*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.015*\"vitamin\" + 0.015*\"love\" + 0.011*\"day\" + 0.010*\"use\" + 0.009*\"supplement\"\n",
      "INFO : topic diff=0.250615, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.073198587, 0.09219113, 0.10596499, 0.082720116, 0.10202267]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.018*\"vitamin\" + 0.017*\"supplement\" + 0.016*\"calcium\" + 0.015*\"pill\" + 0.014*\"day\" + 0.014*\"product\" + 0.013*\"good\" + 0.013*\"need\" + 0.011*\"vitamin_d\" + 0.011*\"d3\"\n",
      "INFO : topic #1 (0.092): 0.031*\"taste\" + 0.024*\"use\" + 0.020*\"like\" + 0.020*\"great\" + 0.018*\"good\" + 0.015*\"water\" + 0.014*\"drink\" + 0.014*\"product\" + 0.013*\"love\" + 0.013*\"mix\"\n",
      "INFO : topic #2 (0.106): 0.026*\"product\" + 0.024*\"good\" + 0.024*\"fish_oil\" + 0.023*\"great\" + 0.023*\"easy\" + 0.021*\"taste\" + 0.018*\"swallow\" + 0.018*\"pill\" + 0.016*\"like\" + 0.014*\"capsule\"\n",
      "INFO : topic #3 (0.083): 0.045*\"good\" + 0.033*\"product\" + 0.015*\"use\" + 0.013*\"quality\" + 0.011*\"price\" + 0.010*\"great\" + 0.010*\"supplement\" + 0.009*\"brand\" + 0.008*\"buy\" + 0.008*\"capsule\"\n",
      "INFO : topic #4 (0.102): 0.034*\"great\" + 0.028*\"good\" + 0.027*\"taste\" + 0.023*\"product\" + 0.017*\"like\" + 0.015*\"vitamin\" + 0.015*\"love\" + 0.011*\"supplement\" + 0.011*\"day\" + 0.010*\"use\"\n",
      "INFO : topic diff=0.291566, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.073847488, 0.08889088, 0.12041916, 0.076654047, 0.10978678]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.074): 0.036*\"vitamin\" + 0.027*\"tablet\" + 0.024*\"supplement\" + 0.016*\"vitamin_d\" + 0.015*\"d3\" + 0.014*\"calcium\" + 0.014*\"need\" + 0.012*\"daily\" + 0.012*\"free\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.089): 0.043*\"taste\" + 0.026*\"like\" + 0.023*\"use\" + 0.023*\"great\" + 0.018*\"product\" + 0.016*\"good\" + 0.016*\"love\" + 0.014*\"flavor\" + 0.014*\"water\" + 0.012*\"try\"\n",
      "INFO : topic #2 (0.120): 0.040*\"fish_oil\" + 0.036*\"taste\" + 0.032*\"fish\" + 0.030*\"product\" + 0.027*\"pill\" + 0.026*\"fishy\" + 0.025*\"good\" + 0.023*\"try\" + 0.022*\"like\" + 0.021*\"great\"\n",
      "INFO : topic #3 (0.077): 0.045*\"good\" + 0.035*\"product\" + 0.024*\"quality\" + 0.015*\"gmo\" + 0.013*\"use\" + 0.011*\"great\" + 0.010*\"brand\" + 0.010*\"price\" + 0.009*\"supplement\" + 0.008*\"try\"\n",
      "INFO : topic #4 (0.110): 0.038*\"great\" + 0.033*\"taste\" + 0.027*\"good\" + 0.025*\"vitamin\" + 0.024*\"product\" + 0.024*\"love\" + 0.018*\"like\" + 0.014*\"supplement\" + 0.012*\"recommend\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.327896, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072774641, 0.093293384, 0.11016252, 0.080353752, 0.10722966]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.034*\"vitamin\" + 0.024*\"tablet\" + 0.022*\"supplement\" + 0.017*\"calcium\" + 0.014*\"need\" + 0.013*\"easy\" + 0.012*\"vitamin_d\" + 0.012*\"d3\" + 0.012*\"day\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.093): 0.039*\"taste\" + 0.024*\"like\" + 0.024*\"use\" + 0.021*\"great\" + 0.017*\"good\" + 0.016*\"product\" + 0.015*\"water\" + 0.014*\"love\" + 0.014*\"drink\" + 0.013*\"flavor\"\n",
      "INFO : topic #2 (0.110): 0.036*\"fish_oil\" + 0.035*\"taste\" + 0.029*\"product\" + 0.029*\"fish\" + 0.027*\"pill\" + 0.025*\"good\" + 0.023*\"fishy\" + 0.022*\"like\" + 0.021*\"oil\" + 0.021*\"try\"\n",
      "INFO : topic #3 (0.080): 0.045*\"good\" + 0.034*\"product\" + 0.017*\"quality\" + 0.014*\"use\" + 0.011*\"brand\" + 0.011*\"great\" + 0.011*\"price\" + 0.009*\"'s\" + 0.009*\"capsule\" + 0.008*\"gmo\"\n",
      "INFO : topic #4 (0.107): 0.038*\"great\" + 0.031*\"taste\" + 0.028*\"good\" + 0.024*\"product\" + 0.024*\"vitamin\" + 0.023*\"love\" + 0.018*\"like\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"recommend\"\n",
      "INFO : topic diff=0.271388, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #14000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072757863, 0.098526284, 0.10491498, 0.084196098, 0.10716662]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.032*\"vitamin\" + 0.022*\"supplement\" + 0.021*\"tablet\" + 0.019*\"calcium\" + 0.014*\"need\" + 0.013*\"easy\" + 0.012*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.011*\"pill\"\n",
      "INFO : topic #1 (0.099): 0.037*\"taste\" + 0.025*\"use\" + 0.023*\"like\" + 0.021*\"great\" + 0.018*\"good\" + 0.017*\"water\" + 0.016*\"product\" + 0.014*\"drink\" + 0.013*\"flavor\" + 0.013*\"love\"\n",
      "INFO : topic #2 (0.105): 0.035*\"fish_oil\" + 0.034*\"taste\" + 0.029*\"product\" + 0.027*\"pill\" + 0.027*\"fish\" + 0.026*\"good\" + 0.022*\"like\" + 0.022*\"great\" + 0.020*\"fishy\" + 0.020*\"oil\"\n",
      "INFO : topic #3 (0.084): 0.046*\"good\" + 0.034*\"product\" + 0.015*\"quality\" + 0.015*\"use\" + 0.012*\"price\" + 0.012*\"brand\" + 0.011*\"great\" + 0.009*\"find\" + 0.009*\"buy\" + 0.009*\"'s\"\n",
      "INFO : topic #4 (0.107): 0.037*\"great\" + 0.030*\"taste\" + 0.029*\"good\" + 0.023*\"product\" + 0.022*\"love\" + 0.021*\"vitamin\" + 0.018*\"like\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"recommend\"\n",
      "INFO : topic diff=0.221993, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #21000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.072795108, 0.10259432, 0.099940039, 0.088904865, 0.10792757]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.073): 0.031*\"vitamin\" + 0.021*\"supplement\" + 0.019*\"tablet\" + 0.019*\"calcium\" + 0.014*\"need\" + 0.013*\"day\" + 0.013*\"pill\" + 0.012*\"easy\" + 0.012*\"good\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.103): 0.035*\"taste\" + 0.026*\"use\" + 0.022*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.017*\"water\" + 0.015*\"product\" + 0.014*\"drink\" + 0.012*\"mix\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.100): 0.032*\"taste\" + 0.032*\"fish_oil\" + 0.028*\"product\" + 0.028*\"pill\" + 0.027*\"good\" + 0.024*\"fish\" + 0.022*\"great\" + 0.021*\"easy\" + 0.021*\"like\" + 0.019*\"oil\"\n",
      "INFO : topic #3 (0.089): 0.047*\"good\" + 0.033*\"product\" + 0.015*\"use\" + 0.014*\"brand\" + 0.013*\"quality\" + 0.013*\"price\" + 0.011*\"great\" + 0.010*\"find\" + 0.009*\"capsule\" + 0.009*\"buy\"\n",
      "INFO : topic #4 (0.108): 0.037*\"great\" + 0.031*\"good\" + 0.028*\"taste\" + 0.022*\"product\" + 0.020*\"love\" + 0.019*\"vitamin\" + 0.017*\"like\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.213269, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #28000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.074599639, 0.10354265, 0.10045706, 0.092124641, 0.10679451]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.075): 0.031*\"vitamin\" + 0.023*\"calcium\" + 0.021*\"supplement\" + 0.017*\"tablet\" + 0.014*\"need\" + 0.013*\"easy\" + 0.013*\"pill\" + 0.013*\"day\" + 0.012*\"good\" + 0.011*\"product\"\n",
      "INFO : topic #1 (0.104): 0.034*\"taste\" + 0.027*\"use\" + 0.021*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.016*\"water\" + 0.015*\"product\" + 0.015*\"drink\" + 0.012*\"love\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.100): 0.033*\"taste\" + 0.032*\"fish_oil\" + 0.028*\"good\" + 0.027*\"product\" + 0.027*\"pill\" + 0.023*\"fish\" + 0.022*\"great\" + 0.022*\"easy\" + 0.021*\"swallow\" + 0.020*\"like\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #3 (0.092): 0.046*\"good\" + 0.033*\"product\" + 0.015*\"use\" + 0.014*\"price\" + 0.014*\"brand\" + 0.012*\"quality\" + 0.012*\"great\" + 0.010*\"find\" + 0.010*\"capsule\" + 0.010*\"buy\"\n",
      "INFO : topic #4 (0.107): 0.037*\"great\" + 0.031*\"good\" + 0.030*\"taste\" + 0.022*\"love\" + 0.021*\"product\" + 0.021*\"vitamin\" + 0.018*\"like\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.011*\"day\"\n",
      "INFO : topic diff=0.197169, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #35000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.075631142, 0.10380811, 0.10231829, 0.095108777, 0.107838]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.076): 0.032*\"vitamin\" + 0.021*\"calcium\" + 0.020*\"supplement\" + 0.015*\"tablet\" + 0.014*\"need\" + 0.014*\"easy\" + 0.014*\"pill\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"product\"\n",
      "INFO : topic #1 (0.104): 0.034*\"taste\" + 0.026*\"use\" + 0.021*\"like\" + 0.020*\"great\" + 0.019*\"good\" + 0.016*\"water\" + 0.015*\"drink\" + 0.014*\"product\" + 0.012*\"mix\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.102): 0.035*\"fish_oil\" + 0.033*\"taste\" + 0.028*\"good\" + 0.026*\"product\" + 0.025*\"pill\" + 0.023*\"fish\" + 0.022*\"great\" + 0.021*\"easy\" + 0.020*\"swallow\" + 0.020*\"oil\"\n",
      "INFO : topic #3 (0.095): 0.046*\"good\" + 0.033*\"product\" + 0.015*\"use\" + 0.015*\"price\" + 0.014*\"brand\" + 0.012*\"quality\" + 0.012*\"great\" + 0.011*\"find\" + 0.010*\"buy\" + 0.009*\"capsule\"\n",
      "INFO : topic #4 (0.108): 0.037*\"great\" + 0.031*\"good\" + 0.031*\"taste\" + 0.022*\"love\" + 0.021*\"product\" + 0.020*\"vitamin\" + 0.018*\"like\" + 0.011*\"supplement\" + 0.011*\"day\" + 0.010*\"use\"\n",
      "INFO : topic diff=0.193778, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #42000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.079053432, 0.10463795, 0.10175104, 0.097407311, 0.10945004]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.079): 0.031*\"vitamin\" + 0.024*\"calcium\" + 0.019*\"supplement\" + 0.016*\"easy\" + 0.015*\"pill\" + 0.014*\"need\" + 0.013*\"day\" + 0.013*\"good\" + 0.013*\"tablet\" + 0.012*\"vitamin_d\"\n",
      "INFO : topic #1 (0.105): 0.034*\"taste\" + 0.026*\"use\" + 0.020*\"like\" + 0.019*\"great\" + 0.019*\"good\" + 0.015*\"water\" + 0.015*\"drink\" + 0.014*\"product\" + 0.012*\"flavor\" + 0.012*\"mix\"\n",
      "INFO : topic #2 (0.102): 0.034*\"fish_oil\" + 0.032*\"taste\" + 0.029*\"good\" + 0.026*\"product\" + 0.024*\"pill\" + 0.022*\"great\" + 0.022*\"fish\" + 0.022*\"easy\" + 0.021*\"swallow\" + 0.020*\"oil\"\n",
      "INFO : topic #3 (0.097): 0.045*\"good\" + 0.033*\"product\" + 0.015*\"use\" + 0.015*\"price\" + 0.014*\"brand\" + 0.013*\"quality\" + 0.011*\"great\" + 0.011*\"find\" + 0.010*\"capsule\" + 0.010*\"buy\"\n",
      "INFO : topic #4 (0.109): 0.037*\"great\" + 0.031*\"good\" + 0.031*\"taste\" + 0.022*\"love\" + 0.021*\"product\" + 0.019*\"like\" + 0.018*\"vitamin\" + 0.011*\"eat\" + 0.010*\"supplement\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.198579, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #49000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.081233308, 0.10647819, 0.10017716, 0.099308282, 0.11292901]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.081): 0.032*\"vitamin\" + 0.022*\"calcium\" + 0.018*\"supplement\" + 0.017*\"easy\" + 0.015*\"pill\" + 0.014*\"need\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"product\" + 0.012*\"vitamin_d\"\n",
      "INFO : topic #1 (0.106): 0.033*\"taste\" + 0.025*\"use\" + 0.020*\"like\" + 0.019*\"great\" + 0.019*\"good\" + 0.016*\"water\" + 0.015*\"drink\" + 0.014*\"product\" + 0.012*\"flavor\" + 0.012*\"mix\"\n",
      "INFO : topic #2 (0.100): 0.032*\"fish_oil\" + 0.031*\"taste\" + 0.028*\"good\" + 0.027*\"product\" + 0.026*\"pill\" + 0.023*\"easy\" + 0.022*\"great\" + 0.022*\"swallow\" + 0.021*\"fish\" + 0.020*\"oil\"\n",
      "INFO : topic #3 (0.099): 0.043*\"good\" + 0.033*\"product\" + 0.014*\"use\" + 0.014*\"price\" + 0.013*\"brand\" + 0.013*\"quality\" + 0.012*\"great\" + 0.011*\"capsule\" + 0.011*\"find\" + 0.010*\"buy\"\n",
      "INFO : topic #4 (0.113): 0.038*\"great\" + 0.032*\"taste\" + 0.032*\"good\" + 0.024*\"love\" + 0.021*\"product\" + 0.019*\"like\" + 0.018*\"vitamin\" + 0.011*\"eat\" + 0.010*\"day\" + 0.010*\"supplement\"\n",
      "INFO : topic diff=0.187527, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56000/56018\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082621761, 0.10741118, 0.10084426, 0.10615785, 0.11566959]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.083): 0.033*\"vitamin\" + 0.020*\"supplement\" + 0.019*\"calcium\" + 0.018*\"easy\" + 0.015*\"pill\" + 0.014*\"need\" + 0.013*\"day\" + 0.013*\"vitamin_d\" + 0.012*\"product\" + 0.012*\"good\"\n",
      "INFO : topic #1 (0.107): 0.032*\"taste\" + 0.025*\"use\" + 0.020*\"like\" + 0.019*\"great\" + 0.018*\"good\" + 0.016*\"water\" + 0.015*\"drink\" + 0.014*\"product\" + 0.014*\"mix\" + 0.012*\"flavor\"\n",
      "INFO : topic #2 (0.101): 0.035*\"fish_oil\" + 0.028*\"taste\" + 0.027*\"product\" + 0.026*\"pill\" + 0.026*\"good\" + 0.022*\"easy\" + 0.022*\"swallow\" + 0.021*\"great\" + 0.021*\"fish\" + 0.019*\"oil\"\n",
      "INFO : topic #3 (0.106): 0.038*\"good\" + 0.035*\"product\" + 0.014*\"use\" + 0.013*\"price\" + 0.013*\"quality\" + 0.012*\"great\" + 0.012*\"brand\" + 0.011*\"supplement\" + 0.011*\"capsule\" + 0.010*\"find\"\n",
      "INFO : topic #4 (0.116): 0.038*\"great\" + 0.031*\"taste\" + 0.030*\"good\" + 0.023*\"love\" + 0.021*\"product\" + 0.020*\"like\" + 0.018*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"day\" + 0.010*\"eat\"\n",
      "INFO : topic diff=0.224677, rho=0.316187\n",
      "INFO : PROGRESS: pass 1, at document #56018/56018\n",
      "DEBUG : performing inference on a chunk of 18 documents\n",
      "DEBUG : 18/18 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.082363188, 0.10124576, 0.11445862, 0.093113147, 0.11772206]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18 documents into a model of 56018 documents\n",
      "INFO : topic #0 (0.082): 0.047*\"vitamin\" + 0.030*\"tablet\" + 0.025*\"supplement\" + 0.016*\"easy\" + 0.015*\"calcium\" + 0.015*\"vitamin_d\" + 0.015*\"need\" + 0.015*\"d3\" + 0.014*\"free\" + 0.014*\"great\"\n",
      "INFO : topic #1 (0.101): 0.042*\"taste\" + 0.025*\"use\" + 0.024*\"like\" + 0.021*\"great\" + 0.018*\"product\" + 0.017*\"good\" + 0.017*\"water\" + 0.015*\"flavor\" + 0.012*\"drink\" + 0.012*\"try\"\n",
      "INFO : topic #2 (0.114): 0.048*\"fish_oil\" + 0.042*\"taste\" + 0.038*\"fish\" + 0.032*\"pill\" + 0.031*\"product\" + 0.030*\"fishy\" + 0.028*\"try\" + 0.026*\"good\" + 0.025*\"oil\" + 0.024*\"like\"\n",
      "INFO : topic #3 (0.093): 0.039*\"good\" + 0.036*\"product\" + 0.021*\"quality\" + 0.013*\"brand\" + 0.013*\"use\" + 0.013*\"great\" + 0.012*\"price\" + 0.011*\"gmo\" + 0.010*\"supplement\" + 0.010*\"capsule\"\n",
      "INFO : topic #4 (0.118): 0.041*\"great\" + 0.038*\"taste\" + 0.034*\"love\" + 0.029*\"good\" + 0.023*\"vitamin\" + 0.022*\"product\" + 0.022*\"like\" + 0.013*\"recommend\" + 0.013*\"supplement\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.318335, rho=0.316187\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=5974, num_topics=5, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 5 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.373443323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 56018 documents\n",
      "DEBUG : 56017/56018 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 3/3 [08:21<00:00, 167.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic1_5.html\n",
      "CPU times: user 8min 18s, sys: 2.59 s, total: 8min 21s\n",
      "Wall time: 8min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [3, 4, 5]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_1 = LdaModel(bow_corpus_1, num_topics=num_topics, id2word=vocab_dictionary_1, chunksize=chunksize, \n",
    "                     passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    save_df_s3(lda_1, bucket_name, filepath='amazon_reviews/kk/lda_tier2_topic1_{}.pkl'.format(num_topics), filetype='pickle')\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda_1, corpus=bow_corpus_1, dictionary=vocab_dictionary_1, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_1, bow_corpus_1, vocab_dictionary_1)\n",
    "    plot_fname = 'pyLDAvis_tier2_topic1_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(13311 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "INFO : adding document #20000 to Dictionary(20414 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "INFO : adding document #30000 to Dictionary(26038 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "INFO : adding document #40000 to Dictionary(31383 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "INFO : adding document #50000 to Dictionary(36542 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "INFO : built Dictionary(41144 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...) from 59465 documents (total 1170796 corpus positions)\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n",
      "INFO : discarding 9786 tokens: [('introduction', 7), ('minutes', 4), ('someplace', 2), ('financial', 6), ('insoluble', 2), ('optimize', 5), ('revgenetics', 2), ('saliva', 2), ('selling_point', 5), ('darker', 5)]...\n",
      "INFO : keeping 4936 tokens which were in no less than 10 and no more than 59465 (=100.0%) documents\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n",
      "INFO : resulting dictionary: Dictionary(4936 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n",
      "DEBUG : rebuilding dictionary, shrinking gaps\n"
     ]
    }
   ],
   "source": [
    "# Topic 2\n",
    "vocab_dictionary_2 = Dictionary(docs_topic_2)\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(vocab_dictionary_2.dfs) if docfreq == 1]\n",
    "vocab_dictionary_2.filter_tokens(bad_ids=once_ids)  # remove words that appear only once\n",
    "vocab_dictionary_2.filter_extremes(no_below=10, no_above=1.0)\n",
    "vocab_dictionary_2.compactify()\n",
    "save_df_s3(vocab_dictionary_2, bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic2.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_dictionary_2 = load_df_s3(bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_topic2.dict', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4936 unique tokens: ['anymore', 'believe', 'claim', 'company', 'contact']...)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dictionary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus_2 = [vocab_dictionary_2.doc2bow(review) for review in docs_topic_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.33333334, 0.33333334, 0.33333334]\n",
      "INFO : using symmetric eta at 0.3333333333333333\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 3 topics, 2 passes over the supplied corpus of 59465 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6759/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19838814, 0.21123424, 0.096921355]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.198): 0.056*\"product\" + 0.032*\"great\" + 0.030*\"use\" + 0.027*\"good\" + 0.017*\"skin\" + 0.017*\"work\" + 0.014*\"recommend\" + 0.012*\"buy\" + 0.011*\"time\" + 0.011*\"order\"\n",
      "INFO : topic #1 (0.211): 0.044*\"product\" + 0.035*\"use\" + 0.030*\"good\" + 0.027*\"great\" + 0.018*\"love\" + 0.014*\"price\" + 0.014*\"buy\" + 0.014*\"work\" + 0.013*\"skin\" + 0.010*\"purchase\"\n",
      "INFO : topic #2 (0.097): 0.040*\"product\" + 0.039*\"great\" + 0.026*\"hair\" + 0.021*\"good\" + 0.021*\"use\" + 0.015*\"work\" + 0.012*\"find\" + 0.011*\"like\" + 0.010*\"skin\" + 0.010*\"price\"\n",
      "INFO : topic diff=2.789182, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6973/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1794299, 0.1681515, 0.084878094]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.179): 0.061*\"product\" + 0.032*\"great\" + 0.029*\"good\" + 0.028*\"use\" + 0.017*\"work\" + 0.016*\"skin\" + 0.015*\"recommend\" + 0.013*\"price\" + 0.013*\"order\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.168): 0.053*\"product\" + 0.036*\"good\" + 0.031*\"use\" + 0.029*\"great\" + 0.019*\"love\" + 0.019*\"price\" + 0.015*\"buy\" + 0.013*\"work\" + 0.011*\"purchase\" + 0.011*\"skin\"\n",
      "INFO : topic #2 (0.085): 0.040*\"product\" + 0.036*\"hair\" + 0.035*\"great\" + 0.020*\"use\" + 0.020*\"good\" + 0.015*\"work\" + 0.014*\"nail\" + 0.011*\"find\" + 0.010*\"like\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.761131, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6985/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16493614, 0.15590477, 0.077798463]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.165): 0.061*\"product\" + 0.033*\"great\" + 0.029*\"use\" + 0.028*\"good\" + 0.017*\"work\" + 0.017*\"skin\" + 0.015*\"recommend\" + 0.013*\"order\" + 0.011*\"price\" + 0.011*\"time\"\n",
      "INFO : topic #1 (0.156): 0.058*\"product\" + 0.040*\"good\" + 0.033*\"great\" + 0.030*\"use\" + 0.021*\"price\" + 0.019*\"love\" + 0.015*\"buy\" + 0.013*\"work\" + 0.011*\"purchase\" + 0.009*\"amazon\"\n",
      "INFO : topic #2 (0.078): 0.045*\"hair\" + 0.034*\"product\" + 0.029*\"great\" + 0.023*\"use\" + 0.017*\"good\" + 0.015*\"nail\" + 0.014*\"work\" + 0.012*\"grow\" + 0.011*\"skin\" + 0.010*\"like\"\n",
      "INFO : topic diff=0.507344, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6988/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14928897, 0.15527973, 0.072167814]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.149): 0.061*\"product\" + 0.033*\"great\" + 0.030*\"use\" + 0.027*\"good\" + 0.017*\"work\" + 0.016*\"skin\" + 0.016*\"recommend\" + 0.013*\"order\" + 0.011*\"time\" + 0.011*\"price\"\n",
      "INFO : topic #1 (0.155): 0.063*\"product\" + 0.044*\"good\" + 0.037*\"great\" + 0.027*\"price\" + 0.026*\"use\" + 0.017*\"love\" + 0.017*\"buy\" + 0.012*\"work\" + 0.011*\"purchase\" + 0.010*\"amazon\"\n",
      "INFO : topic #2 (0.072): 0.049*\"hair\" + 0.030*\"product\" + 0.026*\"great\" + 0.024*\"use\" + 0.017*\"nail\" + 0.016*\"good\" + 0.015*\"skin\" + 0.013*\"grow\" + 0.013*\"work\" + 0.010*\"star\"\n",
      "INFO : topic diff=0.401563, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13606775, 0.16060609, 0.068763755]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.136): 0.060*\"product\" + 0.031*\"great\" + 0.029*\"use\" + 0.026*\"good\" + 0.017*\"work\" + 0.017*\"recommend\" + 0.016*\"skin\" + 0.013*\"order\" + 0.011*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.161): 0.066*\"product\" + 0.047*\"good\" + 0.039*\"great\" + 0.028*\"price\" + 0.023*\"use\" + 0.018*\"buy\" + 0.016*\"love\" + 0.011*\"recommend\" + 0.011*\"purchase\" + 0.011*\"quality\"\n",
      "INFO : topic #2 (0.069): 0.049*\"hair\" + 0.027*\"product\" + 0.025*\"use\" + 0.022*\"great\" + 0.020*\"nail\" + 0.019*\"skin\" + 0.015*\"good\" + 0.014*\"grow\" + 0.013*\"work\" + 0.012*\"love\"\n",
      "INFO : topic diff=0.363187, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12693991, 0.15888068, 0.068669587]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.127): 0.059*\"product\" + 0.030*\"use\" + 0.030*\"great\" + 0.024*\"good\" + 0.017*\"work\" + 0.016*\"recommend\" + 0.016*\"skin\" + 0.012*\"order\" + 0.010*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.159): 0.069*\"product\" + 0.047*\"good\" + 0.040*\"great\" + 0.028*\"price\" + 0.021*\"use\" + 0.018*\"buy\" + 0.016*\"love\" + 0.011*\"recommend\" + 0.011*\"purchase\" + 0.011*\"quality\"\n",
      "INFO : topic #2 (0.069): 0.059*\"hair\" + 0.027*\"nail\" + 0.025*\"product\" + 0.024*\"use\" + 0.020*\"grow\" + 0.019*\"skin\" + 0.018*\"great\" + 0.013*\"work\" + 0.013*\"good\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.363906, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12419234, 0.1652308, 0.06579753]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.124): 0.057*\"product\" + 0.030*\"use\" + 0.028*\"great\" + 0.022*\"good\" + 0.018*\"work\" + 0.016*\"skin\" + 0.015*\"recommend\" + 0.011*\"order\" + 0.011*\"try\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.165): 0.071*\"product\" + 0.046*\"good\" + 0.041*\"great\" + 0.026*\"price\" + 0.020*\"use\" + 0.017*\"buy\" + 0.014*\"love\" + 0.011*\"recommend\" + 0.011*\"quality\" + 0.010*\"purchase\"\n",
      "INFO : topic #2 (0.066): 0.057*\"hair\" + 0.027*\"nail\" + 0.023*\"use\" + 0.023*\"product\" + 0.020*\"skin\" + 0.019*\"grow\" + 0.017*\"great\" + 0.013*\"work\" + 0.012*\"good\" + 0.012*\"love\"\n",
      "INFO : topic diff=0.313758, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1236627, 0.16747525, 0.064128868]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.124): 0.055*\"product\" + 0.031*\"use\" + 0.026*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.017*\"work\" + 0.014*\"recommend\" + 0.011*\"try\" + 0.010*\"order\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.167): 0.073*\"product\" + 0.044*\"good\" + 0.041*\"great\" + 0.024*\"price\" + 0.018*\"use\" + 0.016*\"buy\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"quality\" + 0.010*\"purchase\"\n",
      "INFO : topic #2 (0.064): 0.054*\"hair\" + 0.025*\"nail\" + 0.024*\"skin\" + 0.024*\"use\" + 0.023*\"product\" + 0.017*\"grow\" + 0.016*\"great\" + 0.013*\"work\" + 0.013*\"love\" + 0.012*\"good\"\n",
      "INFO : topic diff=0.307069, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3462/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12798628, 0.16204047, 0.062971964]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.128): 0.052*\"product\" + 0.029*\"use\" + 0.028*\"skin\" + 0.022*\"great\" + 0.019*\"good\" + 0.016*\"work\" + 0.013*\"recommend\" + 0.013*\"try\" + 0.012*\"feel\" + 0.010*\"look\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.162): 0.075*\"product\" + 0.042*\"great\" + 0.040*\"good\" + 0.020*\"price\" + 0.017*\"use\" + 0.014*\"buy\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"quality\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.063): 0.049*\"hair\" + 0.031*\"skin\" + 0.022*\"use\" + 0.022*\"product\" + 0.020*\"nail\" + 0.014*\"grow\" + 0.014*\"great\" + 0.014*\"notice\" + 0.014*\"look\" + 0.013*\"love\"\n",
      "INFO : topic diff=0.358805, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12075604, 0.16356596, 0.062308125]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.121): 0.050*\"product\" + 0.032*\"use\" + 0.028*\"skin\" + 0.022*\"great\" + 0.018*\"good\" + 0.016*\"work\" + 0.013*\"recommend\" + 0.012*\"try\" + 0.011*\"feel\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.164): 0.073*\"product\" + 0.044*\"great\" + 0.041*\"good\" + 0.022*\"price\" + 0.019*\"use\" + 0.015*\"buy\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"work\" + 0.011*\"quality\"\n",
      "INFO : topic #2 (0.062): 0.050*\"hair\" + 0.026*\"skin\" + 0.025*\"use\" + 0.020*\"product\" + 0.018*\"nail\" + 0.015*\"great\" + 0.014*\"grow\" + 0.014*\"love\" + 0.013*\"work\" + 0.011*\"look\"\n",
      "INFO : topic diff=0.292940, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11752956, 0.17081995, 0.060517386]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.118): 0.049*\"product\" + 0.033*\"use\" + 0.028*\"skin\" + 0.022*\"great\" + 0.018*\"good\" + 0.016*\"work\" + 0.012*\"recommend\" + 0.012*\"try\" + 0.011*\"feel\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.171): 0.075*\"product\" + 0.045*\"great\" + 0.043*\"good\" + 0.024*\"price\" + 0.019*\"use\" + 0.016*\"buy\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"work\" + 0.011*\"order\"\n",
      "INFO : topic #2 (0.061): 0.051*\"hair\" + 0.025*\"skin\" + 0.025*\"use\" + 0.019*\"product\" + 0.019*\"nail\" + 0.014*\"grow\" + 0.014*\"great\" + 0.014*\"love\" + 0.013*\"work\" + 0.011*\"notice\"\n",
      "INFO : topic diff=0.237043, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11462095, 0.17640878, 0.059367098]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.115): 0.047*\"product\" + 0.035*\"use\" + 0.027*\"skin\" + 0.022*\"great\" + 0.018*\"good\" + 0.017*\"work\" + 0.012*\"recommend\" + 0.011*\"try\" + 0.010*\"feel\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.176): 0.076*\"product\" + 0.045*\"great\" + 0.044*\"good\" + 0.025*\"price\" + 0.020*\"use\" + 0.016*\"buy\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.012*\"order\"\n",
      "INFO : topic #2 (0.059): 0.054*\"hair\" + 0.026*\"use\" + 0.023*\"skin\" + 0.019*\"product\" + 0.019*\"nail\" + 0.015*\"grow\" + 0.013*\"great\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.230283, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11047253, 0.18669674, 0.058001194]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.110): 0.046*\"product\" + 0.035*\"use\" + 0.027*\"skin\" + 0.022*\"great\" + 0.018*\"good\" + 0.017*\"work\" + 0.012*\"recommend\" + 0.011*\"try\" + 0.010*\"help\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.187): 0.076*\"product\" + 0.046*\"great\" + 0.046*\"good\" + 0.027*\"price\" + 0.020*\"use\" + 0.016*\"buy\" + 0.013*\"recommend\" + 0.013*\"work\" + 0.013*\"love\" + 0.012*\"order\"\n",
      "INFO : topic #2 (0.058): 0.055*\"hair\" + 0.026*\"use\" + 0.022*\"skin\" + 0.019*\"nail\" + 0.019*\"product\" + 0.015*\"grow\" + 0.014*\"great\" + 0.013*\"love\" + 0.012*\"work\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.218607, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10628204, 0.19895226, 0.056982826]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.106): 0.044*\"product\" + 0.035*\"use\" + 0.027*\"skin\" + 0.021*\"great\" + 0.018*\"good\" + 0.017*\"work\" + 0.012*\"recommend\" + 0.011*\"try\" + 0.011*\"help\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.199): 0.076*\"product\" + 0.047*\"good\" + 0.045*\"great\" + 0.027*\"price\" + 0.019*\"use\" + 0.017*\"buy\" + 0.014*\"recommend\" + 0.013*\"order\" + 0.012*\"love\" + 0.012*\"work\"\n",
      "INFO : topic #2 (0.057): 0.055*\"hair\" + 0.027*\"use\" + 0.023*\"skin\" + 0.022*\"nail\" + 0.018*\"product\" + 0.015*\"grow\" + 0.014*\"love\" + 0.013*\"great\" + 0.012*\"work\" + 0.011*\"notice\"\n",
      "INFO : topic diff=0.215297, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10274154, 0.20299606, 0.057586666]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.103): 0.043*\"product\" + 0.036*\"use\" + 0.026*\"skin\" + 0.020*\"great\" + 0.017*\"good\" + 0.016*\"work\" + 0.011*\"recommend\" + 0.011*\"help\" + 0.011*\"try\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.203): 0.077*\"product\" + 0.046*\"good\" + 0.045*\"great\" + 0.027*\"price\" + 0.019*\"use\" + 0.017*\"buy\" + 0.014*\"recommend\" + 0.013*\"order\" + 0.012*\"love\" + 0.012*\"work\"\n",
      "INFO : topic #2 (0.058): 0.063*\"hair\" + 0.027*\"nail\" + 0.025*\"use\" + 0.020*\"skin\" + 0.020*\"grow\" + 0.018*\"product\" + 0.013*\"love\" + 0.013*\"work\" + 0.012*\"great\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.242926, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10224304, 0.21460806, 0.056387577]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.102): 0.042*\"product\" + 0.035*\"use\" + 0.025*\"skin\" + 0.019*\"great\" + 0.016*\"work\" + 0.016*\"good\" + 0.011*\"try\" + 0.011*\"help\" + 0.011*\"recommend\" + 0.011*\"day\"\n",
      "INFO : topic #1 (0.215): 0.077*\"product\" + 0.045*\"good\" + 0.044*\"great\" + 0.025*\"price\" + 0.019*\"use\" + 0.016*\"buy\" + 0.014*\"recommend\" + 0.013*\"order\" + 0.012*\"work\" + 0.012*\"love\"\n",
      "INFO : topic #2 (0.056): 0.062*\"hair\" + 0.029*\"nail\" + 0.024*\"use\" + 0.020*\"skin\" + 0.020*\"grow\" + 0.018*\"product\" + 0.013*\"love\" + 0.013*\"work\" + 0.012*\"strong\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.218356, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10385808, 0.21965042, 0.055199124]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.104): 0.041*\"product\" + 0.035*\"use\" + 0.030*\"skin\" + 0.018*\"great\" + 0.016*\"good\" + 0.015*\"work\" + 0.012*\"feel\" + 0.011*\"try\" + 0.011*\"day\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.220): 0.078*\"product\" + 0.044*\"great\" + 0.043*\"good\" + 0.023*\"price\" + 0.018*\"use\" + 0.015*\"buy\" + 0.014*\"recommend\" + 0.013*\"order\" + 0.012*\"work\" + 0.012*\"love\"\n",
      "INFO : topic #2 (0.055): 0.063*\"hair\" + 0.029*\"nail\" + 0.024*\"use\" + 0.020*\"skin\" + 0.020*\"grow\" + 0.018*\"product\" + 0.013*\"love\" + 0.012*\"work\" + 0.012*\"strong\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.233190, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3464/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10906629, 0.21190165, 0.053790573]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.109): 0.040*\"product\" + 0.038*\"skin\" + 0.032*\"use\" + 0.016*\"great\" + 0.015*\"good\" + 0.014*\"work\" + 0.014*\"feel\" + 0.013*\"look\" + 0.012*\"try\" + 0.010*\"week\"\n",
      "INFO : topic #1 (0.212): 0.079*\"product\" + 0.045*\"great\" + 0.040*\"good\" + 0.020*\"price\" + 0.016*\"use\" + 0.014*\"recommend\" + 0.014*\"buy\" + 0.012*\"order\" + 0.012*\"work\" + 0.012*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.054): 0.063*\"hair\" + 0.025*\"nail\" + 0.022*\"use\" + 0.020*\"skin\" + 0.018*\"product\" + 0.018*\"grow\" + 0.013*\"notice\" + 0.013*\"love\" + 0.012*\"strong\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.302513, rho=0.308680\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=4936, num_topics=3, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 3 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.03372968073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 59465 documents\n",
      "DEBUG : 59456/59465 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 33%|███▎      | 1/3 [02:35<05:11, 155.52s/it]INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 0.25\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 4 topics, 2 passes over the supplied corpus of 59465 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic2_3.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6878/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14096084, 0.15575457, 0.079410776, 0.12455767]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.141): 0.059*\"product\" + 0.031*\"use\" + 0.028*\"great\" + 0.027*\"good\" + 0.018*\"work\" + 0.016*\"skin\" + 0.014*\"buy\" + 0.013*\"recommend\" + 0.013*\"order\" + 0.011*\"price\"\n",
      "INFO : topic #1 (0.156): 0.046*\"product\" + 0.037*\"use\" + 0.030*\"good\" + 0.024*\"great\" + 0.018*\"love\" + 0.017*\"buy\" + 0.016*\"work\" + 0.014*\"price\" + 0.011*\"skin\" + 0.010*\"purchase\"\n",
      "INFO : topic #2 (0.079): 0.040*\"product\" + 0.030*\"great\" + 0.027*\"hair\" + 0.021*\"use\" + 0.020*\"good\" + 0.016*\"work\" + 0.014*\"find\" + 0.012*\"like\" + 0.010*\"help\" + 0.010*\"price\"\n",
      "INFO : topic #3 (0.125): 0.045*\"great\" + 0.043*\"product\" + 0.026*\"good\" + 0.025*\"use\" + 0.019*\"skin\" + 0.016*\"hair\" + 0.012*\"love\" + 0.012*\"price\" + 0.011*\"recommend\" + 0.010*\"work\"\n",
      "INFO : topic diff=3.482182, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13214318, 0.13104248, 0.070488051, 0.11190992]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.132): 0.065*\"product\" + 0.030*\"good\" + 0.029*\"great\" + 0.028*\"use\" + 0.019*\"work\" + 0.015*\"order\" + 0.015*\"recommend\" + 0.014*\"price\" + 0.013*\"buy\" + 0.013*\"skin\"\n",
      "INFO : topic #1 (0.131): 0.056*\"product\" + 0.037*\"good\" + 0.033*\"use\" + 0.026*\"great\" + 0.020*\"price\" + 0.018*\"love\" + 0.017*\"buy\" + 0.015*\"work\" + 0.011*\"purchase\" + 0.009*\"quality\"\n",
      "INFO : topic #2 (0.070): 0.041*\"product\" + 0.036*\"hair\" + 0.027*\"great\" + 0.020*\"use\" + 0.019*\"good\" + 0.016*\"work\" + 0.013*\"find\" + 0.011*\"star\" + 0.011*\"like\" + 0.011*\"love\"\n",
      "INFO : topic #3 (0.112): 0.046*\"product\" + 0.044*\"great\" + 0.026*\"good\" + 0.025*\"use\" + 0.022*\"skin\" + 0.014*\"hair\" + 0.013*\"love\" + 0.013*\"price\" + 0.012*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.733343, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6990/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12863502, 0.12397455, 0.066099547, 0.10249962]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.129): 0.067*\"product\" + 0.031*\"great\" + 0.030*\"good\" + 0.029*\"use\" + 0.019*\"work\" + 0.017*\"order\" + 0.015*\"recommend\" + 0.013*\"price\" + 0.013*\"buy\" + 0.012*\"time\"\n",
      "INFO : topic #1 (0.124): 0.061*\"product\" + 0.043*\"good\" + 0.030*\"use\" + 0.029*\"great\" + 0.023*\"price\" + 0.018*\"buy\" + 0.017*\"love\" + 0.014*\"work\" + 0.012*\"purchase\" + 0.010*\"quality\"\n",
      "INFO : topic #2 (0.066): 0.048*\"hair\" + 0.035*\"product\" + 0.023*\"use\" + 0.023*\"great\" + 0.017*\"good\" + 0.016*\"work\" + 0.012*\"star\" + 0.011*\"like\" + 0.011*\"love\" + 0.011*\"grow\"\n",
      "INFO : topic #3 (0.102): 0.043*\"product\" + 0.041*\"great\" + 0.027*\"use\" + 0.027*\"skin\" + 0.024*\"good\" + 0.013*\"love\" + 0.011*\"hair\" + 0.011*\"recommend\" + 0.010*\"cream\" + 0.010*\"price\"\n",
      "INFO : topic diff=0.470935, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12531394, 0.12571585, 0.063009568, 0.093178064]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.125): 0.069*\"product\" + 0.033*\"great\" + 0.030*\"good\" + 0.027*\"use\" + 0.019*\"work\" + 0.017*\"order\" + 0.016*\"recommend\" + 0.013*\"price\" + 0.013*\"time\" + 0.012*\"buy\"\n",
      "INFO : topic #1 (0.126): 0.065*\"product\" + 0.048*\"good\" + 0.034*\"great\" + 0.029*\"price\" + 0.027*\"use\" + 0.019*\"buy\" + 0.016*\"love\" + 0.013*\"work\" + 0.012*\"purchase\" + 0.012*\"quality\"\n",
      "INFO : topic #2 (0.063): 0.056*\"hair\" + 0.031*\"product\" + 0.024*\"use\" + 0.021*\"great\" + 0.015*\"work\" + 0.015*\"good\" + 0.014*\"grow\" + 0.014*\"nail\" + 0.013*\"star\" + 0.012*\"oil\"\n",
      "INFO : topic #3 (0.093): 0.039*\"product\" + 0.038*\"great\" + 0.033*\"skin\" + 0.029*\"use\" + 0.023*\"good\" + 0.014*\"love\" + 0.011*\"recommend\" + 0.010*\"look\" + 0.010*\"feel\" + 0.009*\"hair\"\n",
      "INFO : topic diff=0.369425, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12136073, 0.13012823, 0.060419623, 0.088944674]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.121): 0.070*\"product\" + 0.033*\"great\" + 0.029*\"good\" + 0.026*\"use\" + 0.019*\"work\" + 0.018*\"order\" + 0.018*\"recommend\" + 0.013*\"time\" + 0.012*\"price\" + 0.012*\"buy\"\n",
      "INFO : topic #1 (0.130): 0.069*\"product\" + 0.052*\"good\" + 0.038*\"great\" + 0.032*\"price\" + 0.024*\"use\" + 0.021*\"buy\" + 0.016*\"love\" + 0.013*\"quality\" + 0.012*\"purchase\" + 0.011*\"amazon\"\n",
      "INFO : topic #2 (0.060): 0.058*\"hair\" + 0.029*\"product\" + 0.025*\"use\" + 0.019*\"nail\" + 0.018*\"great\" + 0.016*\"grow\" + 0.015*\"work\" + 0.015*\"star\" + 0.014*\"good\" + 0.012*\"oil\"\n",
      "INFO : topic #3 (0.089): 0.036*\"product\" + 0.035*\"skin\" + 0.034*\"great\" + 0.029*\"use\" + 0.022*\"good\" + 0.015*\"love\" + 0.011*\"recommend\" + 0.011*\"feel\" + 0.010*\"look\" + 0.009*\"day\"\n",
      "INFO : topic diff=0.333085, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11888791, 0.13138549, 0.0613348, 0.08548145]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.071*\"product\" + 0.033*\"great\" + 0.028*\"good\" + 0.027*\"use\" + 0.019*\"work\" + 0.019*\"order\" + 0.018*\"recommend\" + 0.013*\"time\" + 0.011*\"buy\" + 0.011*\"price\"\n",
      "INFO : topic #1 (0.131): 0.070*\"product\" + 0.052*\"good\" + 0.039*\"great\" + 0.032*\"price\" + 0.022*\"use\" + 0.020*\"buy\" + 0.016*\"love\" + 0.013*\"quality\" + 0.012*\"purchase\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.061): 0.072*\"hair\" + 0.030*\"nail\" + 0.026*\"product\" + 0.024*\"grow\" + 0.024*\"use\" + 0.016*\"work\" + 0.015*\"great\" + 0.013*\"good\" + 0.012*\"month\" + 0.012*\"biotin\"\n",
      "INFO : topic #3 (0.085): 0.037*\"skin\" + 0.034*\"product\" + 0.030*\"great\" + 0.029*\"use\" + 0.021*\"good\" + 0.014*\"love\" + 0.012*\"feel\" + 0.010*\"look\" + 0.010*\"recommend\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.337149, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1192392, 0.13706605, 0.059342336, 0.083903007]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.072*\"product\" + 0.033*\"great\" + 0.026*\"good\" + 0.026*\"use\" + 0.021*\"work\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.010*\"try\" + 0.010*\"buy\"\n",
      "INFO : topic #1 (0.137): 0.072*\"product\" + 0.050*\"good\" + 0.040*\"great\" + 0.030*\"price\" + 0.020*\"use\" + 0.019*\"buy\" + 0.014*\"love\" + 0.013*\"quality\" + 0.012*\"purchase\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.059): 0.071*\"hair\" + 0.031*\"nail\" + 0.025*\"product\" + 0.024*\"grow\" + 0.023*\"use\" + 0.016*\"work\" + 0.015*\"great\" + 0.013*\"strong\" + 0.013*\"biotin\" + 0.012*\"month\"\n",
      "INFO : topic #3 (0.084): 0.038*\"skin\" + 0.032*\"product\" + 0.029*\"use\" + 0.026*\"great\" + 0.019*\"good\" + 0.014*\"love\" + 0.012*\"feel\" + 0.011*\"look\" + 0.010*\"day\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.294380, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11834179, 0.14023943, 0.057776589, 0.085992798]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.118): 0.074*\"product\" + 0.033*\"great\" + 0.025*\"use\" + 0.025*\"good\" + 0.020*\"work\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.011*\"try\" + 0.009*\"buy\"\n",
      "INFO : topic #1 (0.140): 0.075*\"product\" + 0.049*\"good\" + 0.042*\"great\" + 0.028*\"price\" + 0.019*\"use\" + 0.018*\"buy\" + 0.014*\"love\" + 0.014*\"quality\" + 0.011*\"purchase\" + 0.011*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.058): 0.071*\"hair\" + 0.032*\"nail\" + 0.024*\"product\" + 0.023*\"grow\" + 0.022*\"use\" + 0.017*\"work\" + 0.015*\"star\" + 0.015*\"biotin\" + 0.014*\"great\" + 0.013*\"strong\"\n",
      "INFO : topic #3 (0.086): 0.042*\"skin\" + 0.032*\"product\" + 0.031*\"use\" + 0.023*\"great\" + 0.018*\"good\" + 0.014*\"love\" + 0.014*\"feel\" + 0.013*\"look\" + 0.011*\"face\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.292005, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3463/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11919868, 0.13818744, 0.056272469, 0.09181682]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.075*\"product\" + 0.032*\"great\" + 0.024*\"use\" + 0.024*\"good\" + 0.021*\"work\" + 0.018*\"order\" + 0.017*\"recommend\" + 0.013*\"try\" + 0.012*\"time\" + 0.009*\"result\"\n",
      "INFO : topic #1 (0.138): 0.076*\"product\" + 0.045*\"good\" + 0.043*\"great\" + 0.024*\"price\" + 0.017*\"use\" + 0.017*\"buy\" + 0.015*\"quality\" + 0.014*\"love\" + 0.011*\"recommend\" + 0.011*\"purchase\"\n",
      "INFO : topic #2 (0.056): 0.073*\"hair\" + 0.028*\"nail\" + 0.024*\"product\" + 0.021*\"grow\" + 0.021*\"use\" + 0.017*\"star\" + 0.016*\"work\" + 0.014*\"great\" + 0.013*\"strong\" + 0.012*\"notice\"\n",
      "INFO : topic #3 (0.092): 0.051*\"skin\" + 0.032*\"product\" + 0.029*\"use\" + 0.018*\"look\" + 0.018*\"great\" + 0.016*\"feel\" + 0.015*\"good\" + 0.013*\"love\" + 0.012*\"face\" + 0.010*\"notice\"\n",
      "INFO : topic diff=0.335267, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11626883, 0.13765752, 0.056233793, 0.088956617]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.116): 0.073*\"product\" + 0.033*\"great\" + 0.026*\"use\" + 0.023*\"good\" + 0.022*\"work\" + 0.018*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.012*\"try\" + 0.009*\"buy\"\n",
      "INFO : topic #1 (0.138): 0.073*\"product\" + 0.046*\"good\" + 0.044*\"great\" + 0.026*\"price\" + 0.019*\"use\" + 0.018*\"buy\" + 0.014*\"love\" + 0.014*\"quality\" + 0.012*\"purchase\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.056): 0.070*\"hair\" + 0.024*\"use\" + 0.023*\"nail\" + 0.021*\"product\" + 0.020*\"grow\" + 0.017*\"work\" + 0.014*\"great\" + 0.013*\"star\" + 0.012*\"strong\" + 0.011*\"love\"\n",
      "INFO : topic #3 (0.089): 0.051*\"skin\" + 0.031*\"use\" + 0.030*\"product\" + 0.018*\"great\" + 0.017*\"look\" + 0.015*\"good\" + 0.015*\"feel\" + 0.013*\"love\" + 0.013*\"face\" + 0.009*\"day\"\n",
      "INFO : topic diff=0.273643, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11734457, 0.1406049, 0.055052739, 0.086821616]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.117): 0.073*\"product\" + 0.033*\"great\" + 0.026*\"use\" + 0.024*\"good\" + 0.023*\"work\" + 0.018*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.011*\"try\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.141): 0.074*\"product\" + 0.048*\"good\" + 0.045*\"great\" + 0.030*\"price\" + 0.019*\"use\" + 0.018*\"buy\" + 0.015*\"love\" + 0.014*\"quality\" + 0.012*\"purchase\" + 0.011*\"recommend\"\n",
      "INFO : topic #2 (0.055): 0.072*\"hair\" + 0.025*\"nail\" + 0.024*\"use\" + 0.020*\"product\" + 0.020*\"grow\" + 0.017*\"work\" + 0.013*\"great\" + 0.012*\"strong\" + 0.012*\"month\" + 0.011*\"star\"\n",
      "INFO : topic #3 (0.087): 0.052*\"skin\" + 0.033*\"use\" + 0.029*\"product\" + 0.018*\"great\" + 0.015*\"look\" + 0.015*\"good\" + 0.014*\"feel\" + 0.014*\"love\" + 0.013*\"face\" + 0.009*\"cream\"\n",
      "INFO : topic diff=0.217179, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11837633, 0.14249259, 0.054437298, 0.085475154]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.118): 0.074*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.024*\"good\" + 0.023*\"work\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.011*\"try\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.142): 0.075*\"product\" + 0.051*\"good\" + 0.046*\"great\" + 0.031*\"price\" + 0.019*\"use\" + 0.019*\"buy\" + 0.014*\"love\" + 0.014*\"quality\" + 0.012*\"purchase\" + 0.012*\"find\"\n",
      "INFO : topic #2 (0.054): 0.073*\"hair\" + 0.025*\"use\" + 0.024*\"nail\" + 0.020*\"grow\" + 0.019*\"product\" + 0.016*\"work\" + 0.013*\"great\" + 0.012*\"month\" + 0.012*\"strong\" + 0.011*\"love\"\n",
      "INFO : topic #3 (0.085): 0.050*\"skin\" + 0.034*\"use\" + 0.028*\"product\" + 0.017*\"great\" + 0.015*\"good\" + 0.014*\"look\" + 0.013*\"love\" + 0.013*\"feel\" + 0.013*\"face\" + 0.010*\"cream\"\n",
      "INFO : topic diff=0.214615, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11892843, 0.14727657, 0.053651754, 0.08279334]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.074*\"product\" + 0.035*\"great\" + 0.028*\"use\" + 0.024*\"good\" + 0.024*\"work\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.010*\"help\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.147): 0.074*\"product\" + 0.053*\"good\" + 0.047*\"great\" + 0.034*\"price\" + 0.019*\"use\" + 0.019*\"buy\" + 0.014*\"love\" + 0.014*\"quality\" + 0.012*\"find\" + 0.012*\"recommend\"\n",
      "INFO : topic #2 (0.054): 0.074*\"hair\" + 0.025*\"nail\" + 0.025*\"use\" + 0.020*\"grow\" + 0.019*\"product\" + 0.016*\"work\" + 0.013*\"great\" + 0.012*\"month\" + 0.012*\"oil\" + 0.012*\"strong\"\n",
      "INFO : topic #3 (0.083): 0.050*\"skin\" + 0.035*\"use\" + 0.027*\"product\" + 0.017*\"great\" + 0.015*\"good\" + 0.014*\"look\" + 0.013*\"love\" + 0.013*\"face\" + 0.013*\"feel\" + 0.010*\"cream\"\n",
      "INFO : topic diff=0.202884, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11871587, 0.15386428, 0.052911047, 0.081460141]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.074*\"product\" + 0.035*\"great\" + 0.027*\"use\" + 0.025*\"good\" + 0.023*\"work\" + 0.019*\"order\" + 0.018*\"recommend\" + 0.013*\"time\" + 0.010*\"help\" + 0.010*\"buy\"\n",
      "INFO : topic #1 (0.154): 0.075*\"product\" + 0.055*\"good\" + 0.047*\"great\" + 0.035*\"price\" + 0.020*\"buy\" + 0.019*\"use\" + 0.014*\"quality\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.012*\"find\"\n",
      "INFO : topic #2 (0.053): 0.075*\"hair\" + 0.029*\"nail\" + 0.025*\"use\" + 0.021*\"grow\" + 0.019*\"product\" + 0.015*\"work\" + 0.013*\"strong\" + 0.012*\"month\" + 0.012*\"great\" + 0.012*\"oil\"\n",
      "INFO : topic #3 (0.081): 0.050*\"skin\" + 0.035*\"use\" + 0.025*\"product\" + 0.016*\"great\" + 0.015*\"good\" + 0.014*\"love\" + 0.013*\"look\" + 0.013*\"feel\" + 0.012*\"face\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.200809, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11891358, 0.15616876, 0.05384377, 0.079907276]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.119): 0.075*\"product\" + 0.034*\"great\" + 0.028*\"use\" + 0.024*\"good\" + 0.023*\"work\" + 0.019*\"order\" + 0.018*\"recommend\" + 0.013*\"time\" + 0.010*\"help\" + 0.009*\"buy\"\n",
      "INFO : topic #1 (0.156): 0.075*\"product\" + 0.055*\"good\" + 0.046*\"great\" + 0.035*\"price\" + 0.020*\"buy\" + 0.018*\"use\" + 0.014*\"quality\" + 0.014*\"love\" + 0.012*\"find\" + 0.012*\"recommend\"\n",
      "INFO : topic #2 (0.054): 0.082*\"hair\" + 0.035*\"nail\" + 0.026*\"grow\" + 0.023*\"use\" + 0.019*\"product\" + 0.016*\"work\" + 0.014*\"month\" + 0.014*\"strong\" + 0.012*\"biotin\" + 0.011*\"great\"\n",
      "INFO : topic #3 (0.080): 0.049*\"skin\" + 0.035*\"use\" + 0.024*\"product\" + 0.016*\"great\" + 0.014*\"good\" + 0.014*\"love\" + 0.013*\"feel\" + 0.012*\"look\" + 0.012*\"face\" + 0.010*\"oil\"\n",
      "INFO : topic diff=0.228214, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12145299, 0.16197895, 0.052941013, 0.079382017]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.121): 0.076*\"product\" + 0.035*\"great\" + 0.027*\"use\" + 0.024*\"work\" + 0.023*\"good\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.011*\"help\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.162): 0.074*\"product\" + 0.053*\"good\" + 0.046*\"great\" + 0.033*\"price\" + 0.019*\"buy\" + 0.017*\"use\" + 0.014*\"quality\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"find\"\n",
      "INFO : topic #2 (0.053): 0.080*\"hair\" + 0.037*\"nail\" + 0.026*\"grow\" + 0.022*\"use\" + 0.018*\"product\" + 0.016*\"strong\" + 0.016*\"work\" + 0.014*\"month\" + 0.013*\"biotin\" + 0.012*\"notice\"\n",
      "INFO : topic #3 (0.079): 0.048*\"skin\" + 0.034*\"use\" + 0.023*\"product\" + 0.015*\"great\" + 0.014*\"good\" + 0.013*\"love\" + 0.013*\"feel\" + 0.012*\"look\" + 0.012*\"oil\" + 0.012*\"face\"\n",
      "INFO : topic diff=0.205616, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12258182, 0.16520436, 0.051967315, 0.081470817]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.123): 0.077*\"product\" + 0.034*\"great\" + 0.027*\"use\" + 0.023*\"good\" + 0.023*\"work\" + 0.019*\"order\" + 0.017*\"recommend\" + 0.013*\"time\" + 0.010*\"try\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.165): 0.076*\"product\" + 0.052*\"good\" + 0.046*\"great\" + 0.030*\"price\" + 0.018*\"buy\" + 0.016*\"use\" + 0.014*\"quality\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"find\"\n",
      "INFO : topic #2 (0.052): 0.081*\"hair\" + 0.037*\"nail\" + 0.025*\"grow\" + 0.022*\"use\" + 0.018*\"product\" + 0.016*\"strong\" + 0.016*\"work\" + 0.015*\"biotin\" + 0.014*\"month\" + 0.012*\"notice\"\n",
      "INFO : topic #3 (0.081): 0.050*\"skin\" + 0.035*\"use\" + 0.024*\"product\" + 0.014*\"look\" + 0.014*\"great\" + 0.014*\"good\" + 0.014*\"love\" + 0.014*\"feel\" + 0.013*\"face\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.220752, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3464/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12506671, 0.16060002, 0.050750211, 0.086812891]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.125): 0.078*\"product\" + 0.034*\"great\" + 0.025*\"use\" + 0.022*\"work\" + 0.022*\"good\" + 0.018*\"order\" + 0.017*\"recommend\" + 0.012*\"try\" + 0.012*\"time\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.161): 0.077*\"product\" + 0.048*\"good\" + 0.046*\"great\" + 0.027*\"price\" + 0.016*\"buy\" + 0.015*\"quality\" + 0.015*\"use\" + 0.013*\"love\" + 0.013*\"recommend\" + 0.012*\"supplement\"\n",
      "INFO : topic #2 (0.051): 0.083*\"hair\" + 0.033*\"nail\" + 0.024*\"grow\" + 0.020*\"use\" + 0.018*\"product\" + 0.016*\"strong\" + 0.015*\"work\" + 0.013*\"notice\" + 0.013*\"month\" + 0.012*\"biotin\"\n",
      "INFO : topic #3 (0.087): 0.056*\"skin\" + 0.031*\"use\" + 0.025*\"product\" + 0.019*\"look\" + 0.015*\"feel\" + 0.013*\"face\" + 0.013*\"good\" + 0.012*\"love\" + 0.012*\"great\" + 0.010*\"week\"\n",
      "INFO : topic diff=0.284465, rho=0.308680\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=4936, num_topics=4, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 4 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.06143038769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 59465 documents\n",
      "DEBUG : 59461/59465 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      " 67%|██████▋   | 2/3 [05:20<02:40, 160.35s/it]INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "INFO : using symmetric eta at 0.2\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 5 topics, 2 passes over the supplied corpus of 59465 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic2_4.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6889/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12398568, 0.13202353, 0.08236029, 0.11042953, 0.11275282]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.124): 0.055*\"product\" + 0.031*\"use\" + 0.026*\"good\" + 0.026*\"great\" + 0.018*\"skin\" + 0.018*\"work\" + 0.013*\"recommend\" + 0.013*\"buy\" + 0.013*\"order\" + 0.012*\"time\"\n",
      "INFO : topic #1 (0.132): 0.044*\"product\" + 0.038*\"use\" + 0.031*\"good\" + 0.022*\"great\" + 0.021*\"love\" + 0.016*\"buy\" + 0.015*\"work\" + 0.014*\"price\" + 0.012*\"skin\" + 0.011*\"purchase\"\n",
      "INFO : topic #2 (0.082): 0.038*\"product\" + 0.031*\"great\" + 0.021*\"use\" + 0.020*\"good\" + 0.020*\"hair\" + 0.017*\"find\" + 0.015*\"like\" + 0.015*\"work\" + 0.011*\"oil\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.110): 0.041*\"great\" + 0.039*\"product\" + 0.026*\"good\" + 0.025*\"use\" + 0.022*\"skin\" + 0.014*\"love\" + 0.012*\"hair\" + 0.012*\"price\" + 0.011*\"recommend\" + 0.010*\"time\"\n",
      "INFO : topic #4 (0.113): 0.060*\"product\" + 0.042*\"great\" + 0.028*\"use\" + 0.027*\"good\" + 0.024*\"hair\" + 0.019*\"work\" + 0.015*\"buy\" + 0.012*\"price\" + 0.011*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=4.170711, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11377195, 0.11178016, 0.070987999, 0.099726334, 0.1098848]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.114): 0.058*\"product\" + 0.028*\"use\" + 0.027*\"good\" + 0.023*\"great\" + 0.018*\"work\" + 0.016*\"order\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.013*\"time\" + 0.012*\"price\"\n",
      "INFO : topic #1 (0.112): 0.051*\"product\" + 0.039*\"good\" + 0.035*\"use\" + 0.023*\"love\" + 0.022*\"great\" + 0.019*\"price\" + 0.017*\"buy\" + 0.015*\"work\" + 0.011*\"purchase\" + 0.010*\"amazon\"\n",
      "INFO : topic #2 (0.071): 0.038*\"product\" + 0.027*\"great\" + 0.022*\"hair\" + 0.021*\"use\" + 0.020*\"good\" + 0.020*\"find\" + 0.015*\"like\" + 0.014*\"work\" + 0.012*\"help\" + 0.012*\"love\"\n",
      "INFO : topic #3 (0.100): 0.040*\"product\" + 0.037*\"great\" + 0.027*\"skin\" + 0.026*\"use\" + 0.025*\"good\" + 0.015*\"love\" + 0.012*\"price\" + 0.011*\"recommend\" + 0.011*\"cream\" + 0.009*\"hair\"\n",
      "INFO : topic #4 (0.110): 0.074*\"product\" + 0.047*\"great\" + 0.032*\"good\" + 0.024*\"use\" + 0.022*\"hair\" + 0.018*\"work\" + 0.016*\"price\" + 0.015*\"buy\" + 0.013*\"recommend\" + 0.011*\"year\"\n",
      "INFO : topic diff=0.723578, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10763644, 0.10405416, 0.065488666, 0.092143103, 0.11192625]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.108): 0.056*\"product\" + 0.029*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.018*\"work\" + 0.017*\"order\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.013*\"time\" + 0.011*\"try\"\n",
      "INFO : topic #1 (0.104): 0.055*\"product\" + 0.044*\"good\" + 0.034*\"use\" + 0.023*\"great\" + 0.023*\"love\" + 0.022*\"price\" + 0.018*\"buy\" + 0.014*\"work\" + 0.013*\"amazon\" + 0.012*\"purchase\"\n",
      "INFO : topic #2 (0.065): 0.032*\"product\" + 0.029*\"hair\" + 0.025*\"use\" + 0.023*\"great\" + 0.017*\"good\" + 0.017*\"find\" + 0.015*\"like\" + 0.013*\"oil\" + 0.013*\"work\" + 0.012*\"love\"\n",
      "INFO : topic #3 (0.092): 0.036*\"product\" + 0.033*\"great\" + 0.032*\"skin\" + 0.028*\"use\" + 0.023*\"good\" + 0.014*\"love\" + 0.012*\"cream\" + 0.010*\"look\" + 0.010*\"recommend\" + 0.009*\"price\"\n",
      "INFO : topic #4 (0.112): 0.080*\"product\" + 0.052*\"great\" + 0.035*\"good\" + 0.025*\"use\" + 0.021*\"hair\" + 0.019*\"work\" + 0.017*\"price\" + 0.014*\"recommend\" + 0.014*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic diff=0.439123, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1023335, 0.10203075, 0.061788205, 0.085370392, 0.11765358]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.102): 0.054*\"product\" + 0.027*\"use\" + 0.025*\"good\" + 0.021*\"great\" + 0.018*\"work\" + 0.018*\"order\" + 0.014*\"recommend\" + 0.013*\"time\" + 0.011*\"try\" + 0.011*\"skin\"\n",
      "INFO : topic #1 (0.102): 0.057*\"product\" + 0.048*\"good\" + 0.031*\"use\" + 0.028*\"price\" + 0.025*\"great\" + 0.023*\"love\" + 0.021*\"buy\" + 0.015*\"amazon\" + 0.013*\"work\" + 0.012*\"purchase\"\n",
      "INFO : topic #2 (0.062): 0.037*\"hair\" + 0.027*\"product\" + 0.026*\"use\" + 0.021*\"great\" + 0.019*\"oil\" + 0.015*\"good\" + 0.015*\"find\" + 0.014*\"like\" + 0.012*\"nail\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.085): 0.038*\"skin\" + 0.032*\"product\" + 0.030*\"use\" + 0.029*\"great\" + 0.021*\"good\" + 0.014*\"love\" + 0.011*\"look\" + 0.011*\"cream\" + 0.009*\"feel\" + 0.009*\"recommend\"\n",
      "INFO : topic #4 (0.118): 0.083*\"product\" + 0.057*\"great\" + 0.038*\"good\" + 0.023*\"use\" + 0.021*\"price\" + 0.018*\"work\" + 0.018*\"hair\" + 0.016*\"recommend\" + 0.014*\"buy\" + 0.012*\"vitamin\"\n",
      "INFO : topic diff=0.336420, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6994/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.096867807, 0.10356733, 0.058984511, 0.082561381, 0.12229431]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.097): 0.054*\"product\" + 0.026*\"use\" + 0.024*\"good\" + 0.019*\"great\" + 0.019*\"order\" + 0.017*\"work\" + 0.014*\"recommend\" + 0.014*\"time\" + 0.011*\"try\" + 0.011*\"find\"\n",
      "INFO : topic #1 (0.104): 0.058*\"product\" + 0.052*\"good\" + 0.031*\"price\" + 0.028*\"use\" + 0.026*\"great\" + 0.023*\"buy\" + 0.022*\"love\" + 0.016*\"amazon\" + 0.013*\"purchase\" + 0.012*\"find\"\n",
      "INFO : topic #2 (0.059): 0.046*\"hair\" + 0.027*\"use\" + 0.025*\"product\" + 0.021*\"oil\" + 0.018*\"great\" + 0.017*\"nail\" + 0.014*\"like\" + 0.013*\"love\" + 0.013*\"good\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.083): 0.042*\"skin\" + 0.030*\"use\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"good\" + 0.014*\"love\" + 0.011*\"look\" + 0.010*\"feel\" + 0.009*\"help\" + 0.009*\"day\"\n",
      "INFO : topic #4 (0.122): 0.088*\"product\" + 0.060*\"great\" + 0.040*\"good\" + 0.022*\"price\" + 0.022*\"use\" + 0.019*\"recommend\" + 0.018*\"work\" + 0.014*\"buy\" + 0.013*\"hair\" + 0.013*\"vitamin\"\n",
      "INFO : topic diff=0.297142, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6993/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.094223477, 0.1028908, 0.059573125, 0.07993038, 0.12732093]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.094): 0.053*\"product\" + 0.026*\"use\" + 0.022*\"good\" + 0.019*\"order\" + 0.017*\"great\" + 0.017*\"work\" + 0.014*\"recommend\" + 0.013*\"time\" + 0.011*\"try\" + 0.010*\"find\"\n",
      "INFO : topic #1 (0.103): 0.058*\"product\" + 0.053*\"good\" + 0.031*\"price\" + 0.026*\"great\" + 0.025*\"use\" + 0.022*\"love\" + 0.022*\"buy\" + 0.015*\"amazon\" + 0.014*\"find\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.060): 0.070*\"hair\" + 0.031*\"nail\" + 0.025*\"use\" + 0.023*\"grow\" + 0.021*\"product\" + 0.017*\"oil\" + 0.014*\"great\" + 0.013*\"work\" + 0.013*\"biotin\" + 0.012*\"help\"\n",
      "INFO : topic #3 (0.080): 0.044*\"skin\" + 0.029*\"use\" + 0.027*\"product\" + 0.022*\"great\" + 0.019*\"good\" + 0.013*\"love\" + 0.012*\"look\" + 0.011*\"feel\" + 0.010*\"day\" + 0.009*\"face\"\n",
      "INFO : topic #4 (0.127): 0.091*\"product\" + 0.061*\"great\" + 0.040*\"good\" + 0.021*\"use\" + 0.021*\"price\" + 0.019*\"recommend\" + 0.017*\"work\" + 0.014*\"buy\" + 0.013*\"vitamin\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.299696, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.093326196, 0.10512781, 0.058578365, 0.078662217, 0.1318686]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.093): 0.052*\"product\" + 0.025*\"use\" + 0.020*\"good\" + 0.019*\"order\" + 0.017*\"work\" + 0.016*\"great\" + 0.013*\"time\" + 0.013*\"recommend\" + 0.012*\"try\" + 0.010*\"day\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.105): 0.058*\"product\" + 0.052*\"good\" + 0.031*\"price\" + 0.025*\"great\" + 0.023*\"use\" + 0.021*\"buy\" + 0.021*\"love\" + 0.014*\"amazon\" + 0.014*\"find\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.059): 0.066*\"hair\" + 0.032*\"nail\" + 0.025*\"oil\" + 0.024*\"use\" + 0.022*\"grow\" + 0.019*\"product\" + 0.015*\"krill\" + 0.014*\"strong\" + 0.013*\"work\" + 0.013*\"biotin\"\n",
      "INFO : topic #3 (0.079): 0.045*\"skin\" + 0.029*\"use\" + 0.025*\"product\" + 0.020*\"great\" + 0.017*\"good\" + 0.013*\"love\" + 0.013*\"look\" + 0.011*\"feel\" + 0.010*\"day\" + 0.010*\"face\"\n",
      "INFO : topic #4 (0.132): 0.096*\"product\" + 0.064*\"great\" + 0.039*\"good\" + 0.021*\"use\" + 0.021*\"recommend\" + 0.019*\"price\" + 0.019*\"work\" + 0.013*\"excellent\" + 0.012*\"buy\" + 0.012*\"vitamin\"\n",
      "INFO : topic diff=0.265437, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.09234599, 0.10549747, 0.056849372, 0.080685876, 0.13817874]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.092): 0.051*\"product\" + 0.024*\"use\" + 0.019*\"order\" + 0.019*\"good\" + 0.016*\"work\" + 0.015*\"great\" + 0.013*\"time\" + 0.013*\"try\" + 0.012*\"recommend\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.105): 0.058*\"product\" + 0.050*\"good\" + 0.029*\"price\" + 0.025*\"great\" + 0.022*\"use\" + 0.021*\"buy\" + 0.021*\"love\" + 0.014*\"find\" + 0.013*\"amazon\" + 0.013*\"quality\"\n",
      "INFO : topic #2 (0.057): 0.070*\"hair\" + 0.033*\"nail\" + 0.024*\"use\" + 0.023*\"oil\" + 0.023*\"grow\" + 0.018*\"product\" + 0.015*\"biotin\" + 0.014*\"strong\" + 0.014*\"krill\" + 0.013*\"work\"\n",
      "INFO : topic #3 (0.081): 0.050*\"skin\" + 0.032*\"use\" + 0.025*\"product\" + 0.018*\"great\" + 0.017*\"good\" + 0.015*\"look\" + 0.013*\"love\" + 0.013*\"feel\" + 0.012*\"face\" + 0.010*\"day\"\n",
      "INFO : topic #4 (0.138): 0.100*\"product\" + 0.065*\"great\" + 0.039*\"good\" + 0.021*\"recommend\" + 0.021*\"use\" + 0.018*\"work\" + 0.017*\"price\" + 0.013*\"excellent\" + 0.012*\"buy\" + 0.010*\"star\"\n",
      "INFO : topic diff=0.263722, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3465/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.09273006, 0.10306486, 0.055101864, 0.086884089, 0.14357905]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.093): 0.051*\"product\" + 0.023*\"use\" + 0.018*\"order\" + 0.017*\"good\" + 0.016*\"work\" + 0.015*\"try\" + 0.013*\"great\" + 0.012*\"time\" + 0.011*\"recommend\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.103): 0.058*\"product\" + 0.046*\"good\" + 0.025*\"price\" + 0.025*\"great\" + 0.020*\"use\" + 0.020*\"buy\" + 0.020*\"love\" + 0.015*\"find\" + 0.014*\"quality\" + 0.012*\"purchase\"\n",
      "INFO : topic #2 (0.055): 0.076*\"hair\" + 0.030*\"nail\" + 0.022*\"use\" + 0.022*\"grow\" + 0.021*\"oil\" + 0.017*\"product\" + 0.014*\"strong\" + 0.012*\"biotin\" + 0.012*\"work\" + 0.012*\"krill\"\n",
      "INFO : topic #3 (0.087): 0.059*\"skin\" + 0.029*\"use\" + 0.026*\"product\" + 0.021*\"look\" + 0.016*\"feel\" + 0.014*\"great\" + 0.014*\"good\" + 0.013*\"face\" + 0.012*\"love\" + 0.010*\"serum\"\n",
      "INFO : topic #4 (0.144): 0.102*\"product\" + 0.064*\"great\" + 0.036*\"good\" + 0.022*\"recommend\" + 0.019*\"use\" + 0.018*\"work\" + 0.015*\"price\" + 0.012*\"star\" + 0.011*\"excellent\" + 0.011*\"supplement\"\n",
      "INFO : topic diff=0.303901, rho=0.333333\n",
      "INFO : PROGRESS: pass 1, at document #7000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090761386, 0.10418192, 0.055176962, 0.084662244, 0.14045437]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.091): 0.049*\"product\" + 0.024*\"use\" + 0.018*\"order\" + 0.017*\"work\" + 0.017*\"good\" + 0.014*\"try\" + 0.013*\"great\" + 0.012*\"time\" + 0.010*\"day\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.104): 0.054*\"product\" + 0.047*\"good\" + 0.027*\"price\" + 0.026*\"great\" + 0.022*\"use\" + 0.022*\"buy\" + 0.020*\"love\" + 0.015*\"find\" + 0.014*\"purchase\" + 0.013*\"quality\"\n",
      "INFO : topic #2 (0.055): 0.074*\"hair\" + 0.026*\"use\" + 0.025*\"nail\" + 0.023*\"oil\" + 0.020*\"grow\" + 0.015*\"product\" + 0.014*\"work\" + 0.013*\"strong\" + 0.011*\"great\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.085): 0.059*\"skin\" + 0.032*\"use\" + 0.025*\"product\" + 0.019*\"look\" + 0.015*\"great\" + 0.015*\"feel\" + 0.014*\"face\" + 0.014*\"good\" + 0.013*\"love\" + 0.009*\"dry\"\n",
      "INFO : topic #4 (0.140): 0.103*\"product\" + 0.068*\"great\" + 0.037*\"good\" + 0.022*\"recommend\" + 0.021*\"use\" + 0.020*\"work\" + 0.016*\"price\" + 0.012*\"excellent\" + 0.012*\"star\" + 0.011*\"buy\"\n",
      "INFO : topic diff=0.249632, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #14000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090291463, 0.10586522, 0.054260083, 0.083217911, 0.1442363]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.090): 0.048*\"product\" + 0.025*\"use\" + 0.018*\"order\" + 0.017*\"work\" + 0.016*\"good\" + 0.013*\"try\" + 0.013*\"great\" + 0.012*\"time\" + 0.010*\"recommend\" + 0.010*\"day\"\n",
      "INFO : topic #1 (0.106): 0.054*\"product\" + 0.048*\"good\" + 0.031*\"price\" + 0.026*\"great\" + 0.022*\"buy\" + 0.022*\"use\" + 0.019*\"love\" + 0.017*\"find\" + 0.014*\"amazon\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.054): 0.078*\"hair\" + 0.028*\"nail\" + 0.026*\"use\" + 0.022*\"oil\" + 0.022*\"grow\" + 0.015*\"product\" + 0.014*\"strong\" + 0.013*\"work\" + 0.011*\"help\" + 0.011*\"month\"\n",
      "INFO : topic #3 (0.083): 0.060*\"skin\" + 0.033*\"use\" + 0.024*\"product\" + 0.018*\"look\" + 0.015*\"great\" + 0.014*\"face\" + 0.014*\"feel\" + 0.014*\"good\" + 0.013*\"love\" + 0.011*\"cream\"\n",
      "INFO : topic #4 (0.144): 0.104*\"product\" + 0.069*\"great\" + 0.039*\"good\" + 0.022*\"recommend\" + 0.022*\"use\" + 0.020*\"work\" + 0.017*\"price\" + 0.012*\"excellent\" + 0.011*\"buy\" + 0.011*\"star\"\n",
      "INFO : topic diff=0.192631, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #21000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090294003, 0.1070946, 0.054040458, 0.082514234, 0.14775431]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.090): 0.047*\"product\" + 0.025*\"use\" + 0.018*\"order\" + 0.017*\"work\" + 0.016*\"good\" + 0.013*\"try\" + 0.013*\"great\" + 0.013*\"time\" + 0.010*\"day\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.107): 0.055*\"product\" + 0.050*\"good\" + 0.033*\"price\" + 0.027*\"great\" + 0.023*\"buy\" + 0.022*\"use\" + 0.018*\"love\" + 0.018*\"find\" + 0.016*\"amazon\" + 0.014*\"purchase\"\n",
      "INFO : topic #2 (0.054): 0.080*\"hair\" + 0.027*\"use\" + 0.027*\"nail\" + 0.022*\"oil\" + 0.022*\"grow\" + 0.014*\"product\" + 0.014*\"strong\" + 0.013*\"work\" + 0.012*\"month\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.083): 0.059*\"skin\" + 0.035*\"use\" + 0.023*\"product\" + 0.017*\"look\" + 0.015*\"face\" + 0.015*\"great\" + 0.014*\"good\" + 0.013*\"love\" + 0.013*\"feel\" + 0.012*\"cream\"\n",
      "INFO : topic #4 (0.148): 0.105*\"product\" + 0.070*\"great\" + 0.041*\"good\" + 0.023*\"use\" + 0.023*\"recommend\" + 0.021*\"work\" + 0.017*\"price\" + 0.013*\"excellent\" + 0.011*\"buy\" + 0.011*\"star\"\n",
      "INFO : topic diff=0.185980, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #28000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089880221, 0.10981993, 0.053500064, 0.080709159, 0.15282154]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.090): 0.046*\"product\" + 0.025*\"use\" + 0.018*\"order\" + 0.017*\"work\" + 0.015*\"good\" + 0.013*\"time\" + 0.012*\"try\" + 0.012*\"great\" + 0.010*\"day\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.110): 0.054*\"product\" + 0.051*\"good\" + 0.036*\"price\" + 0.027*\"great\" + 0.024*\"buy\" + 0.021*\"use\" + 0.019*\"find\" + 0.017*\"amazon\" + 0.017*\"love\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.054): 0.083*\"hair\" + 0.028*\"nail\" + 0.027*\"use\" + 0.024*\"oil\" + 0.023*\"grow\" + 0.013*\"strong\" + 0.013*\"product\" + 0.013*\"work\" + 0.012*\"month\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.081): 0.059*\"skin\" + 0.036*\"use\" + 0.023*\"product\" + 0.016*\"look\" + 0.015*\"face\" + 0.015*\"great\" + 0.014*\"good\" + 0.013*\"love\" + 0.013*\"feel\" + 0.012*\"cream\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.153): 0.105*\"product\" + 0.071*\"great\" + 0.043*\"good\" + 0.024*\"use\" + 0.023*\"recommend\" + 0.021*\"work\" + 0.018*\"price\" + 0.013*\"excellent\" + 0.011*\"buy\" + 0.011*\"love\"\n",
      "INFO : topic diff=0.178735, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #35000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.088596404, 0.11321811, 0.05307021, 0.080000371, 0.15824954]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.089): 0.045*\"product\" + 0.024*\"use\" + 0.019*\"order\" + 0.016*\"work\" + 0.015*\"good\" + 0.013*\"time\" + 0.012*\"try\" + 0.011*\"great\" + 0.010*\"day\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.113): 0.054*\"product\" + 0.053*\"good\" + 0.037*\"price\" + 0.027*\"great\" + 0.025*\"buy\" + 0.020*\"use\" + 0.020*\"find\" + 0.018*\"amazon\" + 0.016*\"love\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.053): 0.084*\"hair\" + 0.033*\"nail\" + 0.027*\"use\" + 0.023*\"grow\" + 0.023*\"oil\" + 0.014*\"strong\" + 0.013*\"product\" + 0.013*\"work\" + 0.012*\"month\" + 0.011*\"love\"\n",
      "INFO : topic #3 (0.080): 0.059*\"skin\" + 0.036*\"use\" + 0.022*\"product\" + 0.015*\"look\" + 0.014*\"great\" + 0.014*\"love\" + 0.014*\"good\" + 0.014*\"face\" + 0.013*\"feel\" + 0.011*\"dry\"\n",
      "INFO : topic #4 (0.158): 0.106*\"product\" + 0.071*\"great\" + 0.044*\"good\" + 0.025*\"recommend\" + 0.024*\"use\" + 0.021*\"work\" + 0.018*\"price\" + 0.013*\"excellent\" + 0.012*\"love\" + 0.011*\"buy\"\n",
      "INFO : topic diff=0.178164, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #42000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.088758387, 0.11471807, 0.054523118, 0.079077356, 0.16346511]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.089): 0.044*\"product\" + 0.024*\"use\" + 0.019*\"order\" + 0.016*\"work\" + 0.014*\"good\" + 0.012*\"time\" + 0.012*\"try\" + 0.011*\"great\" + 0.011*\"day\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.115): 0.053*\"good\" + 0.053*\"product\" + 0.037*\"price\" + 0.026*\"great\" + 0.024*\"buy\" + 0.020*\"find\" + 0.020*\"use\" + 0.018*\"amazon\" + 0.016*\"love\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.055): 0.092*\"hair\" + 0.040*\"nail\" + 0.030*\"grow\" + 0.025*\"use\" + 0.018*\"oil\" + 0.015*\"strong\" + 0.014*\"month\" + 0.013*\"work\" + 0.013*\"biotin\" + 0.013*\"product\"\n",
      "INFO : topic #3 (0.079): 0.059*\"skin\" + 0.035*\"use\" + 0.021*\"product\" + 0.015*\"look\" + 0.014*\"great\" + 0.014*\"good\" + 0.014*\"face\" + 0.014*\"love\" + 0.013*\"feel\" + 0.011*\"dry\"\n",
      "INFO : topic #4 (0.163): 0.108*\"product\" + 0.070*\"great\" + 0.044*\"good\" + 0.025*\"recommend\" + 0.024*\"use\" + 0.020*\"work\" + 0.018*\"price\" + 0.014*\"excellent\" + 0.012*\"love\" + 0.011*\"buy\"\n",
      "INFO : topic diff=0.197753, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #49000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.089620143, 0.11802489, 0.054442655, 0.078710787, 0.17058983]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.090): 0.043*\"product\" + 0.023*\"use\" + 0.018*\"order\" + 0.017*\"work\" + 0.013*\"good\" + 0.012*\"time\" + 0.012*\"try\" + 0.011*\"day\" + 0.010*\"great\" + 0.010*\"help\"\n",
      "INFO : topic #1 (0.118): 0.052*\"product\" + 0.052*\"good\" + 0.035*\"price\" + 0.025*\"great\" + 0.023*\"buy\" + 0.020*\"find\" + 0.019*\"use\" + 0.017*\"amazon\" + 0.015*\"love\" + 0.013*\"purchase\"\n",
      "INFO : topic #2 (0.054): 0.088*\"hair\" + 0.040*\"nail\" + 0.028*\"grow\" + 0.024*\"use\" + 0.022*\"oil\" + 0.017*\"strong\" + 0.014*\"biotin\" + 0.014*\"month\" + 0.013*\"work\" + 0.012*\"product\"\n",
      "INFO : topic #3 (0.079): 0.058*\"skin\" + 0.035*\"use\" + 0.020*\"product\" + 0.015*\"look\" + 0.014*\"face\" + 0.014*\"great\" + 0.014*\"love\" + 0.013*\"good\" + 0.013*\"feel\" + 0.011*\"dry\"\n",
      "INFO : topic #4 (0.171): 0.109*\"product\" + 0.071*\"great\" + 0.044*\"good\" + 0.025*\"recommend\" + 0.024*\"use\" + 0.021*\"work\" + 0.017*\"price\" + 0.014*\"excellent\" + 0.012*\"love\" + 0.011*\"buy\"\n",
      "INFO : topic diff=0.182874, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #56000/59465\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.090252869, 0.11868967, 0.053761441, 0.081147097, 0.18047377]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.090): 0.042*\"product\" + 0.022*\"use\" + 0.019*\"order\" + 0.016*\"work\" + 0.013*\"good\" + 0.013*\"try\" + 0.012*\"time\" + 0.011*\"day\" + 0.010*\"help\" + 0.009*\"great\"\n",
      "INFO : topic #1 (0.119): 0.052*\"product\" + 0.050*\"good\" + 0.033*\"price\" + 0.024*\"great\" + 0.023*\"buy\" + 0.020*\"find\" + 0.017*\"use\" + 0.015*\"amazon\" + 0.014*\"love\" + 0.014*\"quality\"\n",
      "INFO : topic #2 (0.054): 0.089*\"hair\" + 0.041*\"nail\" + 0.028*\"grow\" + 0.023*\"use\" + 0.020*\"oil\" + 0.018*\"strong\" + 0.016*\"biotin\" + 0.014*\"month\" + 0.013*\"work\" + 0.012*\"notice\"\n",
      "INFO : topic #3 (0.081): 0.060*\"skin\" + 0.036*\"use\" + 0.021*\"product\" + 0.017*\"look\" + 0.015*\"face\" + 0.014*\"love\" + 0.014*\"feel\" + 0.013*\"good\" + 0.013*\"great\" + 0.010*\"day\"\n",
      "INFO : topic #4 (0.180): 0.111*\"product\" + 0.070*\"great\" + 0.043*\"good\" + 0.024*\"recommend\" + 0.023*\"use\" + 0.020*\"work\" + 0.016*\"price\" + 0.013*\"excellent\" + 0.013*\"love\" + 0.012*\"feel\"\n",
      "INFO : topic diff=0.196348, rho=0.308680\n",
      "INFO : PROGRESS: pass 1, at document #59465/59465\n",
      "DEBUG : performing inference on a chunk of 3465 documents\n",
      "DEBUG : 3464/3465 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.09176939, 0.11568287, 0.05290046, 0.0875756, 0.1881981]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 3465 documents into a model of 59465 documents\n",
      "INFO : topic #0 (0.092): 0.042*\"product\" + 0.021*\"use\" + 0.018*\"order\" + 0.015*\"work\" + 0.014*\"try\" + 0.012*\"good\" + 0.011*\"time\" + 0.010*\"day\" + 0.010*\"bottle\" + 0.009*\"help\"\n",
      "INFO : topic #1 (0.116): 0.051*\"product\" + 0.047*\"good\" + 0.030*\"price\" + 0.024*\"great\" + 0.021*\"buy\" + 0.020*\"find\" + 0.016*\"use\" + 0.015*\"quality\" + 0.014*\"amazon\" + 0.014*\"love\"\n",
      "INFO : topic #2 (0.053): 0.093*\"hair\" + 0.037*\"nail\" + 0.027*\"grow\" + 0.022*\"use\" + 0.018*\"oil\" + 0.017*\"strong\" + 0.014*\"biotin\" + 0.013*\"month\" + 0.013*\"notice\" + 0.012*\"work\"\n",
      "INFO : topic #3 (0.088): 0.065*\"skin\" + 0.032*\"use\" + 0.022*\"look\" + 0.022*\"product\" + 0.016*\"feel\" + 0.015*\"face\" + 0.013*\"love\" + 0.012*\"good\" + 0.011*\"serum\" + 0.011*\"great\"\n",
      "INFO : topic #4 (0.188): 0.112*\"product\" + 0.068*\"great\" + 0.040*\"good\" + 0.025*\"recommend\" + 0.021*\"use\" + 0.020*\"work\" + 0.013*\"love\" + 0.013*\"feel\" + 0.013*\"price\" + 0.012*\"excellent\"\n",
      "INFO : topic diff=0.254230, rho=0.308680\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=4936, num_topics=5, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 5 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.11043852988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 59465 documents\n",
      "DEBUG : 59464/59465 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 3/3 [08:06<00:00, 162.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_tier2_topic2_5.html\n",
      "CPU times: user 8min 4s, sys: 2.52 s, total: 8min 6s\n",
      "Wall time: 8min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [3, 4, 5]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 2\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = None           # symmetric prior\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_2 = LdaModel(bow_corpus_2, num_topics=num_topics, id2word=vocab_dictionary_2, chunksize=chunksize, \n",
    "                     passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, random_state=42)\n",
    "    save_df_s3(lda_2, bucket_name, filepath='amazon_reviews/kk/lda_tier2_topic2_{}.pkl'.format(num_topics), filetype='pickle')\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda_2, corpus=bow_corpus_2, dictionary=vocab_dictionary_2, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_2, bow_corpus_2, vocab_dictionary_2)\n",
    "    plot_fname = 'pyLDAvis_tier2_topic2_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Seeded LDA using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eta` can be a scalar for a symmetric prior over topic/word distributions, or a matrix of shape **`num_topics` x `num_words`**, which can be used to impose asymmetric priors over the word distribution on a per-topic basis. This may be useful if you want to seed certain topics with particular words by boosting the priors for those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(13850 unique tokens: ['accord', 'acid', 'anticipate', 'bioavailability', 'break']...)\n"
     ]
    }
   ],
   "source": [
    "vocab_dictionary = load_df_s3(bucket_name, filepath='amazon_reviews/kk/vocab_dictionary_v3.dict', filetype='pickle')\n",
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = load_df_s3(bucket_name, filepath='amazon_reviews/kk/tokenized_reviews_v1.pkl', filetype='pickle')\n",
    "# bag-of-words representation of the corpus/ doc-term matrix\n",
    "bow_corpus = [vocab_dictionary.doc2bow(review) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topics and associated words\n",
    "efficacy = ['quality', 'work', 'help', 'result', 'notice', 'help', 'effect', 'effective', 'outcome', 'high_quality']\n",
    "cost = ['price', 'cost', 'money', 'expensive', 'cheap', 'value', 'worth']\n",
    "seller = ['seller', 'shipping', 'service', 'ship', 'customer', 'customer_service', 'timely', 'company', 'order', 'refund', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert topics' words to ids\n",
    "efficacy_id = [vocab_dictionary.token2id[word] for word in efficacy]\n",
    "cost_id = [vocab_dictionary.token2id[word] for word in cost]\n",
    "seller_id = [vocab_dictionary.token2id[word] for word in seller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[767, 189, 105, 297, 182, 105, 453, 832, 558, 1881]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficacy_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [efficacy_id, cost_id, seller_id]\n",
    "num_topics = len(topics)\n",
    "\n",
    "# initialize eta: matrix of shape num_topics x num_words\n",
    "eta = np.ones((num_topics, len(vocab_dictionary))) * (1.0/num_topics)\n",
    "baseline_prob = eta[0, 0]\n",
    "\n",
    "boosted_prob = 0.9\n",
    "\n",
    "for i in range(num_topics):\n",
    "    \n",
    "    topic_ids = topics[i]   # word ids related to topic i\n",
    "    \n",
    "    for seeded_word_id in topics[i]:\n",
    "        # boost probability of selected word in current topic\n",
    "        eta[i, seeded_word_id] = boosted_prob\n",
    "        # distribute the leftover probability over the remaining topics\n",
    "        leftover_topic_rownums = [x for x in range(num_topics) if x != i]\n",
    "        eta[leftover_topic_rownums, seeded_word_id] = ((1 - boosted_prob) / (num_topics - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[0, vocab_dictionary.token2id['quality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049999999999999989"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[1, vocab_dictionary.token2id['quality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049999999999999989"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[2, vocab_dictionary.token2id['quality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  0.33333333,  0.33333333, ...,  0.33333333,\n",
       "         0.33333333,  0.33333333],\n",
       "       [ 0.33333333,  0.33333333,  0.33333333, ...,  0.33333333,\n",
       "         0.33333333,  0.33333333],\n",
       "       [ 0.33333333,  0.33333333,  0.33333333, ...,  0.33333333,\n",
       "         0.33333333,  0.33333333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13850)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.33333334, 0.33333334, 0.33333334]\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 3 topics, 3 passes over the supplied corpus of 217530 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6555/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10288081, 0.13132362, 0.19620216]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.103): 0.025*\"pedometer\" + 0.023*\"day\" + 0.022*\"step\" + 0.018*\"use\" + 0.016*\"great\" + 0.016*\"good\" + 0.014*\"like\" + 0.014*\"work\" + 0.011*\"product\" + 0.011*\"walk\"\n",
      "INFO : topic #1 (0.131): 0.030*\"pedometer\" + 0.022*\"great\" + 0.022*\"use\" + 0.017*\"product\" + 0.017*\"easy\" + 0.016*\"work\" + 0.015*\"day\" + 0.015*\"good\" + 0.011*\"clip\" + 0.010*\"step\"\n",
      "INFO : topic #2 (0.196): 0.036*\"pedometer\" + 0.025*\"use\" + 0.024*\"product\" + 0.023*\"good\" + 0.018*\"great\" + 0.016*\"day\" + 0.016*\"work\" + 0.016*\"step\" + 0.015*\"love\" + 0.012*\"walk\"\n",
      "INFO : topic diff=4.933672, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6961/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10878263, 0.12703378, 0.15679902]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.109): 0.017*\"good\" + 0.017*\"day\" + 0.014*\"use\" + 0.014*\"product\" + 0.013*\"work\" + 0.013*\"great\" + 0.013*\"like\" + 0.011*\"help\" + 0.009*\"feel\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.127): 0.026*\"product\" + 0.022*\"great\" + 0.018*\"good\" + 0.018*\"use\" + 0.014*\"work\" + 0.012*\"taste\" + 0.010*\"day\" + 0.009*\"recommend\" + 0.009*\"like\" + 0.008*\"pedometer\"\n",
      "INFO : topic #2 (0.157): 0.034*\"product\" + 0.026*\"good\" + 0.024*\"use\" + 0.018*\"great\" + 0.014*\"work\" + 0.013*\"pedometer\" + 0.012*\"love\" + 0.012*\"day\" + 0.011*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=1.450245, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6985/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11271323, 0.12641029, 0.1440265]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.113): 0.017*\"day\" + 0.017*\"good\" + 0.014*\"help\" + 0.014*\"work\" + 0.014*\"product\" + 0.013*\"use\" + 0.011*\"great\" + 0.011*\"like\" + 0.010*\"feel\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.126): 0.029*\"product\" + 0.023*\"great\" + 0.021*\"good\" + 0.018*\"use\" + 0.014*\"work\" + 0.012*\"taste\" + 0.009*\"like\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"help\"\n",
      "INFO : topic #2 (0.144): 0.038*\"product\" + 0.026*\"good\" + 0.025*\"use\" + 0.019*\"great\" + 0.016*\"work\" + 0.011*\"day\" + 0.011*\"love\" + 0.011*\"year\" + 0.010*\"skin\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.773074, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6986/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11491707, 0.13032664, 0.14318213]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.115): 0.017*\"day\" + 0.016*\"good\" + 0.015*\"help\" + 0.015*\"work\" + 0.013*\"product\" + 0.013*\"use\" + 0.011*\"like\" + 0.010*\"feel\" + 0.010*\"great\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.130): 0.030*\"product\" + 0.024*\"great\" + 0.022*\"good\" + 0.019*\"use\" + 0.013*\"work\" + 0.012*\"taste\" + 0.010*\"like\" + 0.009*\"price\" + 0.008*\"recommend\" + 0.008*\"find\"\n",
      "INFO : topic #2 (0.143): 0.039*\"product\" + 0.027*\"use\" + 0.025*\"good\" + 0.020*\"great\" + 0.016*\"work\" + 0.012*\"love\" + 0.010*\"skin\" + 0.010*\"year\" + 0.010*\"help\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.590217, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6992/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12025099, 0.13341348, 0.13835461]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.120): 0.016*\"day\" + 0.016*\"help\" + 0.015*\"work\" + 0.015*\"good\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"like\" + 0.009*\"try\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.133): 0.032*\"product\" + 0.025*\"great\" + 0.025*\"good\" + 0.018*\"use\" + 0.014*\"taste\" + 0.012*\"work\" + 0.011*\"like\" + 0.010*\"price\" + 0.009*\"recommend\" + 0.008*\"find\"\n",
      "INFO : topic #2 (0.138): 0.040*\"product\" + 0.026*\"use\" + 0.025*\"good\" + 0.020*\"great\" + 0.016*\"work\" + 0.012*\"love\" + 0.011*\"skin\" + 0.010*\"year\" + 0.010*\"help\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.529911, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12495694, 0.13665146, 0.13546844]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.125): 0.017*\"day\" + 0.016*\"help\" + 0.015*\"work\" + 0.015*\"good\" + 0.013*\"product\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.137): 0.032*\"product\" + 0.027*\"good\" + 0.026*\"great\" + 0.018*\"use\" + 0.015*\"taste\" + 0.011*\"like\" + 0.011*\"price\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.008*\"find\"\n",
      "INFO : topic #2 (0.135): 0.042*\"product\" + 0.027*\"use\" + 0.024*\"good\" + 0.020*\"great\" + 0.016*\"work\" + 0.012*\"skin\" + 0.011*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"day\"\n",
      "INFO : topic diff=0.474684, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.12957631, 0.14089073, 0.13424966]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.130): 0.017*\"day\" + 0.016*\"work\" + 0.016*\"help\" + 0.014*\"good\" + 0.014*\"product\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.141): 0.033*\"product\" + 0.028*\"good\" + 0.026*\"great\" + 0.018*\"use\" + 0.016*\"taste\" + 0.012*\"like\" + 0.011*\"price\" + 0.010*\"work\" + 0.009*\"find\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.134): 0.043*\"product\" + 0.028*\"use\" + 0.023*\"good\" + 0.020*\"great\" + 0.017*\"work\" + 0.012*\"skin\" + 0.011*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.009*\"day\"\n",
      "INFO : topic diff=0.442087, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13751084, 0.1431246, 0.12971774]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.138): 0.018*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.014*\"product\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.143): 0.034*\"product\" + 0.029*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.017*\"use\" + 0.012*\"like\" + 0.011*\"price\" + 0.010*\"work\" + 0.009*\"find\" + 0.008*\"recommend\"\n",
      "INFO : topic #2 (0.130): 0.045*\"product\" + 0.029*\"use\" + 0.023*\"good\" + 0.021*\"great\" + 0.018*\"work\" + 0.013*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.403104, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14261632, 0.14624219, 0.12725346]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.143): 0.017*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"use\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.146): 0.034*\"product\" + 0.031*\"good\" + 0.027*\"great\" + 0.018*\"taste\" + 0.017*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.009*\"work\" + 0.009*\"find\" + 0.009*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.127): 0.046*\"product\" + 0.029*\"use\" + 0.024*\"good\" + 0.022*\"great\" + 0.018*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.360838, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14829192, 0.14594503, 0.12849703]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.148): 0.018*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.146): 0.034*\"product\" + 0.032*\"good\" + 0.027*\"great\" + 0.017*\"use\" + 0.017*\"taste\" + 0.012*\"like\" + 0.012*\"price\" + 0.009*\"find\" + 0.009*\"love\" + 0.009*\"work\"\n",
      "INFO : topic #2 (0.128): 0.047*\"product\" + 0.029*\"use\" + 0.023*\"good\" + 0.022*\"great\" + 0.018*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.358150, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15109277, 0.15036719, 0.12712698]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.151): 0.018*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.150): 0.033*\"good\" + 0.033*\"product\" + 0.026*\"great\" + 0.019*\"use\" + 0.018*\"taste\" + 0.012*\"like\" + 0.012*\"price\" + 0.009*\"find\" + 0.009*\"love\" + 0.008*\"buy\"\n",
      "INFO : topic #2 (0.127): 0.046*\"product\" + 0.031*\"use\" + 0.022*\"great\" + 0.022*\"good\" + 0.018*\"work\" + 0.012*\"skin\" + 0.011*\"love\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.338813, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15644275, 0.15249054, 0.12702277]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.156): 0.017*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.015*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.152): 0.035*\"good\" + 0.033*\"product\" + 0.026*\"great\" + 0.018*\"use\" + 0.017*\"taste\" + 0.013*\"like\" + 0.012*\"price\" + 0.009*\"find\" + 0.009*\"brand\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.127): 0.047*\"product\" + 0.030*\"use\" + 0.022*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.012*\"skin\" + 0.011*\"love\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.311316, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16238703, 0.15593781, 0.12408254]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.162): 0.017*\"day\" + 0.017*\"help\" + 0.016*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.156): 0.035*\"good\" + 0.033*\"product\" + 0.026*\"great\" + 0.017*\"use\" + 0.016*\"taste\" + 0.013*\"price\" + 0.012*\"like\" + 0.009*\"find\" + 0.009*\"supplement\" + 0.009*\"vitamin\"\n",
      "INFO : topic #2 (0.124): 0.047*\"product\" + 0.030*\"use\" + 0.023*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.013*\"skin\" + 0.011*\"love\" + 0.010*\"year\" + 0.010*\"help\" + 0.010*\"hair\"\n",
      "INFO : topic diff=0.305171, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16237308, 0.16153675, 0.12311038]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.162): 0.018*\"day\" + 0.017*\"help\" + 0.017*\"work\" + 0.014*\"product\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.162): 0.035*\"good\" + 0.033*\"product\" + 0.027*\"great\" + 0.018*\"use\" + 0.017*\"taste\" + 0.013*\"price\" + 0.013*\"like\" + 0.010*\"vitamin\" + 0.009*\"find\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.123): 0.047*\"product\" + 0.031*\"use\" + 0.023*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.013*\"skin\" + 0.012*\"love\" + 0.010*\"recommend\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.271182, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16561809, 0.16217564, 0.12387951]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.166): 0.018*\"day\" + 0.017*\"work\" + 0.017*\"help\" + 0.014*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.008*\"supplement\"\n",
      "INFO : topic #1 (0.162): 0.035*\"good\" + 0.033*\"product\" + 0.027*\"great\" + 0.018*\"use\" + 0.017*\"taste\" + 0.013*\"price\" + 0.013*\"like\" + 0.010*\"vitamin\" + 0.009*\"find\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.124): 0.047*\"product\" + 0.031*\"use\" + 0.024*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.013*\"skin\" + 0.012*\"love\" + 0.010*\"help\" + 0.010*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.273785, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16751239, 0.16945679, 0.1227808]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.168): 0.018*\"day\" + 0.017*\"work\" + 0.017*\"help\" + 0.014*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.169): 0.035*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.017*\"taste\" + 0.016*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.011*\"vitamin\" + 0.010*\"find\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.123): 0.047*\"product\" + 0.031*\"use\" + 0.023*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.263530, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6996/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16776013, 0.17399335, 0.12178251]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.168): 0.018*\"day\" + 0.017*\"work\" + 0.017*\"help\" + 0.014*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.174): 0.035*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.015*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.012*\"vitamin\" + 0.010*\"find\" + 0.009*\"easy\"\n",
      "INFO : topic #2 (0.122): 0.047*\"product\" + 0.031*\"use\" + 0.023*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.010*\"help\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.245886, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1655412, 0.18215849, 0.1212571]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.166): 0.018*\"day\" + 0.017*\"work\" + 0.016*\"help\" + 0.014*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.010*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.182): 0.036*\"good\" + 0.032*\"product\" + 0.027*\"great\" + 0.020*\"taste\" + 0.014*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"find\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.121): 0.047*\"product\" + 0.030*\"use\" + 0.023*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.013*\"skin\" + 0.012*\"love\" + 0.010*\"recommend\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.231784, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1626647, 0.18799931, 0.12085097]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.163): 0.018*\"day\" + 0.018*\"work\" + 0.017*\"help\" + 0.015*\"product\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.188): 0.036*\"good\" + 0.031*\"product\" + 0.027*\"great\" + 0.021*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.121): 0.047*\"product\" + 0.031*\"use\" + 0.024*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.010*\"oil\" + 0.010*\"recommend\" + 0.010*\"hair\"\n",
      "INFO : topic diff=0.233327, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1674556, 0.1884758, 0.12058856]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.167): 0.018*\"day\" + 0.018*\"work\" + 0.016*\"help\" + 0.015*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.188): 0.036*\"good\" + 0.032*\"product\" + 0.027*\"great\" + 0.020*\"taste\" + 0.014*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.009*\"easy\"\n",
      "INFO : topic #2 (0.121): 0.049*\"product\" + 0.031*\"use\" + 0.024*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.014*\"love\" + 0.013*\"skin\" + 0.010*\"recommend\" + 0.010*\"help\" + 0.010*\"year\"\n",
      "INFO : topic diff=0.233975, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1675826, 0.191443, 0.12066473]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.168): 0.018*\"day\" + 0.018*\"work\" + 0.016*\"help\" + 0.016*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.191): 0.035*\"good\" + 0.031*\"product\" + 0.027*\"great\" + 0.021*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.011*\"vitamin\" + 0.010*\"love\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.121): 0.049*\"product\" + 0.032*\"use\" + 0.024*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.014*\"love\" + 0.013*\"skin\" + 0.010*\"recommend\" + 0.010*\"help\" + 0.010*\"oil\"\n",
      "INFO : topic diff=0.223931, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16937315, 0.194326, 0.12028788]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.169): 0.018*\"work\" + 0.018*\"day\" + 0.016*\"help\" + 0.016*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.194): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.021*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.011*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.120): 0.050*\"product\" + 0.032*\"use\" + 0.024*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.014*\"love\" + 0.013*\"skin\" + 0.010*\"recommend\" + 0.010*\"help\" + 0.009*\"year\"\n",
      "INFO : topic diff=0.218580, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16738439, 0.19102934, 0.12271131]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.167): 0.018*\"work\" + 0.018*\"day\" + 0.016*\"help\" + 0.016*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.191): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.123): 0.048*\"product\" + 0.031*\"use\" + 0.023*\"great\" + 0.019*\"good\" + 0.017*\"hair\" + 0.017*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.010*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.224335, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17031863, 0.19273028, 0.12271693]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.170): 0.018*\"work\" + 0.017*\"day\" + 0.016*\"help\" + 0.016*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.193): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.123): 0.049*\"product\" + 0.030*\"use\" + 0.024*\"great\" + 0.019*\"good\" + 0.017*\"work\" + 0.017*\"hair\" + 0.013*\"skin\" + 0.013*\"love\" + 0.011*\"oil\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.210496, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17264499, 0.19356495, 0.12390333]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.173): 0.018*\"work\" + 0.017*\"day\" + 0.017*\"help\" + 0.017*\"product\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.194): 0.035*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.124): 0.050*\"product\" + 0.030*\"use\" + 0.024*\"great\" + 0.019*\"good\" + 0.017*\"work\" + 0.014*\"hair\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"oil\" + 0.010*\"help\"\n",
      "INFO : topic diff=0.206346, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17503396, 0.19532003, 0.12403023]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.175): 0.018*\"work\" + 0.017*\"product\" + 0.017*\"day\" + 0.017*\"help\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.195): 0.034*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.124): 0.051*\"product\" + 0.030*\"use\" + 0.025*\"great\" + 0.019*\"good\" + 0.017*\"work\" + 0.014*\"hair\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"oil\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.208916, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17752697, 0.19559376, 0.12385885]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.178): 0.018*\"work\" + 0.017*\"product\" + 0.017*\"day\" + 0.016*\"help\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.196): 0.034*\"good\" + 0.032*\"product\" + 0.027*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.011*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.124): 0.052*\"product\" + 0.030*\"use\" + 0.025*\"great\" + 0.019*\"good\" + 0.018*\"work\" + 0.014*\"hair\" + 0.013*\"love\" + 0.013*\"skin\" + 0.011*\"oil\" + 0.010*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.195451, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18187711, 0.19608344, 0.12382407]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.182): 0.018*\"work\" + 0.017*\"product\" + 0.017*\"day\" + 0.016*\"help\" + 0.013*\"good\" + 0.013*\"feel\" + 0.011*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.196): 0.034*\"good\" + 0.032*\"product\" + 0.027*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.124): 0.052*\"product\" + 0.030*\"use\" + 0.026*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.013*\"hair\" + 0.010*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.194153, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17947996, 0.19426626, 0.12574847]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.179): 0.018*\"work\" + 0.018*\"product\" + 0.017*\"day\" + 0.016*\"help\" + 0.013*\"feel\" + 0.013*\"good\" + 0.011*\"use\" + 0.011*\"try\" + 0.009*\"supplement\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.194): 0.034*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.010*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.126): 0.053*\"product\" + 0.031*\"use\" + 0.025*\"great\" + 0.020*\"good\" + 0.017*\"work\" + 0.016*\"skin\" + 0.014*\"love\" + 0.011*\"hair\" + 0.010*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.201603, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1814362, 0.19713826, 0.1266171]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.181): 0.018*\"product\" + 0.017*\"work\" + 0.016*\"day\" + 0.016*\"help\" + 0.014*\"feel\" + 0.013*\"good\" + 0.011*\"use\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.008*\"recommend\"\n",
      "INFO : topic #1 (0.197): 0.033*\"product\" + 0.032*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.012*\"use\" + 0.012*\"supplement\" + 0.011*\"price\" + 0.011*\"vitamin\" + 0.011*\"easy\"\n",
      "INFO : topic #2 (0.127): 0.055*\"product\" + 0.030*\"use\" + 0.026*\"great\" + 0.019*\"good\" + 0.017*\"skin\" + 0.016*\"work\" + 0.014*\"love\" + 0.011*\"feel\" + 0.010*\"hair\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.203599, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18276414, 0.19067417, 0.12987047]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.183): 0.018*\"product\" + 0.017*\"work\" + 0.016*\"day\" + 0.016*\"help\" + 0.015*\"feel\" + 0.013*\"good\" + 0.011*\"try\" + 0.011*\"use\" + 0.010*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.033*\"product\" + 0.031*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.011*\"easy\" + 0.011*\"price\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.130): 0.055*\"product\" + 0.029*\"use\" + 0.025*\"great\" + 0.021*\"skin\" + 0.019*\"good\" + 0.016*\"work\" + 0.013*\"love\" + 0.012*\"feel\" + 0.010*\"recommend\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.200985, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 529/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1837043, 0.18289787, 0.13254486]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.184): 0.019*\"product\" + 0.017*\"work\" + 0.017*\"help\" + 0.016*\"feel\" + 0.016*\"day\" + 0.014*\"good\" + 0.012*\"try\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.183): 0.034*\"product\" + 0.031*\"good\" + 0.027*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.013*\"supplement\" + 0.012*\"easy\" + 0.012*\"use\" + 0.011*\"price\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.133): 0.058*\"product\" + 0.027*\"use\" + 0.027*\"great\" + 0.020*\"skin\" + 0.019*\"good\" + 0.017*\"work\" + 0.014*\"love\" + 0.013*\"feel\" + 0.011*\"recommend\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.184281, rho=0.176777\n",
      "INFO : PROGRESS: pass 1, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18681586, 0.18445125, 0.13674645]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.187): 0.018*\"day\" + 0.018*\"product\" + 0.018*\"work\" + 0.016*\"help\" + 0.014*\"feel\" + 0.014*\"good\" + 0.011*\"use\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.184): 0.032*\"product\" + 0.031*\"good\" + 0.027*\"great\" + 0.016*\"taste\" + 0.015*\"easy\" + 0.014*\"like\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.137): 0.052*\"product\" + 0.029*\"use\" + 0.027*\"great\" + 0.019*\"good\" + 0.018*\"skin\" + 0.018*\"work\" + 0.015*\"love\" + 0.012*\"pedometer\" + 0.011*\"feel\" + 0.011*\"recommend\"\n",
      "INFO : topic diff=0.231639, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1905165, 0.18664451, 0.1342376]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.191): 0.018*\"work\" + 0.018*\"day\" + 0.018*\"product\" + 0.016*\"help\" + 0.014*\"feel\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.031*\"product\" + 0.031*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.014*\"easy\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.134): 0.051*\"product\" + 0.030*\"use\" + 0.027*\"great\" + 0.019*\"good\" + 0.018*\"skin\" + 0.017*\"work\" + 0.015*\"love\" + 0.011*\"feel\" + 0.011*\"recommend\" + 0.010*\"pedometer\"\n",
      "INFO : topic diff=0.197645, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19546196, 0.18612768, 0.13323537]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.195): 0.018*\"work\" + 0.018*\"day\" + 0.017*\"product\" + 0.017*\"help\" + 0.014*\"good\" + 0.013*\"feel\" + 0.012*\"use\" + 0.010*\"try\" + 0.009*\"supplement\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.186): 0.032*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.013*\"easy\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.133): 0.051*\"product\" + 0.031*\"use\" + 0.027*\"great\" + 0.019*\"good\" + 0.018*\"skin\" + 0.018*\"work\" + 0.015*\"love\" + 0.011*\"feel\" + 0.010*\"recommend\" + 0.010*\"hair\"\n",
      "INFO : topic diff=0.179702, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1956066, 0.18532112, 0.13376437]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.196): 0.018*\"work\" + 0.018*\"day\" + 0.018*\"help\" + 0.017*\"product\" + 0.014*\"good\" + 0.013*\"feel\" + 0.012*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.185): 0.032*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.012*\"easy\" + 0.010*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.134): 0.049*\"product\" + 0.031*\"use\" + 0.027*\"great\" + 0.018*\"good\" + 0.018*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.011*\"hair\" + 0.010*\"feel\" + 0.010*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.179011, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19869582, 0.18542501, 0.131529]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.199): 0.018*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.017*\"product\" + 0.013*\"good\" + 0.013*\"feel\" + 0.012*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.185): 0.033*\"good\" + 0.031*\"product\" + 0.027*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.012*\"easy\" + 0.010*\"vitamin\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.132): 0.049*\"product\" + 0.031*\"use\" + 0.026*\"great\" + 0.019*\"good\" + 0.018*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.010*\"recommend\" + 0.010*\"hair\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.170773, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20062537, 0.18463527, 0.13007396]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.201): 0.018*\"work\" + 0.018*\"help\" + 0.018*\"day\" + 0.017*\"product\" + 0.014*\"good\" + 0.013*\"feel\" + 0.012*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.185): 0.033*\"good\" + 0.031*\"product\" + 0.027*\"great\" + 0.018*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.011*\"easy\" + 0.010*\"love\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.130): 0.049*\"product\" + 0.032*\"use\" + 0.026*\"great\" + 0.018*\"good\" + 0.018*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.011*\"hair\" + 0.010*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.170473, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20225038, 0.18515471, 0.12801513]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.202): 0.019*\"work\" + 0.018*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.185): 0.033*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.011*\"easy\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.128): 0.049*\"product\" + 0.032*\"use\" + 0.026*\"great\" + 0.018*\"work\" + 0.018*\"good\" + 0.018*\"skin\" + 0.015*\"love\" + 0.011*\"hair\" + 0.010*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic diff=0.168913, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20747322, 0.18398789, 0.12459028]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.207): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.184): 0.033*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.010*\"easy\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.125): 0.050*\"product\" + 0.033*\"use\" + 0.026*\"great\" + 0.019*\"work\" + 0.019*\"skin\" + 0.019*\"good\" + 0.015*\"love\" + 0.010*\"hair\" + 0.010*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic diff=0.164677, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21063291, 0.18431705, 0.12200711]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.211): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.184): 0.034*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.010*\"easy\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.122): 0.051*\"product\" + 0.033*\"use\" + 0.027*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.018*\"skin\" + 0.015*\"love\" + 0.010*\"hair\" + 0.010*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic diff=0.157209, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21427536, 0.18159485, 0.12152475]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.214): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.018*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.182): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.014*\"price\" + 0.010*\"easy\" + 0.010*\"love\" + 0.009*\"supplement\"\n",
      "INFO : topic #2 (0.122): 0.051*\"product\" + 0.033*\"use\" + 0.027*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.017*\"skin\" + 0.015*\"love\" + 0.012*\"hair\" + 0.010*\"recommend\" + 0.009*\"buy\"\n",
      "INFO : topic diff=0.167849, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21514739, 0.18218155, 0.11974898]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.215): 0.019*\"work\" + 0.018*\"product\" + 0.017*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.182): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.010*\"love\" + 0.009*\"easy\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.120): 0.050*\"product\" + 0.034*\"use\" + 0.027*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.017*\"skin\" + 0.015*\"love\" + 0.011*\"hair\" + 0.010*\"recommend\" + 0.009*\"buy\"\n",
      "INFO : topic diff=0.165971, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21841305, 0.18196763, 0.11831821]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.218): 0.019*\"work\" + 0.019*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.182): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.014*\"price\" + 0.009*\"find\" + 0.009*\"easy\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.118): 0.051*\"product\" + 0.034*\"use\" + 0.027*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.013*\"hair\" + 0.009*\"recommend\" + 0.009*\"buy\"\n",
      "INFO : topic diff=0.157959, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22204, 0.18360646, 0.11608248]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.222): 0.018*\"product\" + 0.018*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.184): 0.035*\"good\" + 0.031*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.015*\"use\" + 0.014*\"price\" + 0.014*\"like\" + 0.010*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.116): 0.051*\"product\" + 0.034*\"use\" + 0.027*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.018*\"skin\" + 0.014*\"love\" + 0.013*\"hair\" + 0.009*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.161225, rho=0.173878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22039965, 0.18670149, 0.11479571]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.220): 0.018*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.018*\"taste\" + 0.016*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.010*\"vitamin\" + 0.009*\"supplement\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.115): 0.051*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.017*\"skin\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.150769, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22147015, 0.18604013, 0.11461824]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.221): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.186): 0.035*\"good\" + 0.030*\"product\" + 0.026*\"great\" + 0.018*\"taste\" + 0.016*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.011*\"vitamin\" + 0.009*\"supplement\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.115): 0.050*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.017*\"skin\" + 0.014*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.158260, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22114496, 0.19181268, 0.11244938]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.221): 0.019*\"work\" + 0.018*\"help\" + 0.018*\"product\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.192): 0.035*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.015*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.112): 0.051*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"work\" + 0.019*\"good\" + 0.016*\"skin\" + 0.015*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.156949, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21925095, 0.19454543, 0.11119624]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.219): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.195): 0.035*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"use\" + 0.014*\"price\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.111): 0.051*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.016*\"skin\" + 0.015*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.151462, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21477208, 0.20154969, 0.1099445]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.215): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.012*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.202): 0.035*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"price\" + 0.014*\"use\" + 0.013*\"like\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.110): 0.051*\"product\" + 0.033*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"recommend\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.148484, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21024576, 0.20560467, 0.1095146]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.210): 0.019*\"work\" + 0.018*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.012*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.206): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.014*\"price\" + 0.014*\"like\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"love\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.110): 0.051*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.016*\"love\" + 0.013*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.154229, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2132794, 0.2048064, 0.10914949]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.019*\"work\" + 0.019*\"product\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.012*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.205): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"price\" + 0.014*\"like\" + 0.013*\"use\" + 0.011*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.109): 0.053*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.017*\"skin\" + 0.016*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.155361, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21122681, 0.20667659, 0.10903821]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.211): 0.019*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.109): 0.053*\"product\" + 0.034*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.016*\"skin\" + 0.016*\"love\" + 0.011*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.156696, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21148786, 0.20811966, 0.10890099]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.211): 0.019*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.208): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.011*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.109): 0.053*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.016*\"love\" + 0.016*\"skin\" + 0.011*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.158250, rho=0.173878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20708156, 0.20466927, 0.11017092]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.207): 0.019*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.205): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.110): 0.051*\"product\" + 0.033*\"use\" + 0.027*\"great\" + 0.020*\"hair\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"skin\" + 0.015*\"love\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.170839, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20887226, 0.2063435, 0.11015551]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.209): 0.019*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.206): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.110): 0.052*\"product\" + 0.032*\"use\" + 0.027*\"great\" + 0.019*\"good\" + 0.019*\"hair\" + 0.018*\"work\" + 0.016*\"skin\" + 0.015*\"love\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.155733, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21046023, 0.20663616, 0.11129088]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.210): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.013*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.012*\"use\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.111): 0.053*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"hair\" + 0.016*\"skin\" + 0.015*\"love\" + 0.010*\"oil\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.159663, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21129893, 0.20740312, 0.11144476]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.211): 0.020*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.034*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.111): 0.053*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"skin\" + 0.016*\"hair\" + 0.015*\"love\" + 0.010*\"oil\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.163415, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21306953, 0.20660169, 0.11134984]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.020*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.034*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.111): 0.054*\"product\" + 0.032*\"use\" + 0.029*\"great\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"hair\" + 0.016*\"skin\" + 0.015*\"love\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.155779, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21682183, 0.20674402, 0.11139912]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.217): 0.020*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.111): 0.055*\"product\" + 0.031*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.016*\"skin\" + 0.015*\"love\" + 0.015*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.158646, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21267799, 0.20448558, 0.11287224]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.020*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"feel\" + 0.012*\"use\" + 0.011*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.204): 0.033*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.113): 0.056*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.020*\"good\" + 0.019*\"skin\" + 0.018*\"work\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"feel\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.173033, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21368051, 0.20683847, 0.11393912]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.214): 0.021*\"product\" + 0.018*\"work\" + 0.017*\"help\" + 0.016*\"day\" + 0.014*\"feel\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"try\" + 0.009*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.031*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.012*\"supplement\" + 0.012*\"vitamin\" + 0.012*\"use\" + 0.011*\"price\" + 0.011*\"easy\"\n",
      "INFO : topic #2 (0.114): 0.057*\"product\" + 0.031*\"use\" + 0.029*\"great\" + 0.020*\"skin\" + 0.019*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.012*\"hair\" + 0.011*\"feel\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.175326, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21346869, 0.19935356, 0.11700351]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.213): 0.021*\"product\" + 0.018*\"work\" + 0.017*\"help\" + 0.016*\"day\" + 0.015*\"feel\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"try\" + 0.010*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.199): 0.031*\"good\" + 0.031*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.013*\"supplement\" + 0.012*\"easy\" + 0.011*\"use\" + 0.011*\"vitamin\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.117): 0.057*\"product\" + 0.030*\"use\" + 0.027*\"great\" + 0.024*\"skin\" + 0.019*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.011*\"feel\" + 0.011*\"hair\" + 0.011*\"look\"\n",
      "INFO : topic diff=0.180356, rho=0.173878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 529/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21238394, 0.19002634, 0.11915652]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.212): 0.022*\"product\" + 0.018*\"work\" + 0.018*\"help\" + 0.016*\"feel\" + 0.015*\"day\" + 0.015*\"good\" + 0.012*\"use\" + 0.012*\"try\" + 0.011*\"supplement\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.190): 0.031*\"product\" + 0.031*\"good\" + 0.025*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.013*\"supplement\" + 0.012*\"easy\" + 0.011*\"price\" + 0.011*\"use\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.119): 0.061*\"product\" + 0.029*\"great\" + 0.028*\"use\" + 0.023*\"skin\" + 0.019*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.012*\"feel\" + 0.011*\"look\" + 0.011*\"recommend\"\n",
      "INFO : topic diff=0.167850, rho=0.173878\n",
      "INFO : PROGRESS: pass 2, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2154734, 0.18997902, 0.12509552]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.215): 0.021*\"product\" + 0.018*\"work\" + 0.017*\"day\" + 0.016*\"help\" + 0.015*\"feel\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"try\" + 0.009*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.190): 0.031*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.017*\"taste\" + 0.016*\"easy\" + 0.014*\"like\" + 0.012*\"supplement\" + 0.012*\"price\" + 0.012*\"use\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.125): 0.053*\"product\" + 0.030*\"use\" + 0.029*\"great\" + 0.020*\"skin\" + 0.019*\"good\" + 0.018*\"work\" + 0.017*\"love\" + 0.016*\"pedometer\" + 0.011*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.218357, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21933754, 0.1923812, 0.12317548]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.219): 0.020*\"product\" + 0.018*\"work\" + 0.017*\"day\" + 0.017*\"help\" + 0.014*\"good\" + 0.014*\"feel\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.192): 0.031*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"easy\" + 0.014*\"like\" + 0.012*\"use\" + 0.012*\"price\" + 0.012*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.123): 0.052*\"product\" + 0.031*\"use\" + 0.029*\"great\" + 0.020*\"skin\" + 0.018*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.014*\"pedometer\" + 0.011*\"recommend\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.177610, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22426017, 0.19159514, 0.12249531]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.224): 0.020*\"product\" + 0.018*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"feel\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.192): 0.032*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.014*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.122): 0.051*\"product\" + 0.031*\"use\" + 0.029*\"great\" + 0.020*\"skin\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.012*\"pedometer\" + 0.011*\"hair\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.162127, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22361949, 0.19045928, 0.12333424]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.224): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.013*\"use\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.190): 0.032*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"easy\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.123): 0.050*\"product\" + 0.032*\"use\" + 0.029*\"great\" + 0.019*\"skin\" + 0.018*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.012*\"hair\" + 0.010*\"pedometer\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.162749, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22689737, 0.19055417, 0.12173309]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.227): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.013*\"use\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.032*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"easy\" + 0.011*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.122): 0.050*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.019*\"skin\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.011*\"hair\" + 0.010*\"recommend\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.155533, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22885175, 0.18991821, 0.12063045]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.229): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.190): 0.033*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.011*\"easy\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.121): 0.050*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.019*\"skin\" + 0.019*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.155408, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2300247, 0.19062085, 0.11898798]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.230): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.013*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.033*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.011*\"easy\" + 0.010*\"vitamin\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.119): 0.050*\"product\" + 0.033*\"use\" + 0.028*\"great\" + 0.020*\"skin\" + 0.019*\"work\" + 0.018*\"good\" + 0.016*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.154358, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23528247, 0.18920016, 0.11609914]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.235): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.189): 0.033*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.010*\"easy\" + 0.010*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.116): 0.051*\"product\" + 0.033*\"use\" + 0.028*\"great\" + 0.020*\"skin\" + 0.019*\"work\" + 0.019*\"good\" + 0.016*\"love\" + 0.011*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.150295, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23828362, 0.18943489, 0.1141187]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.238): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.189): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.010*\"easy\" + 0.010*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.114): 0.051*\"product\" + 0.033*\"use\" + 0.029*\"great\" + 0.020*\"skin\" + 0.019*\"good\" + 0.019*\"work\" + 0.016*\"love\" + 0.011*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.144754, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24234971, 0.18655102, 0.11390673]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.242): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.014*\"use\" + 0.013*\"price\" + 0.010*\"easy\" + 0.010*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.114): 0.051*\"product\" + 0.033*\"use\" + 0.029*\"great\" + 0.019*\"good\" + 0.019*\"skin\" + 0.019*\"work\" + 0.016*\"love\" + 0.013*\"hair\" + 0.010*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.154423, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24313669, 0.18725705, 0.11266106]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.243): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.034*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.010*\"easy\" + 0.009*\"vitamin\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.113): 0.051*\"product\" + 0.035*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.012*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.152044, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24635534, 0.18688411, 0.11130953]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.246): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.187): 0.035*\"good\" + 0.029*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.014*\"price\" + 0.010*\"supplement\" + 0.010*\"easy\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.111): 0.051*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.014*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.145087, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24984919, 0.18871261, 0.10957688]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.250): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.189): 0.035*\"good\" + 0.029*\"product\" + 0.024*\"great\" + 0.019*\"taste\" + 0.014*\"use\" + 0.014*\"price\" + 0.014*\"like\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.110): 0.052*\"product\" + 0.034*\"use\" + 0.028*\"great\" + 0.019*\"good\" + 0.019*\"skin\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.149974, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24714102, 0.1917364, 0.10861248]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.247): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.192): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.015*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.109): 0.052*\"product\" + 0.034*\"use\" + 0.029*\"great\" + 0.019*\"good\" + 0.019*\"skin\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.139555, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24789804, 0.19099581, 0.10880718]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.248): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.014*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.191): 0.035*\"good\" + 0.029*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.015*\"use\" + 0.014*\"price\" + 0.014*\"like\" + 0.011*\"vitamin\" + 0.010*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.109): 0.051*\"product\" + 0.034*\"use\" + 0.029*\"great\" + 0.019*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.146790, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24691316, 0.19684723, 0.10700822]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.247): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.197): 0.034*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.018*\"taste\" + 0.014*\"use\" + 0.014*\"price\" + 0.013*\"like\" + 0.012*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.107): 0.052*\"product\" + 0.035*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.145551, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24429128, 0.19951987, 0.10605336]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.244): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.200): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"price\" + 0.014*\"use\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.106): 0.052*\"product\" + 0.034*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.019*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.013*\"hair\" + 0.011*\"buy\" + 0.009*\"order\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.140711, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23893443, 0.20653255, 0.10512221]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.239): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.207): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"price\" + 0.013*\"like\" + 0.013*\"use\" + 0.013*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.105): 0.053*\"product\" + 0.033*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.018*\"skin\" + 0.015*\"love\" + 0.014*\"hair\" + 0.011*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.138213, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23367842, 0.21040404, 0.104924]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.234): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.210): 0.035*\"good\" + 0.028*\"product\" + 0.025*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.105): 0.053*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.020*\"good\" + 0.019*\"skin\" + 0.018*\"work\" + 0.016*\"love\" + 0.014*\"hair\" + 0.011*\"buy\" + 0.010*\"recommend\"\n",
      "INFO : topic diff=0.144968, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23657852, 0.20933332, 0.10469849]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.237): 0.020*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.209): 0.035*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.009*\"love\"\n",
      "INFO : topic #2 (0.105): 0.054*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.020*\"good\" + 0.018*\"skin\" + 0.018*\"work\" + 0.016*\"love\" + 0.013*\"hair\" + 0.011*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.144277, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23379557, 0.21117844, 0.10478087]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.234): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.211): 0.035*\"good\" + 0.027*\"product\" + 0.024*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.105): 0.055*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.018*\"skin\" + 0.017*\"love\" + 0.012*\"hair\" + 0.011*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.145864, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23340252, 0.21226729, 0.10491581]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.034*\"good\" + 0.027*\"product\" + 0.024*\"great\" + 0.021*\"taste\" + 0.014*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.105): 0.055*\"product\" + 0.034*\"use\" + 0.030*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.016*\"love\" + 0.012*\"hair\" + 0.011*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.149056, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22810446, 0.20883448, 0.10631708]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.228): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.011*\"year\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.209): 0.034*\"good\" + 0.027*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.106): 0.053*\"product\" + 0.032*\"use\" + 0.028*\"great\" + 0.021*\"hair\" + 0.020*\"good\" + 0.017*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.011*\"buy\" + 0.009*\"recommend\"\n",
      "INFO : topic diff=0.162415, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22979483, 0.21085003, 0.10577558]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.230): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.211): 0.034*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.013*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.106): 0.054*\"product\" + 0.032*\"use\" + 0.029*\"great\" + 0.020*\"hair\" + 0.020*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.011*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.145408, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23087566, 0.21096848, 0.10671318]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.021*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.211): 0.034*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.013*\"vitamin\" + 0.013*\"price\" + 0.012*\"use\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.107): 0.054*\"product\" + 0.032*\"use\" + 0.029*\"great\" + 0.020*\"good\" + 0.018*\"skin\" + 0.018*\"hair\" + 0.017*\"work\" + 0.015*\"love\" + 0.010*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.149647, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2312164, 0.21159841, 0.10681025]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.022*\"product\" + 0.019*\"work\" + 0.018*\"help\" + 0.016*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.010*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.212): 0.033*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.012*\"price\" + 0.012*\"use\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.107): 0.055*\"product\" + 0.032*\"use\" + 0.030*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.017*\"hair\" + 0.015*\"love\" + 0.011*\"buy\" + 0.010*\"order\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.153572, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23273267, 0.21040648, 0.10698713]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.233): 0.022*\"product\" + 0.020*\"work\" + 0.018*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"use\" + 0.013*\"feel\" + 0.011*\"try\" + 0.010*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.210): 0.033*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"price\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.107): 0.056*\"product\" + 0.032*\"use\" + 0.030*\"great\" + 0.020*\"good\" + 0.018*\"work\" + 0.017*\"hair\" + 0.017*\"skin\" + 0.016*\"love\" + 0.011*\"buy\" + 0.010*\"order\"\n",
      "INFO : topic diff=0.146273, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23634379, 0.21036173, 0.10711391]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.236): 0.022*\"product\" + 0.020*\"work\" + 0.017*\"help\" + 0.017*\"day\" + 0.014*\"good\" + 0.013*\"feel\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.210): 0.033*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.012*\"price\" + 0.012*\"use\" + 0.011*\"easy\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.107): 0.057*\"product\" + 0.031*\"use\" + 0.031*\"great\" + 0.021*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.016*\"hair\" + 0.016*\"love\" + 0.011*\"buy\" + 0.011*\"order\"\n",
      "INFO : topic diff=0.149464, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23120692, 0.20767246, 0.10857385]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.022*\"product\" + 0.019*\"work\" + 0.017*\"help\" + 0.016*\"day\" + 0.014*\"good\" + 0.014*\"feel\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.208): 0.033*\"good\" + 0.028*\"product\" + 0.024*\"great\" + 0.020*\"taste\" + 0.014*\"like\" + 0.012*\"vitamin\" + 0.012*\"price\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"easy\"\n",
      "INFO : topic #2 (0.109): 0.058*\"product\" + 0.032*\"use\" + 0.030*\"great\" + 0.021*\"good\" + 0.020*\"skin\" + 0.017*\"work\" + 0.016*\"love\" + 0.014*\"hair\" + 0.010*\"order\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.164923, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23141079, 0.2097044, 0.10955536]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.022*\"product\" + 0.018*\"work\" + 0.017*\"help\" + 0.016*\"day\" + 0.015*\"feel\" + 0.014*\"good\" + 0.013*\"use\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"great\"\n",
      "INFO : topic #1 (0.210): 0.031*\"good\" + 0.029*\"product\" + 0.024*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.012*\"supplement\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"easy\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.110): 0.059*\"product\" + 0.032*\"use\" + 0.030*\"great\" + 0.021*\"skin\" + 0.020*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.013*\"hair\" + 0.010*\"order\" + 0.010*\"buy\"\n",
      "INFO : topic diff=0.166030, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23102166, 0.20183843, 0.11253216]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.231): 0.022*\"product\" + 0.018*\"work\" + 0.017*\"help\" + 0.016*\"day\" + 0.015*\"feel\" + 0.014*\"good\" + 0.012*\"use\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.202): 0.030*\"good\" + 0.029*\"product\" + 0.024*\"great\" + 0.018*\"taste\" + 0.014*\"like\" + 0.013*\"supplement\" + 0.012*\"easy\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.113): 0.059*\"product\" + 0.030*\"use\" + 0.029*\"great\" + 0.025*\"skin\" + 0.019*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.011*\"hair\" + 0.011*\"look\" + 0.010*\"feel\"\n",
      "INFO : topic diff=0.172675, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23012961, 0.19193047, 0.11466032]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.230): 0.024*\"product\" + 0.018*\"work\" + 0.018*\"help\" + 0.017*\"feel\" + 0.015*\"day\" + 0.015*\"good\" + 0.012*\"use\" + 0.012*\"try\" + 0.010*\"supplement\" + 0.010*\"recommend\"\n",
      "INFO : topic #1 (0.192): 0.030*\"good\" + 0.030*\"product\" + 0.025*\"great\" + 0.019*\"taste\" + 0.014*\"like\" + 0.014*\"supplement\" + 0.012*\"easy\" + 0.011*\"use\" + 0.011*\"vitamin\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.115): 0.063*\"product\" + 0.030*\"great\" + 0.028*\"use\" + 0.024*\"skin\" + 0.019*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.011*\"look\" + 0.011*\"feel\" + 0.011*\"recommend\"\n",
      "INFO : topic diff=0.158575, rho=0.171308\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=3, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 3 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.06992543002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217515/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 1/1 [11:03<00:00, 663.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_3.html\n",
      "CPU times: user 11min 3s, sys: 1.58 s, total: 11min 4s\n",
      "Wall time: 11min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [3]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 3\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = eta           # asymmetric prior (seeded)\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_seeded = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                          passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, \n",
    "                          random_state=42)\n",
    "    save_df_s3(lda_seeded, bucket_name, filepath='amazon_reviews/kk/lda_seeded_{}.pkl'.format(num_topics), \n",
    "               filetype='pickle')\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda_seeded, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_seeded, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 270420 words, keeping 17012 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 538225 words, keeping 27237 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 794667 words, keeping 35107 word types\n",
      "INFO : PROGRESS: at sentence #40000, processed 1060869 words, keeping 42511 word types\n",
      "INFO : PROGRESS: at sentence #50000, processed 1334096 words, keeping 49488 word types\n",
      "INFO : PROGRESS: at sentence #60000, processed 1599525 words, keeping 55872 word types\n",
      "INFO : PROGRESS: at sentence #70000, processed 1871260 words, keeping 62397 word types\n",
      "INFO : PROGRESS: at sentence #80000, processed 2136438 words, keeping 67941 word types\n",
      "INFO : PROGRESS: at sentence #90000, processed 2416725 words, keeping 73936 word types\n",
      "INFO : PROGRESS: at sentence #100000, processed 2670691 words, keeping 79120 word types\n",
      "INFO : PROGRESS: at sentence #110000, processed 2940695 words, keeping 84203 word types\n",
      "INFO : PROGRESS: at sentence #120000, processed 3191308 words, keeping 88797 word types\n",
      "INFO : PROGRESS: at sentence #130000, processed 3432936 words, keeping 93258 word types\n",
      "INFO : PROGRESS: at sentence #140000, processed 3693413 words, keeping 97791 word types\n",
      "INFO : PROGRESS: at sentence #150000, processed 3946208 words, keeping 102571 word types\n",
      "INFO : PROGRESS: at sentence #160000, processed 4195758 words, keeping 107266 word types\n",
      "INFO : PROGRESS: at sentence #170000, processed 4456605 words, keeping 112008 word types\n",
      "INFO : PROGRESS: at sentence #180000, processed 4708794 words, keeping 116498 word types\n",
      "INFO : PROGRESS: at sentence #190000, processed 4973866 words, keeping 121045 word types\n",
      "INFO : PROGRESS: at sentence #200000, processed 5245855 words, keeping 125859 word types\n",
      "INFO : PROGRESS: at sentence #210000, processed 5544212 words, keeping 130136 word types\n",
      "INFO : collected 133384 word types from a corpus of 5767227 raw words and 217530 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=5 retains 22056 unique words (16% of original 133384, drops 111328)\n",
      "INFO : min_count=5 leaves 5621225 word corpus (97% of original 5767227, drops 146002)\n",
      "INFO : deleting the raw counts dictionary of 133384 items\n",
      "INFO : sample=0.001 downsamples 53 most-common words\n",
      "INFO : downsampling leaves estimated 4819222 word corpus (85.7% of prior 5621225)\n",
      "INFO : estimated required memory for 22056 words and 100 dimensions: 28672800 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 3 workers on 22056 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : EPOCH 1 - PROGRESS: at 6.98% examples, 336543 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 15.02% examples, 355868 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 22.40% examples, 355808 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 29.56% examples, 353963 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 36.93% examples, 353679 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 44.58% examples, 354455 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 52.27% examples, 356235 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 60.65% examples, 357923 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 68.68% examples, 359361 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 76.77% examples, 360052 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 84.98% examples, 361497 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 92.43% examples, 360932 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "INFO : EPOCH 1 - PROGRESS: at 99.52% examples, 361905 words/s, in_qsize 4, out_qsize 0\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 5767227 raw words (4818668 effective words) took 13.3s, 362022 effective words/s\n",
      "INFO : EPOCH 2 - PROGRESS: at 6.78% examples, 330057 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 14.68% examples, 346058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 22.40% examples, 353410 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 30.10% examples, 357747 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 37.29% examples, 355278 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 44.92% examples, 356703 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 52.86% examples, 358935 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 61.51% examples, 362141 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 69.03% examples, 360467 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 77.15% examples, 360673 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 84.58% examples, 359312 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 92.28% examples, 359378 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 98.93% examples, 359232 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 5767227 raw words (4819232 effective words) took 13.4s, 359041 effective words/s\n",
      "INFO : EPOCH 3 - PROGRESS: at 7.32% examples, 352574 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 15.19% examples, 358403 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 22.57% examples, 357806 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 30.27% examples, 361322 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 37.48% examples, 358834 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 44.92% examples, 357922 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 52.86% examples, 358277 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 60.82% examples, 358138 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 68.50% examples, 357534 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 76.50% examples, 358124 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 84.08% examples, 357975 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 91.81% examples, 358562 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 98.79% examples, 359093 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 195 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 5767227 raw words (4820033 effective words) took 13.4s, 359789 effective words/s\n",
      "INFO : EPOCH 4 - PROGRESS: at 7.32% examples, 348336 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 15.19% examples, 360407 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 22.86% examples, 362754 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 31.10% examples, 367160 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 38.69% examples, 368742 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 46.35% examples, 368067 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 54.24% examples, 367998 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 62.63% examples, 368061 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : EPOCH 4 - PROGRESS: at 70.72% examples, 368500 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 79.35% examples, 371133 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 87.09% examples, 370536 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 94.53% examples, 370537 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 5767227 raw words (4819483 effective words) took 13.0s, 371372 effective words/s\n",
      "INFO : EPOCH 5 - PROGRESS: at 7.32% examples, 347254 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 15.19% examples, 359726 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 22.73% examples, 361295 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 30.27% examples, 361881 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 37.65% examples, 361105 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 45.28% examples, 363400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 53.37% examples, 363734 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 62.27% examples, 365120 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 70.53% examples, 366335 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 78.76% examples, 366876 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 86.93% examples, 368162 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 94.24% examples, 366739 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 5767227 raw words (4819506 effective words) took 13.1s, 367029 effective words/s\n",
      "INFO : training on a 28836135 raw words (24096922 effective words) took 66.3s, 363652 effective words/s\n"
     ]
    }
   ],
   "source": [
    "review2vec = Word2Vec(tokenized_reviews, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : Effective 'alpha' higher than previous training cycles\n",
      "INFO : training model with 3 workers on 22056 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : EPOCH 1 - PROGRESS: at 7.32% examples, 351169 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 15.19% examples, 361941 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 23.03% examples, 365989 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 30.44% examples, 362596 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 38.29% examples, 365926 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 45.66% examples, 364522 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 53.93% examples, 368171 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 62.45% examples, 369269 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 70.36% examples, 368644 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 78.56% examples, 368899 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 86.93% examples, 370018 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 94.37% examples, 369335 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 5767227 raw words (4818774 effective words) took 13.1s, 368356 effective words/s\n",
      "INFO : EPOCH 2 - PROGRESS: at 7.15% examples, 350780 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 15.19% examples, 365727 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 22.57% examples, 361988 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 30.27% examples, 365125 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 37.76% examples, 364386 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 45.28% examples, 363903 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 53.02% examples, 364323 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 60.99% examples, 362921 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 69.06% examples, 363800 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 77.15% examples, 364644 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 85.16% examples, 365434 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 92.60% examples, 365197 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 99.07% examples, 363843 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 5767227 raw words (4819199 effective words) took 13.2s, 363977 effective words/s\n",
      "INFO : EPOCH 3 - PROGRESS: at 7.32% examples, 358921 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 15.19% examples, 367043 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 22.73% examples, 364906 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 30.10% examples, 363043 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 37.81% examples, 363213 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 45.48% examples, 362208 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 53.56% examples, 362989 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 61.91% examples, 362806 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 70.17% examples, 364014 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 77.83% examples, 362545 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 85.82% examples, 363441 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 92.79% examples, 361467 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "INFO : EPOCH 3 - PROGRESS: at 99.39% examples, 361245 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 5767227 raw words (4820401 effective words) took 13.4s, 360835 effective words/s\n",
      "INFO : EPOCH 4 - PROGRESS: at 7.49% examples, 363469 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 15.38% examples, 369183 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 22.86% examples, 368021 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 30.62% examples, 369573 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 38.58% examples, 373516 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 46.19% examples, 371697 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 53.93% examples, 370494 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 62.09% examples, 369234 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 69.61% examples, 366505 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 77.84% examples, 367908 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 85.82% examples, 368060 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 93.45% examples, 368078 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 367164 words/s, in_qsize 0, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 5767227 raw words (4818015 effective words) took 13.1s, 367101 effective words/s\n",
      "INFO : EPOCH 5 - PROGRESS: at 7.83% examples, 366731 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 15.71% examples, 367804 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 23.34% examples, 368413 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 31.10% examples, 369798 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 38.58% examples, 368785 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 46.02% examples, 366672 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 54.08% examples, 368640 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 62.63% examples, 369013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 70.88% examples, 370112 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 79.14% examples, 370887 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 87.09% examples, 370960 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 94.24% examples, 369411 words/s, in_qsize 5, out_qsize 0\n",
      "DEBUG : job loop exiting, total 579 jobs\n",
      "DEBUG : worker exiting, processed 194 jobs\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG : worker exiting, processed 192 jobs\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG : worker exiting, processed 193 jobs\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 5767227 raw words (4819931 effective words) took 13.1s, 368792 effective words/s\n",
      "INFO : training on a 28836135 raw words (24096320 effective words) took 65.9s, 365651 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24096320, 28836135)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2vec.train(tokenized_reviews, total_examples=len(tokenized_reviews), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_df_s3(review2vec, bucket_name, filepath='amazon_reviews/kk/reviews2vec.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>-0.074639</td>\n",
       "      <td>0.354219</td>\n",
       "      <td>-0.063841</td>\n",
       "      <td>0.089670</td>\n",
       "      <td>0.124870</td>\n",
       "      <td>-0.068005</td>\n",
       "      <td>-0.013207</td>\n",
       "      <td>0.077418</td>\n",
       "      <td>-0.072039</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090658</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.283028</td>\n",
       "      <td>0.135874</td>\n",
       "      <td>-0.020405</td>\n",
       "      <td>-0.348821</td>\n",
       "      <td>0.081809</td>\n",
       "      <td>-0.030812</td>\n",
       "      <td>-0.143888</td>\n",
       "      <td>0.085219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.060326</td>\n",
       "      <td>0.231529</td>\n",
       "      <td>-0.074902</td>\n",
       "      <td>0.085871</td>\n",
       "      <td>0.125798</td>\n",
       "      <td>-0.210218</td>\n",
       "      <td>-0.254523</td>\n",
       "      <td>0.022960</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>0.107868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112741</td>\n",
       "      <td>0.317170</td>\n",
       "      <td>0.317087</td>\n",
       "      <td>0.147866</td>\n",
       "      <td>0.027891</td>\n",
       "      <td>-0.198109</td>\n",
       "      <td>-0.161131</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>-0.188499</td>\n",
       "      <td>0.284355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-0.023763</td>\n",
       "      <td>0.227623</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>-0.032541</td>\n",
       "      <td>0.218707</td>\n",
       "      <td>-0.283889</td>\n",
       "      <td>-0.056582</td>\n",
       "      <td>-0.082675</td>\n",
       "      <td>-0.077025</td>\n",
       "      <td>-0.132305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172068</td>\n",
       "      <td>0.273706</td>\n",
       "      <td>0.362132</td>\n",
       "      <td>0.212433</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>-0.145674</td>\n",
       "      <td>-0.378388</td>\n",
       "      <td>0.332170</td>\n",
       "      <td>-0.047167</td>\n",
       "      <td>0.048488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>0.285413</td>\n",
       "      <td>-0.048341</td>\n",
       "      <td>0.064590</td>\n",
       "      <td>-0.048613</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.090392</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.154475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163690</td>\n",
       "      <td>0.172842</td>\n",
       "      <td>0.176484</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.064181</td>\n",
       "      <td>-0.306245</td>\n",
       "      <td>-0.179612</td>\n",
       "      <td>0.168094</td>\n",
       "      <td>-0.231233</td>\n",
       "      <td>0.250193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.131723</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.025562</td>\n",
       "      <td>-0.039717</td>\n",
       "      <td>0.244143</td>\n",
       "      <td>-0.032734</td>\n",
       "      <td>-0.207010</td>\n",
       "      <td>0.207864</td>\n",
       "      <td>0.275477</td>\n",
       "      <td>-0.119843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228646</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.179578</td>\n",
       "      <td>0.260665</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>-0.443599</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.112038</td>\n",
       "      <td>-0.188126</td>\n",
       "      <td>0.271196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>-0.019957</td>\n",
       "      <td>-0.195679</td>\n",
       "      <td>0.167550</td>\n",
       "      <td>-0.076098</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.005643</td>\n",
       "      <td>0.103599</td>\n",
       "      <td>0.043114</td>\n",
       "      <td>-0.069537</td>\n",
       "      <td>0.089945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292399</td>\n",
       "      <td>0.245713</td>\n",
       "      <td>-0.204933</td>\n",
       "      <td>0.163944</td>\n",
       "      <td>-0.119181</td>\n",
       "      <td>-0.182083</td>\n",
       "      <td>0.054396</td>\n",
       "      <td>0.083126</td>\n",
       "      <td>-0.313821</td>\n",
       "      <td>0.242774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.070785</td>\n",
       "      <td>-0.172536</td>\n",
       "      <td>-0.329890</td>\n",
       "      <td>0.100656</td>\n",
       "      <td>0.269875</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>-0.067899</td>\n",
       "      <td>0.083343</td>\n",
       "      <td>0.031652</td>\n",
       "      <td>-0.123470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213810</td>\n",
       "      <td>0.254655</td>\n",
       "      <td>0.104915</td>\n",
       "      <td>0.258943</td>\n",
       "      <td>-0.088309</td>\n",
       "      <td>-0.146966</td>\n",
       "      <td>-0.194560</td>\n",
       "      <td>0.276072</td>\n",
       "      <td>-0.179112</td>\n",
       "      <td>0.399372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.073624</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.156824</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-0.129977</td>\n",
       "      <td>-0.087569</td>\n",
       "      <td>-0.062197</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>-0.046887</td>\n",
       "      <td>0.224330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107319</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.165261</td>\n",
       "      <td>0.395306</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>-0.164832</td>\n",
       "      <td>-0.232992</td>\n",
       "      <td>0.114996</td>\n",
       "      <td>0.029391</td>\n",
       "      <td>0.416531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>0.294066</td>\n",
       "      <td>0.640802</td>\n",
       "      <td>0.152148</td>\n",
       "      <td>-0.030043</td>\n",
       "      <td>0.277109</td>\n",
       "      <td>-0.604638</td>\n",
       "      <td>-0.013796</td>\n",
       "      <td>0.093345</td>\n",
       "      <td>0.078040</td>\n",
       "      <td>0.171017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179922</td>\n",
       "      <td>0.101642</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.195905</td>\n",
       "      <td>-0.165352</td>\n",
       "      <td>-0.126587</td>\n",
       "      <td>-0.250627</td>\n",
       "      <td>0.189282</td>\n",
       "      <td>-0.168884</td>\n",
       "      <td>0.025101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>-0.116470</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.083074</td>\n",
       "      <td>0.211078</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>-0.031157</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.054648</td>\n",
       "      <td>0.059417</td>\n",
       "      <td>-0.110198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038766</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>-0.047384</td>\n",
       "      <td>0.411964</td>\n",
       "      <td>-0.135511</td>\n",
       "      <td>-0.198599</td>\n",
       "      <td>-0.211693</td>\n",
       "      <td>0.150326</td>\n",
       "      <td>-0.428607</td>\n",
       "      <td>0.217542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.125145</td>\n",
       "      <td>0.032734</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>0.026181</td>\n",
       "      <td>0.029319</td>\n",
       "      <td>0.221195</td>\n",
       "      <td>0.162209</td>\n",
       "      <td>0.193188</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.032388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521883</td>\n",
       "      <td>-0.129289</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.160578</td>\n",
       "      <td>-0.154719</td>\n",
       "      <td>-0.197546</td>\n",
       "      <td>-0.065869</td>\n",
       "      <td>0.279988</td>\n",
       "      <td>-0.332211</td>\n",
       "      <td>0.411903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-0.042094</td>\n",
       "      <td>-0.064606</td>\n",
       "      <td>-0.131619</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>-0.356923</td>\n",
       "      <td>0.356033</td>\n",
       "      <td>-0.194826</td>\n",
       "      <td>0.071787</td>\n",
       "      <td>-0.143158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206210</td>\n",
       "      <td>0.089591</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.276289</td>\n",
       "      <td>0.169822</td>\n",
       "      <td>0.036459</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>-0.138762</td>\n",
       "      <td>-0.548343</td>\n",
       "      <td>0.410395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supplement</th>\n",
       "      <td>-0.159490</td>\n",
       "      <td>0.137338</td>\n",
       "      <td>0.225708</td>\n",
       "      <td>0.174277</td>\n",
       "      <td>0.122478</td>\n",
       "      <td>-0.019642</td>\n",
       "      <td>0.167143</td>\n",
       "      <td>-0.089559</td>\n",
       "      <td>-0.331553</td>\n",
       "      <td>0.406758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063646</td>\n",
       "      <td>0.239245</td>\n",
       "      <td>0.248464</td>\n",
       "      <td>0.362987</td>\n",
       "      <td>-0.039085</td>\n",
       "      <td>-0.394169</td>\n",
       "      <td>-0.106171</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>-0.343865</td>\n",
       "      <td>0.209764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.044152</td>\n",
       "      <td>-0.071927</td>\n",
       "      <td>0.043411</td>\n",
       "      <td>0.048013</td>\n",
       "      <td>0.072494</td>\n",
       "      <td>-0.300624</td>\n",
       "      <td>-0.209743</td>\n",
       "      <td>-0.027427</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>-0.039381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333111</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.161670</td>\n",
       "      <td>0.204192</td>\n",
       "      <td>0.162375</td>\n",
       "      <td>-0.286837</td>\n",
       "      <td>-0.563885</td>\n",
       "      <td>0.328455</td>\n",
       "      <td>0.131724</td>\n",
       "      <td>0.407444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste</th>\n",
       "      <td>0.036556</td>\n",
       "      <td>0.139370</td>\n",
       "      <td>-0.389828</td>\n",
       "      <td>0.180753</td>\n",
       "      <td>0.032585</td>\n",
       "      <td>0.086729</td>\n",
       "      <td>0.098552</td>\n",
       "      <td>0.488643</td>\n",
       "      <td>-0.255265</td>\n",
       "      <td>0.128057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342056</td>\n",
       "      <td>-0.041710</td>\n",
       "      <td>0.586251</td>\n",
       "      <td>0.307798</td>\n",
       "      <td>-0.041044</td>\n",
       "      <td>-0.068655</td>\n",
       "      <td>-0.319548</td>\n",
       "      <td>0.264359</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>0.283535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-0.084595</td>\n",
       "      <td>0.102417</td>\n",
       "      <td>0.083796</td>\n",
       "      <td>-0.252505</td>\n",
       "      <td>0.224015</td>\n",
       "      <td>-0.033471</td>\n",
       "      <td>-0.016940</td>\n",
       "      <td>0.069439</td>\n",
       "      <td>-0.157311</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247823</td>\n",
       "      <td>0.135666</td>\n",
       "      <td>-0.104256</td>\n",
       "      <td>0.205925</td>\n",
       "      <td>0.051673</td>\n",
       "      <td>0.072584</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.174208</td>\n",
       "      <td>-0.220510</td>\n",
       "      <td>0.133960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>0.174731</td>\n",
       "      <td>0.244685</td>\n",
       "      <td>-0.100288</td>\n",
       "      <td>0.175566</td>\n",
       "      <td>0.183345</td>\n",
       "      <td>-0.273145</td>\n",
       "      <td>0.199735</td>\n",
       "      <td>-0.089706</td>\n",
       "      <td>-0.274569</td>\n",
       "      <td>-0.366918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031165</td>\n",
       "      <td>0.167517</td>\n",
       "      <td>0.454833</td>\n",
       "      <td>0.138319</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>-0.076133</td>\n",
       "      <td>-0.150704</td>\n",
       "      <td>0.384565</td>\n",
       "      <td>-0.455265</td>\n",
       "      <td>0.035062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>0.219255</td>\n",
       "      <td>0.181165</td>\n",
       "      <td>0.069085</td>\n",
       "      <td>0.323918</td>\n",
       "      <td>0.324614</td>\n",
       "      <td>-0.303967</td>\n",
       "      <td>-0.140795</td>\n",
       "      <td>-0.290850</td>\n",
       "      <td>-0.062919</td>\n",
       "      <td>-0.171420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121557</td>\n",
       "      <td>-0.049086</td>\n",
       "      <td>0.101042</td>\n",
       "      <td>0.211965</td>\n",
       "      <td>0.159734</td>\n",
       "      <td>-0.325682</td>\n",
       "      <td>-0.241131</td>\n",
       "      <td>0.381319</td>\n",
       "      <td>-0.432989</td>\n",
       "      <td>0.185540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.599139</td>\n",
       "      <td>-0.275831</td>\n",
       "      <td>0.156092</td>\n",
       "      <td>0.496220</td>\n",
       "      <td>-0.048745</td>\n",
       "      <td>-0.015859</td>\n",
       "      <td>-0.100425</td>\n",
       "      <td>0.048610</td>\n",
       "      <td>0.068822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055685</td>\n",
       "      <td>0.028206</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.429793</td>\n",
       "      <td>0.101703</td>\n",
       "      <td>-0.380758</td>\n",
       "      <td>-0.200654</td>\n",
       "      <td>0.137072</td>\n",
       "      <td>-0.296962</td>\n",
       "      <td>0.129422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.017732</td>\n",
       "      <td>-0.087444</td>\n",
       "      <td>0.038792</td>\n",
       "      <td>-0.052732</td>\n",
       "      <td>0.061442</td>\n",
       "      <td>0.091422</td>\n",
       "      <td>0.209542</td>\n",
       "      <td>-0.108499</td>\n",
       "      <td>-0.207357</td>\n",
       "      <td>-0.124101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092765</td>\n",
       "      <td>0.130748</td>\n",
       "      <td>-0.032792</td>\n",
       "      <td>0.256342</td>\n",
       "      <td>0.339566</td>\n",
       "      <td>-0.058128</td>\n",
       "      <td>-0.011886</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>-0.428215</td>\n",
       "      <td>0.155962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>0.058791</td>\n",
       "      <td>0.148452</td>\n",
       "      <td>0.168208</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>-0.122640</td>\n",
       "      <td>-0.310814</td>\n",
       "      <td>-0.051594</td>\n",
       "      <td>0.187401</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>-0.295230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482748</td>\n",
       "      <td>0.147437</td>\n",
       "      <td>0.352033</td>\n",
       "      <td>0.433890</td>\n",
       "      <td>-0.072178</td>\n",
       "      <td>-0.234170</td>\n",
       "      <td>-0.290749</td>\n",
       "      <td>0.127692</td>\n",
       "      <td>-0.210966</td>\n",
       "      <td>0.529318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.063553</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>-0.014181</td>\n",
       "      <td>0.300003</td>\n",
       "      <td>-0.085526</td>\n",
       "      <td>-0.138211</td>\n",
       "      <td>0.372830</td>\n",
       "      <td>-0.075299</td>\n",
       "      <td>0.142291</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200874</td>\n",
       "      <td>0.135577</td>\n",
       "      <td>-0.273246</td>\n",
       "      <td>0.073827</td>\n",
       "      <td>0.295260</td>\n",
       "      <td>-0.316404</td>\n",
       "      <td>0.225864</td>\n",
       "      <td>-0.071610</td>\n",
       "      <td>-0.332199</td>\n",
       "      <td>0.439058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitamin</th>\n",
       "      <td>-0.366853</td>\n",
       "      <td>0.328901</td>\n",
       "      <td>-0.170687</td>\n",
       "      <td>0.687110</td>\n",
       "      <td>0.059167</td>\n",
       "      <td>-0.095928</td>\n",
       "      <td>-0.316688</td>\n",
       "      <td>0.070234</td>\n",
       "      <td>-0.249089</td>\n",
       "      <td>0.343130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229457</td>\n",
       "      <td>0.245903</td>\n",
       "      <td>0.247457</td>\n",
       "      <td>0.223266</td>\n",
       "      <td>0.230666</td>\n",
       "      <td>0.015494</td>\n",
       "      <td>-0.179398</td>\n",
       "      <td>-0.034338</td>\n",
       "      <td>-0.432433</td>\n",
       "      <td>0.069087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pill</th>\n",
       "      <td>-0.413312</td>\n",
       "      <td>-0.211963</td>\n",
       "      <td>-0.175645</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>0.295040</td>\n",
       "      <td>-0.138634</td>\n",
       "      <td>0.110146</td>\n",
       "      <td>-0.111251</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>0.140678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112558</td>\n",
       "      <td>0.270521</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.418959</td>\n",
       "      <td>-0.211729</td>\n",
       "      <td>-0.387660</td>\n",
       "      <td>-0.283485</td>\n",
       "      <td>0.330883</td>\n",
       "      <td>-0.223896</td>\n",
       "      <td>0.160311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>0.086338</td>\n",
       "      <td>-0.227266</td>\n",
       "      <td>-0.012759</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>-0.040900</td>\n",
       "      <td>-0.019057</td>\n",
       "      <td>0.291444</td>\n",
       "      <td>0.238701</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.100624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187810</td>\n",
       "      <td>0.295749</td>\n",
       "      <td>-0.351078</td>\n",
       "      <td>0.186532</td>\n",
       "      <td>0.111317</td>\n",
       "      <td>-0.159204</td>\n",
       "      <td>0.239019</td>\n",
       "      <td>-0.090472</td>\n",
       "      <td>-0.334906</td>\n",
       "      <td>0.284010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>-0.155015</td>\n",
       "      <td>0.033533</td>\n",
       "      <td>-0.265588</td>\n",
       "      <td>0.015096</td>\n",
       "      <td>0.103521</td>\n",
       "      <td>-0.722320</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.253991</td>\n",
       "      <td>-0.249581</td>\n",
       "      <td>0.078105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296834</td>\n",
       "      <td>-0.044441</td>\n",
       "      <td>0.528226</td>\n",
       "      <td>0.691648</td>\n",
       "      <td>0.044114</td>\n",
       "      <td>0.172196</td>\n",
       "      <td>-0.142819</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.036553</td>\n",
       "      <td>0.399739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>0.030870</td>\n",
       "      <td>0.057877</td>\n",
       "      <td>0.191278</td>\n",
       "      <td>0.106899</td>\n",
       "      <td>-0.490074</td>\n",
       "      <td>-0.443010</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>-0.123883</td>\n",
       "      <td>-0.339054</td>\n",
       "      <td>0.031029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001506</td>\n",
       "      <td>0.046969</td>\n",
       "      <td>0.427737</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.255049</td>\n",
       "      <td>-0.426744</td>\n",
       "      <td>0.274066</td>\n",
       "      <td>-0.106740</td>\n",
       "      <td>-0.506388</td>\n",
       "      <td>0.284273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>0.102201</td>\n",
       "      <td>0.376216</td>\n",
       "      <td>-0.110119</td>\n",
       "      <td>0.393529</td>\n",
       "      <td>0.272431</td>\n",
       "      <td>-0.319818</td>\n",
       "      <td>-0.194908</td>\n",
       "      <td>-0.170285</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>0.330109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145177</td>\n",
       "      <td>-0.051594</td>\n",
       "      <td>0.226128</td>\n",
       "      <td>0.270867</td>\n",
       "      <td>0.074406</td>\n",
       "      <td>-0.183991</td>\n",
       "      <td>0.293989</td>\n",
       "      <td>0.146933</td>\n",
       "      <td>-0.538798</td>\n",
       "      <td>0.058115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>0.070894</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.262246</td>\n",
       "      <td>-0.008310</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>-0.195191</td>\n",
       "      <td>0.046805</td>\n",
       "      <td>-0.136272</td>\n",
       "      <td>-0.159358</td>\n",
       "      <td>-0.052452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120025</td>\n",
       "      <td>-0.073303</td>\n",
       "      <td>-0.092533</td>\n",
       "      <td>0.180256</td>\n",
       "      <td>0.306997</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>-0.393777</td>\n",
       "      <td>0.393329</td>\n",
       "      <td>-0.367802</td>\n",
       "      <td>0.021518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.266091</td>\n",
       "      <td>0.191161</td>\n",
       "      <td>0.325512</td>\n",
       "      <td>-0.030606</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>-0.158457</td>\n",
       "      <td>0.155003</td>\n",
       "      <td>-0.019165</td>\n",
       "      <td>0.095027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188344</td>\n",
       "      <td>0.053784</td>\n",
       "      <td>0.222148</td>\n",
       "      <td>0.258554</td>\n",
       "      <td>0.297347</td>\n",
       "      <td>-0.237132</td>\n",
       "      <td>0.029280</td>\n",
       "      <td>0.301460</td>\n",
       "      <td>-0.153841</td>\n",
       "      <td>0.170455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teraputic</th>\n",
       "      <td>-0.092083</td>\n",
       "      <td>-0.124443</td>\n",
       "      <td>0.089372</td>\n",
       "      <td>0.037424</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>-0.361411</td>\n",
       "      <td>-0.117675</td>\n",
       "      <td>0.242479</td>\n",
       "      <td>-0.179045</td>\n",
       "      <td>-0.165160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132472</td>\n",
       "      <td>0.104583</td>\n",
       "      <td>0.261208</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>0.117015</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.121669</td>\n",
       "      <td>-0.026444</td>\n",
       "      <td>0.009354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pounds/</th>\n",
       "      <td>0.070952</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>-0.110843</td>\n",
       "      <td>-0.178000</td>\n",
       "      <td>-0.171696</td>\n",
       "      <td>-0.105441</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.046410</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.046702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273317</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>-0.010609</td>\n",
       "      <td>0.284996</td>\n",
       "      <td>0.057435</td>\n",
       "      <td>-0.194035</td>\n",
       "      <td>0.049657</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>-0.155203</td>\n",
       "      <td>0.064236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoned</th>\n",
       "      <td>0.044097</td>\n",
       "      <td>-0.248290</td>\n",
       "      <td>-0.205611</td>\n",
       "      <td>-0.166538</td>\n",
       "      <td>-0.146275</td>\n",
       "      <td>-0.198867</td>\n",
       "      <td>0.090246</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>-0.024464</td>\n",
       "      <td>-0.114401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063838</td>\n",
       "      <td>-0.022030</td>\n",
       "      <td>0.079599</td>\n",
       "      <td>0.170870</td>\n",
       "      <td>-0.121658</td>\n",
       "      <td>-0.153488</td>\n",
       "      <td>-0.190933</td>\n",
       "      <td>0.124171</td>\n",
       "      <td>-0.363535</td>\n",
       "      <td>0.050479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nzt-48</th>\n",
       "      <td>0.005303</td>\n",
       "      <td>-0.205386</td>\n",
       "      <td>-0.131887</td>\n",
       "      <td>0.014337</td>\n",
       "      <td>-0.259626</td>\n",
       "      <td>-0.043607</td>\n",
       "      <td>-0.110312</td>\n",
       "      <td>0.258017</td>\n",
       "      <td>-0.027220</td>\n",
       "      <td>0.153967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075089</td>\n",
       "      <td>-0.110982</td>\n",
       "      <td>-0.108912</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>-0.020723</td>\n",
       "      <td>-0.134190</td>\n",
       "      <td>-0.066174</td>\n",
       "      <td>0.049259</td>\n",
       "      <td>-0.226343</td>\n",
       "      <td>0.116528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oriveda</th>\n",
       "      <td>0.037119</td>\n",
       "      <td>0.130811</td>\n",
       "      <td>0.087657</td>\n",
       "      <td>0.135634</td>\n",
       "      <td>-0.090650</td>\n",
       "      <td>-0.241515</td>\n",
       "      <td>-0.204503</td>\n",
       "      <td>0.185346</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.180912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124376</td>\n",
       "      <td>0.145381</td>\n",
       "      <td>0.110305</td>\n",
       "      <td>0.117297</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>-0.028480</td>\n",
       "      <td>0.095463</td>\n",
       "      <td>0.116485</td>\n",
       "      <td>-0.103227</td>\n",
       "      <td>-0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mellit</th>\n",
       "      <td>0.097637</td>\n",
       "      <td>0.084301</td>\n",
       "      <td>0.066986</td>\n",
       "      <td>-0.071723</td>\n",
       "      <td>-0.188055</td>\n",
       "      <td>-0.197099</td>\n",
       "      <td>0.057731</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>-0.196670</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201438</td>\n",
       "      <td>-0.084792</td>\n",
       "      <td>0.062416</td>\n",
       "      <td>0.266798</td>\n",
       "      <td>0.211469</td>\n",
       "      <td>-0.194042</td>\n",
       "      <td>-0.017395</td>\n",
       "      <td>0.180961</td>\n",
       "      <td>-0.230396</td>\n",
       "      <td>0.102034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rpo</th>\n",
       "      <td>-0.029305</td>\n",
       "      <td>0.110522</td>\n",
       "      <td>0.182452</td>\n",
       "      <td>0.086598</td>\n",
       "      <td>-0.091644</td>\n",
       "      <td>-0.264849</td>\n",
       "      <td>-0.007200</td>\n",
       "      <td>-0.054683</td>\n",
       "      <td>-0.213463</td>\n",
       "      <td>0.144923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019412</td>\n",
       "      <td>0.107950</td>\n",
       "      <td>-0.040507</td>\n",
       "      <td>0.035652</td>\n",
       "      <td>0.250510</td>\n",
       "      <td>-0.108254</td>\n",
       "      <td>0.175073</td>\n",
       "      <td>0.135199</td>\n",
       "      <td>-0.089545</td>\n",
       "      <td>0.106613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vivalabs</th>\n",
       "      <td>-0.086138</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>0.203631</td>\n",
       "      <td>0.162113</td>\n",
       "      <td>-0.020137</td>\n",
       "      <td>-0.232859</td>\n",
       "      <td>-0.130851</td>\n",
       "      <td>0.148494</td>\n",
       "      <td>-0.219388</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032948</td>\n",
       "      <td>0.230580</td>\n",
       "      <td>0.281966</td>\n",
       "      <td>0.227362</td>\n",
       "      <td>-0.012388</td>\n",
       "      <td>-0.155119</td>\n",
       "      <td>-0.070332</td>\n",
       "      <td>0.188419</td>\n",
       "      <td>-0.285609</td>\n",
       "      <td>0.165456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tobia</th>\n",
       "      <td>-0.005888</td>\n",
       "      <td>0.230921</td>\n",
       "      <td>0.126370</td>\n",
       "      <td>0.062466</td>\n",
       "      <td>-0.144890</td>\n",
       "      <td>-0.399964</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.057604</td>\n",
       "      <td>-0.143738</td>\n",
       "      <td>0.035837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009937</td>\n",
       "      <td>0.077386</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.146478</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>-0.295386</td>\n",
       "      <td>-0.056745</td>\n",
       "      <td>0.115966</td>\n",
       "      <td>-0.156297</td>\n",
       "      <td>0.127978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gladstar</th>\n",
       "      <td>0.084265</td>\n",
       "      <td>-0.212785</td>\n",
       "      <td>-0.092294</td>\n",
       "      <td>0.051867</td>\n",
       "      <td>-0.182724</td>\n",
       "      <td>-0.182495</td>\n",
       "      <td>-0.018736</td>\n",
       "      <td>0.196715</td>\n",
       "      <td>-0.235993</td>\n",
       "      <td>0.142907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>-0.100794</td>\n",
       "      <td>0.066472</td>\n",
       "      <td>0.138234</td>\n",
       "      <td>-0.161978</td>\n",
       "      <td>-0.035209</td>\n",
       "      <td>0.180157</td>\n",
       "      <td>-0.048405</td>\n",
       "      <td>-0.031697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>options</th>\n",
       "      <td>0.033210</td>\n",
       "      <td>0.230411</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>0.060131</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>-0.161935</td>\n",
       "      <td>-0.068473</td>\n",
       "      <td>0.135116</td>\n",
       "      <td>-0.209624</td>\n",
       "      <td>-0.150311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336904</td>\n",
       "      <td>0.119382</td>\n",
       "      <td>0.201134</td>\n",
       "      <td>0.223792</td>\n",
       "      <td>0.151605</td>\n",
       "      <td>-0.220975</td>\n",
       "      <td>-0.041315</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>-0.334388</td>\n",
       "      <td>0.119626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wfit</th>\n",
       "      <td>-0.043736</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>-0.010825</td>\n",
       "      <td>-0.096660</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>-0.188938</td>\n",
       "      <td>-0.118121</td>\n",
       "      <td>0.085043</td>\n",
       "      <td>-0.223312</td>\n",
       "      <td>0.076130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061527</td>\n",
       "      <td>0.059485</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.105294</td>\n",
       "      <td>0.117724</td>\n",
       "      <td>-0.125748</td>\n",
       "      <td>-0.122238</td>\n",
       "      <td>0.154360</td>\n",
       "      <td>-0.165689</td>\n",
       "      <td>0.036690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suckers</th>\n",
       "      <td>0.153882</td>\n",
       "      <td>-0.223844</td>\n",
       "      <td>-0.141472</td>\n",
       "      <td>-0.123518</td>\n",
       "      <td>0.044388</td>\n",
       "      <td>-0.229427</td>\n",
       "      <td>-0.072420</td>\n",
       "      <td>0.330999</td>\n",
       "      <td>-0.075459</td>\n",
       "      <td>0.102671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074271</td>\n",
       "      <td>0.095356</td>\n",
       "      <td>-0.074736</td>\n",
       "      <td>0.271663</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>-0.064718</td>\n",
       "      <td>-0.105327</td>\n",
       "      <td>0.254351</td>\n",
       "      <td>-0.048380</td>\n",
       "      <td>-0.005935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confident_ingesting</th>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.100944</td>\n",
       "      <td>0.246085</td>\n",
       "      <td>-0.193033</td>\n",
       "      <td>-0.361242</td>\n",
       "      <td>-0.148765</td>\n",
       "      <td>0.214760</td>\n",
       "      <td>-0.220893</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000960</td>\n",
       "      <td>-0.020849</td>\n",
       "      <td>0.113725</td>\n",
       "      <td>0.223191</td>\n",
       "      <td>-0.102895</td>\n",
       "      <td>-0.322052</td>\n",
       "      <td>-0.108456</td>\n",
       "      <td>0.085614</td>\n",
       "      <td>-0.040634</td>\n",
       "      <td>0.204622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doan</th>\n",
       "      <td>0.094831</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>-0.087324</td>\n",
       "      <td>0.113434</td>\n",
       "      <td>-0.029369</td>\n",
       "      <td>-0.211192</td>\n",
       "      <td>-0.090740</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>-0.077442</td>\n",
       "      <td>-0.211864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152295</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>0.030969</td>\n",
       "      <td>0.308125</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>-0.034401</td>\n",
       "      <td>-0.090219</td>\n",
       "      <td>-0.084728</td>\n",
       "      <td>-0.259388</td>\n",
       "      <td>0.028181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kilogram</th>\n",
       "      <td>-0.063745</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>-0.085556</td>\n",
       "      <td>0.052024</td>\n",
       "      <td>0.109468</td>\n",
       "      <td>-0.320659</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.024682</td>\n",
       "      <td>-0.173919</td>\n",
       "      <td>0.107272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074002</td>\n",
       "      <td>0.015139</td>\n",
       "      <td>0.116638</td>\n",
       "      <td>0.242956</td>\n",
       "      <td>0.073958</td>\n",
       "      <td>-0.125051</td>\n",
       "      <td>-0.322164</td>\n",
       "      <td>0.250545</td>\n",
       "      <td>-0.112651</td>\n",
       "      <td>0.063130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everpures</th>\n",
       "      <td>0.041022</td>\n",
       "      <td>-0.109575</td>\n",
       "      <td>0.070570</td>\n",
       "      <td>0.138544</td>\n",
       "      <td>-0.048680</td>\n",
       "      <td>-0.149593</td>\n",
       "      <td>-0.077308</td>\n",
       "      <td>0.168427</td>\n",
       "      <td>-0.017311</td>\n",
       "      <td>-0.136165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139955</td>\n",
       "      <td>0.047553</td>\n",
       "      <td>-0.059025</td>\n",
       "      <td>0.307445</td>\n",
       "      <td>0.079754</td>\n",
       "      <td>-0.171739</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.159626</td>\n",
       "      <td>-0.062977</td>\n",
       "      <td>0.101971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phytoceramid</th>\n",
       "      <td>-0.043796</td>\n",
       "      <td>-0.014813</td>\n",
       "      <td>0.145667</td>\n",
       "      <td>0.101619</td>\n",
       "      <td>-0.062781</td>\n",
       "      <td>-0.252995</td>\n",
       "      <td>0.038963</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>-0.126385</td>\n",
       "      <td>0.026924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076730</td>\n",
       "      <td>0.060204</td>\n",
       "      <td>-0.019415</td>\n",
       "      <td>0.090084</td>\n",
       "      <td>-0.018240</td>\n",
       "      <td>-0.192086</td>\n",
       "      <td>0.151854</td>\n",
       "      <td>0.201663</td>\n",
       "      <td>-0.232599</td>\n",
       "      <td>0.066306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kiva</th>\n",
       "      <td>-0.057261</td>\n",
       "      <td>-0.248978</td>\n",
       "      <td>0.077619</td>\n",
       "      <td>-0.035737</td>\n",
       "      <td>-0.103786</td>\n",
       "      <td>-0.395502</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.056534</td>\n",
       "      <td>-0.417699</td>\n",
       "      <td>0.062389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209599</td>\n",
       "      <td>0.103361</td>\n",
       "      <td>0.067264</td>\n",
       "      <td>0.270575</td>\n",
       "      <td>0.138054</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.267571</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.039597</td>\n",
       "      <td>0.101869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dalvia</th>\n",
       "      <td>-0.191226</td>\n",
       "      <td>0.093267</td>\n",
       "      <td>-0.058116</td>\n",
       "      <td>0.205157</td>\n",
       "      <td>-0.042237</td>\n",
       "      <td>-0.054442</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>-0.030677</td>\n",
       "      <td>-0.150494</td>\n",
       "      <td>0.060661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.052034</td>\n",
       "      <td>0.085326</td>\n",
       "      <td>0.323895</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>0.021195</td>\n",
       "      <td>-0.004798</td>\n",
       "      <td>0.088653</td>\n",
       "      <td>-0.215114</td>\n",
       "      <td>0.050069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phytospectrum</th>\n",
       "      <td>-0.052582</td>\n",
       "      <td>-0.114368</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>-0.027669</td>\n",
       "      <td>-0.232475</td>\n",
       "      <td>-0.371245</td>\n",
       "      <td>-0.018146</td>\n",
       "      <td>-0.049761</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.064410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302900</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.059470</td>\n",
       "      <td>0.410360</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>-0.283512</td>\n",
       "      <td>-0.135284</td>\n",
       "      <td>0.110340</td>\n",
       "      <td>0.126224</td>\n",
       "      <td>0.140529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bulksupplements.com</th>\n",
       "      <td>-0.076248</td>\n",
       "      <td>0.166548</td>\n",
       "      <td>0.139707</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>-0.215474</td>\n",
       "      <td>-0.305885</td>\n",
       "      <td>-0.139216</td>\n",
       "      <td>0.185868</td>\n",
       "      <td>-0.065516</td>\n",
       "      <td>0.175562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045457</td>\n",
       "      <td>-0.110961</td>\n",
       "      <td>0.067663</td>\n",
       "      <td>0.117055</td>\n",
       "      <td>0.145933</td>\n",
       "      <td>-0.130218</td>\n",
       "      <td>-0.048750</td>\n",
       "      <td>0.156457</td>\n",
       "      <td>-0.308669</td>\n",
       "      <td>0.080387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omegak</th>\n",
       "      <td>-0.065428</td>\n",
       "      <td>0.116332</td>\n",
       "      <td>0.108486</td>\n",
       "      <td>-0.094366</td>\n",
       "      <td>-0.031351</td>\n",
       "      <td>-0.179973</td>\n",
       "      <td>0.086045</td>\n",
       "      <td>0.160428</td>\n",
       "      <td>-0.056044</td>\n",
       "      <td>0.181879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137487</td>\n",
       "      <td>0.062624</td>\n",
       "      <td>0.202995</td>\n",
       "      <td>0.124344</td>\n",
       "      <td>0.074680</td>\n",
       "      <td>-0.330316</td>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.171782</td>\n",
       "      <td>-0.068234</td>\n",
       "      <td>0.245853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorogenic_acid</th>\n",
       "      <td>-0.056752</td>\n",
       "      <td>-0.121555</td>\n",
       "      <td>-0.015458</td>\n",
       "      <td>0.214998</td>\n",
       "      <td>-0.163376</td>\n",
       "      <td>-0.358154</td>\n",
       "      <td>-0.061092</td>\n",
       "      <td>0.041010</td>\n",
       "      <td>-0.138097</td>\n",
       "      <td>-0.151927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007281</td>\n",
       "      <td>0.340944</td>\n",
       "      <td>0.085821</td>\n",
       "      <td>0.121572</td>\n",
       "      <td>-0.178815</td>\n",
       "      <td>-0.298724</td>\n",
       "      <td>-0.050234</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>-0.070546</td>\n",
       "      <td>0.183379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yakon</th>\n",
       "      <td>0.079964</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>0.021480</td>\n",
       "      <td>-0.048927</td>\n",
       "      <td>-0.021359</td>\n",
       "      <td>-0.290300</td>\n",
       "      <td>-0.068816</td>\n",
       "      <td>0.136128</td>\n",
       "      <td>-0.020694</td>\n",
       "      <td>0.131674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061106</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.159846</td>\n",
       "      <td>0.123387</td>\n",
       "      <td>0.043820</td>\n",
       "      <td>-0.198122</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>0.201311</td>\n",
       "      <td>-0.193441</td>\n",
       "      <td>0.179965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eurycoma</th>\n",
       "      <td>0.170617</td>\n",
       "      <td>-0.051745</td>\n",
       "      <td>0.079057</td>\n",
       "      <td>0.033811</td>\n",
       "      <td>-0.249741</td>\n",
       "      <td>-0.302313</td>\n",
       "      <td>-0.007940</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>-0.011764</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.092681</td>\n",
       "      <td>-0.077533</td>\n",
       "      <td>0.069870</td>\n",
       "      <td>0.030444</td>\n",
       "      <td>-0.236505</td>\n",
       "      <td>0.111392</td>\n",
       "      <td>-0.032792</td>\n",
       "      <td>-0.103529</td>\n",
       "      <td>-0.038317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longifolia</th>\n",
       "      <td>0.142105</td>\n",
       "      <td>-0.015737</td>\n",
       "      <td>0.086692</td>\n",
       "      <td>0.068783</td>\n",
       "      <td>-0.211042</td>\n",
       "      <td>-0.309462</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.079716</td>\n",
       "      <td>-0.102950</td>\n",
       "      <td>0.144938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134007</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>-0.089553</td>\n",
       "      <td>0.066732</td>\n",
       "      <td>-0.028061</td>\n",
       "      <td>-0.288525</td>\n",
       "      <td>0.108296</td>\n",
       "      <td>-0.034813</td>\n",
       "      <td>-0.068922</td>\n",
       "      <td>-0.058274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthwell</th>\n",
       "      <td>0.083053</td>\n",
       "      <td>0.101627</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>-0.104599</td>\n",
       "      <td>0.021734</td>\n",
       "      <td>-0.217836</td>\n",
       "      <td>-0.025667</td>\n",
       "      <td>0.050503</td>\n",
       "      <td>0.048703</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196635</td>\n",
       "      <td>0.095783</td>\n",
       "      <td>0.104818</td>\n",
       "      <td>0.223341</td>\n",
       "      <td>0.099048</td>\n",
       "      <td>-0.112303</td>\n",
       "      <td>-0.102712</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>-0.258930</td>\n",
       "      <td>0.122011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digestwise</th>\n",
       "      <td>0.001305</td>\n",
       "      <td>-0.154512</td>\n",
       "      <td>0.122562</td>\n",
       "      <td>-0.051745</td>\n",
       "      <td>-0.060934</td>\n",
       "      <td>-0.279857</td>\n",
       "      <td>-0.007258</td>\n",
       "      <td>0.223152</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>-0.215347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256749</td>\n",
       "      <td>-0.062228</td>\n",
       "      <td>-0.024103</td>\n",
       "      <td>0.254028</td>\n",
       "      <td>-0.303741</td>\n",
       "      <td>-0.254536</td>\n",
       "      <td>-0.006890</td>\n",
       "      <td>0.083679</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>0.103709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restiflex</th>\n",
       "      <td>0.106358</td>\n",
       "      <td>0.172584</td>\n",
       "      <td>0.165156</td>\n",
       "      <td>-0.310054</td>\n",
       "      <td>0.058647</td>\n",
       "      <td>-0.286208</td>\n",
       "      <td>0.021586</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>-0.091494</td>\n",
       "      <td>-0.038275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233712</td>\n",
       "      <td>0.131703</td>\n",
       "      <td>0.063685</td>\n",
       "      <td>0.333880</td>\n",
       "      <td>0.073968</td>\n",
       "      <td>-0.196640</td>\n",
       "      <td>-0.034836</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>-0.336317</td>\n",
       "      <td>0.096005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22056 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1         2         3         4   \\\n",
       "product             -0.074639  0.354219 -0.063841  0.089670  0.124870   \n",
       "good                 0.060326  0.231529 -0.074902  0.085871  0.125798   \n",
       "great               -0.023763  0.227623  0.004929 -0.032541  0.218707   \n",
       "use                  0.155800  0.033982  0.285413 -0.048341  0.064590   \n",
       "work                -0.131723  0.010586  0.025562 -0.039717  0.244143   \n",
       "day                 -0.019957 -0.195679  0.167550 -0.076098  0.005321   \n",
       "help                 0.070785 -0.172536 -0.329890  0.100656  0.269875   \n",
       "like                 0.073624  0.001295  0.156824  0.017900 -0.129977   \n",
       "recommend            0.294066  0.640802  0.152148 -0.030043  0.277109   \n",
       "try                 -0.116470  0.008058  0.083074  0.211078  0.020380   \n",
       "feel                 0.125145  0.032734 -0.004857  0.026181  0.029319   \n",
       "year                -0.042094 -0.064606 -0.131619  0.185157  0.018280   \n",
       "supplement          -0.159490  0.137338  0.225708  0.174277  0.122478   \n",
       "love                -0.044152 -0.071927  0.043411  0.048013  0.072494   \n",
       "taste                0.036556  0.139370 -0.389828  0.180753  0.032585   \n",
       "time                -0.084595  0.102417  0.083796 -0.252505  0.224015   \n",
       "find                 0.174731  0.244685 -0.100288  0.175566  0.183345   \n",
       "buy                  0.219255  0.181165  0.069085  0.323918  0.324614   \n",
       "price                0.109502  0.599139 -0.275831  0.156092  0.496220   \n",
       "start                0.017732 -0.087444  0.038792 -0.052732  0.061442   \n",
       "need                 0.058791  0.148452  0.168208  0.008048 -0.122640   \n",
       "month                0.063553  0.010345 -0.014181  0.300003 -0.085526   \n",
       "vitamin             -0.366853  0.328901 -0.170687  0.687110  0.059167   \n",
       "pill                -0.413312 -0.211963 -0.175645  0.161435  0.295040   \n",
       "week                 0.086338 -0.227266 -0.012759  0.143300 -0.040900   \n",
       "easy                -0.155015  0.033533 -0.265588  0.015096  0.103521   \n",
       "'s                   0.030870  0.057877  0.191278  0.106899 -0.490074   \n",
       "brand                0.102201  0.376216 -0.110119  0.393529  0.272431   \n",
       "order                0.070894  0.208236  0.262246 -0.008310  0.125490   \n",
       "know                 0.032000  0.266091  0.191161  0.325512 -0.030606   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "teraputic           -0.092083 -0.124443  0.089372  0.037424  0.059566   \n",
       "pounds/              0.070952 -0.030996 -0.110843 -0.178000 -0.171696   \n",
       "zoned                0.044097 -0.248290 -0.205611 -0.166538 -0.146275   \n",
       "nzt-48               0.005303 -0.205386 -0.131887  0.014337 -0.259626   \n",
       "oriveda              0.037119  0.130811  0.087657  0.135634 -0.090650   \n",
       "mellit               0.097637  0.084301  0.066986 -0.071723 -0.188055   \n",
       "rpo                 -0.029305  0.110522  0.182452  0.086598 -0.091644   \n",
       "vivalabs            -0.086138  0.215816  0.203631  0.162113 -0.020137   \n",
       "tobia               -0.005888  0.230921  0.126370  0.062466 -0.144890   \n",
       "gladstar             0.084265 -0.212785 -0.092294  0.051867 -0.182724   \n",
       "options              0.033210  0.230411  0.226316  0.060131  0.001231   \n",
       "wfit                -0.043736 -0.001802 -0.010825 -0.096660  0.055147   \n",
       "suckers              0.153882 -0.223844 -0.141472 -0.123518  0.044388   \n",
       "confident_ingesting  0.013311  0.113372  0.100944  0.246085 -0.193033   \n",
       "doan                 0.094831  0.042687 -0.087324  0.113434 -0.029369   \n",
       "kilogram            -0.063745  0.068974 -0.085556  0.052024  0.109468   \n",
       "everpures            0.041022 -0.109575  0.070570  0.138544 -0.048680   \n",
       "phytoceramid        -0.043796 -0.014813  0.145667  0.101619 -0.062781   \n",
       "kiva                -0.057261 -0.248978  0.077619 -0.035737 -0.103786   \n",
       "dalvia              -0.191226  0.093267 -0.058116  0.205157 -0.042237   \n",
       "phytospectrum       -0.052582 -0.114368  0.058192 -0.027669 -0.232475   \n",
       "bulksupplements.com -0.076248  0.166548  0.139707  0.105100 -0.215474   \n",
       "omegak              -0.065428  0.116332  0.108486 -0.094366 -0.031351   \n",
       "chlorogenic_acid    -0.056752 -0.121555 -0.015458  0.214998 -0.163376   \n",
       "yakon                0.079964 -0.040887  0.021480 -0.048927 -0.021359   \n",
       "eurycoma             0.170617 -0.051745  0.079057  0.033811 -0.249741   \n",
       "longifolia           0.142105 -0.015737  0.086692  0.068783 -0.211042   \n",
       "earthwell            0.083053  0.101627 -0.034360 -0.104599  0.021734   \n",
       "digestwise           0.001305 -0.154512  0.122562 -0.051745 -0.060934   \n",
       "restiflex            0.106358  0.172584  0.165156 -0.310054  0.058647   \n",
       "\n",
       "                           5         6         7         8         9   \\\n",
       "product             -0.068005 -0.013207  0.077418 -0.072039  0.329119   \n",
       "good                -0.210218 -0.254523  0.022960 -0.008771  0.107868   \n",
       "great               -0.283889 -0.056582 -0.082675 -0.077025 -0.132305   \n",
       "use                 -0.048613  0.025682 -0.090392  0.168781 -0.154475   \n",
       "work                -0.032734 -0.207010  0.207864  0.275477 -0.119843   \n",
       "day                 -0.005643  0.103599  0.043114 -0.069537  0.089945   \n",
       "help                -0.180524 -0.067899  0.083343  0.031652 -0.123470   \n",
       "like                -0.087569 -0.062197  0.110463 -0.046887  0.224330   \n",
       "recommend           -0.604638 -0.013796  0.093345  0.078040  0.171017   \n",
       "try                 -0.031157  0.010769 -0.054648  0.059417 -0.110198   \n",
       "feel                 0.221195  0.162209  0.193188  0.028210  0.032388   \n",
       "year                -0.356923  0.356033 -0.194826  0.071787 -0.143158   \n",
       "supplement          -0.019642  0.167143 -0.089559 -0.331553  0.406758   \n",
       "love                -0.300624 -0.209743 -0.027427  0.126003 -0.039381   \n",
       "taste                0.086729  0.098552  0.488643 -0.255265  0.128057   \n",
       "time                -0.033471 -0.016940  0.069439 -0.157311  0.098750   \n",
       "find                -0.273145  0.199735 -0.089706 -0.274569 -0.366918   \n",
       "buy                 -0.303967 -0.140795 -0.290850 -0.062919 -0.171420   \n",
       "price               -0.048745 -0.015859 -0.100425  0.048610  0.068822   \n",
       "start                0.091422  0.209542 -0.108499 -0.207357 -0.124101   \n",
       "need                -0.310814 -0.051594  0.187401  0.017567 -0.295230   \n",
       "month               -0.138211  0.372830 -0.075299  0.142291  0.001231   \n",
       "vitamin             -0.095928 -0.316688  0.070234 -0.249089  0.343130   \n",
       "pill                -0.138634  0.110146 -0.111251 -0.011163  0.140678   \n",
       "week                -0.019057  0.291444  0.238701  0.095929  0.100624   \n",
       "easy                -0.722320  0.005318  0.253991 -0.249581  0.078105   \n",
       "'s                  -0.443010  0.021550 -0.123883 -0.339054  0.031029   \n",
       "brand               -0.319818 -0.194908 -0.170285 -0.216213  0.330109   \n",
       "order               -0.195191  0.046805 -0.136272 -0.159358 -0.052452   \n",
       "know                 0.020722 -0.158457  0.155003 -0.019165  0.095027   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "teraputic           -0.361411 -0.117675  0.242479 -0.179045 -0.165160   \n",
       "pounds/             -0.105441  0.069332  0.046410  0.115051  0.046702   \n",
       "zoned               -0.198867  0.090246  0.011051 -0.024464 -0.114401   \n",
       "nzt-48              -0.043607 -0.110312  0.258017 -0.027220  0.153967   \n",
       "oriveda             -0.241515 -0.204503  0.185346  0.009003  0.180912   \n",
       "mellit              -0.197099  0.057731  0.001320 -0.196670  0.020644   \n",
       "rpo                 -0.264849 -0.007200 -0.054683 -0.213463  0.144923   \n",
       "vivalabs            -0.232859 -0.130851  0.148494 -0.219388 -0.000878   \n",
       "tobia               -0.399964  0.025915  0.057604 -0.143738  0.035837   \n",
       "gladstar            -0.182495 -0.018736  0.196715 -0.235993  0.142907   \n",
       "options             -0.161935 -0.068473  0.135116 -0.209624 -0.150311   \n",
       "wfit                -0.188938 -0.118121  0.085043 -0.223312  0.076130   \n",
       "suckers             -0.229427 -0.072420  0.330999 -0.075459  0.102671   \n",
       "confident_ingesting -0.361242 -0.148765  0.214760 -0.220893 -0.029398   \n",
       "doan                -0.211192 -0.090740  0.016396 -0.077442 -0.211864   \n",
       "kilogram            -0.320659  0.040698  0.024682 -0.173919  0.107272   \n",
       "everpures           -0.149593 -0.077308  0.168427 -0.017311 -0.136165   \n",
       "phytoceramid        -0.252995  0.038963  0.008514 -0.126385  0.026924   \n",
       "kiva                -0.395502  0.004893  0.056534 -0.417699  0.062389   \n",
       "dalvia              -0.054442  0.022549 -0.030677 -0.150494  0.060661   \n",
       "phytospectrum       -0.371245 -0.018146 -0.049761 -0.078872 -0.064410   \n",
       "bulksupplements.com -0.305885 -0.139216  0.185868 -0.065516  0.175562   \n",
       "omegak              -0.179973  0.086045  0.160428 -0.056044  0.181879   \n",
       "chlorogenic_acid    -0.358154 -0.061092  0.041010 -0.138097 -0.151927   \n",
       "yakon               -0.290300 -0.068816  0.136128 -0.020694  0.131674   \n",
       "eurycoma            -0.302313 -0.007940  0.102119 -0.011764  0.063928   \n",
       "longifolia          -0.309462  0.005288  0.079716 -0.102950  0.144938   \n",
       "earthwell           -0.217836 -0.025667  0.050503  0.048703 -0.142857   \n",
       "digestwise          -0.279857 -0.007258  0.223152  0.024974 -0.215347   \n",
       "restiflex           -0.286208  0.021586  0.105660 -0.091494 -0.038275   \n",
       "\n",
       "                       ...           90        91        92        93  \\\n",
       "product                ...    -0.090658  0.023709  0.283028  0.135874   \n",
       "good                   ...    -0.112741  0.317170  0.317087  0.147866   \n",
       "great                  ...    -0.172068  0.273706  0.362132  0.212433   \n",
       "use                    ...    -0.163690  0.172842  0.176484  0.065678   \n",
       "work                   ...    -0.228646  0.007168  0.179578  0.260665   \n",
       "day                    ...    -0.292399  0.245713 -0.204933  0.163944   \n",
       "help                   ...     0.213810  0.254655  0.104915  0.258943   \n",
       "like                   ...    -0.107319  0.047285  0.165261  0.395306   \n",
       "recommend              ...    -0.179922  0.101642  0.061390  0.195905   \n",
       "try                    ...    -0.038766  0.008201 -0.047384  0.411964   \n",
       "feel                   ...    -0.521883 -0.129289  0.129402  0.160578   \n",
       "year                   ...     0.206210  0.089591  0.005881  0.276289   \n",
       "supplement             ...    -0.063646  0.239245  0.248464  0.362987   \n",
       "love                   ...    -0.333111  0.393974  0.161670  0.204192   \n",
       "taste                  ...    -0.342056 -0.041710  0.586251  0.307798   \n",
       "time                   ...    -0.247823  0.135666 -0.104256  0.205925   \n",
       "find                   ...    -0.031165  0.167517  0.454833  0.138319   \n",
       "buy                    ...    -0.121557 -0.049086  0.101042  0.211965   \n",
       "price                  ...     0.055685  0.028206  0.449800  0.429793   \n",
       "start                  ...    -0.092765  0.130748 -0.032792  0.256342   \n",
       "need                   ...    -0.482748  0.147437  0.352033  0.433890   \n",
       "month                  ...    -0.200874  0.135577 -0.273246  0.073827   \n",
       "vitamin                ...    -0.229457  0.245903  0.247457  0.223266   \n",
       "pill                   ...    -0.112558  0.270521  0.169798  0.418959   \n",
       "week                   ...    -0.187810  0.295749 -0.351078  0.186532   \n",
       "easy                   ...    -0.296834 -0.044441  0.528226  0.691648   \n",
       "'s                     ...    -0.001506  0.046969  0.427737  0.099789   \n",
       "brand                  ...    -0.145177 -0.051594  0.226128  0.270867   \n",
       "order                  ...    -0.120025 -0.073303 -0.092533  0.180256   \n",
       "know                   ...    -0.188344  0.053784  0.222148  0.258554   \n",
       "...                    ...          ...       ...       ...       ...   \n",
       "teraputic              ...     0.132472  0.104583  0.261208  0.256348   \n",
       "pounds/                ...    -0.273317  0.030526 -0.010609  0.284996   \n",
       "zoned                  ...    -0.063838 -0.022030  0.079599  0.170870   \n",
       "nzt-48                 ...     0.075089 -0.110982 -0.108912  0.160500   \n",
       "oriveda                ...     0.124376  0.145381  0.110305  0.117297   \n",
       "mellit                 ...    -0.201438 -0.084792  0.062416  0.266798   \n",
       "rpo                    ...     0.019412  0.107950 -0.040507  0.035652   \n",
       "vivalabs               ...    -0.032948  0.230580  0.281966  0.227362   \n",
       "tobia                  ...    -0.009937  0.077386  0.192137  0.146478   \n",
       "gladstar               ...     0.046948  0.003252 -0.100794  0.066472   \n",
       "options                ...     0.336904  0.119382  0.201134  0.223792   \n",
       "wfit                   ...     0.061527  0.059485  0.030739  0.105294   \n",
       "suckers                ...     0.074271  0.095356 -0.074736  0.271663   \n",
       "confident_ingesting    ...    -0.000960 -0.020849  0.113725  0.223191   \n",
       "doan                   ...     0.152295  0.070999  0.030969  0.308125   \n",
       "kilogram               ...    -0.074002  0.015139  0.116638  0.242956   \n",
       "everpures              ...     0.139955  0.047553 -0.059025  0.307445   \n",
       "phytoceramid           ...     0.076730  0.060204 -0.019415  0.090084   \n",
       "kiva                   ...    -0.209599  0.103361  0.067264  0.270575   \n",
       "dalvia                 ...     0.018450  0.052034  0.085326  0.323895   \n",
       "phytospectrum          ...    -0.302900  0.043489  0.059470  0.410360   \n",
       "bulksupplements.com    ...     0.045457 -0.110961  0.067663  0.117055   \n",
       "omegak                 ...     0.137487  0.062624  0.202995  0.124344   \n",
       "chlorogenic_acid       ...    -0.007281  0.340944  0.085821  0.121572   \n",
       "yakon                  ...    -0.061106  0.086482  0.159846  0.123387   \n",
       "eurycoma               ...     0.083142  0.092681 -0.077533  0.069870   \n",
       "longifolia             ...     0.134007  0.025270 -0.089553  0.066732   \n",
       "earthwell              ...     0.196635  0.095783  0.104818  0.223341   \n",
       "digestwise             ...    -0.256749 -0.062228 -0.024103  0.254028   \n",
       "restiflex              ...     0.233712  0.131703  0.063685  0.333880   \n",
       "\n",
       "                           94        95        96        97        98  \\\n",
       "product             -0.020405 -0.348821  0.081809 -0.030812 -0.143888   \n",
       "good                 0.027891 -0.198109 -0.161131  0.182401 -0.188499   \n",
       "great                0.006429 -0.145674 -0.378388  0.332170 -0.047167   \n",
       "use                  0.064181 -0.306245 -0.179612  0.168094 -0.231233   \n",
       "work                 0.066437 -0.443599  0.041221  0.112038 -0.188126   \n",
       "day                 -0.119181 -0.182083  0.054396  0.083126 -0.313821   \n",
       "help                -0.088309 -0.146966 -0.194560  0.276072 -0.179112   \n",
       "like                 0.015415 -0.164832 -0.232992  0.114996  0.029391   \n",
       "recommend           -0.165352 -0.126587 -0.250627  0.189282 -0.168884   \n",
       "try                 -0.135511 -0.198599 -0.211693  0.150326 -0.428607   \n",
       "feel                -0.154719 -0.197546 -0.065869  0.279988 -0.332211   \n",
       "year                 0.169822  0.036459  0.016410 -0.138762 -0.548343   \n",
       "supplement          -0.039085 -0.394169 -0.106171  0.091398 -0.343865   \n",
       "love                 0.162375 -0.286837 -0.563885  0.328455  0.131724   \n",
       "taste               -0.041044 -0.068655 -0.319548  0.264359  0.087646   \n",
       "time                 0.051673  0.072584  0.062897  0.174208 -0.220510   \n",
       "find                 0.019366 -0.076133 -0.150704  0.384565 -0.455265   \n",
       "buy                  0.159734 -0.325682 -0.241131  0.381319 -0.432989   \n",
       "price                0.101703 -0.380758 -0.200654  0.137072 -0.296962   \n",
       "start                0.339566 -0.058128 -0.011886  0.110957 -0.428215   \n",
       "need                -0.072178 -0.234170 -0.290749  0.127692 -0.210966   \n",
       "month                0.295260 -0.316404  0.225864 -0.071610 -0.332199   \n",
       "vitamin              0.230666  0.015494 -0.179398 -0.034338 -0.432433   \n",
       "pill                -0.211729 -0.387660 -0.283485  0.330883 -0.223896   \n",
       "week                 0.111317 -0.159204  0.239019 -0.090472 -0.334906   \n",
       "easy                 0.044114  0.172196 -0.142819  0.117321  0.036553   \n",
       "'s                   0.255049 -0.426744  0.274066 -0.106740 -0.506388   \n",
       "brand                0.074406 -0.183991  0.293989  0.146933 -0.538798   \n",
       "order                0.306997 -0.039837 -0.393777  0.393329 -0.367802   \n",
       "know                 0.297347 -0.237132  0.029280  0.301460 -0.153841   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "teraputic            0.117015 -0.047054 -0.126535  0.121669 -0.026444   \n",
       "pounds/              0.057435 -0.194035  0.049657  0.146881 -0.155203   \n",
       "zoned               -0.121658 -0.153488 -0.190933  0.124171 -0.363535   \n",
       "nzt-48              -0.020723 -0.134190 -0.066174  0.049259 -0.226343   \n",
       "oriveda              0.005986 -0.028480  0.095463  0.116485 -0.103227   \n",
       "mellit               0.211469 -0.194042 -0.017395  0.180961 -0.230396   \n",
       "rpo                  0.250510 -0.108254  0.175073  0.135199 -0.089545   \n",
       "vivalabs            -0.012388 -0.155119 -0.070332  0.188419 -0.285609   \n",
       "tobia                0.028577 -0.295386 -0.056745  0.115966 -0.156297   \n",
       "gladstar             0.138234 -0.161978 -0.035209  0.180157 -0.048405   \n",
       "options              0.151605 -0.220975 -0.041315  0.063636 -0.334388   \n",
       "wfit                 0.117724 -0.125748 -0.122238  0.154360 -0.165689   \n",
       "suckers              0.132231 -0.064718 -0.105327  0.254351 -0.048380   \n",
       "confident_ingesting -0.102895 -0.322052 -0.108456  0.085614 -0.040634   \n",
       "doan                 0.263525 -0.034401 -0.090219 -0.084728 -0.259388   \n",
       "kilogram             0.073958 -0.125051 -0.322164  0.250545 -0.112651   \n",
       "everpures            0.079754 -0.171739  0.031415  0.159626 -0.062977   \n",
       "phytoceramid        -0.018240 -0.192086  0.151854  0.201663 -0.232599   \n",
       "kiva                 0.138054 -0.118546 -0.267571  0.195279  0.039597   \n",
       "dalvia               0.094811  0.021195 -0.004798  0.088653 -0.215114   \n",
       "phytospectrum        0.023485 -0.283512 -0.135284  0.110340  0.126224   \n",
       "bulksupplements.com  0.145933 -0.130218 -0.048750  0.156457 -0.308669   \n",
       "omegak               0.074680 -0.330316  0.022037  0.171782 -0.068234   \n",
       "chlorogenic_acid    -0.178815 -0.298724 -0.050234  0.161212 -0.070546   \n",
       "yakon                0.043820 -0.198122 -0.015747  0.201311 -0.193441   \n",
       "eurycoma             0.030444 -0.236505  0.111392 -0.032792 -0.103529   \n",
       "longifolia          -0.028061 -0.288525  0.108296 -0.034813 -0.068922   \n",
       "earthwell            0.099048 -0.112303 -0.102712  0.206593 -0.258930   \n",
       "digestwise          -0.303741 -0.254536 -0.006890  0.083679 -0.002360   \n",
       "restiflex            0.073968 -0.196640 -0.034836  0.024225 -0.336317   \n",
       "\n",
       "                           99  \n",
       "product              0.085219  \n",
       "good                 0.284355  \n",
       "great                0.048488  \n",
       "use                  0.250193  \n",
       "work                 0.271196  \n",
       "day                  0.242774  \n",
       "help                 0.399372  \n",
       "like                 0.416531  \n",
       "recommend            0.025101  \n",
       "try                  0.217542  \n",
       "feel                 0.411903  \n",
       "year                 0.410395  \n",
       "supplement           0.209764  \n",
       "love                 0.407444  \n",
       "taste                0.283535  \n",
       "time                 0.133960  \n",
       "find                 0.035062  \n",
       "buy                  0.185540  \n",
       "price                0.129422  \n",
       "start                0.155962  \n",
       "need                 0.529318  \n",
       "month                0.439058  \n",
       "vitamin              0.069087  \n",
       "pill                 0.160311  \n",
       "week                 0.284010  \n",
       "easy                 0.399739  \n",
       "'s                   0.284273  \n",
       "brand                0.058115  \n",
       "order                0.021518  \n",
       "know                 0.170455  \n",
       "...                       ...  \n",
       "teraputic            0.009354  \n",
       "pounds/              0.064236  \n",
       "zoned                0.050479  \n",
       "nzt-48               0.116528  \n",
       "oriveda             -0.004135  \n",
       "mellit               0.102034  \n",
       "rpo                  0.106613  \n",
       "vivalabs             0.165456  \n",
       "tobia                0.127978  \n",
       "gladstar            -0.031697  \n",
       "options              0.119626  \n",
       "wfit                 0.036690  \n",
       "suckers             -0.005935  \n",
       "confident_ingesting  0.204622  \n",
       "doan                 0.028181  \n",
       "kilogram             0.063130  \n",
       "everpures            0.101971  \n",
       "phytoceramid         0.066306  \n",
       "kiva                 0.101869  \n",
       "dalvia               0.050069  \n",
       "phytospectrum        0.140529  \n",
       "bulksupplements.com  0.080387  \n",
       "omegak               0.245853  \n",
       "chlorogenic_acid     0.183379  \n",
       "yakon                0.179965  \n",
       "eurycoma            -0.038317  \n",
       "longifolia          -0.058274  \n",
       "earthwell            0.122011  \n",
       "digestwise           0.103709  \n",
       "restiflex            0.096005  \n",
       "\n",
       "[22056 rows x 100 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count) for term, voc in review2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda x: -x[2])\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "# print(ordered_terms)\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(review2vec.wv.syn0[term_indices, :], index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "    similar_words = []\n",
    "    for word, similarity in review2vec.wv.most_similar(positive=[token], topn=topn):\n",
    "        similar_words.append(word)\n",
    "        print('{:20} {}'.format(word, round(similarity, 3)))\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effectiveness        0.781\n",
      "usefulness           0.72\n",
      "validity             0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "efficacy = get_related_terms('efficacy', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_quality         0.857\n",
      "purity               0.706\n",
      "value                0.702\n",
      "quality!.            0.701\n",
      "quaility             0.685\n",
      "products!!.          0.685\n",
      "unsurpassed          0.68\n",
      "quality!!.           0.68\n",
      "excellence!.         0.679\n",
      "reputation           0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('quality', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "efficacy = []\n",
    "cost = []\n",
    "service = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_side_effect 0.697\n",
      "side_effect          0.692\n",
      "positive_effect      0.683\n",
      "side_affect          0.676\n",
      "affect               0.662\n",
      "unmistakable         0.66\n",
      "benefit              0.659\n",
      "adverse_effect       0.653\n",
      "result               0.645\n",
      "personal_observation 0.645\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('effect', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side_affect          0.679\n",
      "side_effect          0.647\n",
      "effects!.            0.592\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('effects', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noticeable           0.805\n",
      "noticable            0.769\n",
      "noticible            0.767\n",
      "noticed              0.737\n",
      "improvement!.        0.705\n",
      "uplifted_mood        0.7\n",
      "weeksi               0.689\n",
      "noticiable           0.688\n",
      "feel                 0.677\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('notice', topn=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results              0.713\n",
      "results!.            0.706\n",
      "outcome              0.684\n",
      "improvement          0.656\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('result', topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality              0.857\n",
      "pure_encapsulation   0.718\n",
      "unsurpassed          0.717\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('high_quality', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficacy += ['works', 'work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instant              0.694\n",
      "instantaneous        0.668\n",
      "gradual              0.666\n",
      "dramatic             0.643\n",
      "profound             0.642\n",
      "marked               0.637\n",
      "noticeable           0.63\n",
      "cumulative           0.628\n",
      "remarkable           0.625\n",
      "substantial          0.619\n",
      "significant          0.614\n",
      "positive             0.613\n",
      "notable              0.61\n",
      "immediately!.        0.604\n",
      "tremendous           0.604\n",
      "drastic              0.602\n",
      "unmistakable         0.601\n",
      "measurable           0.601\n",
      "immediately          0.594\n",
      "tangible             0.591\n",
      "immediatley          0.589\n",
      "definate             0.588\n",
      "appreciable          0.586\n",
      "imediately           0.585\n",
      "miraculous           0.584\n",
      "analgesic            0.583\n",
      "enegry               0.581\n",
      "stunned              0.581\n",
      "energy-              0.581\n",
      "noticed              0.577\n",
      "improved             0.815\n",
      "improved!.           0.692\n",
      "dramatically         0.69\n",
      "improvement          0.689\n",
      "enhance              0.68\n",
      "significant_improvement 0.674\n",
      "wellbe               0.67\n",
      "overall              0.66\n",
      "marked_improvement   0.641\n",
      "increases            0.639\n",
      "definite_improvement 0.636\n",
      "mental_sharpness     0.635\n",
      "greatly              0.634\n",
      "increase             0.63\n",
      "clearer              0.628\n",
      "help                 0.625\n",
      "persistently         0.624\n",
      "diminsh              0.619\n",
      "noticeably           0.615\n",
      "clearness            0.613\n",
      "dramatic_improvement 0.612\n",
      "drastically          0.612\n",
      "urogenital           0.607\n",
      "improvements         0.602\n",
      "strengthens          0.594\n",
      "sharper              0.592\n",
      "noticeable_difference 0.591\n",
      "blepharitis          0.591\n",
      "promotes             0.591\n",
      "resilience           0.59\n",
      "big_difference       0.87\n",
      "difference           0.845\n",
      "noticeable_difference 0.781\n",
      "differance           0.682\n",
      "differece            0.677\n",
      "deference            0.67\n",
      "vast_improvement     0.668\n",
      "definite_improvement 0.666\n",
      "improvement          0.664\n",
      "marked_difference    0.658\n",
      "difference!.         0.652\n",
      "diference            0.642\n",
      "clearness            0.637\n",
      "improvment           0.635\n",
      "improvement!.        0.619\n",
      "significant_improvement 0.618\n"
     ]
    }
   ],
   "source": [
    "efficacy += get_related_terms('immediate', topn=30)\n",
    "\n",
    "efficacy += get_related_terms('improve', topn=30)\n",
    "\n",
    "efficacy += get_related_terms('huge_difference', topn=16)\n",
    "\n",
    "efficacy = [e for e in efficacy if e not in ['wellbe', 'urogenital', 'blepharitis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(efficacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expensive            0.762\n",
      "pay                  0.756\n",
      "price                0.725\n",
      "cheap                0.722\n",
      "dollar               0.707\n",
      "costly               0.696\n",
      "fraction             0.667\n",
      "markup               0.66\n",
      "monie                0.658\n",
      "rite_aid             0.653\n",
      "costing              0.652\n",
      "costwise             0.643\n",
      "expense              0.642\n",
      "money                0.642\n"
     ]
    }
   ],
   "source": [
    "cost = get_related_terms('cost', topn=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.remove('rite_aid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pricing              0.821\n",
      "price-               0.775\n",
      "prices!.             0.764\n",
      "selling              0.757\n",
      "reasonable_price     0.742\n",
      "price!.              0.739\n",
      "quantity             0.731\n",
      "quanity              0.731\n",
      "cost!.               0.725\n",
      "cost                 0.725\n",
      "value                0.723\n",
      "price!!!.            0.72\n",
      "value-               0.718\n",
      "priced!.             0.714\n",
      "brick_and_mortar     0.711\n",
      "pricei               0.711\n",
      "price!i              0.706\n",
      "bargain              0.702\n",
      "bargin               0.701\n",
      "product/             0.699\n",
      "wholesale_club       0.698\n",
      "affordability        0.697\n",
      "prices               0.696\n",
      "quality!!.           0.695\n",
      "potency!.            0.694\n",
      "savings!.            0.692\n",
      "affordable           0.688\n"
     ]
    }
   ],
   "source": [
    "cost += get_related_terms('price', topn=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost.remove('brick_and_mortar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monie                0.78\n",
      "cash                 0.762\n",
      "money!.              0.746\n",
      "dollar               0.732\n",
      "buck                 0.725\n",
      "buying               0.708\n",
      "cost!.               0.68\n",
      "expensive            0.67\n",
      "costing              0.665\n",
      "save                 0.662\n",
      "spend                0.654\n",
      "penny                0.653\n",
      "wisely               0.644\n",
      "savings!.            0.643\n",
      "cost                 0.642\n",
      "expense              0.633\n",
      "selling              0.633\n",
      "convienence          0.632\n",
      "pay                  0.629\n",
      "price                0.626\n"
     ]
    }
   ],
   "source": [
    "cost += get_related_terms('money', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pricey               0.823\n",
      "pricy                0.812\n",
      "cheap                0.809\n",
      "costly               0.788\n",
      "cost                 0.762\n",
      "pay                  0.694\n",
      "spendy               0.677\n",
      "overpriced           0.673\n",
      "money                0.67\n",
      "expensive-           0.655\n",
      "skinceuticals        0.651\n",
      "rite_aid             0.646\n",
      "inexpensive          0.645\n",
      "monie                0.642\n",
      "cheaper              0.639\n",
      "opinon               0.635\n",
      "prohibitively        0.633\n",
      "affordable           0.633\n",
      "dollar               0.631\n",
      "compare              0.628\n",
      "exorbitantly         0.623\n",
      "savings!.            0.621\n"
     ]
    }
   ],
   "source": [
    "cost += get_related_terms('expensive', topn=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost.remove('skinceuticals')\n",
    "cost.remove('opinon')\n",
    "cost.remove('rite_aid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost += ['substantial_saving', 'overpay', 'pay', 'thrifty', 'deal', 'costing', 'worth_every_penny',\n",
    "        'worthwhile_investment', 'worthwhile', 'investment', 'pricey', 'competitive_pricing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor               0.886\n",
      "merchandise          0.777\n",
      "resend               0.775\n",
      "sevice               0.773\n",
      "promply              0.772\n",
      "sellers              0.77\n",
      "customer_service     0.762\n",
      "service              0.759\n",
      "promptness           0.758\n",
      "courteous            0.756\n",
      "prompt               0.752\n",
      "delievery            0.751\n",
      "a+++++++             0.748\n",
      "merchant             0.747\n",
      "shipment             0.742\n",
      "lightn               0.74\n",
      "product/             0.74\n",
      "discrib              0.737\n",
      "packageing           0.732\n",
      "nutricity            0.732\n",
      "sender               0.732\n",
      "vender               0.731\n",
      "supplier             0.73\n",
      "company              0.728\n",
      "a++++++++++          0.727\n",
      "shipping!!.          0.726\n",
      "fulfillment          0.725\n"
     ]
    }
   ],
   "source": [
    "service = get_related_terms('seller', topn=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship                 0.834\n",
      "shipment             0.812\n",
      "prime_member         0.78\n",
      "packageing           0.771\n",
      "delivery             0.77\n",
      "super_saver          0.763\n",
      "lightn               0.757\n",
      "handling             0.755\n",
      "sevice               0.755\n",
      "delievery            0.753\n",
      "prime_shipping       0.735\n",
      "free_shipping        0.734\n",
      "effecient            0.727\n",
      "fulfillment          0.724\n",
      "seller               0.714\n",
      "delivery!.           0.713\n",
      "product/             0.711\n",
      "supersaver           0.71\n",
      "merchandise          0.709\n",
      "prime_membership     0.709\n",
      "vendor               0.708\n",
      "eligible             0.706\n",
      "shipper              0.703\n",
      "shipping!.           0.703\n",
      "shippment            0.698\n",
      "prime_account        0.697\n",
      "service              0.694\n",
      "prime                0.694\n",
      "packing              0.693\n",
      "turnaround           0.693\n",
      "mailing              0.69\n",
      "msrp                 0.687\n",
      "courteous            0.686\n",
      "amazons              0.686\n",
      "timeliness           0.685\n",
      "euro                 0.684\n",
      "sellers              0.683\n",
      "apo                  0.682\n",
      "received             0.679\n",
      "checkout             0.676\n",
      "prompt               0.676\n",
      "aj                   0.676\n",
      "s_h                  0.673\n",
      "paypal               0.672\n",
      "thanks!!.            0.671\n",
      "experation_date      0.67\n",
      "maui                 0.669\n",
      "promply              0.669\n",
      "usps                 0.669\n",
      "promptness           0.669\n",
      "competative          0.667\n",
      "amazone              0.667\n",
      "ordering             0.667\n",
      "deliverd             0.667\n",
      "quality-             0.666\n",
      "vendor!.             0.666\n",
      "vitaminlife          0.665\n",
      "quallity             0.665\n",
      "shipping!!.          0.661\n",
      "payment              0.661\n"
     ]
    }
   ],
   "source": [
    "service += get_related_terms('shipping', topn=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service!!.           0.856\n",
      "service!!!.          0.821\n",
      "product/             0.81\n",
      "service!.            0.809\n",
      "sevice               0.806\n",
      "delievery            0.8\n",
      "fulfillment          0.792\n",
      "delivery!.           0.786\n",
      "prompt_service       0.78\n",
      "thanks!!.            0.779\n",
      "shipping!!.          0.777\n",
      "quick_delivery       0.774\n",
      "packageing           0.774\n",
      "proudct              0.774\n",
      "reliable!.           0.769\n",
      "promptness           0.768\n",
      "merchandise          0.765\n",
      "lightn               0.765\n",
      "a+++++++             0.763\n",
      "buyherbs             0.763\n",
      "prompt               0.763\n",
      "superfast            0.761\n",
      "fast_delivery        0.759\n",
      "merchant             0.759\n",
      "seller               0.759\n",
      "values4u             0.756\n",
      "prompt_delivery      0.756\n",
      "transaction          0.754\n",
      "courteous            0.751\n",
      "vendor!.             0.75\n",
      "aaa+                 0.75\n",
      "quality-             0.749\n",
      "timely_delivery      0.749\n",
      "shipping!.           0.744\n",
      "expedient            0.743\n",
      "shippment            0.743\n",
      "promply              0.743\n",
      "timeliness           0.742\n",
      "fast_shipping        0.742\n",
      "sup_fast_shipping    0.74\n",
      "customer_service     0.739\n",
      "company!.            0.738\n",
      "quallity             0.738\n",
      "a++++++++++          0.738\n",
      "received             0.738\n",
      "turnaround           0.737\n",
      "swift                0.737\n",
      "fast_shipping!.      0.732\n",
      "silver_wire          0.73\n",
      "speedy_delivery      0.73\n",
      "delver               0.728\n",
      "lifelong_customer    0.727\n",
      "qulaity              0.726\n",
      "effecient            0.724\n",
      "vendor               0.724\n",
      "aaa                  0.721\n",
      "arrived              0.72\n",
      "ontime               0.718\n",
      "customer_service!.   0.717\n",
      "reciv                0.717\n"
     ]
    }
   ],
   "source": [
    "service += get_related_terms('service', topn=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service += ['ship', 'customer_satisfaction', 'kindness', 'promptness', 'lifelong_customer', 'satisfied_customer', \n",
    "            'policy', 'business', 'timeliness', 'timely', 'timely_manner', 'superfast', 'expediently', 'packaged',\n",
    "           'swift', 'timely_arrival', 'return', 'order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resend               0.764\n",
      "gracious             0.757\n",
      "credit_card          0.735\n",
      "reimburse            0.725\n",
      "email                0.725\n",
      "apologize            0.724\n",
      "e_mail               0.721\n",
      "invoice              0.711\n",
      "compensation         0.708\n",
      "apology              0.703\n",
      "inquire              0.7\n",
      "sender               0.7\n",
      "paypal               0.693\n",
      "unethical            0.688\n",
      "costumer             0.687\n",
      "vitanherbs           0.687\n",
      "considerate          0.685\n",
      "send                 0.683\n",
      "notify               0.678\n",
      "policy               0.674\n",
      "backorder            0.671\n",
      "customer_satisfaction 0.669\n",
      "phone_call           0.667\n",
      "representative       0.666\n",
      "sellers              0.664\n",
      "honor                0.663\n",
      "customer_service     0.662\n",
      "vitaminlife          0.657\n",
      "reshipp              0.657\n",
      "oversight            0.654\n"
     ]
    }
   ],
   "source": [
    "service += get_related_terms('refund', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_service     0.784\n",
      "manufacturer         0.767\n",
      "unsurpassed          0.767\n",
      "helpfully            0.745\n",
      "professionalism      0.744\n",
      "company!.            0.739\n",
      "customer_satisfaction 0.739\n",
      "implicitly           0.735\n",
      "ethic                0.728\n",
      "seller               0.728\n",
      "puritans             0.723\n",
      "naturwise            0.718\n",
      "reputable_company    0.717\n",
      "uphold               0.717\n",
      "costumer_service     0.712\n",
      "meticulous           0.711\n",
      "top_notch            0.711\n",
      "supplier             0.709\n",
      "exemplary            0.706\n",
      "manufacturing_practice 0.703\n",
      "shady                0.703\n",
      "bulksuppliments      0.702\n",
      "pure_encapsulation   0.702\n",
      "stellar_reputation   0.699\n",
      "bulksupplements.com  0.697\n",
      "vendor               0.697\n",
      "considerate          0.695\n",
      "verification         0.695\n",
      "sourcing             0.693\n",
      "kindness             0.693\n"
     ]
    }
   ],
   "source": [
    "service += get_related_terms('company', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = [e for e in service if e not in ['product/', 'lightn', 'discrib', 'nutricity', 'a++++++++++', 'apo', 'aj', 's_h', 'maui', \n",
    "                          'vitaminlife', 'a+++++++', 'aaa+', 'buyherbs', 'reciv', 'vitanherbs', 'vitaminlife',\n",
    "                          'puritans', 'naturwise', 'bulksupplements.com', 'pure_encapsulation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = []\n",
    "for k, v in vocab_dictionary.token2id.items():\n",
    "    vocab_words.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficacy = [e for e in efficacy if e in vocab_words]\n",
    "cost = [e for e in cost if e in vocab_words]\n",
    "service = [e for e in service if e in vocab_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficacy = list(set(efficacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = list(set(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = list(set(service))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quality', 'marked_difference', 'mental_sharpness', 'effects!.', 'difference', 'analgesic', 'differance', 'negative_side_effect', 'drastic', 'noticible', 'instant', 'improvement!.', 'dramatic', 'enhance', 'notable', 'benefit', 'results', 'definate', 'significant', 'significant_improvement', 'adverse_effect', 'overall', 'tangible', 'increases', 'stunned', 'appreciable', 'clearer', 'gradual', 'vast_improvement', 'works', 'tremendous', 'substantial', 'dramatically', 'noticeably', 'miraculous', 'side_affect', 'remarkable', 'drastically', 'affect', 'noticeable', 'increase', 'marked', 'dramatic_improvement', 'outcome', 'definite_improvement', 'immediately!.', 'marked_improvement', 'feel', 'positive_effect', 'measurable', 'result', 'instantaneous', 'improved', 'deference', 'promotes', 'greatly', 'noticable', 'diference', 'noticed', 'difference!.', 'work', 'results!.', 'immediately', 'side_effect', 'improvment', 'big_difference', 'cumulative', 'help', 'noticeable_difference', 'positive', 'improvement', 'unsurpassed', 'profound']\n"
     ]
    }
   ],
   "source": [
    "print(efficacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bargin', 'investment', 'compare', 'overpriced', 'spend', 'inexpensive', 'cost!.', 'price-', 'price', 'dollar', 'expensive', 'competitive_pricing', 'selling', 'quanity', 'save', 'monie', 'worth_every_penny', 'pricing', 'bargain', 'product/', 'affordability', 'pricy', 'worthwhile', 'money', 'value', 'buck', 'reasonable_price', 'price!i', 'quantity', 'overpay', 'spendy', 'money!.', 'deal', 'cheap', 'buying', 'wisely', 'penny', 'cost', 'price!!!.', 'pay', 'expense', 'affordable', 'pricey', 'fraction', 'cash', 'price!.', 'prices', 'cheaper', 'costly']\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['payment', 'quick_delivery', 'shipping!.', 'prime_membership', 'delivery', 'service!!.', 'aaa', 'backorder', 'return', 'supplier', 'fast_shipping', 'packing', 'verification', 'order', 'service', 'merchant', 'transaction', 'seller', 'satisfied_customer', 'courteous', 'business', 'fulfillment', 'amazons', 'vendor', 'inquire', 'delievery', 'shipper', 'superfast', 'timely', 'costumer', 'sender', 'apologize', 'credit_card', 'ship', 'supersaver', 'top_notch', 'promptness', 'prompt', 'vender', 'customer_service!.', 'customer_satisfaction', 'shippment', 'service!.', 'company!.', 'customer_service', 'send', 'prime_member', 'timely_delivery', 'sourcing', 'ordering', 'merchandise', 'company', 'manufacturer', 'free_shipping', 'service!!!.', 'timely_manner', 'ontime', 'email', 'eligible', 'considerate', 'prime_shipping', 'phone_call', 'ethic', 'representative', 'notify', 'invoice', 'delivery!.', 'prompt_service', 'unsurpassed', 'e_mail', 'honor', 'shipment', 'proudct', 'swift', 'speedy_delivery', 'prompt_delivery', 'turnaround', 'policy', 'compensation', 'reputable_company', 'fast_delivery', 'usps', 'exemplary', 'lifelong_customer', 'apology', 'mailing', 'super_saver', 'prime', 'handling']\n"
     ]
    }
   ],
   "source": [
    "print(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert topics' words to ids\n",
    "efficacy_id = [vocab_dictionary.token2id[word] for word in efficacy]\n",
    "cost_id = [vocab_dictionary.token2id[word] for word in cost]\n",
    "service_id = [vocab_dictionary.token2id[word] for word in service]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [efficacy_id, cost_id, service_id]\n",
    "num_topics = 5\n",
    "\n",
    "# initialize eta: matrix of shape num_topics x num_words\n",
    "eta = np.ones((num_topics, len(vocab_dictionary))) * (1.0/num_topics)\n",
    "baseline_prob = eta[0, 0]\n",
    "\n",
    "boosted_prob = 0.9\n",
    "\n",
    "for i in range(len(topics)):\n",
    "    \n",
    "    topic_ids = topics[i]   # word ids related to topic i\n",
    "    \n",
    "    for seeded_word_id in topics[i]:\n",
    "        # boost probability of selected word in current topic\n",
    "        eta[i, seeded_word_id] = boosted_prob\n",
    "        # distribute the leftover probability over the remaining topics\n",
    "        leftover_topic_rownums = [x for x in range(num_topics) if x != i]\n",
    "        eta[leftover_topic_rownums, seeded_word_id] = ((1 - boosted_prob) / (num_topics - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[0, vocab_dictionary.token2id['significant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024999999999999994"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[1, vocab_dictionary.token2id['significant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024999999999999994"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[2, vocab_dictionary.token2id['significant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.sum(eta, axis=0))   # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13850)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 5 topics, 4 passes over the supplied corpus of 217530 documents, updating model once every 7000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6772/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.077892676, 0.082816936, 0.10946905, 0.12279554, 0.10918184]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.078): 0.022*\"pedometer\" + 0.019*\"day\" + 0.018*\"step\" + 0.017*\"like\" + 0.016*\"good\" + 0.016*\"great\" + 0.016*\"work\" + 0.014*\"use\" + 0.010*\"product\" + 0.010*\"walk\"\n",
      "INFO : topic #1 (0.083): 0.026*\"pedometer\" + 0.022*\"great\" + 0.018*\"work\" + 0.016*\"good\" + 0.015*\"product\" + 0.015*\"use\" + 0.012*\"easy\" + 0.012*\"day\" + 0.010*\"accurate\" + 0.010*\"recommend\"\n",
      "INFO : topic #2 (0.109): 0.035*\"pedometer\" + 0.025*\"good\" + 0.025*\"product\" + 0.021*\"use\" + 0.019*\"great\" + 0.016*\"work\" + 0.013*\"love\" + 0.012*\"walk\" + 0.012*\"day\" + 0.012*\"step\"\n",
      "INFO : topic #3 (0.123): 0.028*\"pedometer\" + 0.024*\"day\" + 0.024*\"use\" + 0.023*\"product\" + 0.020*\"great\" + 0.020*\"good\" + 0.017*\"step\" + 0.013*\"work\" + 0.011*\"time\" + 0.011*\"easy\"\n",
      "INFO : topic #4 (0.109): 0.039*\"pedometer\" + 0.029*\"use\" + 0.022*\"step\" + 0.021*\"easy\" + 0.019*\"day\" + 0.018*\"great\" + 0.016*\"love\" + 0.015*\"work\" + 0.015*\"omron\" + 0.015*\"good\"\n",
      "INFO : topic diff=6.844054, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6987/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.088982493, 0.095432617, 0.11091473, 0.10039218, 0.079516605]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.089): 0.018*\"good\" + 0.014*\"day\" + 0.014*\"like\" + 0.013*\"product\" + 0.013*\"work\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"great\" + 0.009*\"feel\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.095): 0.024*\"product\" + 0.022*\"great\" + 0.019*\"good\" + 0.017*\"taste\" + 0.014*\"use\" + 0.014*\"work\" + 0.009*\"recommend\" + 0.009*\"like\" + 0.008*\"help\" + 0.008*\"day\"\n",
      "INFO : topic #2 (0.111): 0.036*\"product\" + 0.027*\"good\" + 0.024*\"use\" + 0.018*\"great\" + 0.014*\"work\" + 0.012*\"love\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"skin\" + 0.009*\"day\"\n",
      "INFO : topic #3 (0.100): 0.032*\"product\" + 0.023*\"use\" + 0.021*\"day\" + 0.020*\"good\" + 0.019*\"great\" + 0.014*\"pedometer\" + 0.013*\"work\" + 0.011*\"time\" + 0.009*\"year\" + 0.009*\"like\"\n",
      "INFO : topic #4 (0.080): 0.029*\"pedometer\" + 0.028*\"use\" + 0.016*\"day\" + 0.016*\"easy\" + 0.016*\"great\" + 0.016*\"step\" + 0.015*\"product\" + 0.015*\"love\" + 0.015*\"good\" + 0.015*\"work\"\n",
      "INFO : topic diff=1.623967, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.095525578, 0.10540243, 0.11456104, 0.094202906, 0.068934739]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.096): 0.017*\"good\" + 0.016*\"help\" + 0.015*\"day\" + 0.014*\"work\" + 0.013*\"product\" + 0.012*\"like\" + 0.012*\"use\" + 0.010*\"great\" + 0.010*\"feel\" + 0.010*\"supplement\"\n",
      "INFO : topic #1 (0.105): 0.026*\"product\" + 0.022*\"great\" + 0.022*\"good\" + 0.015*\"taste\" + 0.014*\"use\" + 0.014*\"work\" + 0.009*\"like\" + 0.009*\"help\" + 0.009*\"supplement\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.115): 0.040*\"product\" + 0.027*\"good\" + 0.025*\"use\" + 0.019*\"great\" + 0.015*\"work\" + 0.012*\"skin\" + 0.012*\"year\" + 0.011*\"love\" + 0.010*\"help\" + 0.010*\"recommend\"\n",
      "INFO : topic #3 (0.094): 0.032*\"product\" + 0.023*\"use\" + 0.021*\"day\" + 0.019*\"good\" + 0.018*\"great\" + 0.015*\"work\" + 0.012*\"time\" + 0.010*\"help\" + 0.009*\"year\" + 0.008*\"pain\"\n",
      "INFO : topic #4 (0.069): 0.029*\"use\" + 0.019*\"pedometer\" + 0.016*\"work\" + 0.016*\"day\" + 0.015*\"product\" + 0.015*\"great\" + 0.015*\"love\" + 0.015*\"hair\" + 0.014*\"good\" + 0.014*\"easy\"\n",
      "INFO : topic diff=0.770599, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1000009, 0.11508932, 0.12001495, 0.090725504, 0.064648375]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.100): 0.017*\"help\" + 0.016*\"good\" + 0.015*\"day\" + 0.014*\"work\" + 0.012*\"product\" + 0.012*\"like\" + 0.011*\"use\" + 0.011*\"feel\" + 0.009*\"great\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.115): 0.028*\"product\" + 0.024*\"great\" + 0.024*\"good\" + 0.016*\"use\" + 0.014*\"taste\" + 0.013*\"work\" + 0.010*\"like\" + 0.009*\"price\" + 0.009*\"find\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.120): 0.041*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.021*\"great\" + 0.016*\"work\" + 0.012*\"skin\" + 0.012*\"love\" + 0.011*\"oil\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.091): 0.031*\"product\" + 0.023*\"use\" + 0.020*\"day\" + 0.017*\"great\" + 0.017*\"good\" + 0.016*\"work\" + 0.012*\"time\" + 0.010*\"help\" + 0.009*\"year\" + 0.009*\"pain\"\n",
      "INFO : topic #4 (0.065): 0.029*\"use\" + 0.022*\"hair\" + 0.016*\"work\" + 0.015*\"love\" + 0.015*\"product\" + 0.014*\"great\" + 0.013*\"day\" + 0.013*\"good\" + 0.011*\"pedometer\" + 0.010*\"easy\"\n",
      "INFO : topic diff=0.538937, rho=0.500000\n",
      "INFO : PROGRESS: pass 0, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.10666326, 0.12458566, 0.12033514, 0.089786969, 0.061730817]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.107): 0.017*\"help\" + 0.015*\"good\" + 0.015*\"day\" + 0.015*\"work\" + 0.012*\"product\" + 0.012*\"feel\" + 0.011*\"like\" + 0.011*\"use\" + 0.009*\"try\" + 0.008*\"supplement\"\n",
      "INFO : topic #1 (0.125): 0.030*\"product\" + 0.026*\"good\" + 0.025*\"great\" + 0.016*\"taste\" + 0.015*\"use\" + 0.012*\"work\" + 0.011*\"like\" + 0.010*\"price\" + 0.009*\"recommend\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.120): 0.044*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.022*\"great\" + 0.016*\"work\" + 0.014*\"skin\" + 0.012*\"love\" + 0.011*\"year\" + 0.011*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.090): 0.030*\"product\" + 0.022*\"use\" + 0.020*\"day\" + 0.016*\"good\" + 0.016*\"work\" + 0.016*\"great\" + 0.012*\"time\" + 0.011*\"help\" + 0.010*\"year\" + 0.009*\"pain\"\n",
      "INFO : topic #4 (0.062): 0.028*\"use\" + 0.018*\"hair\" + 0.015*\"product\" + 0.015*\"work\" + 0.014*\"love\" + 0.014*\"swanson\" + 0.013*\"day\" + 0.013*\"great\" + 0.012*\"good\" + 0.009*\"easy\"\n",
      "INFO : topic diff=0.466250, rho=0.447214\n",
      "INFO : PROGRESS: pass 0, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6995/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11242127, 0.13244016, 0.12074873, 0.090954259, 0.060245335]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.112): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"good\" + 0.014*\"work\" + 0.012*\"product\" + 0.012*\"feel\" + 0.010*\"like\" + 0.010*\"use\" + 0.010*\"try\" + 0.009*\"start\"\n",
      "INFO : topic #1 (0.132): 0.030*\"product\" + 0.029*\"good\" + 0.025*\"great\" + 0.017*\"taste\" + 0.015*\"use\" + 0.011*\"price\" + 0.011*\"like\" + 0.010*\"work\" + 0.009*\"find\" + 0.009*\"recommend\"\n",
      "INFO : topic #2 (0.121): 0.046*\"product\" + 0.027*\"use\" + 0.026*\"good\" + 0.023*\"great\" + 0.016*\"work\" + 0.015*\"skin\" + 0.012*\"love\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.091): 0.030*\"product\" + 0.022*\"use\" + 0.021*\"day\" + 0.017*\"work\" + 0.015*\"good\" + 0.014*\"great\" + 0.014*\"pain\" + 0.012*\"time\" + 0.011*\"help\" + 0.011*\"year\"\n",
      "INFO : topic #4 (0.060): 0.032*\"hair\" + 0.028*\"use\" + 0.015*\"product\" + 0.014*\"work\" + 0.012*\"love\" + 0.012*\"day\" + 0.011*\"good\" + 0.011*\"great\" + 0.010*\"grow\" + 0.009*\"nail\"\n",
      "INFO : topic diff=0.416723, rho=0.408248\n",
      "INFO : PROGRESS: pass 0, at document #49000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.11758024, 0.14169693, 0.12300795, 0.092379905, 0.058417365]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.118): 0.017*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.014*\"good\" + 0.012*\"product\" + 0.012*\"feel\" + 0.010*\"try\" + 0.010*\"like\" + 0.010*\"use\" + 0.009*\"supplement\"\n",
      "INFO : topic #1 (0.142): 0.031*\"product\" + 0.030*\"good\" + 0.026*\"great\" + 0.018*\"taste\" + 0.015*\"use\" + 0.012*\"like\" + 0.011*\"price\" + 0.010*\"work\" + 0.010*\"supplement\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.123): 0.048*\"product\" + 0.030*\"use\" + 0.025*\"good\" + 0.024*\"great\" + 0.016*\"work\" + 0.016*\"skin\" + 0.012*\"love\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.092): 0.030*\"product\" + 0.021*\"use\" + 0.020*\"day\" + 0.019*\"work\" + 0.015*\"good\" + 0.014*\"pain\" + 0.013*\"great\" + 0.012*\"time\" + 0.011*\"year\" + 0.011*\"help\"\n",
      "INFO : topic #4 (0.058): 0.033*\"hair\" + 0.026*\"use\" + 0.014*\"product\" + 0.014*\"work\" + 0.012*\"love\" + 0.011*\"grow\" + 0.011*\"nail\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"great\"\n",
      "INFO : topic diff=0.372998, rho=0.377964\n",
      "INFO : PROGRESS: pass 0, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.1253535, 0.14862336, 0.12238678, 0.095103689, 0.0567942]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.125): 0.018*\"help\" + 0.015*\"work\" + 0.015*\"day\" + 0.014*\"good\" + 0.012*\"product\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"use\" + 0.009*\"supplement\" + 0.009*\"like\"\n",
      "INFO : topic #1 (0.149): 0.032*\"product\" + 0.031*\"good\" + 0.026*\"great\" + 0.020*\"taste\" + 0.015*\"use\" + 0.012*\"like\" + 0.011*\"price\" + 0.010*\"supplement\" + 0.010*\"work\" + 0.009*\"find\"\n",
      "INFO : topic #2 (0.122): 0.051*\"product\" + 0.030*\"use\" + 0.025*\"good\" + 0.025*\"great\" + 0.017*\"skin\" + 0.017*\"work\" + 0.013*\"love\" + 0.012*\"year\" + 0.011*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.095): 0.029*\"product\" + 0.021*\"day\" + 0.021*\"use\" + 0.020*\"work\" + 0.014*\"pain\" + 0.014*\"good\" + 0.012*\"great\" + 0.012*\"time\" + 0.011*\"year\" + 0.011*\"help\"\n",
      "INFO : topic #4 (0.057): 0.030*\"hair\" + 0.024*\"use\" + 0.014*\"product\" + 0.013*\"work\" + 0.012*\"love\" + 0.010*\"grow\" + 0.010*\"nail\" + 0.010*\"day\" + 0.010*\"good\" + 0.009*\"great\"\n",
      "INFO : topic diff=0.344267, rho=0.353553\n",
      "INFO : PROGRESS: pass 0, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13243665, 0.15589084, 0.12449123, 0.094790503, 0.05573]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.132): 0.018*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.014*\"good\" + 0.012*\"product\" + 0.011*\"feel\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.009*\"use\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.156): 0.033*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.015*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.010*\"supplement\" + 0.010*\"find\" + 0.009*\"work\"\n",
      "INFO : topic #2 (0.124): 0.053*\"product\" + 0.030*\"use\" + 0.027*\"great\" + 0.026*\"good\" + 0.017*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.012*\"year\" + 0.012*\"recommend\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.095): 0.029*\"product\" + 0.021*\"day\" + 0.021*\"work\" + 0.020*\"use\" + 0.015*\"pain\" + 0.014*\"good\" + 0.012*\"time\" + 0.011*\"great\" + 0.011*\"help\" + 0.011*\"year\"\n",
      "INFO : topic #4 (0.056): 0.032*\"hair\" + 0.023*\"use\" + 0.013*\"product\" + 0.012*\"grow\" + 0.012*\"work\" + 0.011*\"nail\" + 0.011*\"love\" + 0.010*\"day\" + 0.009*\"good\" + 0.008*\"great\"\n",
      "INFO : topic diff=0.298149, rho=0.333333\n",
      "INFO : PROGRESS: pass 0, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.13877419, 0.15895346, 0.12765411, 0.097106114, 0.055798668]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.139): 0.017*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.012*\"product\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.009*\"year\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.159): 0.034*\"good\" + 0.032*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.015*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.010*\"supplement\" + 0.010*\"find\" + 0.009*\"work\"\n",
      "INFO : topic #2 (0.128): 0.055*\"product\" + 0.031*\"use\" + 0.028*\"great\" + 0.026*\"good\" + 0.017*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.012*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.097): 0.028*\"product\" + 0.021*\"day\" + 0.021*\"work\" + 0.020*\"use\" + 0.016*\"pain\" + 0.013*\"good\" + 0.011*\"year\" + 0.011*\"help\" + 0.011*\"time\" + 0.011*\"great\"\n",
      "INFO : topic #4 (0.056): 0.042*\"hair\" + 0.023*\"use\" + 0.014*\"camera\" + 0.014*\"grow\" + 0.013*\"nail\" + 0.012*\"product\" + 0.011*\"work\" + 0.010*\"love\" + 0.009*\"good\" + 0.009*\"great\"\n",
      "INFO : topic diff=0.320549, rho=0.316228\n",
      "INFO : PROGRESS: pass 0, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.14390583, 0.16830036, 0.13108593, 0.098244347, 0.055316102]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.144): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.010*\"supplement\" + 0.009*\"year\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.168): 0.035*\"good\" + 0.031*\"product\" + 0.026*\"great\" + 0.020*\"taste\" + 0.017*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.010*\"find\" + 0.009*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.131): 0.054*\"product\" + 0.033*\"use\" + 0.028*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.098): 0.027*\"product\" + 0.021*\"day\" + 0.021*\"work\" + 0.020*\"use\" + 0.016*\"pain\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"help\" + 0.011*\"time\" + 0.011*\"try\"\n",
      "INFO : topic #4 (0.055): 0.041*\"hair\" + 0.023*\"use\" + 0.014*\"nail\" + 0.013*\"grow\" + 0.012*\"product\" + 0.010*\"camera\" + 0.010*\"work\" + 0.010*\"love\" + 0.008*\"great\" + 0.008*\"good\"\n",
      "INFO : topic diff=0.271638, rho=0.301511\n",
      "INFO : PROGRESS: pass 0, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15063046, 0.17348415, 0.13468431, 0.099252418, 0.055412017]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.151): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.012*\"product\" + 0.010*\"supplement\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"use\"\n",
      "INFO : topic #1 (0.173): 0.036*\"good\" + 0.031*\"product\" + 0.025*\"great\" + 0.020*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.012*\"price\" + 0.010*\"find\" + 0.010*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.135): 0.056*\"product\" + 0.033*\"use\" + 0.029*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.014*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.099): 0.027*\"product\" + 0.021*\"work\" + 0.021*\"day\" + 0.019*\"use\" + 0.018*\"pain\" + 0.012*\"good\" + 0.011*\"help\" + 0.011*\"year\" + 0.011*\"time\" + 0.011*\"try\"\n",
      "INFO : topic #4 (0.055): 0.052*\"hair\" + 0.021*\"use\" + 0.016*\"grow\" + 0.016*\"nail\" + 0.012*\"product\" + 0.010*\"work\" + 0.009*\"love\" + 0.008*\"good\" + 0.008*\"month\" + 0.008*\"great\"\n",
      "INFO : topic diff=0.258992, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.15832831, 0.18082471, 0.13599241, 0.10064782, 0.055536192]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.158): 0.018*\"help\" + 0.016*\"day\" + 0.014*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.012*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.008*\"recommend\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.181): 0.037*\"good\" + 0.031*\"product\" + 0.025*\"great\" + 0.018*\"taste\" + 0.016*\"use\" + 0.013*\"price\" + 0.013*\"like\" + 0.011*\"supplement\" + 0.010*\"find\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.136): 0.058*\"product\" + 0.033*\"use\" + 0.030*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.101): 0.027*\"product\" + 0.021*\"day\" + 0.021*\"work\" + 0.019*\"pain\" + 0.019*\"use\" + 0.012*\"help\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #4 (0.056): 0.048*\"hair\" + 0.019*\"use\" + 0.018*\"nail\" + 0.016*\"grow\" + 0.011*\"product\" + 0.011*\"eye\" + 0.009*\"work\" + 0.008*\"notice\" + 0.008*\"love\" + 0.008*\"month\"\n",
      "INFO : topic diff=0.249931, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16080891, 0.18968856, 0.13901208, 0.10165233, 0.055594034]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.161): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.190): 0.037*\"good\" + 0.030*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.013*\"like\" + 0.013*\"price\" + 0.010*\"supplement\" + 0.010*\"vitamin\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.139): 0.058*\"product\" + 0.034*\"use\" + 0.031*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.015*\"skin\" + 0.013*\"love\" + 0.012*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.102): 0.026*\"product\" + 0.022*\"day\" + 0.022*\"work\" + 0.019*\"use\" + 0.019*\"pain\" + 0.012*\"help\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"time\" + 0.010*\"try\"\n",
      "INFO : topic #4 (0.056): 0.046*\"hair\" + 0.019*\"use\" + 0.015*\"nail\" + 0.015*\"grow\" + 0.015*\"eye\" + 0.011*\"product\" + 0.008*\"notice\" + 0.008*\"work\" + 0.007*\"month\" + 0.007*\"day\"\n",
      "INFO : topic diff=0.215759, rho=0.267261\n",
      "INFO : PROGRESS: pass 0, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.16591291, 0.19375616, 0.14309028, 0.10383224, 0.055490542]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.166): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.009*\"year\" + 0.008*\"recommend\"\n",
      "INFO : topic #1 (0.194): 0.037*\"good\" + 0.030*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.016*\"use\" + 0.014*\"like\" + 0.013*\"price\" + 0.010*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"find\"\n",
      "INFO : topic #2 (0.143): 0.058*\"product\" + 0.034*\"use\" + 0.032*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.015*\"skin\" + 0.014*\"love\" + 0.012*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.104): 0.024*\"product\" + 0.022*\"day\" + 0.022*\"work\" + 0.019*\"use\" + 0.019*\"pain\" + 0.012*\"help\" + 0.011*\"good\" + 0.011*\"time\" + 0.011*\"try\" + 0.011*\"year\"\n",
      "INFO : topic #4 (0.055): 0.045*\"hair\" + 0.019*\"use\" + 0.017*\"nail\" + 0.015*\"grow\" + 0.014*\"eye\" + 0.011*\"product\" + 0.009*\"notice\" + 0.008*\"work\" + 0.007*\"month\" + 0.007*\"love\"\n",
      "INFO : topic diff=0.219792, rho=0.258199\n",
      "INFO : PROGRESS: pass 0, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17042828, 0.20538764, 0.14688115, 0.10446575, 0.055824753]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.170): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.008*\"recommend\"\n",
      "INFO : topic #1 (0.205): 0.036*\"good\" + 0.030*\"product\" + 0.026*\"great\" + 0.019*\"taste\" + 0.015*\"use\" + 0.014*\"like\" + 0.014*\"price\" + 0.011*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"find\"\n",
      "INFO : topic #2 (0.147): 0.058*\"product\" + 0.033*\"use\" + 0.032*\"great\" + 0.025*\"good\" + 0.017*\"work\" + 0.013*\"love\" + 0.013*\"skin\" + 0.012*\"recommend\" + 0.012*\"year\" + 0.011*\"help\"\n",
      "INFO : topic #3 (0.104): 0.024*\"product\" + 0.022*\"work\" + 0.022*\"day\" + 0.019*\"use\" + 0.018*\"pain\" + 0.012*\"help\" + 0.011*\"try\" + 0.011*\"good\" + 0.011*\"time\" + 0.011*\"year\"\n",
      "INFO : topic #4 (0.056): 0.043*\"hair\" + 0.018*\"use\" + 0.017*\"nail\" + 0.016*\"eye\" + 0.014*\"grow\" + 0.010*\"product\" + 0.009*\"notice\" + 0.008*\"work\" + 0.007*\"month\" + 0.007*\"day\"\n",
      "INFO : topic diff=0.204325, rho=0.250000\n",
      "INFO : PROGRESS: pass 0, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17385529, 0.21390888, 0.14964499, 0.10539138, 0.05581139]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.174): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.214): 0.037*\"good\" + 0.029*\"product\" + 0.026*\"great\" + 0.022*\"taste\" + 0.014*\"use\" + 0.014*\"like\" + 0.014*\"price\" + 0.013*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.150): 0.060*\"product\" + 0.033*\"use\" + 0.033*\"great\" + 0.026*\"good\" + 0.017*\"work\" + 0.014*\"love\" + 0.014*\"skin\" + 0.013*\"recommend\" + 0.011*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.105): 0.024*\"product\" + 0.022*\"work\" + 0.022*\"day\" + 0.018*\"use\" + 0.017*\"pain\" + 0.012*\"help\" + 0.011*\"try\" + 0.011*\"good\" + 0.011*\"time\" + 0.011*\"year\"\n",
      "INFO : topic #4 (0.056): 0.045*\"hair\" + 0.020*\"nail\" + 0.017*\"use\" + 0.017*\"grow\" + 0.016*\"eye\" + 0.010*\"product\" + 0.010*\"notice\" + 0.008*\"month\" + 0.007*\"work\" + 0.007*\"strong\"\n",
      "INFO : topic diff=0.193251, rho=0.242536\n",
      "INFO : PROGRESS: pass 0, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17498688, 0.22680223, 0.15355992, 0.10558034, 0.056005839]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.175): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.227): 0.038*\"good\" + 0.029*\"product\" + 0.026*\"great\" + 0.023*\"taste\" + 0.014*\"like\" + 0.014*\"price\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"easy\"\n",
      "INFO : topic #2 (0.154): 0.062*\"product\" + 0.033*\"great\" + 0.032*\"use\" + 0.027*\"good\" + 0.017*\"work\" + 0.014*\"love\" + 0.014*\"skin\" + 0.013*\"recommend\" + 0.012*\"year\" + 0.010*\"help\"\n",
      "INFO : topic #3 (0.106): 0.023*\"product\" + 0.023*\"work\" + 0.022*\"day\" + 0.018*\"use\" + 0.017*\"pain\" + 0.012*\"help\" + 0.011*\"try\" + 0.011*\"week\" + 0.011*\"good\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.056): 0.047*\"hair\" + 0.023*\"nail\" + 0.017*\"grow\" + 0.016*\"use\" + 0.015*\"eye\" + 0.011*\"notice\" + 0.010*\"product\" + 0.008*\"month\" + 0.008*\"strong\" + 0.007*\"work\"\n",
      "INFO : topic diff=0.179816, rho=0.235702\n",
      "INFO : PROGRESS: pass 0, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.17490941, 0.23665768, 0.15801139, 0.10590887, 0.056515653]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.175): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.237): 0.038*\"good\" + 0.028*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.013*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.010*\"easy\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.158): 0.062*\"product\" + 0.034*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.015*\"skin\" + 0.013*\"recommend\" + 0.012*\"oil\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.106): 0.023*\"work\" + 0.023*\"product\" + 0.022*\"day\" + 0.018*\"use\" + 0.016*\"pain\" + 0.012*\"help\" + 0.012*\"try\" + 0.011*\"week\" + 0.011*\"time\" + 0.011*\"good\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.057): 0.050*\"hair\" + 0.021*\"nail\" + 0.018*\"grow\" + 0.016*\"use\" + 0.015*\"eye\" + 0.010*\"notice\" + 0.010*\"coconut_oil\" + 0.009*\"product\" + 0.008*\"month\" + 0.008*\"strong\"\n",
      "INFO : topic diff=0.179453, rho=0.229416\n",
      "INFO : PROGRESS: pass 0, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18053673, 0.24050231, 0.16356511, 0.10862023, 0.057097871]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.181): 0.018*\"help\" + 0.016*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.241): 0.038*\"good\" + 0.028*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.014*\"price\" + 0.013*\"use\" + 0.012*\"vitamin\" + 0.011*\"easy\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.164): 0.064*\"product\" + 0.034*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.014*\"skin\" + 0.013*\"recommend\" + 0.011*\"year\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.109): 0.024*\"work\" + 0.023*\"day\" + 0.022*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.012*\"help\" + 0.012*\"try\" + 0.011*\"week\" + 0.011*\"feel\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.057): 0.045*\"hair\" + 0.019*\"nail\" + 0.017*\"grow\" + 0.016*\"use\" + 0.014*\"eye\" + 0.012*\"gaia\" + 0.010*\"notice\" + 0.009*\"product\" + 0.008*\"month\" + 0.008*\"strong\"\n",
      "INFO : topic diff=0.183613, rho=0.223607\n",
      "INFO : PROGRESS: pass 0, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18360043, 0.24720038, 0.16748771, 0.11047845, 0.057201512]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.184): 0.018*\"help\" + 0.016*\"day\" + 0.014*\"work\" + 0.012*\"feel\" + 0.012*\"good\" + 0.011*\"product\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.247): 0.038*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.167): 0.065*\"product\" + 0.034*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.016*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.011*\"year\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.110): 0.024*\"work\" + 0.023*\"day\" + 0.022*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.012*\"help\" + 0.012*\"week\" + 0.012*\"try\" + 0.011*\"feel\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.057): 0.042*\"hair\" + 0.019*\"nail\" + 0.017*\"grow\" + 0.016*\"use\" + 0.014*\"eye\" + 0.011*\"gaia\" + 0.010*\"notice\" + 0.009*\"product\" + 0.008*\"coconut_oil\" + 0.008*\"strong\"\n",
      "INFO : topic diff=0.167395, rho=0.218218\n",
      "INFO : PROGRESS: pass 0, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18650256, 0.25303122, 0.17063263, 0.11242078, 0.057561576]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.187): 0.018*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.011*\"supplement\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.253): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.015*\"like\" + 0.013*\"use\" + 0.013*\"price\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.171): 0.067*\"product\" + 0.035*\"great\" + 0.034*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.016*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.011*\"year\" + 0.010*\"oil\"\n",
      "INFO : topic #3 (0.112): 0.024*\"work\" + 0.022*\"day\" + 0.022*\"product\" + 0.018*\"use\" + 0.016*\"pain\" + 0.012*\"week\" + 0.012*\"help\" + 0.012*\"try\" + 0.011*\"feel\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.058): 0.043*\"hair\" + 0.019*\"nail\" + 0.018*\"eye\" + 0.018*\"grow\" + 0.016*\"use\" + 0.010*\"notice\" + 0.009*\"product\" + 0.008*\"month\" + 0.008*\"gaia\" + 0.008*\"strong\"\n",
      "INFO : topic diff=0.170804, rho=0.213201\n",
      "INFO : PROGRESS: pass 0, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.18716867, 0.25236276, 0.17459171, 0.11248195, 0.059487954]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.187): 0.018*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.252): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.023*\"taste\" + 0.015*\"like\" + 0.013*\"price\" + 0.013*\"vitamin\" + 0.013*\"use\" + 0.012*\"easy\" + 0.010*\"find\"\n",
      "INFO : topic #2 (0.175): 0.069*\"product\" + 0.035*\"great\" + 0.034*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.016*\"love\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.011*\"year\" + 0.011*\"oil\"\n",
      "INFO : topic #3 (0.112): 0.024*\"work\" + 0.023*\"day\" + 0.022*\"product\" + 0.018*\"use\" + 0.016*\"pain\" + 0.012*\"week\" + 0.012*\"help\" + 0.012*\"try\" + 0.012*\"feel\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.059): 0.069*\"hair\" + 0.031*\"nail\" + 0.026*\"grow\" + 0.016*\"use\" + 0.013*\"eye\" + 0.012*\"biotin\" + 0.012*\"notice\" + 0.011*\"month\" + 0.011*\"strong\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.205365, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19173339, 0.25752404, 0.17937198, 0.11421018, 0.060074206]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.192): 0.018*\"help\" + 0.015*\"day\" + 0.015*\"work\" + 0.012*\"good\" + 0.011*\"supplement\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.258): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.023*\"taste\" + 0.015*\"like\" + 0.013*\"vitamin\" + 0.013*\"price\" + 0.012*\"use\" + 0.012*\"easy\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.179): 0.070*\"product\" + 0.036*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.014*\"skin\" + 0.013*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.114): 0.024*\"work\" + 0.023*\"day\" + 0.022*\"product\" + 0.018*\"use\" + 0.017*\"pain\" + 0.012*\"help\" + 0.012*\"try\" + 0.012*\"week\" + 0.012*\"feel\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.060): 0.066*\"hair\" + 0.030*\"nail\" + 0.026*\"grow\" + 0.015*\"use\" + 0.014*\"eye\" + 0.012*\"biotin\" + 0.012*\"notice\" + 0.011*\"month\" + 0.011*\"strong\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.159913, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19548355, 0.26082456, 0.18712266, 0.11532995, 0.060214002]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.195): 0.018*\"help\" + 0.015*\"day\" + 0.014*\"work\" + 0.012*\"good\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.261): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.023*\"taste\" + 0.015*\"like\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.013*\"price\" + 0.012*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.187): 0.070*\"product\" + 0.036*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.014*\"skin\" + 0.014*\"oil\" + 0.014*\"recommend\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.115): 0.024*\"work\" + 0.023*\"day\" + 0.021*\"product\" + 0.018*\"use\" + 0.017*\"pain\" + 0.012*\"help\" + 0.012*\"try\" + 0.012*\"feel\" + 0.012*\"week\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.060): 0.061*\"hair\" + 0.030*\"nail\" + 0.024*\"grow\" + 0.015*\"use\" + 0.014*\"eye\" + 0.013*\"biotin\" + 0.012*\"notice\" + 0.012*\"strong\" + 0.011*\"month\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.153310, rho=0.200000\n",
      "INFO : PROGRESS: pass 0, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : optimized alpha [0.19820184, 0.26557565, 0.19120696, 0.11822807, 0.060456198]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.198): 0.018*\"help\" + 0.015*\"day\" + 0.014*\"work\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.266): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.023*\"taste\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"price\" + 0.012*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.191): 0.072*\"product\" + 0.036*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.016*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.013*\"oil\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.118): 0.025*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.013*\"week\" + 0.013*\"try\" + 0.012*\"feel\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.060): 0.060*\"hair\" + 0.029*\"nail\" + 0.024*\"grow\" + 0.015*\"use\" + 0.013*\"eye\" + 0.012*\"notice\" + 0.011*\"strong\" + 0.011*\"biotin\" + 0.011*\"month\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.159854, rho=0.196116\n",
      "INFO : PROGRESS: pass 0, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6997/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19989599, 0.26774004, 0.19467771, 0.12165238, 0.060899697]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.200): 0.018*\"help\" + 0.015*\"day\" + 0.014*\"work\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.268): 0.037*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.012*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.195): 0.074*\"product\" + 0.037*\"great\" + 0.033*\"use\" + 0.027*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.012*\"oil\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.122): 0.026*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.013*\"try\" + 0.013*\"feel\" + 0.013*\"week\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.061): 0.060*\"hair\" + 0.028*\"nail\" + 0.023*\"grow\" + 0.015*\"use\" + 0.013*\"eye\" + 0.012*\"notice\" + 0.012*\"biotin\" + 0.012*\"strong\" + 0.011*\"month\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.147507, rho=0.192450\n",
      "INFO : PROGRESS: pass 0, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20234191, 0.26981637, 0.19867198, 0.12570035, 0.061084185]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.202): 0.018*\"help\" + 0.014*\"day\" + 0.014*\"work\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.270): 0.037*\"good\" + 0.026*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"price\" + 0.012*\"use\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.199): 0.075*\"product\" + 0.038*\"great\" + 0.032*\"use\" + 0.028*\"good\" + 0.017*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.011*\"oil\" + 0.010*\"feel\"\n",
      "INFO : topic #3 (0.126): 0.026*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.061): 0.057*\"hair\" + 0.027*\"nail\" + 0.022*\"grow\" + 0.014*\"use\" + 0.014*\"eye\" + 0.013*\"notice\" + 0.012*\"strong\" + 0.011*\"biotin\" + 0.011*\"month\" + 0.009*\"product\"\n",
      "INFO : topic diff=0.148658, rho=0.188982\n",
      "INFO : PROGRESS: pass 0, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20321582, 0.26997188, 0.20548128, 0.12744211, 0.061465759]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.203): 0.018*\"help\" + 0.014*\"day\" + 0.014*\"work\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.270): 0.036*\"good\" + 0.026*\"product\" + 0.026*\"great\" + 0.024*\"taste\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.012*\"price\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.205): 0.075*\"product\" + 0.037*\"great\" + 0.033*\"use\" + 0.028*\"good\" + 0.016*\"skin\" + 0.016*\"work\" + 0.016*\"love\" + 0.014*\"recommend\" + 0.011*\"feel\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.127): 0.025*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.014*\"feel\" + 0.013*\"week\" + 0.013*\"try\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.061): 0.055*\"hair\" + 0.026*\"nail\" + 0.021*\"grow\" + 0.014*\"use\" + 0.014*\"eye\" + 0.013*\"notice\" + 0.012*\"biotin\" + 0.011*\"strong\" + 0.011*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.145140, rho=0.185695\n",
      "INFO : PROGRESS: pass 0, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20621297, 0.27543709, 0.2135212, 0.1305443, 0.06224224]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.206): 0.018*\"help\" + 0.015*\"supplement\" + 0.014*\"day\" + 0.013*\"work\" + 0.013*\"feel\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.275): 0.034*\"good\" + 0.026*\"product\" + 0.025*\"great\" + 0.022*\"taste\" + 0.016*\"like\" + 0.014*\"easy\" + 0.013*\"supplement\" + 0.013*\"vitamin\" + 0.011*\"use\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.214): 0.076*\"product\" + 0.037*\"great\" + 0.032*\"use\" + 0.027*\"good\" + 0.016*\"skin\" + 0.015*\"love\" + 0.015*\"work\" + 0.014*\"recommend\" + 0.012*\"feel\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.131): 0.024*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.016*\"use\" + 0.015*\"feel\" + 0.015*\"pain\" + 0.014*\"week\" + 0.013*\"try\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.062): 0.050*\"hair\" + 0.024*\"nail\" + 0.019*\"grow\" + 0.017*\"naturewise\" + 0.015*\"notice\" + 0.014*\"use\" + 0.014*\"eye\" + 0.011*\"strong\" + 0.011*\"month\" + 0.010*\"biotin\"\n",
      "INFO : topic diff=0.151428, rho=0.182574\n",
      "INFO : PROGRESS: pass 0, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20682895, 0.26886842, 0.22180484, 0.13546628, 0.062776178]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.207): 0.018*\"help\" + 0.016*\"supplement\" + 0.014*\"day\" + 0.013*\"feel\" + 0.013*\"work\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.007*\"year\"\n",
      "INFO : topic #1 (0.269): 0.034*\"good\" + 0.026*\"product\" + 0.025*\"great\" + 0.022*\"taste\" + 0.016*\"like\" + 0.014*\"supplement\" + 0.014*\"easy\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.222): 0.076*\"product\" + 0.036*\"great\" + 0.031*\"use\" + 0.026*\"good\" + 0.020*\"skin\" + 0.015*\"love\" + 0.014*\"work\" + 0.014*\"recommend\" + 0.012*\"feel\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.135): 0.024*\"work\" + 0.022*\"day\" + 0.021*\"product\" + 0.016*\"feel\" + 0.016*\"use\" + 0.015*\"week\" + 0.014*\"try\" + 0.014*\"pain\" + 0.012*\"help\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.063): 0.050*\"hair\" + 0.022*\"nail\" + 0.018*\"grow\" + 0.016*\"notice\" + 0.014*\"naturewise\" + 0.014*\"use\" + 0.013*\"eye\" + 0.011*\"strong\" + 0.011*\"look\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.149960, rho=0.179605\n",
      "INFO : PROGRESS: pass 0, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20595516, 0.2622149, 0.22901541, 0.1401573, 0.06223705]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.206): 0.019*\"help\" + 0.017*\"supplement\" + 0.014*\"feel\" + 0.013*\"good\" + 0.013*\"work\" + 0.013*\"day\" + 0.012*\"product\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.008*\"probiotic\"\n",
      "INFO : topic #1 (0.262): 0.034*\"good\" + 0.027*\"product\" + 0.026*\"great\" + 0.022*\"taste\" + 0.017*\"like\" + 0.015*\"supplement\" + 0.015*\"easy\" + 0.012*\"fish_oil\" + 0.011*\"vitamin\" + 0.011*\"price\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.229): 0.080*\"product\" + 0.038*\"great\" + 0.029*\"use\" + 0.026*\"good\" + 0.020*\"skin\" + 0.015*\"love\" + 0.015*\"work\" + 0.015*\"recommend\" + 0.013*\"feel\" + 0.010*\"order\"\n",
      "INFO : topic #3 (0.140): 0.023*\"work\" + 0.022*\"product\" + 0.020*\"day\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.016*\"use\" + 0.015*\"try\" + 0.014*\"week\" + 0.013*\"help\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.062): 0.049*\"hair\" + 0.022*\"nail\" + 0.018*\"notice\" + 0.017*\"grow\" + 0.014*\"eye\" + 0.013*\"use\" + 0.012*\"naturewise\" + 0.012*\"look\" + 0.010*\"strong\" + 0.010*\"month\"\n",
      "INFO : topic diff=0.143115, rho=0.176777\n",
      "INFO : PROGRESS: pass 1, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.19576883, 0.25683141, 0.22354899, 0.14582995, 0.06572742]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.196): 0.019*\"help\" + 0.016*\"supplement\" + 0.013*\"feel\" + 0.013*\"day\" + 0.013*\"work\" + 0.013*\"good\" + 0.011*\"product\" + 0.011*\"try\" + 0.009*\"recommend\" + 0.007*\"probiotic\"\n",
      "INFO : topic #1 (0.257): 0.034*\"good\" + 0.027*\"great\" + 0.025*\"product\" + 0.020*\"taste\" + 0.019*\"easy\" + 0.017*\"like\" + 0.014*\"supplement\" + 0.012*\"price\" + 0.011*\"use\" + 0.011*\"love\"\n",
      "INFO : topic #2 (0.224): 0.077*\"product\" + 0.039*\"great\" + 0.031*\"use\" + 0.026*\"good\" + 0.019*\"skin\" + 0.016*\"love\" + 0.016*\"work\" + 0.015*\"recommend\" + 0.013*\"feel\" + 0.009*\"order\"\n",
      "INFO : topic #3 (0.146): 0.024*\"day\" + 0.024*\"work\" + 0.020*\"product\" + 0.017*\"use\" + 0.015*\"feel\" + 0.013*\"try\" + 0.013*\"pain\" + 0.013*\"week\" + 0.012*\"help\" + 0.011*\"good\"\n",
      "INFO : topic #4 (0.066): 0.059*\"pedometer\" + 0.027*\"hair\" + 0.019*\"omron\" + 0.017*\"use\" + 0.016*\"clip\" + 0.014*\"step\" + 0.012*\"accurate\" + 0.011*\"nail\" + 0.010*\"notice\" + 0.009*\"grow\"\n",
      "INFO : topic diff=0.251649, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20332193, 0.26176962, 0.22760974, 0.14608856, 0.065592453]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.203): 0.019*\"help\" + 0.015*\"supplement\" + 0.013*\"day\" + 0.013*\"work\" + 0.013*\"feel\" + 0.013*\"good\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.262): 0.034*\"good\" + 0.026*\"great\" + 0.025*\"product\" + 0.022*\"taste\" + 0.017*\"easy\" + 0.017*\"like\" + 0.013*\"supplement\" + 0.012*\"use\" + 0.011*\"price\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.228): 0.076*\"product\" + 0.038*\"great\" + 0.032*\"use\" + 0.026*\"good\" + 0.019*\"skin\" + 0.016*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"feel\" + 0.009*\"order\"\n",
      "INFO : topic #3 (0.146): 0.025*\"day\" + 0.024*\"work\" + 0.019*\"product\" + 0.017*\"use\" + 0.015*\"feel\" + 0.014*\"pain\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"help\" + 0.011*\"good\"\n",
      "INFO : topic #4 (0.066): 0.053*\"pedometer\" + 0.035*\"hair\" + 0.017*\"use\" + 0.016*\"omron\" + 0.015*\"clip\" + 0.013*\"step\" + 0.012*\"nail\" + 0.012*\"grow\" + 0.010*\"accurate\" + 0.010*\"notice\"\n",
      "INFO : topic diff=0.141892, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.20910043, 0.26272154, 0.23449343, 0.14728537, 0.065503515]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.209): 0.020*\"help\" + 0.015*\"supplement\" + 0.014*\"day\" + 0.013*\"work\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.263): 0.035*\"good\" + 0.026*\"great\" + 0.025*\"product\" + 0.023*\"taste\" + 0.016*\"like\" + 0.016*\"easy\" + 0.012*\"supplement\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.234): 0.074*\"product\" + 0.038*\"great\" + 0.033*\"use\" + 0.026*\"good\" + 0.018*\"skin\" + 0.017*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.012*\"feel\" + 0.009*\"buy\"\n",
      "INFO : topic #3 (0.147): 0.025*\"day\" + 0.025*\"work\" + 0.019*\"product\" + 0.017*\"use\" + 0.015*\"pain\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.066): 0.047*\"pedometer\" + 0.041*\"hair\" + 0.017*\"use\" + 0.015*\"omron\" + 0.014*\"nail\" + 0.014*\"grow\" + 0.013*\"clip\" + 0.011*\"step\" + 0.010*\"notice\" + 0.009*\"accurate\"\n",
      "INFO : topic diff=0.129133, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2124171, 0.26515257, 0.24258529, 0.14831981, 0.065782651]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.212): 0.020*\"help\" + 0.014*\"work\" + 0.014*\"supplement\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.265): 0.035*\"good\" + 0.025*\"great\" + 0.024*\"product\" + 0.022*\"taste\" + 0.016*\"like\" + 0.015*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.012*\"supplement\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.243): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"use\" + 0.026*\"good\" + 0.017*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.011*\"feel\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.148): 0.025*\"day\" + 0.025*\"work\" + 0.018*\"product\" + 0.017*\"use\" + 0.014*\"pain\" + 0.014*\"feel\" + 0.013*\"week\" + 0.013*\"try\" + 0.012*\"help\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.066): 0.045*\"hair\" + 0.041*\"pedometer\" + 0.017*\"use\" + 0.016*\"nail\" + 0.015*\"grow\" + 0.013*\"omron\" + 0.011*\"clip\" + 0.010*\"step\" + 0.010*\"notice\" + 0.009*\"eye\"\n",
      "INFO : topic diff=0.124941, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.21744177, 0.26753289, 0.24676155, 0.15000458, 0.065784387]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.217): 0.021*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.013*\"good\" + 0.012*\"feel\" + 0.011*\"product\" + 0.010*\"try\" + 0.009*\"year\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.268): 0.035*\"good\" + 0.025*\"great\" + 0.024*\"product\" + 0.023*\"taste\" + 0.016*\"like\" + 0.015*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"vitamin\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.247): 0.072*\"product\" + 0.038*\"great\" + 0.034*\"use\" + 0.026*\"good\" + 0.017*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.011*\"feel\" + 0.009*\"buy\"\n",
      "INFO : topic #3 (0.150): 0.025*\"day\" + 0.025*\"work\" + 0.018*\"product\" + 0.017*\"use\" + 0.014*\"pain\" + 0.014*\"feel\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"try\" + 0.011*\"start\"\n",
      "INFO : topic #4 (0.066): 0.043*\"hair\" + 0.036*\"pedometer\" + 0.017*\"use\" + 0.015*\"nail\" + 0.015*\"grow\" + 0.011*\"omron\" + 0.010*\"eye\" + 0.010*\"clip\" + 0.010*\"notice\" + 0.009*\"step\"\n",
      "INFO : topic diff=0.122433, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2222304, 0.2690123, 0.24948634, 0.1517465, 0.066022985]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.222): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.269): 0.036*\"good\" + 0.025*\"great\" + 0.024*\"product\" + 0.023*\"taste\" + 0.016*\"like\" + 0.014*\"easy\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.249): 0.073*\"product\" + 0.038*\"great\" + 0.034*\"use\" + 0.027*\"good\" + 0.017*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.011*\"feel\" + 0.009*\"year\"\n",
      "INFO : topic #3 (0.152): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"product\" + 0.017*\"use\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"help\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.047*\"hair\" + 0.030*\"pedometer\" + 0.017*\"nail\" + 0.017*\"use\" + 0.016*\"grow\" + 0.014*\"eye\" + 0.010*\"notice\" + 0.009*\"omron\" + 0.009*\"month\" + 0.008*\"clip\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.117777, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.22500134, 0.27286011, 0.25201145, 0.15467738, 0.065746218]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.225): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"feel\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.273): 0.036*\"good\" + 0.025*\"great\" + 0.024*\"product\" + 0.023*\"taste\" + 0.016*\"like\" + 0.013*\"easy\" + 0.013*\"use\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.011*\"vitamin\"\n",
      "INFO : topic #2 (0.252): 0.073*\"product\" + 0.038*\"great\" + 0.035*\"use\" + 0.027*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.010*\"feel\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.155): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"product\" + 0.017*\"use\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"help\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.047*\"hair\" + 0.027*\"pedometer\" + 0.018*\"nail\" + 0.017*\"use\" + 0.016*\"grow\" + 0.013*\"eye\" + 0.010*\"notice\" + 0.009*\"month\" + 0.008*\"strong\" + 0.008*\"omron\"\n",
      "INFO : topic diff=0.119469, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23198245, 0.27367395, 0.25324029, 0.15794615, 0.065417171]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.232): 0.021*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"product\" + 0.011*\"feel\" + 0.010*\"year\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.274): 0.036*\"good\" + 0.025*\"great\" + 0.024*\"taste\" + 0.024*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.013*\"easy\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.253): 0.074*\"product\" + 0.038*\"great\" + 0.036*\"use\" + 0.027*\"good\" + 0.018*\"work\" + 0.017*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.010*\"feel\" + 0.010*\"year\"\n",
      "INFO : topic #3 (0.158): 0.026*\"day\" + 0.026*\"work\" + 0.017*\"product\" + 0.017*\"use\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.065): 0.046*\"hair\" + 0.024*\"pedometer\" + 0.017*\"nail\" + 0.016*\"use\" + 0.016*\"grow\" + 0.013*\"eye\" + 0.010*\"notice\" + 0.009*\"month\" + 0.008*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.122286, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.23979425, 0.2759288, 0.25680459, 0.15873256, 0.06515824]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.240): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.276): 0.036*\"good\" + 0.025*\"great\" + 0.024*\"taste\" + 0.024*\"product\" + 0.017*\"like\" + 0.013*\"use\" + 0.012*\"easy\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.257): 0.075*\"product\" + 0.039*\"great\" + 0.035*\"use\" + 0.028*\"good\" + 0.018*\"work\" + 0.015*\"skin\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.010*\"year\" + 0.010*\"feel\"\n",
      "INFO : topic #3 (0.159): 0.026*\"day\" + 0.026*\"work\" + 0.017*\"product\" + 0.017*\"use\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.065): 0.046*\"hair\" + 0.021*\"pedometer\" + 0.017*\"nail\" + 0.017*\"grow\" + 0.016*\"use\" + 0.013*\"eye\" + 0.010*\"notice\" + 0.009*\"month\" + 0.008*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.110192, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24585961, 0.27401894, 0.26182351, 0.16138469, 0.065587245]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.246): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"product\" + 0.011*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.274): 0.037*\"good\" + 0.025*\"great\" + 0.024*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.013*\"use\" + 0.012*\"easy\" + 0.012*\"price\" + 0.011*\"supplement\" + 0.010*\"vitamin\"\n",
      "INFO : topic #2 (0.262): 0.075*\"product\" + 0.039*\"great\" + 0.036*\"use\" + 0.028*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.015*\"skin\" + 0.014*\"recommend\" + 0.011*\"year\" + 0.010*\"feel\"\n",
      "INFO : topic #3 (0.161): 0.026*\"day\" + 0.026*\"work\" + 0.017*\"product\" + 0.016*\"use\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"try\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.051*\"hair\" + 0.017*\"grow\" + 0.017*\"nail\" + 0.017*\"pedometer\" + 0.016*\"use\" + 0.013*\"eye\" + 0.011*\"notice\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"camera\"\n",
      "INFO : topic diff=0.122991, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.24998035, 0.27932602, 0.26555517, 0.16311111, 0.065651223]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.250): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.011*\"supplement\" + 0.011*\"product\" + 0.011*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.279): 0.037*\"good\" + 0.024*\"great\" + 0.024*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"price\" + 0.012*\"easy\" + 0.010*\"brand\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.266): 0.075*\"product\" + 0.039*\"great\" + 0.037*\"use\" + 0.028*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"skin\" + 0.014*\"recommend\" + 0.011*\"year\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.163): 0.026*\"day\" + 0.026*\"work\" + 0.017*\"use\" + 0.016*\"product\" + 0.016*\"pain\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"help\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.050*\"hair\" + 0.018*\"nail\" + 0.017*\"grow\" + 0.016*\"use\" + 0.014*\"pedometer\" + 0.013*\"eye\" + 0.011*\"notice\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.110745, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.25652161, 0.28050265, 0.26979926, 0.16466397, 0.066023372]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.257): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"good\" + 0.012*\"supplement\" + 0.011*\"product\" + 0.011*\"year\" + 0.010*\"feel\" + 0.010*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.281): 0.038*\"good\" + 0.024*\"great\" + 0.024*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"price\" + 0.012*\"easy\" + 0.011*\"brand\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.270): 0.076*\"product\" + 0.039*\"great\" + 0.037*\"use\" + 0.028*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.014*\"skin\" + 0.011*\"year\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.165): 0.026*\"day\" + 0.025*\"work\" + 0.017*\"pain\" + 0.017*\"product\" + 0.016*\"use\" + 0.014*\"feel\" + 0.013*\"try\" + 0.013*\"help\" + 0.013*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.056*\"hair\" + 0.019*\"grow\" + 0.018*\"nail\" + 0.016*\"use\" + 0.013*\"eye\" + 0.012*\"pedometer\" + 0.011*\"notice\" + 0.011*\"month\" + 0.008*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.110005, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6998/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26335403, 0.28443462, 0.27099115, 0.16640675, 0.066431798]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.263): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.013*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"feel\" + 0.009*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.284): 0.038*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.023*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"price\" + 0.012*\"easy\" + 0.011*\"vitamin\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.271): 0.077*\"product\" + 0.040*\"great\" + 0.037*\"use\" + 0.029*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.014*\"skin\" + 0.011*\"year\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.166): 0.026*\"day\" + 0.025*\"work\" + 0.018*\"pain\" + 0.016*\"product\" + 0.016*\"use\" + 0.014*\"feel\" + 0.013*\"help\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.066): 0.052*\"hair\" + 0.019*\"nail\" + 0.019*\"eye\" + 0.018*\"grow\" + 0.015*\"use\" + 0.012*\"notice\" + 0.011*\"month\" + 0.009*\"pedometer\" + 0.008*\"lutein\" + 0.008*\"strong\"\n",
      "INFO : topic diff=0.115394, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26455876, 0.2912479, 0.27480018, 0.16717996, 0.066791505]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.265): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.013*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"feel\" + 0.009*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.291): 0.038*\"good\" + 0.024*\"great\" + 0.022*\"taste\" + 0.022*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"price\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.275): 0.077*\"product\" + 0.041*\"great\" + 0.037*\"use\" + 0.029*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.015*\"recommend\" + 0.014*\"skin\" + 0.011*\"year\" + 0.010*\"buy\"\n",
      "INFO : topic #3 (0.167): 0.027*\"day\" + 0.026*\"work\" + 0.018*\"pain\" + 0.016*\"use\" + 0.016*\"product\" + 0.015*\"feel\" + 0.013*\"help\" + 0.013*\"try\" + 0.012*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.067): 0.051*\"hair\" + 0.023*\"eye\" + 0.018*\"grow\" + 0.017*\"nail\" + 0.015*\"use\" + 0.012*\"notice\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"lutein\" + 0.008*\"pedometer\"\n",
      "INFO : topic diff=0.101092, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.26967186, 0.29330403, 0.28010958, 0.17014506, 0.066876441]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.270): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"feel\" + 0.009*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.293): 0.037*\"good\" + 0.024*\"great\" + 0.023*\"taste\" + 0.022*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.012*\"price\" + 0.012*\"vitamin\" + 0.012*\"easy\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.280): 0.077*\"product\" + 0.041*\"great\" + 0.037*\"use\" + 0.029*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.013*\"skin\" + 0.011*\"year\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.170): 0.027*\"day\" + 0.026*\"work\" + 0.018*\"pain\" + 0.016*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"help\" + 0.013*\"try\" + 0.012*\"start\" + 0.012*\"week\"\n",
      "INFO : topic #4 (0.067): 0.051*\"hair\" + 0.022*\"eye\" + 0.018*\"nail\" + 0.018*\"grow\" + 0.015*\"use\" + 0.012*\"notice\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"look\" + 0.007*\"lutein\"\n",
      "INFO : topic diff=0.106360, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27457541, 0.30362114, 0.28508449, 0.17051893, 0.067272447]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.275): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic #1 (0.304): 0.037*\"good\" + 0.023*\"great\" + 0.022*\"taste\" + 0.022*\"product\" + 0.016*\"like\" + 0.014*\"use\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.285): 0.076*\"product\" + 0.041*\"great\" + 0.037*\"use\" + 0.029*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.014*\"recommend\" + 0.012*\"skin\" + 0.011*\"year\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.171): 0.027*\"day\" + 0.026*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"start\" + 0.012*\"week\"\n",
      "INFO : topic #4 (0.067): 0.049*\"hair\" + 0.024*\"eye\" + 0.018*\"nail\" + 0.017*\"grow\" + 0.015*\"use\" + 0.012*\"notice\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"lutein\" + 0.007*\"look\"\n",
      "INFO : topic diff=0.104386, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27773196, 0.31041121, 0.28824329, 0.17113504, 0.067330495]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.278): 0.020*\"help\" + 0.014*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"try\" + 0.010*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic #1 (0.310): 0.037*\"good\" + 0.024*\"taste\" + 0.023*\"great\" + 0.021*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"use\" + 0.013*\"easy\" + 0.012*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.288): 0.078*\"product\" + 0.041*\"great\" + 0.036*\"use\" + 0.030*\"good\" + 0.019*\"work\" + 0.015*\"love\" + 0.015*\"recommend\" + 0.012*\"skin\" + 0.011*\"year\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.171): 0.027*\"day\" + 0.026*\"work\" + 0.017*\"pain\" + 0.016*\"use\" + 0.015*\"product\" + 0.015*\"feel\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.067): 0.050*\"hair\" + 0.023*\"eye\" + 0.021*\"nail\" + 0.019*\"grow\" + 0.014*\"use\" + 0.012*\"notice\" + 0.010*\"month\" + 0.009*\"strong\" + 0.008*\"look\" + 0.007*\"vitamin\"\n",
      "INFO : topic diff=0.099190, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27773201, 0.32384551, 0.29335386, 0.1713205, 0.067588747]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.278): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.009*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.324): 0.037*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.021*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.293): 0.078*\"product\" + 0.042*\"great\" + 0.035*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.015*\"love\" + 0.015*\"recommend\" + 0.012*\"skin\" + 0.011*\"year\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.171): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"pain\" + 0.016*\"use\" + 0.015*\"feel\" + 0.015*\"product\" + 0.013*\"help\" + 0.013*\"try\" + 0.013*\"week\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.068): 0.052*\"hair\" + 0.023*\"nail\" + 0.022*\"eye\" + 0.019*\"grow\" + 0.014*\"use\" + 0.013*\"notice\" + 0.010*\"month\" + 0.009*\"strong\" + 0.008*\"look\" + 0.008*\"vitamin\"\n",
      "INFO : topic diff=0.097940, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.27643043, 0.33332634, 0.29974133, 0.17104179, 0.0681815]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.276): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"day\" + 0.013*\"supplement\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"product\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.009*\"doctor\"\n",
      "INFO : topic #1 (0.333): 0.037*\"good\" + 0.026*\"taste\" + 0.024*\"great\" + 0.020*\"product\" + 0.016*\"like\" + 0.014*\"vitamin\" + 0.012*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.300): 0.078*\"product\" + 0.042*\"great\" + 0.036*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"buy\" + 0.011*\"price\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #3 (0.171): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"pain\" + 0.015*\"use\" + 0.015*\"feel\" + 0.015*\"product\" + 0.013*\"help\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"week\"\n",
      "INFO : topic #4 (0.068): 0.054*\"hair\" + 0.022*\"nail\" + 0.021*\"eye\" + 0.019*\"grow\" + 0.014*\"use\" + 0.013*\"notice\" + 0.010*\"month\" + 0.010*\"coconut_oil\" + 0.009*\"strong\" + 0.008*\"skin\"\n",
      "INFO : topic diff=0.100091, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28222606, 0.33599225, 0.30903876, 0.17446494, 0.068782054]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.282): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.011*\"year\" + 0.010*\"recommend\" + 0.010*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.336): 0.037*\"good\" + 0.026*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.016*\"like\" + 0.013*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.012*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.309): 0.079*\"product\" + 0.042*\"great\" + 0.035*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"buy\" + 0.012*\"skin\" + 0.012*\"price\"\n",
      "INFO : topic #3 (0.174): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"feel\" + 0.015*\"use\" + 0.015*\"pain\" + 0.015*\"product\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"help\" + 0.013*\"week\"\n",
      "INFO : topic #4 (0.069): 0.050*\"hair\" + 0.020*\"eye\" + 0.020*\"nail\" + 0.018*\"grow\" + 0.014*\"use\" + 0.012*\"notice\" + 0.010*\"gaia\" + 0.010*\"month\" + 0.009*\"strong\" + 0.008*\"coconut_oil\"\n",
      "INFO : topic diff=0.105161, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28450298, 0.34331617, 0.31509048, 0.17652984, 0.068907604]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.285): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.343): 0.037*\"good\" + 0.027*\"taste\" + 0.023*\"great\" + 0.020*\"product\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.013*\"easy\" + 0.012*\"use\" + 0.011*\"price\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.315): 0.080*\"product\" + 0.042*\"great\" + 0.036*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"buy\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.177): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"feel\" + 0.015*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"week\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.069): 0.047*\"hair\" + 0.020*\"nail\" + 0.020*\"eye\" + 0.018*\"grow\" + 0.014*\"use\" + 0.012*\"notice\" + 0.010*\"month\" + 0.009*\"strong\" + 0.009*\"gaia\" + 0.009*\"coconut_oil\"\n",
      "INFO : topic diff=0.099978, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28760475, 0.34941512, 0.32153869, 0.17886849, 0.069333643]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.288): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"day\" + 0.012*\"supplement\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"recommend\" + 0.010*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.349): 0.036*\"good\" + 0.026*\"taste\" + 0.023*\"great\" + 0.019*\"product\" + 0.017*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"use\" + 0.011*\"price\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.322): 0.081*\"product\" + 0.042*\"great\" + 0.036*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"buy\" + 0.012*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.179): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"start\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.069): 0.048*\"hair\" + 0.024*\"eye\" + 0.020*\"nail\" + 0.020*\"grow\" + 0.014*\"use\" + 0.013*\"notice\" + 0.010*\"month\" + 0.009*\"strong\" + 0.008*\"skin\" + 0.008*\"gaia\"\n",
      "INFO : topic diff=0.104438, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.28763947, 0.34794933, 0.32886431, 0.1785145, 0.07120645]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.288): 0.020*\"help\" + 0.013*\"work\" + 0.013*\"supplement\" + 0.013*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"doctor\" + 0.010*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.348): 0.036*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.019*\"product\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.012*\"use\" + 0.011*\"price\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.329): 0.082*\"product\" + 0.042*\"great\" + 0.035*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"buy\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.179): 0.027*\"day\" + 0.026*\"work\" + 0.016*\"feel\" + 0.015*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"week\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.071): 0.071*\"hair\" + 0.032*\"nail\" + 0.027*\"grow\" + 0.018*\"eye\" + 0.014*\"use\" + 0.013*\"notice\" + 0.013*\"month\" + 0.012*\"biotin\" + 0.012*\"strong\" + 0.009*\"skin\"\n",
      "INFO : topic diff=0.140454, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29329032, 0.35414696, 0.3373248, 0.18052034, 0.071864359]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.293): 0.019*\"help\" + 0.013*\"work\" + 0.013*\"supplement\" + 0.012*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"doctor\" + 0.010*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.354): 0.036*\"good\" + 0.025*\"taste\" + 0.023*\"great\" + 0.019*\"product\" + 0.017*\"like\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.011*\"price\"\n",
      "INFO : topic #2 (0.337): 0.083*\"product\" + 0.043*\"great\" + 0.035*\"use\" + 0.032*\"good\" + 0.017*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.181): 0.027*\"day\" + 0.026*\"work\" + 0.017*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"start\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.072): 0.069*\"hair\" + 0.031*\"nail\" + 0.027*\"grow\" + 0.018*\"eye\" + 0.014*\"notice\" + 0.014*\"use\" + 0.013*\"month\" + 0.012*\"biotin\" + 0.012*\"strong\" + 0.009*\"skin\"\n",
      "INFO : topic diff=0.102563, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.29842186, 0.35836658, 0.35169277, 0.18225397, 0.072138973]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.298): 0.020*\"help\" + 0.013*\"supplement\" + 0.013*\"work\" + 0.012*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"year\" + 0.010*\"doctor\" + 0.010*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.358): 0.036*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.017*\"like\" + 0.015*\"vitamin\" + 0.015*\"easy\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"price\"\n",
      "INFO : topic #2 (0.352): 0.083*\"product\" + 0.043*\"great\" + 0.035*\"use\" + 0.031*\"good\" + 0.017*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"order\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.182): 0.027*\"day\" + 0.026*\"work\" + 0.017*\"feel\" + 0.016*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"start\"\n",
      "INFO : topic #4 (0.072): 0.065*\"hair\" + 0.031*\"nail\" + 0.026*\"grow\" + 0.018*\"eye\" + 0.013*\"use\" + 0.013*\"notice\" + 0.013*\"biotin\" + 0.013*\"strong\" + 0.012*\"month\" + 0.011*\"skin\"\n",
      "INFO : topic diff=0.103231, rho=0.173878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30088302, 0.3636083, 0.36006504, 0.18604954, 0.072438091]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.301): 0.019*\"help\" + 0.014*\"supplement\" + 0.013*\"work\" + 0.012*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"recommend\" + 0.010*\"year\" + 0.010*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.364): 0.036*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.019*\"product\" + 0.017*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.360): 0.085*\"product\" + 0.043*\"great\" + 0.035*\"use\" + 0.032*\"good\" + 0.017*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"order\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.186): 0.027*\"day\" + 0.026*\"work\" + 0.017*\"feel\" + 0.015*\"pain\" + 0.015*\"use\" + 0.014*\"product\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.072): 0.064*\"hair\" + 0.030*\"nail\" + 0.025*\"grow\" + 0.017*\"eye\" + 0.014*\"notice\" + 0.013*\"use\" + 0.013*\"month\" + 0.012*\"strong\" + 0.012*\"biotin\" + 0.011*\"skin\"\n",
      "INFO : topic diff=0.108853, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30250818, 0.36630028, 0.36864763, 0.19144136, 0.07296589]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.303): 0.019*\"help\" + 0.014*\"supplement\" + 0.013*\"work\" + 0.012*\"day\" + 0.012*\"good\" + 0.011*\"product\" + 0.010*\"recommend\" + 0.010*\"doctor\" + 0.010*\"year\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.366): 0.036*\"good\" + 0.026*\"taste\" + 0.023*\"great\" + 0.018*\"product\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.369): 0.086*\"product\" + 0.044*\"great\" + 0.034*\"use\" + 0.032*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.016*\"recommend\" + 0.012*\"order\" + 0.012*\"price\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.191): 0.027*\"work\" + 0.026*\"day\" + 0.018*\"feel\" + 0.015*\"use\" + 0.014*\"pain\" + 0.014*\"try\" + 0.014*\"product\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"start\"\n",
      "INFO : topic #4 (0.073): 0.064*\"hair\" + 0.030*\"nail\" + 0.025*\"grow\" + 0.017*\"eye\" + 0.014*\"notice\" + 0.013*\"use\" + 0.013*\"strong\" + 0.012*\"month\" + 0.012*\"biotin\" + 0.011*\"skin\"\n",
      "INFO : topic diff=0.103316, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30547228, 0.36912757, 0.37791544, 0.19752595, 0.073291592]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.305): 0.019*\"help\" + 0.014*\"supplement\" + 0.013*\"work\" + 0.012*\"good\" + 0.012*\"day\" + 0.010*\"product\" + 0.010*\"recommend\" + 0.009*\"year\" + 0.009*\"doctor\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.369): 0.035*\"good\" + 0.026*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.018*\"like\" + 0.014*\"easy\" + 0.014*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"love\"\n",
      "INFO : topic #2 (0.378): 0.087*\"product\" + 0.045*\"great\" + 0.034*\"use\" + 0.032*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"order\" + 0.012*\"price\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.198): 0.026*\"work\" + 0.026*\"day\" + 0.018*\"feel\" + 0.014*\"use\" + 0.014*\"pain\" + 0.014*\"try\" + 0.014*\"product\" + 0.014*\"week\" + 0.012*\"help\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.073): 0.061*\"hair\" + 0.028*\"nail\" + 0.023*\"grow\" + 0.018*\"eye\" + 0.014*\"notice\" + 0.013*\"use\" + 0.013*\"strong\" + 0.012*\"month\" + 0.012*\"biotin\" + 0.012*\"skin\"\n",
      "INFO : topic diff=0.107614, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30621091, 0.37037957, 0.39224261, 0.20039019, 0.073916815]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.306): 0.019*\"help\" + 0.015*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.012*\"day\" + 0.010*\"product\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.009*\"year\" + 0.009*\"doctor\"\n",
      "INFO : topic #1 (0.370): 0.035*\"good\" + 0.026*\"taste\" + 0.022*\"great\" + 0.018*\"like\" + 0.018*\"product\" + 0.015*\"easy\" + 0.014*\"vitamin\" + 0.012*\"supplement\" + 0.010*\"fish_oil\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.392): 0.087*\"product\" + 0.044*\"great\" + 0.034*\"use\" + 0.032*\"good\" + 0.017*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"order\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.200): 0.026*\"day\" + 0.026*\"work\" + 0.019*\"feel\" + 0.015*\"pain\" + 0.014*\"use\" + 0.014*\"try\" + 0.014*\"week\" + 0.013*\"product\" + 0.012*\"help\" + 0.012*\"start\"\n",
      "INFO : topic #4 (0.074): 0.059*\"hair\" + 0.028*\"nail\" + 0.022*\"grow\" + 0.018*\"eye\" + 0.015*\"notice\" + 0.014*\"skin\" + 0.013*\"use\" + 0.012*\"month\" + 0.012*\"strong\" + 0.012*\"biotin\"\n",
      "INFO : topic diff=0.107470, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31001976, 0.37827542, 0.40878406, 0.20538169, 0.074919462]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.310): 0.019*\"help\" + 0.016*\"supplement\" + 0.012*\"good\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"product\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.009*\"feel\" + 0.009*\"doctor\"\n",
      "INFO : topic #1 (0.378): 0.033*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.018*\"product\" + 0.018*\"like\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.013*\"supplement\" + 0.010*\"fish_oil\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.409): 0.089*\"product\" + 0.044*\"great\" + 0.033*\"use\" + 0.031*\"good\" + 0.016*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"order\" + 0.011*\"buy\"\n",
      "INFO : topic #3 (0.205): 0.025*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"week\" + 0.014*\"pain\" + 0.014*\"try\" + 0.014*\"use\" + 0.014*\"product\" + 0.013*\"start\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.075): 0.053*\"hair\" + 0.025*\"nail\" + 0.020*\"grow\" + 0.018*\"eye\" + 0.018*\"naturewise\" + 0.016*\"notice\" + 0.015*\"skin\" + 0.012*\"use\" + 0.012*\"strong\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.116537, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.31049606, 0.37033635, 0.42633626, 0.21378326, 0.075731665]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.310): 0.019*\"help\" + 0.017*\"supplement\" + 0.012*\"good\" + 0.011*\"work\" + 0.011*\"day\" + 0.010*\"product\" + 0.010*\"try\" + 0.010*\"recommend\" + 0.009*\"feel\" + 0.008*\"year\"\n",
      "INFO : topic #1 (0.370): 0.032*\"good\" + 0.024*\"taste\" + 0.022*\"great\" + 0.018*\"like\" + 0.017*\"product\" + 0.016*\"easy\" + 0.014*\"supplement\" + 0.014*\"vitamin\" + 0.010*\"fish_oil\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.426): 0.088*\"product\" + 0.043*\"great\" + 0.032*\"use\" + 0.030*\"good\" + 0.017*\"skin\" + 0.016*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"order\" + 0.011*\"feel\"\n",
      "INFO : topic #3 (0.214): 0.025*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"week\" + 0.015*\"try\" + 0.014*\"product\" + 0.013*\"use\" + 0.013*\"pain\" + 0.013*\"start\" + 0.012*\"help\"\n",
      "INFO : topic #4 (0.076): 0.053*\"hair\" + 0.023*\"nail\" + 0.019*\"grow\" + 0.018*\"skin\" + 0.018*\"notice\" + 0.018*\"eye\" + 0.014*\"naturewise\" + 0.013*\"look\" + 0.012*\"use\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.119268, rho=0.173878\n",
      "INFO : PROGRESS: pass 1, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30848935, 0.36049446, 0.44262713, 0.22111745, 0.075161308]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.308): 0.020*\"help\" + 0.019*\"supplement\" + 0.012*\"good\" + 0.011*\"work\" + 0.010*\"day\" + 0.010*\"product\" + 0.010*\"try\" + 0.010*\"feel\" + 0.010*\"recommend\" + 0.009*\"probiotic\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.360): 0.033*\"good\" + 0.024*\"taste\" + 0.022*\"great\" + 0.018*\"like\" + 0.018*\"product\" + 0.017*\"easy\" + 0.015*\"supplement\" + 0.014*\"fish_oil\" + 0.013*\"vitamin\" + 0.011*\"pill\"\n",
      "INFO : topic #2 (0.443): 0.092*\"product\" + 0.044*\"great\" + 0.031*\"use\" + 0.030*\"good\" + 0.016*\"recommend\" + 0.016*\"work\" + 0.016*\"love\" + 0.016*\"skin\" + 0.013*\"feel\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.221): 0.024*\"work\" + 0.023*\"day\" + 0.022*\"feel\" + 0.016*\"try\" + 0.015*\"pain\" + 0.015*\"week\" + 0.014*\"product\" + 0.013*\"start\" + 0.013*\"use\" + 0.013*\"help\"\n",
      "INFO : topic #4 (0.075): 0.052*\"hair\" + 0.022*\"nail\" + 0.020*\"notice\" + 0.020*\"skin\" + 0.018*\"eye\" + 0.018*\"grow\" + 0.014*\"look\" + 0.013*\"naturewise\" + 0.012*\"use\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.119266, rho=0.173878\n",
      "INFO : PROGRESS: pass 2, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.2935712, 0.35446182, 0.44151226, 0.22936036, 0.080484331]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.294): 0.020*\"help\" + 0.018*\"supplement\" + 0.012*\"good\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"product\" + 0.010*\"try\" + 0.010*\"recommend\" + 0.009*\"feel\" + 0.009*\"probiotic\"\n",
      "INFO : topic #1 (0.354): 0.033*\"good\" + 0.022*\"great\" + 0.022*\"taste\" + 0.022*\"easy\" + 0.018*\"like\" + 0.017*\"product\" + 0.014*\"supplement\" + 0.012*\"vitamin\" + 0.012*\"fish_oil\" + 0.011*\"pill\"\n",
      "INFO : topic #2 (0.442): 0.088*\"product\" + 0.046*\"great\" + 0.033*\"use\" + 0.031*\"good\" + 0.018*\"love\" + 0.017*\"work\" + 0.017*\"recommend\" + 0.016*\"skin\" + 0.012*\"feel\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.229): 0.027*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.014*\"try\" + 0.014*\"use\" + 0.014*\"week\" + 0.013*\"pain\" + 0.013*\"product\" + 0.012*\"start\" + 0.012*\"help\"\n",
      "INFO : topic #4 (0.080): 0.074*\"pedometer\" + 0.027*\"hair\" + 0.019*\"step\" + 0.019*\"omron\" + 0.016*\"clip\" + 0.015*\"accurate\" + 0.015*\"use\" + 0.011*\"pocket\" + 0.011*\"nail\" + 0.011*\"notice\"\n",
      "INFO : topic diff=0.221422, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.30491313, 0.36207458, 0.45187292, 0.23100457, 0.080391474]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.305): 0.020*\"help\" + 0.017*\"supplement\" + 0.012*\"good\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"product\" + 0.010*\"try\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.009*\"feel\"\n",
      "INFO : topic #1 (0.362): 0.033*\"good\" + 0.025*\"taste\" + 0.022*\"great\" + 0.020*\"easy\" + 0.018*\"like\" + 0.016*\"product\" + 0.013*\"supplement\" + 0.013*\"vitamin\" + 0.010*\"use\" + 0.010*\"fish_oil\"\n",
      "INFO : topic #2 (0.452): 0.087*\"product\" + 0.046*\"great\" + 0.034*\"use\" + 0.031*\"good\" + 0.017*\"work\" + 0.017*\"love\" + 0.016*\"recommend\" + 0.015*\"skin\" + 0.012*\"buy\" + 0.011*\"feel\"\n",
      "INFO : topic #3 (0.231): 0.028*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.014*\"use\" + 0.014*\"try\" + 0.014*\"week\" + 0.014*\"pain\" + 0.012*\"start\" + 0.012*\"product\" + 0.012*\"help\"\n",
      "INFO : topic #4 (0.080): 0.066*\"pedometer\" + 0.035*\"hair\" + 0.017*\"step\" + 0.017*\"omron\" + 0.015*\"use\" + 0.015*\"clip\" + 0.014*\"accurate\" + 0.012*\"nail\" + 0.012*\"grow\" + 0.011*\"eye\"\n",
      "INFO : topic diff=0.119940, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3149412, 0.36400601, 0.46756572, 0.23359829, 0.080441415]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.315): 0.021*\"help\" + 0.016*\"supplement\" + 0.012*\"good\" + 0.012*\"work\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"try\" + 0.009*\"product\" + 0.009*\"recommend\" + 0.009*\"feel\"\n",
      "INFO : topic #1 (0.364): 0.033*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.019*\"easy\" + 0.018*\"like\" + 0.016*\"product\" + 0.013*\"vitamin\" + 0.013*\"supplement\" + 0.010*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.468): 0.085*\"product\" + 0.045*\"great\" + 0.035*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.017*\"love\" + 0.016*\"recommend\" + 0.015*\"skin\" + 0.011*\"buy\" + 0.011*\"feel\"\n",
      "INFO : topic #3 (0.234): 0.029*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.014*\"use\" + 0.014*\"pain\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"start\" + 0.013*\"help\" + 0.012*\"product\"\n",
      "INFO : topic #4 (0.080): 0.059*\"pedometer\" + 0.041*\"hair\" + 0.015*\"step\" + 0.015*\"omron\" + 0.015*\"use\" + 0.014*\"nail\" + 0.014*\"grow\" + 0.013*\"clip\" + 0.012*\"accurate\" + 0.011*\"eye\"\n",
      "INFO : topic diff=0.108502, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.32058987, 0.36826855, 0.48618034, 0.23601843, 0.080927573]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.321): 0.021*\"help\" + 0.015*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"try\" + 0.009*\"product\" + 0.009*\"recommend\" + 0.008*\"feel\"\n",
      "INFO : topic #1 (0.368): 0.033*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.018*\"easy\" + 0.015*\"product\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.486): 0.082*\"product\" + 0.045*\"great\" + 0.036*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.014*\"skin\" + 0.012*\"buy\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.236): 0.029*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.014*\"use\" + 0.014*\"pain\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"help\" + 0.013*\"start\" + 0.011*\"product\"\n",
      "INFO : topic #4 (0.081): 0.052*\"pedometer\" + 0.045*\"hair\" + 0.016*\"nail\" + 0.016*\"grow\" + 0.015*\"use\" + 0.014*\"step\" + 0.013*\"omron\" + 0.012*\"clip\" + 0.011*\"eye\" + 0.011*\"accurate\"\n",
      "INFO : topic diff=0.104525, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.32913107, 0.37218133, 0.49777868, 0.2396806, 0.081067234]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.329): 0.022*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"year\" + 0.009*\"try\" + 0.009*\"product\" + 0.009*\"recommend\" + 0.008*\"feel\"\n",
      "INFO : topic #1 (0.372): 0.033*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.017*\"easy\" + 0.015*\"product\" + 0.013*\"vitamin\" + 0.012*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.498): 0.082*\"product\" + 0.045*\"great\" + 0.036*\"use\" + 0.031*\"good\" + 0.018*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.014*\"skin\" + 0.012*\"buy\" + 0.011*\"price\"\n",
      "INFO : topic #3 (0.240): 0.029*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.014*\"use\" + 0.014*\"week\" + 0.014*\"pain\" + 0.013*\"try\" + 0.013*\"help\" + 0.013*\"start\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.081): 0.046*\"pedometer\" + 0.044*\"hair\" + 0.016*\"nail\" + 0.015*\"grow\" + 0.015*\"use\" + 0.013*\"eye\" + 0.012*\"step\" + 0.012*\"omron\" + 0.011*\"notice\" + 0.011*\"skin\"\n",
      "INFO : topic diff=0.102288, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.33802524, 0.37459913, 0.50751847, 0.24266085, 0.081455477]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.338): 0.022*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"day\" + 0.011*\"year\" + 0.009*\"try\" + 0.009*\"product\" + 0.009*\"recommend\" + 0.009*\"doctor\"\n",
      "INFO : topic #1 (0.375): 0.034*\"good\" + 0.025*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"easy\" + 0.014*\"product\" + 0.012*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.508): 0.083*\"product\" + 0.044*\"great\" + 0.036*\"use\" + 0.032*\"good\" + 0.019*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"buy\" + 0.012*\"price\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #3 (0.243): 0.029*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.015*\"pain\" + 0.014*\"use\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.081): 0.049*\"hair\" + 0.039*\"pedometer\" + 0.018*\"nail\" + 0.016*\"eye\" + 0.016*\"grow\" + 0.015*\"use\" + 0.011*\"notice\" + 0.011*\"skin\" + 0.010*\"step\" + 0.010*\"omron\"\n",
      "INFO : topic diff=0.097050, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.3435728, 0.38023067, 0.5165931, 0.24776863, 0.08119566]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.344): 0.021*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"doctor\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.380): 0.034*\"good\" + 0.026*\"taste\" + 0.021*\"great\" + 0.018*\"like\" + 0.016*\"easy\" + 0.014*\"product\" + 0.013*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.517): 0.083*\"product\" + 0.044*\"great\" + 0.037*\"use\" + 0.032*\"good\" + 0.019*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"buy\" + 0.012*\"price\"\n",
      "INFO : topic #3 (0.248): 0.029*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.015*\"pain\" + 0.014*\"use\" + 0.013*\"week\" + 0.013*\"try\" + 0.013*\"start\" + 0.013*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.081): 0.049*\"hair\" + 0.035*\"pedometer\" + 0.019*\"nail\" + 0.017*\"grow\" + 0.016*\"eye\" + 0.015*\"use\" + 0.011*\"skin\" + 0.011*\"notice\" + 0.010*\"month\" + 0.009*\"step\"\n",
      "INFO : topic diff=0.101062, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.35631526, 0.38220969, 0.52342159, 0.25345767, 0.080885828]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.356): 0.021*\"help\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.009*\"try\" + 0.009*\"doctor\" + 0.009*\"recommend\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.382): 0.034*\"good\" + 0.027*\"taste\" + 0.020*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"product\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.523): 0.085*\"product\" + 0.045*\"great\" + 0.038*\"use\" + 0.032*\"good\" + 0.020*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.013*\"skin\" + 0.012*\"price\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.253): 0.029*\"day\" + 0.026*\"work\" + 0.017*\"feel\" + 0.015*\"pain\" + 0.014*\"use\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"start\" + 0.013*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.081): 0.047*\"hair\" + 0.032*\"pedometer\" + 0.018*\"nail\" + 0.017*\"grow\" + 0.016*\"eye\" + 0.015*\"use\" + 0.011*\"notice\" + 0.011*\"skin\" + 0.009*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.104373, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 6999/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.37021521, 0.38598207, 0.53520715, 0.25576875, 0.080724008]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.370): 0.021*\"help\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.009*\"doctor\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.386): 0.034*\"good\" + 0.027*\"taste\" + 0.020*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"product\" + 0.012*\"vitamin\" + 0.011*\"use\" + 0.011*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.535): 0.085*\"product\" + 0.045*\"great\" + 0.037*\"use\" + 0.033*\"good\" + 0.020*\"work\" + 0.016*\"love\" + 0.015*\"recommend\" + 0.012*\"price\" + 0.012*\"skin\" + 0.012*\"buy\"\n",
      "INFO : topic #3 (0.256): 0.029*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.015*\"pain\" + 0.014*\"use\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"start\" + 0.013*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.081): 0.048*\"hair\" + 0.028*\"pedometer\" + 0.018*\"nail\" + 0.018*\"grow\" + 0.016*\"eye\" + 0.014*\"use\" + 0.011*\"notice\" + 0.011*\"skin\" + 0.009*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.093810, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.38273185, 0.38423917, 0.54994518, 0.26130348, 0.081433199]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.383): 0.021*\"help\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.384): 0.034*\"good\" + 0.026*\"taste\" + 0.020*\"great\" + 0.018*\"like\" + 0.015*\"easy\" + 0.014*\"product\" + 0.012*\"vitamin\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.550): 0.085*\"product\" + 0.045*\"great\" + 0.038*\"use\" + 0.034*\"good\" + 0.020*\"work\" + 0.016*\"love\" + 0.016*\"recommend\" + 0.012*\"price\" + 0.012*\"buy\" + 0.012*\"skin\"\n",
      "INFO : topic #3 (0.261): 0.030*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.014*\"week\" + 0.014*\"start\" + 0.013*\"use\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.081): 0.053*\"hair\" + 0.022*\"pedometer\" + 0.018*\"grow\" + 0.018*\"nail\" + 0.016*\"eye\" + 0.014*\"use\" + 0.012*\"notice\" + 0.010*\"skin\" + 0.010*\"month\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.103046, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.39112994, 0.39228359, 0.56119627, 0.26457021, 0.081643529]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.391): 0.021*\"help\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.012*\"year\" + 0.012*\"good\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.392): 0.034*\"good\" + 0.027*\"taste\" + 0.019*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.013*\"product\" + 0.012*\"use\" + 0.012*\"vitamin\" + 0.010*\"brand\" + 0.010*\"supplement\"\n",
      "INFO : topic #2 (0.561): 0.085*\"product\" + 0.045*\"great\" + 0.039*\"use\" + 0.034*\"good\" + 0.020*\"work\" + 0.016*\"love\" + 0.016*\"recommend\" + 0.013*\"price\" + 0.012*\"buy\" + 0.011*\"skin\"\n",
      "INFO : topic #3 (0.265): 0.030*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.016*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"use\" + 0.013*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.082): 0.052*\"hair\" + 0.019*\"pedometer\" + 0.018*\"nail\" + 0.018*\"grow\" + 0.017*\"eye\" + 0.014*\"use\" + 0.011*\"notice\" + 0.010*\"skin\" + 0.010*\"month\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.094220, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.4037894, 0.39448521, 0.57299459, 0.2684797, 0.082236871]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.404): 0.021*\"help\" + 0.013*\"supplement\" + 0.012*\"work\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.009*\"product\"\n",
      "INFO : topic #1 (0.394): 0.035*\"good\" + 0.027*\"taste\" + 0.019*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.013*\"product\" + 0.012*\"vitamin\" + 0.012*\"use\" + 0.011*\"supplement\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.573): 0.086*\"product\" + 0.045*\"great\" + 0.039*\"use\" + 0.035*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.015*\"love\" + 0.013*\"price\" + 0.012*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.268): 0.029*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.017*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"use\" + 0.013*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.082): 0.059*\"hair\" + 0.020*\"grow\" + 0.019*\"nail\" + 0.016*\"eye\" + 0.015*\"pedometer\" + 0.014*\"use\" + 0.012*\"notice\" + 0.011*\"month\" + 0.011*\"skin\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.091765, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #91000/217530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.4168134, 0.40069965, 0.57897353, 0.27179897, 0.082858197]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.417): 0.021*\"help\" + 0.015*\"supplement\" + 0.012*\"work\" + 0.011*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.009*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"product\"\n",
      "INFO : topic #1 (0.401): 0.035*\"good\" + 0.025*\"taste\" + 0.019*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.013*\"vitamin\" + 0.012*\"product\" + 0.012*\"use\" + 0.011*\"supplement\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.579): 0.087*\"product\" + 0.046*\"great\" + 0.039*\"use\" + 0.035*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.015*\"love\" + 0.014*\"price\" + 0.013*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.272): 0.030*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.017*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"help\" + 0.013*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.083): 0.055*\"hair\" + 0.022*\"eye\" + 0.020*\"nail\" + 0.019*\"grow\" + 0.013*\"use\" + 0.013*\"notice\" + 0.012*\"pedometer\" + 0.011*\"skin\" + 0.011*\"month\" + 0.009*\"lutein\"\n",
      "INFO : topic diff=0.097381, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.42173147, 0.41177726, 0.59305567, 0.27355847, 0.083492354]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.422): 0.021*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.011*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"product\"\n",
      "INFO : topic #1 (0.412): 0.034*\"good\" + 0.025*\"taste\" + 0.019*\"great\" + 0.018*\"like\" + 0.014*\"easy\" + 0.014*\"vitamin\" + 0.012*\"use\" + 0.012*\"product\" + 0.011*\"brand\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.593): 0.087*\"product\" + 0.047*\"great\" + 0.039*\"use\" + 0.035*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.015*\"love\" + 0.014*\"price\" + 0.013*\"buy\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.274): 0.030*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.017*\"pain\" + 0.014*\"start\" + 0.013*\"week\" + 0.013*\"try\" + 0.013*\"help\" + 0.013*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.083): 0.054*\"hair\" + 0.027*\"eye\" + 0.018*\"grow\" + 0.018*\"nail\" + 0.013*\"use\" + 0.012*\"notice\" + 0.011*\"pedometer\" + 0.011*\"skin\" + 0.011*\"month\" + 0.008*\"lutein\"\n",
      "INFO : topic diff=0.085485, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.43194473, 0.41634622, 0.60734439, 0.27967578, 0.083682336]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.432): 0.021*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.011*\"good\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"product\"\n",
      "INFO : topic #1 (0.416): 0.034*\"good\" + 0.026*\"taste\" + 0.019*\"great\" + 0.018*\"like\" + 0.014*\"vitamin\" + 0.014*\"easy\" + 0.012*\"use\" + 0.012*\"product\" + 0.011*\"brand\" + 0.011*\"supplement\"\n",
      "INFO : topic #2 (0.607): 0.086*\"product\" + 0.047*\"great\" + 0.039*\"use\" + 0.036*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.015*\"love\" + 0.014*\"price\" + 0.013*\"buy\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.280): 0.030*\"day\" + 0.025*\"work\" + 0.018*\"feel\" + 0.017*\"pain\" + 0.014*\"start\" + 0.013*\"try\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.084): 0.053*\"hair\" + 0.026*\"eye\" + 0.019*\"nail\" + 0.018*\"grow\" + 0.013*\"use\" + 0.012*\"notice\" + 0.011*\"skin\" + 0.010*\"month\" + 0.009*\"pedometer\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.090264, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.44204938, 0.43177989, 0.62025416, 0.28063643, 0.084276602]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.442): 0.021*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.011*\"year\" + 0.011*\"good\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.432): 0.034*\"good\" + 0.025*\"taste\" + 0.018*\"great\" + 0.018*\"like\" + 0.015*\"vitamin\" + 0.015*\"easy\" + 0.011*\"supplement\" + 0.011*\"use\" + 0.011*\"product\" + 0.011*\"brand\"\n",
      "INFO : topic #2 (0.620): 0.086*\"product\" + 0.047*\"great\" + 0.039*\"use\" + 0.036*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.015*\"love\" + 0.015*\"price\" + 0.013*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.281): 0.030*\"day\" + 0.026*\"work\" + 0.018*\"feel\" + 0.017*\"pain\" + 0.014*\"start\" + 0.014*\"try\" + 0.013*\"week\" + 0.013*\"help\" + 0.013*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.084): 0.052*\"hair\" + 0.028*\"eye\" + 0.019*\"nail\" + 0.018*\"grow\" + 0.013*\"use\" + 0.012*\"notice\" + 0.011*\"skin\" + 0.010*\"month\" + 0.008*\"strong\" + 0.008*\"lutein\"\n",
      "INFO : topic diff=0.089639, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.44842479, 0.44239777, 0.62998307, 0.28210914, 0.084455281]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.448): 0.021*\"help\" + 0.014*\"supplement\" + 0.012*\"work\" + 0.011*\"year\" + 0.011*\"good\" + 0.011*\"day\" + 0.010*\"doctor\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.442): 0.034*\"good\" + 0.028*\"taste\" + 0.018*\"like\" + 0.018*\"great\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.011*\"supplement\" + 0.011*\"brand\" + 0.011*\"product\" + 0.011*\"use\"\n",
      "INFO : topic #2 (0.630): 0.087*\"product\" + 0.047*\"great\" + 0.038*\"use\" + 0.037*\"good\" + 0.020*\"work\" + 0.016*\"recommend\" + 0.016*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.282): 0.030*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.016*\"pain\" + 0.014*\"start\" + 0.014*\"try\" + 0.014*\"week\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.084): 0.053*\"hair\" + 0.027*\"eye\" + 0.022*\"nail\" + 0.020*\"grow\" + 0.013*\"notice\" + 0.013*\"use\" + 0.012*\"skin\" + 0.010*\"month\" + 0.009*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.084001, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.45139551, 0.46275753, 0.6456095, 0.28293025, 0.084891327]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.451): 0.021*\"help\" + 0.015*\"supplement\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.011*\"day\" + 0.011*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.463): 0.034*\"good\" + 0.028*\"taste\" + 0.018*\"like\" + 0.018*\"great\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.011*\"supplement\" + 0.011*\"brand\" + 0.010*\"product\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.646): 0.087*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.037*\"use\" + 0.020*\"work\" + 0.017*\"recommend\" + 0.016*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.283): 0.030*\"day\" + 0.025*\"work\" + 0.019*\"feel\" + 0.016*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.085): 0.055*\"hair\" + 0.026*\"eye\" + 0.025*\"nail\" + 0.020*\"grow\" + 0.013*\"notice\" + 0.013*\"skin\" + 0.012*\"use\" + 0.010*\"month\" + 0.010*\"strong\" + 0.008*\"look\"\n",
      "INFO : topic diff=0.084083, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.45156422, 0.47804701, 0.66227782, 0.28274193, 0.085722454]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.452): 0.021*\"help\" + 0.014*\"supplement\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.011*\"day\" + 0.011*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.478): 0.034*\"good\" + 0.030*\"taste\" + 0.018*\"like\" + 0.018*\"great\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.011*\"brand\" + 0.011*\"supplement\" + 0.011*\"fish_oil\" + 0.010*\"product\"\n",
      "INFO : topic #2 (0.662): 0.087*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.038*\"use\" + 0.020*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.283): 0.030*\"day\" + 0.026*\"work\" + 0.019*\"feel\" + 0.016*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.086): 0.057*\"hair\" + 0.025*\"eye\" + 0.023*\"nail\" + 0.020*\"grow\" + 0.015*\"skin\" + 0.013*\"notice\" + 0.012*\"use\" + 0.010*\"month\" + 0.010*\"coconut_oil\" + 0.010*\"strong\"\n",
      "INFO : topic diff=0.085676, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.46319792, 0.4829013, 0.68407869, 0.28952405, 0.086589858]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.463): 0.021*\"help\" + 0.014*\"supplement\" + 0.011*\"work\" + 0.011*\"doctor\" + 0.011*\"year\" + 0.011*\"day\" + 0.011*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.009*\"'s\"\n",
      "INFO : topic #1 (0.483): 0.033*\"good\" + 0.030*\"taste\" + 0.018*\"like\" + 0.017*\"great\" + 0.016*\"vitamin\" + 0.015*\"easy\" + 0.011*\"brand\" + 0.011*\"supplement\" + 0.010*\"fish_oil\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.684): 0.087*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.290): 0.031*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.087): 0.054*\"hair\" + 0.024*\"eye\" + 0.021*\"nail\" + 0.020*\"grow\" + 0.015*\"skin\" + 0.013*\"notice\" + 0.012*\"use\" + 0.011*\"gaia\" + 0.010*\"month\" + 0.010*\"strong\"\n",
      "INFO : topic diff=0.090239, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.46906471, 0.49515536, 0.69896525, 0.29333949, 0.086828627]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.469): 0.020*\"help\" + 0.014*\"supplement\" + 0.012*\"doctor\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.009*\"'s\"\n",
      "INFO : topic #1 (0.495): 0.033*\"good\" + 0.030*\"taste\" + 0.019*\"like\" + 0.017*\"great\" + 0.016*\"vitamin\" + 0.016*\"easy\" + 0.011*\"brand\" + 0.010*\"supplement\" + 0.010*\"fish_oil\" + 0.010*\"use\"\n",
      "INFO : topic #2 (0.699): 0.087*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.293): 0.031*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.013*\"try\" + 0.013*\"help\" + 0.012*\"use\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.087): 0.050*\"hair\" + 0.024*\"eye\" + 0.021*\"nail\" + 0.020*\"grow\" + 0.015*\"skin\" + 0.013*\"notice\" + 0.012*\"use\" + 0.010*\"month\" + 0.010*\"strong\" + 0.010*\"gaia\"\n",
      "INFO : topic diff=0.086017, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.47626412, 0.50551337, 0.71386606, 0.29797706, 0.087501578]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.476): 0.020*\"help\" + 0.014*\"supplement\" + 0.012*\"doctor\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.506): 0.032*\"good\" + 0.030*\"taste\" + 0.019*\"like\" + 0.017*\"great\" + 0.016*\"easy\" + 0.015*\"vitamin\" + 0.010*\"supplement\" + 0.010*\"brand\" + 0.010*\"use\" + 0.009*\"pill\"\n",
      "INFO : topic #2 (0.714): 0.088*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.038*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.298): 0.030*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"pain\" + 0.014*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"use\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.088): 0.051*\"hair\" + 0.028*\"eye\" + 0.021*\"nail\" + 0.021*\"grow\" + 0.015*\"skin\" + 0.013*\"notice\" + 0.012*\"use\" + 0.010*\"month\" + 0.010*\"strong\" + 0.008*\"gaia\"\n",
      "INFO : topic diff=0.089132, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.47864264, 0.50440758, 0.72682905, 0.29791579, 0.089935713]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.479): 0.020*\"help\" + 0.015*\"supplement\" + 0.012*\"doctor\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.504): 0.032*\"good\" + 0.029*\"taste\" + 0.019*\"like\" + 0.017*\"great\" + 0.016*\"easy\" + 0.016*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"pill\" + 0.009*\"fish_oil\"\n",
      "INFO : topic #2 (0.727): 0.089*\"product\" + 0.048*\"great\" + 0.038*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.298): 0.031*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"pain\" + 0.015*\"start\" + 0.015*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"use\"\n",
      "INFO : topic #4 (0.090): 0.076*\"hair\" + 0.034*\"nail\" + 0.029*\"grow\" + 0.021*\"eye\" + 0.017*\"skin\" + 0.014*\"notice\" + 0.013*\"biotin\" + 0.013*\"month\" + 0.012*\"strong\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.124768, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.49024028, 0.51430362, 0.74322098, 0.30173421, 0.090784423]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.490): 0.020*\"help\" + 0.015*\"supplement\" + 0.012*\"doctor\" + 0.011*\"work\" + 0.011*\"year\" + 0.011*\"day\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.514): 0.032*\"good\" + 0.029*\"taste\" + 0.019*\"like\" + 0.016*\"great\" + 0.016*\"vitamin\" + 0.016*\"easy\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"pill\" + 0.010*\"fish_oil\"\n",
      "INFO : topic #2 (0.743): 0.090*\"product\" + 0.048*\"great\" + 0.039*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.016*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.302): 0.031*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.016*\"pain\" + 0.015*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"use\"\n",
      "INFO : topic #4 (0.091): 0.074*\"hair\" + 0.034*\"nail\" + 0.029*\"grow\" + 0.021*\"eye\" + 0.016*\"skin\" + 0.014*\"notice\" + 0.013*\"biotin\" + 0.013*\"month\" + 0.013*\"strong\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.089263, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.50144792, 0.52172279, 0.77046442, 0.30607957, 0.091228731]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.501): 0.020*\"help\" + 0.015*\"supplement\" + 0.012*\"doctor\" + 0.011*\"work\" + 0.010*\"year\" + 0.010*\"day\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.522): 0.032*\"good\" + 0.029*\"taste\" + 0.019*\"like\" + 0.017*\"vitamin\" + 0.017*\"easy\" + 0.016*\"great\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"brand\" + 0.010*\"fish_oil\"\n",
      "INFO : topic #2 (0.770): 0.090*\"product\" + 0.048*\"great\" + 0.039*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.306): 0.031*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.016*\"pain\" + 0.015*\"start\" + 0.014*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.011*\"time\" + 0.011*\"use\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.091): 0.070*\"hair\" + 0.033*\"nail\" + 0.027*\"grow\" + 0.021*\"eye\" + 0.018*\"skin\" + 0.014*\"notice\" + 0.014*\"biotin\" + 0.013*\"strong\" + 0.012*\"month\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.089441, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.50707346, 0.529697, 0.78378522, 0.31303346, 0.091632359]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.507): 0.020*\"help\" + 0.016*\"supplement\" + 0.011*\"doctor\" + 0.011*\"work\" + 0.010*\"year\" + 0.010*\"day\" + 0.010*\"good\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.530): 0.031*\"good\" + 0.029*\"taste\" + 0.019*\"like\" + 0.017*\"easy\" + 0.017*\"vitamin\" + 0.016*\"great\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"brand\" + 0.009*\"fish_oil\"\n",
      "INFO : topic #2 (0.784): 0.091*\"product\" + 0.049*\"great\" + 0.039*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.313): 0.030*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.015*\"week\" + 0.015*\"pain\" + 0.014*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"energy\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.092): 0.069*\"hair\" + 0.033*\"nail\" + 0.027*\"grow\" + 0.021*\"eye\" + 0.019*\"skin\" + 0.014*\"notice\" + 0.013*\"strong\" + 0.012*\"biotin\" + 0.012*\"month\" + 0.011*\"use\"\n",
      "INFO : topic diff=0.095436, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.51015514, 0.53387702, 0.7972753, 0.32294667, 0.09239161]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.510): 0.020*\"help\" + 0.016*\"supplement\" + 0.011*\"doctor\" + 0.011*\"work\" + 0.010*\"day\" + 0.010*\"year\" + 0.010*\"good\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.534): 0.031*\"good\" + 0.030*\"taste\" + 0.020*\"like\" + 0.017*\"easy\" + 0.016*\"great\" + 0.016*\"vitamin\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"brand\" + 0.009*\"fish_oil\"\n",
      "INFO : topic #2 (0.797): 0.093*\"product\" + 0.050*\"great\" + 0.039*\"good\" + 0.036*\"use\" + 0.020*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.323): 0.030*\"day\" + 0.026*\"work\" + 0.021*\"feel\" + 0.015*\"week\" + 0.014*\"pain\" + 0.014*\"try\" + 0.014*\"start\" + 0.013*\"help\" + 0.012*\"energy\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.092): 0.069*\"hair\" + 0.032*\"nail\" + 0.027*\"grow\" + 0.020*\"eye\" + 0.019*\"skin\" + 0.015*\"notice\" + 0.013*\"strong\" + 0.013*\"biotin\" + 0.012*\"month\" + 0.011*\"use\"\n",
      "INFO : topic diff=0.089645, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.51601273, 0.53835398, 0.81097686, 0.3340911, 0.092876077]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.516): 0.020*\"help\" + 0.016*\"supplement\" + 0.011*\"doctor\" + 0.011*\"work\" + 0.010*\"day\" + 0.010*\"year\" + 0.010*\"good\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.538): 0.031*\"good\" + 0.030*\"taste\" + 0.020*\"like\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.016*\"great\" + 0.012*\"supplement\" + 0.010*\"pill\" + 0.009*\"brand\" + 0.009*\"fish_oil\"\n",
      "INFO : topic #2 (0.811): 0.093*\"product\" + 0.050*\"great\" + 0.040*\"good\" + 0.036*\"use\" + 0.020*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.334): 0.029*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.015*\"week\" + 0.014*\"pain\" + 0.014*\"try\" + 0.014*\"start\" + 0.013*\"energy\" + 0.012*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.093): 0.066*\"hair\" + 0.030*\"nail\" + 0.025*\"grow\" + 0.021*\"eye\" + 0.020*\"skin\" + 0.015*\"notice\" + 0.013*\"strong\" + 0.013*\"biotin\" + 0.012*\"month\" + 0.011*\"use\"\n",
      "INFO : topic diff=0.094295, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.51848084, 0.5413413, 0.83419728, 0.34037796, 0.093834713]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.518): 0.020*\"help\" + 0.017*\"supplement\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.010*\"day\" + 0.010*\"good\" + 0.010*\"year\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.541): 0.030*\"good\" + 0.029*\"taste\" + 0.020*\"like\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.016*\"great\" + 0.012*\"supplement\" + 0.012*\"fish_oil\" + 0.010*\"pill\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.834): 0.093*\"product\" + 0.049*\"great\" + 0.040*\"good\" + 0.036*\"use\" + 0.019*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.014*\"price\" + 0.014*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.340): 0.029*\"day\" + 0.025*\"work\" + 0.022*\"feel\" + 0.016*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.014*\"start\" + 0.013*\"energy\" + 0.012*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.094): 0.063*\"hair\" + 0.030*\"nail\" + 0.025*\"skin\" + 0.024*\"grow\" + 0.022*\"eye\" + 0.015*\"notice\" + 0.013*\"biotin\" + 0.013*\"strong\" + 0.012*\"month\" + 0.011*\"look\"\n",
      "INFO : topic diff=0.093992, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.5257355, 0.55302769, 0.858518, 0.35010332, 0.095164649]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.526): 0.020*\"help\" + 0.019*\"supplement\" + 0.010*\"doctor\" + 0.010*\"work\" + 0.010*\"day\" + 0.009*\"good\" + 0.009*\"year\" + 0.009*\"recommend\" + 0.009*\"try\" + 0.008*\"probiotic\"\n",
      "INFO : topic #1 (0.553): 0.029*\"good\" + 0.028*\"taste\" + 0.020*\"like\" + 0.018*\"easy\" + 0.017*\"vitamin\" + 0.015*\"great\" + 0.014*\"supplement\" + 0.012*\"fish_oil\" + 0.011*\"pill\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.859): 0.095*\"product\" + 0.049*\"great\" + 0.038*\"good\" + 0.035*\"use\" + 0.018*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.013*\"price\" + 0.013*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.350): 0.029*\"day\" + 0.024*\"work\" + 0.023*\"feel\" + 0.016*\"week\" + 0.014*\"try\" + 0.014*\"pain\" + 0.014*\"start\" + 0.014*\"energy\" + 0.012*\"help\" + 0.010*\"use\"\n",
      "INFO : topic #4 (0.095): 0.057*\"hair\" + 0.027*\"nail\" + 0.027*\"skin\" + 0.022*\"grow\" + 0.021*\"eye\" + 0.019*\"naturewise\" + 0.017*\"notice\" + 0.013*\"strong\" + 0.012*\"month\" + 0.012*\"biotin\"\n",
      "INFO : topic diff=0.102846, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.5272668, 0.542216, 0.88403708, 0.36630422, 0.096433029]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.527): 0.020*\"supplement\" + 0.020*\"help\" + 0.010*\"doctor\" + 0.010*\"day\" + 0.009*\"work\" + 0.009*\"good\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"probiotic\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.542): 0.028*\"good\" + 0.027*\"taste\" + 0.020*\"like\" + 0.019*\"easy\" + 0.016*\"vitamin\" + 0.015*\"great\" + 0.014*\"supplement\" + 0.012*\"fish_oil\" + 0.011*\"pill\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.884): 0.095*\"product\" + 0.048*\"great\" + 0.037*\"good\" + 0.034*\"use\" + 0.017*\"work\" + 0.017*\"recommend\" + 0.016*\"love\" + 0.013*\"buy\" + 0.012*\"price\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.366): 0.028*\"day\" + 0.024*\"work\" + 0.024*\"feel\" + 0.017*\"week\" + 0.015*\"try\" + 0.015*\"start\" + 0.014*\"energy\" + 0.013*\"pain\" + 0.012*\"help\" + 0.011*\"notice\"\n",
      "INFO : topic #4 (0.096): 0.056*\"hair\" + 0.036*\"skin\" + 0.024*\"nail\" + 0.021*\"eye\" + 0.020*\"grow\" + 0.019*\"notice\" + 0.015*\"naturewise\" + 0.014*\"look\" + 0.012*\"strong\" + 0.012*\"month\"\n",
      "INFO : topic diff=0.107526, rho=0.171308\n",
      "INFO : PROGRESS: pass 2, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : optimized alpha [0.52648866, 0.52974385, 0.91011214, 0.38191921, 0.09584409]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.526): 0.023*\"supplement\" + 0.021*\"help\" + 0.010*\"probiotic\" + 0.010*\"good\" + 0.010*\"try\" + 0.010*\"doctor\" + 0.010*\"work\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.009*\"health\"\n",
      "INFO : topic #1 (0.530): 0.028*\"good\" + 0.027*\"taste\" + 0.020*\"like\" + 0.019*\"easy\" + 0.016*\"supplement\" + 0.015*\"great\" + 0.015*\"fish_oil\" + 0.015*\"vitamin\" + 0.013*\"pill\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.910): 0.098*\"product\" + 0.049*\"great\" + 0.037*\"good\" + 0.033*\"use\" + 0.018*\"recommend\" + 0.018*\"work\" + 0.017*\"love\" + 0.012*\"buy\" + 0.012*\"feel\" + 0.012*\"price\"\n",
      "INFO : topic #3 (0.382): 0.026*\"day\" + 0.026*\"feel\" + 0.023*\"work\" + 0.017*\"week\" + 0.016*\"try\" + 0.016*\"pain\" + 0.015*\"start\" + 0.014*\"energy\" + 0.013*\"help\" + 0.011*\"notice\"\n",
      "INFO : topic #4 (0.096): 0.054*\"hair\" + 0.039*\"skin\" + 0.023*\"nail\" + 0.022*\"eye\" + 0.021*\"notice\" + 0.019*\"grow\" + 0.015*\"look\" + 0.013*\"naturewise\" + 0.011*\"strong\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.106639, rho=0.171308\n",
      "INFO : PROGRESS: pass 3, at document #7000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.50256038, 0.51789081, 0.90007925, 0.39554074, 0.10300027]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.503): 0.021*\"supplement\" + 0.021*\"help\" + 0.010*\"work\" + 0.010*\"good\" + 0.010*\"probiotic\" + 0.010*\"day\" + 0.009*\"doctor\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.009*\"year\"\n",
      "INFO : topic #1 (0.518): 0.029*\"good\" + 0.026*\"easy\" + 0.025*\"taste\" + 0.020*\"like\" + 0.015*\"great\" + 0.014*\"supplement\" + 0.014*\"vitamin\" + 0.014*\"fish_oil\" + 0.013*\"pill\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.900): 0.092*\"product\" + 0.050*\"great\" + 0.038*\"good\" + 0.035*\"use\" + 0.019*\"love\" + 0.019*\"work\" + 0.018*\"recommend\" + 0.014*\"buy\" + 0.013*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.396): 0.031*\"day\" + 0.024*\"work\" + 0.022*\"feel\" + 0.016*\"week\" + 0.015*\"try\" + 0.014*\"pain\" + 0.014*\"start\" + 0.012*\"help\" + 0.012*\"energy\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.103): 0.080*\"pedometer\" + 0.029*\"hair\" + 0.023*\"step\" + 0.021*\"skin\" + 0.020*\"omron\" + 0.017*\"clip\" + 0.016*\"accurate\" + 0.014*\"pocket\" + 0.012*\"eye\" + 0.012*\"use\"\n",
      "INFO : topic diff=0.205968, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #14000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.52241844, 0.52933544, 0.91252619, 0.40117079, 0.10295732]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.522): 0.021*\"help\" + 0.020*\"supplement\" + 0.010*\"work\" + 0.010*\"good\" + 0.010*\"year\" + 0.009*\"day\" + 0.009*\"doctor\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"probiotic\"\n",
      "INFO : topic #1 (0.529): 0.029*\"good\" + 0.028*\"taste\" + 0.023*\"easy\" + 0.020*\"like\" + 0.015*\"vitamin\" + 0.015*\"great\" + 0.013*\"supplement\" + 0.012*\"pill\" + 0.012*\"fish_oil\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.913): 0.091*\"product\" + 0.050*\"great\" + 0.038*\"good\" + 0.036*\"use\" + 0.019*\"work\" + 0.018*\"love\" + 0.018*\"recommend\" + 0.014*\"buy\" + 0.013*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.401): 0.031*\"day\" + 0.024*\"work\" + 0.022*\"feel\" + 0.016*\"week\" + 0.014*\"try\" + 0.014*\"pain\" + 0.014*\"start\" + 0.012*\"help\" + 0.012*\"energy\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.103): 0.072*\"pedometer\" + 0.037*\"hair\" + 0.021*\"skin\" + 0.021*\"step\" + 0.018*\"omron\" + 0.015*\"clip\" + 0.014*\"accurate\" + 0.013*\"eye\" + 0.013*\"nail\" + 0.013*\"grow\"\n",
      "INFO : topic diff=0.109338, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #21000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.54121912, 0.5331977, 0.93521976, 0.40937364, 0.1031094]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.541): 0.022*\"help\" + 0.019*\"supplement\" + 0.010*\"year\" + 0.010*\"work\" + 0.010*\"day\" + 0.010*\"good\" + 0.009*\"doctor\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.533): 0.029*\"good\" + 0.028*\"taste\" + 0.022*\"easy\" + 0.020*\"like\" + 0.015*\"vitamin\" + 0.015*\"great\" + 0.013*\"supplement\" + 0.011*\"pill\" + 0.010*\"fish_oil\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.935): 0.089*\"product\" + 0.049*\"great\" + 0.038*\"good\" + 0.037*\"use\" + 0.019*\"work\" + 0.018*\"love\" + 0.018*\"recommend\" + 0.014*\"buy\" + 0.013*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.409): 0.032*\"day\" + 0.025*\"work\" + 0.022*\"feel\" + 0.016*\"week\" + 0.014*\"pain\" + 0.014*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.011*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.103): 0.064*\"pedometer\" + 0.044*\"hair\" + 0.021*\"skin\" + 0.019*\"step\" + 0.016*\"omron\" + 0.015*\"nail\" + 0.015*\"grow\" + 0.014*\"clip\" + 0.014*\"eye\" + 0.013*\"accurate\"\n",
      "INFO : topic diff=0.097851, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #28000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.55115169, 0.53982913, 0.96299934, 0.41618836, 0.10382343]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.551): 0.022*\"help\" + 0.018*\"supplement\" + 0.010*\"year\" + 0.010*\"work\" + 0.010*\"day\" + 0.009*\"good\" + 0.009*\"doctor\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"stomach\"\n",
      "INFO : topic #1 (0.540): 0.028*\"good\" + 0.028*\"taste\" + 0.021*\"easy\" + 0.020*\"like\" + 0.015*\"vitamin\" + 0.014*\"great\" + 0.012*\"supplement\" + 0.011*\"pill\" + 0.010*\"brand\" + 0.009*\"fish_oil\"\n",
      "INFO : topic #2 (0.963): 0.087*\"product\" + 0.049*\"great\" + 0.038*\"use\" + 0.038*\"good\" + 0.020*\"work\" + 0.017*\"love\" + 0.017*\"recommend\" + 0.014*\"buy\" + 0.013*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.416): 0.032*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.015*\"week\" + 0.014*\"start\" + 0.014*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.011*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.104): 0.057*\"pedometer\" + 0.048*\"hair\" + 0.022*\"skin\" + 0.017*\"nail\" + 0.016*\"step\" + 0.016*\"grow\" + 0.014*\"eye\" + 0.014*\"omron\" + 0.012*\"use\" + 0.012*\"clip\"\n",
      "INFO : topic diff=0.093972, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #35000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.56568557, 0.54568422, 0.97678357, 0.42563352, 0.1040479]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.566): 0.023*\"help\" + 0.017*\"supplement\" + 0.011*\"year\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"doctor\" + 0.009*\"good\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"stomach\"\n",
      "INFO : topic #1 (0.546): 0.029*\"good\" + 0.028*\"taste\" + 0.020*\"like\" + 0.020*\"easy\" + 0.015*\"vitamin\" + 0.014*\"great\" + 0.012*\"supplement\" + 0.011*\"pill\" + 0.010*\"brand\" + 0.009*\"capsule\"\n",
      "INFO : topic #2 (0.977): 0.086*\"product\" + 0.049*\"great\" + 0.038*\"use\" + 0.038*\"good\" + 0.020*\"work\" + 0.017*\"love\" + 0.017*\"recommend\" + 0.014*\"buy\" + 0.014*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.426): 0.032*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"week\" + 0.015*\"start\" + 0.014*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.104): 0.051*\"pedometer\" + 0.047*\"hair\" + 0.022*\"skin\" + 0.017*\"nail\" + 0.016*\"grow\" + 0.016*\"eye\" + 0.015*\"step\" + 0.013*\"omron\" + 0.012*\"use\" + 0.011*\"notice\"\n",
      "INFO : topic diff=0.093170, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #42000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.57994717, 0.54852968, 0.98532826, 0.43216124, 0.10458305]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.580): 0.023*\"help\" + 0.016*\"supplement\" + 0.011*\"year\" + 0.010*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"'s\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.549): 0.029*\"good\" + 0.028*\"taste\" + 0.020*\"like\" + 0.019*\"easy\" + 0.015*\"vitamin\" + 0.014*\"great\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"water\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (0.985): 0.087*\"product\" + 0.049*\"great\" + 0.039*\"use\" + 0.039*\"good\" + 0.020*\"work\" + 0.017*\"love\" + 0.017*\"recommend\" + 0.014*\"buy\" + 0.014*\"price\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.432): 0.033*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"pain\" + 0.015*\"week\" + 0.015*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"like\"\n",
      "INFO : topic #4 (0.105): 0.051*\"hair\" + 0.043*\"pedometer\" + 0.022*\"skin\" + 0.020*\"eye\" + 0.019*\"nail\" + 0.017*\"grow\" + 0.013*\"step\" + 0.012*\"use\" + 0.011*\"notice\" + 0.011*\"omron\"\n",
      "INFO : topic diff=0.087162, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #49000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.58832252, 0.55591452, 0.99328715, 0.44250309, 0.10425998]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.588): 0.022*\"help\" + 0.016*\"supplement\" + 0.011*\"year\" + 0.010*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"try\" + 0.009*\"recommend\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.556): 0.029*\"taste\" + 0.029*\"good\" + 0.020*\"like\" + 0.018*\"easy\" + 0.015*\"vitamin\" + 0.014*\"great\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"water\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.993): 0.087*\"product\" + 0.048*\"great\" + 0.040*\"use\" + 0.039*\"good\" + 0.020*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.014*\"price\" + 0.014*\"buy\" + 0.011*\"order\"\n",
      "INFO : topic #3 (0.443): 0.033*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.015*\"week\" + 0.015*\"pain\" + 0.015*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"like\"\n",
      "INFO : topic #4 (0.104): 0.052*\"hair\" + 0.038*\"pedometer\" + 0.023*\"skin\" + 0.020*\"nail\" + 0.020*\"eye\" + 0.018*\"grow\" + 0.012*\"use\" + 0.012*\"notice\" + 0.011*\"step\" + 0.009*\"omron\"\n",
      "INFO : topic diff=0.092046, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #56000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.60846967, 0.55845636, 0.99931765, 0.45422611, 0.10385715]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.608): 0.022*\"help\" + 0.016*\"supplement\" + 0.012*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"'s\" + 0.009*\"try\" + 0.009*\"recommend\"\n",
      "INFO : topic #1 (0.558): 0.030*\"taste\" + 0.029*\"good\" + 0.020*\"like\" + 0.017*\"easy\" + 0.014*\"vitamin\" + 0.014*\"great\" + 0.011*\"supplement\" + 0.010*\"water\" + 0.010*\"pill\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (0.999): 0.089*\"product\" + 0.048*\"great\" + 0.040*\"use\" + 0.040*\"good\" + 0.021*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.014*\"price\" + 0.014*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.454): 0.033*\"day\" + 0.026*\"work\" + 0.020*\"feel\" + 0.016*\"pain\" + 0.015*\"week\" + 0.015*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"use\"\n",
      "INFO : topic #4 (0.104): 0.050*\"hair\" + 0.035*\"pedometer\" + 0.023*\"skin\" + 0.019*\"eye\" + 0.019*\"nail\" + 0.018*\"grow\" + 0.012*\"use\" + 0.012*\"notice\" + 0.010*\"step\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.095638, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #63000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.63007599, 0.56327814, 1.0128204, 0.4602859, 0.10365838]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.630): 0.022*\"help\" + 0.015*\"supplement\" + 0.012*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"'s\" + 0.009*\"try\" + 0.008*\"recommend\"\n",
      "INFO : topic #1 (0.563): 0.030*\"taste\" + 0.029*\"good\" + 0.021*\"like\" + 0.017*\"easy\" + 0.014*\"vitamin\" + 0.014*\"great\" + 0.011*\"supplement\" + 0.011*\"water\" + 0.010*\"pill\" + 0.009*\"drink\"\n",
      "INFO : topic #2 (1.013): 0.089*\"product\" + 0.049*\"great\" + 0.041*\"good\" + 0.040*\"use\" + 0.021*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.460): 0.033*\"day\" + 0.026*\"work\" + 0.020*\"feel\" + 0.015*\"pain\" + 0.015*\"week\" + 0.015*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"use\"\n",
      "INFO : topic #4 (0.104): 0.051*\"hair\" + 0.031*\"pedometer\" + 0.023*\"skin\" + 0.020*\"eye\" + 0.019*\"nail\" + 0.019*\"grow\" + 0.012*\"notice\" + 0.012*\"use\" + 0.009*\"step\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.085452, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #70000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.6489408, 0.55952513, 1.0295793, 0.47188967, 0.10461122]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.649): 0.022*\"help\" + 0.015*\"supplement\" + 0.012*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"'s\" + 0.009*\"good\" + 0.009*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.560): 0.030*\"taste\" + 0.029*\"good\" + 0.021*\"like\" + 0.017*\"easy\" + 0.014*\"vitamin\" + 0.013*\"great\" + 0.011*\"supplement\" + 0.011*\"water\" + 0.010*\"pill\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (1.030): 0.089*\"product\" + 0.049*\"great\" + 0.041*\"good\" + 0.040*\"use\" + 0.021*\"work\" + 0.017*\"recommend\" + 0.017*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.472): 0.034*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"pain\" + 0.016*\"week\" + 0.016*\"start\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"like\"\n",
      "INFO : topic #4 (0.105): 0.056*\"hair\" + 0.025*\"pedometer\" + 0.021*\"skin\" + 0.020*\"eye\" + 0.019*\"grow\" + 0.019*\"nail\" + 0.012*\"notice\" + 0.012*\"use\" + 0.010*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.093095, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #77000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.65988982, 0.57037055, 1.0422271, 0.47882223, 0.10484273]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.660): 0.022*\"help\" + 0.015*\"supplement\" + 0.012*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"'s\" + 0.009*\"good\" + 0.009*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.570): 0.030*\"taste\" + 0.029*\"good\" + 0.020*\"like\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.013*\"great\" + 0.010*\"water\" + 0.010*\"supplement\" + 0.010*\"brand\" + 0.009*\"add\"\n",
      "INFO : topic #2 (1.042): 0.088*\"product\" + 0.049*\"great\" + 0.042*\"use\" + 0.041*\"good\" + 0.021*\"work\" + 0.017*\"recommend\" + 0.016*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.479): 0.034*\"day\" + 0.025*\"work\" + 0.020*\"feel\" + 0.016*\"pain\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"like\"\n",
      "INFO : topic #4 (0.105): 0.056*\"hair\" + 0.021*\"pedometer\" + 0.021*\"skin\" + 0.021*\"eye\" + 0.020*\"nail\" + 0.019*\"grow\" + 0.012*\"notice\" + 0.012*\"use\" + 0.010*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.085478, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #84000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.67723644, 0.57177299, 1.0521353, 0.48753858, 0.10561686]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.677): 0.022*\"help\" + 0.016*\"supplement\" + 0.012*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"'s\" + 0.009*\"good\" + 0.009*\"recommend\" + 0.009*\"try\"\n",
      "INFO : topic #1 (0.572): 0.030*\"taste\" + 0.030*\"good\" + 0.020*\"like\" + 0.016*\"easy\" + 0.014*\"vitamin\" + 0.013*\"great\" + 0.011*\"supplement\" + 0.010*\"water\" + 0.010*\"brand\" + 0.009*\"drink\"\n",
      "INFO : topic #2 (1.052): 0.090*\"product\" + 0.049*\"great\" + 0.042*\"good\" + 0.042*\"use\" + 0.021*\"work\" + 0.017*\"recommend\" + 0.016*\"love\" + 0.015*\"price\" + 0.014*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.488): 0.033*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.017*\"pain\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"like\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.106): 0.062*\"hair\" + 0.022*\"skin\" + 0.021*\"grow\" + 0.020*\"nail\" + 0.020*\"eye\" + 0.017*\"pedometer\" + 0.012*\"notice\" + 0.011*\"use\" + 0.011*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.082622, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #91000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.6948871, 0.57869518, 1.0531214, 0.49431226, 0.10628991]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.695): 0.022*\"help\" + 0.017*\"supplement\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"day\" + 0.009*\"'s\" + 0.009*\"good\" + 0.009*\"recommend\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.579): 0.029*\"good\" + 0.029*\"taste\" + 0.020*\"like\" + 0.017*\"easy\" + 0.016*\"vitamin\" + 0.013*\"great\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"water\" + 0.009*\"pill\"\n",
      "INFO : topic #2 (1.053): 0.090*\"product\" + 0.049*\"great\" + 0.043*\"good\" + 0.042*\"use\" + 0.021*\"work\" + 0.018*\"recommend\" + 0.016*\"love\" + 0.016*\"price\" + 0.015*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.494): 0.034*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.017*\"pain\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"energy\"\n",
      "INFO : topic #4 (0.106): 0.059*\"hair\" + 0.027*\"eye\" + 0.022*\"skin\" + 0.021*\"nail\" + 0.021*\"grow\" + 0.014*\"pedometer\" + 0.013*\"notice\" + 0.011*\"use\" + 0.010*\"month\" + 0.009*\"lutein\"\n",
      "INFO : topic diff=0.088132, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #98000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.69885004, 0.59312189, 1.0713453, 0.49797213, 0.10706154]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.699): 0.022*\"help\" + 0.017*\"supplement\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.009*\"work\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.009*\"'s\" + 0.009*\"good\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.593): 0.029*\"good\" + 0.029*\"taste\" + 0.020*\"like\" + 0.016*\"vitamin\" + 0.016*\"easy\" + 0.013*\"great\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"water\" + 0.009*\"drink\"\n",
      "INFO : topic #2 (1.071): 0.090*\"product\" + 0.050*\"great\" + 0.043*\"good\" + 0.042*\"use\" + 0.021*\"work\" + 0.018*\"recommend\" + 0.016*\"price\" + 0.016*\"love\" + 0.015*\"buy\" + 0.011*\"year\"\n",
      "INFO : topic #3 (0.498): 0.034*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.017*\"pain\" + 0.016*\"start\" + 0.015*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.107): 0.058*\"hair\" + 0.031*\"eye\" + 0.022*\"skin\" + 0.020*\"grow\" + 0.020*\"nail\" + 0.013*\"notice\" + 0.012*\"pedometer\" + 0.011*\"use\" + 0.010*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.077412, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #105000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.71110398, 0.59861171, 1.0873184, 0.51059324, 0.10728222]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.711): 0.022*\"help\" + 0.016*\"supplement\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.010*\"work\" + 0.009*\"'s\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"good\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.599): 0.029*\"good\" + 0.029*\"taste\" + 0.020*\"like\" + 0.016*\"vitamin\" + 0.016*\"easy\" + 0.013*\"great\" + 0.010*\"supplement\" + 0.010*\"water\" + 0.010*\"brand\" + 0.010*\"drink\"\n",
      "INFO : topic #2 (1.087): 0.089*\"product\" + 0.050*\"great\" + 0.043*\"good\" + 0.042*\"use\" + 0.021*\"work\" + 0.018*\"recommend\" + 0.016*\"price\" + 0.016*\"love\" + 0.015*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.511): 0.034*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.017*\"pain\" + 0.016*\"start\" + 0.015*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.107): 0.057*\"hair\" + 0.031*\"eye\" + 0.022*\"skin\" + 0.021*\"nail\" + 0.020*\"grow\" + 0.012*\"notice\" + 0.011*\"use\" + 0.011*\"pedometer\" + 0.010*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.081825, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #112000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.72267038, 0.61772639, 1.0998449, 0.51212394, 0.10789857]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.723): 0.022*\"help\" + 0.017*\"supplement\" + 0.011*\"year\" + 0.011*\"doctor\" + 0.009*\"work\" + 0.009*\"'s\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"try\" + 0.008*\"good\"\n",
      "INFO : topic #1 (0.618): 0.029*\"taste\" + 0.028*\"good\" + 0.020*\"like\" + 0.017*\"vitamin\" + 0.017*\"easy\" + 0.012*\"great\" + 0.011*\"supplement\" + 0.010*\"brand\" + 0.010*\"pill\" + 0.009*\"water\"\n",
      "INFO : topic #2 (1.100): 0.089*\"product\" + 0.050*\"great\" + 0.044*\"good\" + 0.042*\"use\" + 0.021*\"work\" + 0.018*\"recommend\" + 0.017*\"price\" + 0.016*\"love\" + 0.015*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.512): 0.035*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.017*\"pain\" + 0.016*\"start\" + 0.015*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.010*\"energy\"\n",
      "INFO : topic #4 (0.108): 0.056*\"hair\" + 0.033*\"eye\" + 0.022*\"skin\" + 0.021*\"nail\" + 0.019*\"grow\" + 0.012*\"notice\" + 0.011*\"use\" + 0.010*\"month\" + 0.009*\"pedometer\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.081783, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #119000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.72646469, 0.63001752, 1.1057924, 0.51536083, 0.10801802]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.726): 0.022*\"help\" + 0.017*\"supplement\" + 0.012*\"doctor\" + 0.012*\"year\" + 0.009*\"'s\" + 0.009*\"work\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"try\" + 0.008*\"good\"\n",
      "INFO : topic #1 (0.630): 0.031*\"taste\" + 0.028*\"good\" + 0.020*\"like\" + 0.019*\"vitamin\" + 0.017*\"easy\" + 0.012*\"great\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"brand\" + 0.009*\"water\"\n",
      "INFO : topic #2 (1.106): 0.089*\"product\" + 0.050*\"great\" + 0.044*\"good\" + 0.041*\"use\" + 0.021*\"work\" + 0.018*\"recommend\" + 0.017*\"price\" + 0.016*\"love\" + 0.016*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.515): 0.035*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"pain\" + 0.016*\"start\" + 0.015*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.108): 0.057*\"hair\" + 0.032*\"eye\" + 0.024*\"nail\" + 0.023*\"skin\" + 0.021*\"grow\" + 0.013*\"notice\" + 0.010*\"use\" + 0.010*\"month\" + 0.010*\"strong\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.075995, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #126000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.72685403, 0.65538442, 1.1249812, 0.51699489, 0.10846896]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.727): 0.022*\"help\" + 0.017*\"supplement\" + 0.012*\"doctor\" + 0.012*\"year\" + 0.009*\"'s\" + 0.009*\"work\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.655): 0.032*\"taste\" + 0.029*\"good\" + 0.020*\"like\" + 0.019*\"vitamin\" + 0.017*\"easy\" + 0.012*\"great\" + 0.011*\"fish_oil\" + 0.011*\"supplement\" + 0.011*\"pill\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (1.125): 0.090*\"product\" + 0.051*\"great\" + 0.045*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"price\" + 0.017*\"love\" + 0.016*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.517): 0.035*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"pain\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.012*\"energy\"\n",
      "INFO : topic #4 (0.108): 0.059*\"hair\" + 0.030*\"eye\" + 0.027*\"nail\" + 0.025*\"skin\" + 0.022*\"grow\" + 0.013*\"notice\" + 0.010*\"strong\" + 0.010*\"use\" + 0.010*\"month\" + 0.009*\"look\"\n",
      "INFO : topic diff=0.076158, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #133000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.72215754, 0.67394555, 1.1435027, 0.51689726, 0.1094664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.722): 0.022*\"help\" + 0.017*\"supplement\" + 0.013*\"doctor\" + 0.012*\"year\" + 0.010*\"'s\" + 0.009*\"work\" + 0.009*\"day\" + 0.009*\"recommend\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.674): 0.033*\"taste\" + 0.028*\"good\" + 0.020*\"like\" + 0.018*\"vitamin\" + 0.017*\"easy\" + 0.012*\"great\" + 0.012*\"fish_oil\" + 0.011*\"supplement\" + 0.010*\"pill\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (1.144): 0.089*\"product\" + 0.051*\"great\" + 0.045*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.018*\"recommend\" + 0.018*\"price\" + 0.017*\"love\" + 0.016*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.517): 0.035*\"day\" + 0.025*\"work\" + 0.021*\"feel\" + 0.016*\"start\" + 0.015*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.011*\"energy\"\n",
      "INFO : topic #4 (0.109): 0.061*\"hair\" + 0.029*\"eye\" + 0.029*\"skin\" + 0.025*\"nail\" + 0.022*\"grow\" + 0.013*\"notice\" + 0.011*\"coconut_oil\" + 0.010*\"use\" + 0.010*\"strong\" + 0.010*\"month\"\n",
      "INFO : topic diff=0.077560, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #140000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.73558772, 0.6777941, 1.169037, 0.52945954, 0.11039574]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.736): 0.022*\"help\" + 0.017*\"supplement\" + 0.013*\"doctor\" + 0.012*\"year\" + 0.010*\"'s\" + 0.009*\"work\" + 0.009*\"recommend\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.678): 0.033*\"taste\" + 0.028*\"good\" + 0.020*\"like\" + 0.018*\"vitamin\" + 0.017*\"easy\" + 0.012*\"great\" + 0.011*\"fish_oil\" + 0.010*\"pill\" + 0.010*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (1.169): 0.089*\"product\" + 0.051*\"great\" + 0.046*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.018*\"recommend\" + 0.018*\"price\" + 0.017*\"love\" + 0.016*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.529): 0.035*\"day\" + 0.025*\"work\" + 0.022*\"feel\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"energy\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.110): 0.058*\"hair\" + 0.028*\"eye\" + 0.028*\"skin\" + 0.023*\"nail\" + 0.021*\"grow\" + 0.013*\"notice\" + 0.011*\"gaia\" + 0.010*\"strong\" + 0.010*\"use\" + 0.010*\"month\"\n",
      "INFO : topic diff=0.082315, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #147000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.73975337, 0.69239402, 1.1836246, 0.5357489, 0.11060176]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.740): 0.021*\"help\" + 0.016*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.010*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.692): 0.033*\"taste\" + 0.028*\"good\" + 0.020*\"like\" + 0.018*\"vitamin\" + 0.018*\"easy\" + 0.012*\"great\" + 0.011*\"fish_oil\" + 0.010*\"pill\" + 0.010*\"supplement\" + 0.010*\"brand\"\n",
      "INFO : topic #2 (1.184): 0.090*\"product\" + 0.051*\"great\" + 0.046*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"year\"\n",
      "INFO : topic #3 (0.536): 0.035*\"day\" + 0.025*\"work\" + 0.023*\"feel\" + 0.016*\"start\" + 0.016*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"energy\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.111): 0.055*\"hair\" + 0.029*\"skin\" + 0.028*\"eye\" + 0.023*\"nail\" + 0.021*\"grow\" + 0.013*\"notice\" + 0.010*\"gaia\" + 0.010*\"coconut_oil\" + 0.010*\"strong\" + 0.010*\"use\"\n",
      "INFO : topic diff=0.078747, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #154000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.74577546, 0.70357549, 1.1983434, 0.54361171, 0.11136749]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.746): 0.021*\"help\" + 0.017*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.010*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.704): 0.033*\"taste\" + 0.027*\"good\" + 0.021*\"like\" + 0.018*\"easy\" + 0.018*\"vitamin\" + 0.011*\"great\" + 0.010*\"pill\" + 0.010*\"fish_oil\" + 0.010*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (1.198): 0.090*\"product\" + 0.051*\"great\" + 0.045*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.544): 0.035*\"day\" + 0.025*\"work\" + 0.022*\"feel\" + 0.016*\"start\" + 0.016*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"energy\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.111): 0.055*\"hair\" + 0.032*\"eye\" + 0.028*\"skin\" + 0.023*\"nail\" + 0.023*\"grow\" + 0.013*\"notice\" + 0.010*\"strong\" + 0.010*\"use\" + 0.010*\"month\" + 0.010*\"dry\"\n",
      "INFO : topic diff=0.080653, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #161000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.74282563, 0.69743854, 1.2039766, 0.54362404, 0.11438257]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.743): 0.021*\"help\" + 0.017*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.010*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"problem\"\n",
      "INFO : topic #1 (0.697): 0.032*\"taste\" + 0.027*\"good\" + 0.021*\"like\" + 0.019*\"vitamin\" + 0.018*\"easy\" + 0.011*\"great\" + 0.011*\"pill\" + 0.010*\"supplement\" + 0.010*\"fish_oil\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (1.204): 0.091*\"product\" + 0.050*\"great\" + 0.046*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.544): 0.035*\"day\" + 0.025*\"work\" + 0.023*\"feel\" + 0.017*\"start\" + 0.016*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.012*\"energy\"\n",
      "INFO : topic #4 (0.114): 0.083*\"hair\" + 0.037*\"nail\" + 0.031*\"grow\" + 0.028*\"skin\" + 0.024*\"eye\" + 0.014*\"biotin\" + 0.014*\"notice\" + 0.013*\"strong\" + 0.012*\"month\" + 0.010*\"use\"\n",
      "INFO : topic diff=0.115641, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #168000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.7559818, 0.70860189, 1.2185836, 0.55080873, 0.11526524]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.756): 0.021*\"help\" + 0.017*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.009*\"day\" + 0.008*\"try\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.709): 0.032*\"taste\" + 0.027*\"good\" + 0.021*\"like\" + 0.019*\"vitamin\" + 0.018*\"easy\" + 0.011*\"pill\" + 0.011*\"great\" + 0.011*\"fish_oil\" + 0.011*\"supplement\" + 0.009*\"brand\"\n",
      "INFO : topic #2 (1.219): 0.092*\"product\" + 0.051*\"great\" + 0.046*\"good\" + 0.040*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.017*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.551): 0.035*\"day\" + 0.025*\"work\" + 0.023*\"feel\" + 0.017*\"start\" + 0.016*\"week\" + 0.016*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.012*\"time\" + 0.012*\"energy\"\n",
      "INFO : topic #4 (0.115): 0.081*\"hair\" + 0.037*\"nail\" + 0.031*\"grow\" + 0.028*\"skin\" + 0.024*\"eye\" + 0.015*\"biotin\" + 0.014*\"notice\" + 0.013*\"strong\" + 0.012*\"month\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.081944, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #175000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.76851642, 0.71608806, 1.2498375, 0.55911183, 0.11572336]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.769): 0.022*\"help\" + 0.018*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.008*\"day\" + 0.008*\"problem\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.716): 0.031*\"taste\" + 0.026*\"good\" + 0.021*\"like\" + 0.020*\"vitamin\" + 0.019*\"easy\" + 0.011*\"pill\" + 0.011*\"great\" + 0.011*\"supplement\" + 0.011*\"fish_oil\" + 0.010*\"oil\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (1.250): 0.092*\"product\" + 0.051*\"great\" + 0.046*\"good\" + 0.039*\"use\" + 0.019*\"work\" + 0.019*\"recommend\" + 0.017*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.559): 0.035*\"day\" + 0.025*\"work\" + 0.023*\"feel\" + 0.017*\"start\" + 0.016*\"week\" + 0.015*\"pain\" + 0.014*\"try\" + 0.013*\"help\" + 0.013*\"energy\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.116): 0.076*\"hair\" + 0.036*\"nail\" + 0.031*\"skin\" + 0.030*\"grow\" + 0.024*\"eye\" + 0.015*\"biotin\" + 0.014*\"notice\" + 0.014*\"strong\" + 0.011*\"month\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.080739, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #182000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.77105194, 0.72351152, 1.2593049, 0.57020313, 0.11601435]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.771): 0.021*\"help\" + 0.018*\"supplement\" + 0.013*\"doctor\" + 0.011*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.009*\"work\" + 0.008*\"day\" + 0.008*\"health\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.724): 0.032*\"taste\" + 0.026*\"good\" + 0.021*\"like\" + 0.019*\"vitamin\" + 0.019*\"easy\" + 0.011*\"pill\" + 0.011*\"supplement\" + 0.011*\"great\" + 0.010*\"fish_oil\" + 0.010*\"oil\"\n",
      "INFO : topic #2 (1.259): 0.094*\"product\" + 0.051*\"great\" + 0.046*\"good\" + 0.039*\"use\" + 0.019*\"work\" + 0.019*\"recommend\" + 0.017*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.570): 0.035*\"day\" + 0.025*\"work\" + 0.023*\"feel\" + 0.017*\"week\" + 0.017*\"start\" + 0.014*\"try\" + 0.014*\"pain\" + 0.014*\"energy\" + 0.013*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.116): 0.075*\"hair\" + 0.035*\"nail\" + 0.032*\"skin\" + 0.029*\"grow\" + 0.024*\"eye\" + 0.014*\"notice\" + 0.013*\"strong\" + 0.013*\"biotin\" + 0.012*\"month\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.087450, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #189000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.76982582, 0.72586656, 1.2701968, 0.5866316, 0.11679231]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.770): 0.021*\"help\" + 0.018*\"supplement\" + 0.013*\"doctor\" + 0.010*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.008*\"work\" + 0.008*\"day\" + 0.008*\"try\" + 0.008*\"health\"\n",
      "INFO : topic #1 (0.726): 0.033*\"taste\" + 0.026*\"good\" + 0.021*\"like\" + 0.019*\"easy\" + 0.018*\"vitamin\" + 0.011*\"pill\" + 0.011*\"great\" + 0.011*\"supplement\" + 0.010*\"fish_oil\" + 0.009*\"oil\"\n",
      "INFO : topic #2 (1.270): 0.095*\"product\" + 0.052*\"great\" + 0.046*\"good\" + 0.039*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.017*\"price\" + 0.016*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.587): 0.034*\"day\" + 0.026*\"work\" + 0.023*\"feel\" + 0.017*\"week\" + 0.016*\"start\" + 0.015*\"try\" + 0.014*\"energy\" + 0.014*\"pain\" + 0.012*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.117): 0.076*\"hair\" + 0.035*\"nail\" + 0.032*\"skin\" + 0.029*\"grow\" + 0.023*\"eye\" + 0.014*\"biotin\" + 0.014*\"notice\" + 0.014*\"strong\" + 0.011*\"month\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.081888, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #196000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.77368599, 0.72839767, 1.2814915, 0.60473365, 0.11723571]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.774): 0.021*\"help\" + 0.019*\"supplement\" + 0.012*\"doctor\" + 0.010*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.008*\"work\" + 0.008*\"health\" + 0.008*\"try\" + 0.008*\"day\"\n",
      "INFO : topic #1 (0.728): 0.032*\"taste\" + 0.026*\"good\" + 0.022*\"like\" + 0.019*\"vitamin\" + 0.018*\"easy\" + 0.011*\"supplement\" + 0.011*\"great\" + 0.011*\"pill\" + 0.010*\"fish_oil\" + 0.009*\"capsule\"\n",
      "INFO : topic #2 (1.281): 0.095*\"product\" + 0.053*\"great\" + 0.047*\"good\" + 0.039*\"use\" + 0.020*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.016*\"price\" + 0.016*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.605): 0.034*\"day\" + 0.026*\"work\" + 0.024*\"feel\" + 0.017*\"week\" + 0.016*\"start\" + 0.015*\"try\" + 0.015*\"energy\" + 0.014*\"pain\" + 0.012*\"help\" + 0.012*\"time\"\n",
      "INFO : topic #4 (0.117): 0.072*\"hair\" + 0.033*\"skin\" + 0.033*\"nail\" + 0.028*\"grow\" + 0.024*\"eye\" + 0.014*\"notice\" + 0.014*\"biotin\" + 0.014*\"strong\" + 0.011*\"month\" + 0.010*\"look\"\n",
      "INFO : topic diff=0.086309, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #203000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.77161694, 0.72934729, 1.3049535, 0.61370128, 0.11846669]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.772): 0.021*\"help\" + 0.020*\"supplement\" + 0.012*\"doctor\" + 0.010*\"year\" + 0.009*\"'s\" + 0.009*\"recommend\" + 0.008*\"health\" + 0.008*\"work\" + 0.008*\"try\" + 0.008*\"day\"\n",
      "INFO : topic #1 (0.729): 0.032*\"taste\" + 0.026*\"good\" + 0.022*\"like\" + 0.019*\"vitamin\" + 0.019*\"easy\" + 0.013*\"fish_oil\" + 0.011*\"supplement\" + 0.011*\"pill\" + 0.010*\"great\" + 0.009*\"capsule\"\n",
      "INFO : topic #2 (1.305): 0.095*\"product\" + 0.052*\"great\" + 0.046*\"good\" + 0.039*\"use\" + 0.019*\"work\" + 0.019*\"recommend\" + 0.018*\"love\" + 0.016*\"price\" + 0.015*\"buy\" + 0.013*\"order\"\n",
      "INFO : topic #3 (0.614): 0.034*\"day\" + 0.025*\"work\" + 0.025*\"feel\" + 0.018*\"week\" + 0.016*\"start\" + 0.015*\"try\" + 0.015*\"energy\" + 0.014*\"pain\" + 0.012*\"help\" + 0.011*\"time\"\n",
      "INFO : topic #4 (0.118): 0.068*\"hair\" + 0.043*\"skin\" + 0.032*\"nail\" + 0.026*\"grow\" + 0.024*\"eye\" + 0.015*\"notice\" + 0.014*\"biotin\" + 0.013*\"strong\" + 0.012*\"look\" + 0.011*\"month\"\n",
      "INFO : topic diff=0.087327, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #210000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.77725559, 0.74135983, 1.3284314, 0.62766427, 0.11998156]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.777): 0.022*\"supplement\" + 0.021*\"help\" + 0.011*\"doctor\" + 0.010*\"year\" + 0.009*\"'s\" + 0.009*\"health\" + 0.009*\"recommend\" + 0.008*\"probiotic\" + 0.008*\"body\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.741): 0.031*\"taste\" + 0.024*\"good\" + 0.022*\"like\" + 0.020*\"easy\" + 0.019*\"vitamin\" + 0.013*\"fish_oil\" + 0.013*\"supplement\" + 0.011*\"pill\" + 0.010*\"great\" + 0.009*\"capsule\"\n",
      "INFO : topic #2 (1.328): 0.097*\"product\" + 0.052*\"great\" + 0.045*\"good\" + 0.038*\"use\" + 0.019*\"recommend\" + 0.018*\"work\" + 0.017*\"love\" + 0.015*\"buy\" + 0.015*\"price\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.628): 0.033*\"day\" + 0.027*\"feel\" + 0.024*\"work\" + 0.019*\"week\" + 0.016*\"start\" + 0.016*\"energy\" + 0.015*\"try\" + 0.014*\"pain\" + 0.012*\"help\" + 0.012*\"notice\"\n",
      "INFO : topic #4 (0.120): 0.061*\"hair\" + 0.046*\"skin\" + 0.029*\"nail\" + 0.024*\"eye\" + 0.023*\"grow\" + 0.020*\"naturewise\" + 0.017*\"notice\" + 0.013*\"strong\" + 0.013*\"look\" + 0.012*\"biotin\"\n",
      "INFO : topic diff=0.095449, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #217000/217530\n",
      "DEBUG : performing inference on a chunk of 7000 documents\n",
      "DEBUG : 7000/7000 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.7741518, 0.7240175, 1.3532621, 0.65354717, 0.12175106]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 7000 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.774): 0.024*\"supplement\" + 0.021*\"help\" + 0.011*\"doctor\" + 0.009*\"year\" + 0.009*\"probiotic\" + 0.009*\"health\" + 0.009*\"recommend\" + 0.008*\"'s\" + 0.008*\"body\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.724): 0.030*\"taste\" + 0.024*\"good\" + 0.022*\"like\" + 0.020*\"easy\" + 0.018*\"vitamin\" + 0.014*\"supplement\" + 0.013*\"fish_oil\" + 0.012*\"pill\" + 0.010*\"great\" + 0.010*\"capsule\"\n",
      "INFO : topic #2 (1.353): 0.097*\"product\" + 0.051*\"great\" + 0.043*\"good\" + 0.037*\"use\" + 0.019*\"recommend\" + 0.018*\"work\" + 0.017*\"love\" + 0.015*\"buy\" + 0.014*\"price\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.654): 0.032*\"day\" + 0.027*\"feel\" + 0.024*\"work\" + 0.020*\"week\" + 0.017*\"start\" + 0.016*\"try\" + 0.016*\"energy\" + 0.013*\"pain\" + 0.013*\"notice\" + 0.012*\"help\"\n",
      "INFO : topic #4 (0.122): 0.062*\"skin\" + 0.059*\"hair\" + 0.025*\"nail\" + 0.023*\"eye\" + 0.022*\"grow\" + 0.018*\"notice\" + 0.016*\"look\" + 0.016*\"naturewise\" + 0.012*\"dry\" + 0.012*\"strong\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.102289, rho=0.168848\n",
      "INFO : PROGRESS: pass 3, at document #217530/217530\n",
      "DEBUG : performing inference on a chunk of 530 documents\n",
      "DEBUG : 530/530 documents converged within 400 iterations\n",
      "INFO : optimized alpha [0.76974219, 0.70556438, 1.3840002, 0.67875957, 0.12086617]\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 530 documents into a model of 217530 documents\n",
      "INFO : topic #0 (0.770): 0.026*\"supplement\" + 0.022*\"help\" + 0.010*\"doctor\" + 0.010*\"probiotic\" + 0.010*\"health\" + 0.009*\"body\" + 0.009*\"recommend\" + 0.009*\"year\" + 0.009*\"try\" + 0.008*\"'s\"\n",
      "INFO : topic #1 (0.706): 0.030*\"taste\" + 0.024*\"good\" + 0.022*\"like\" + 0.021*\"easy\" + 0.017*\"vitamin\" + 0.017*\"fish_oil\" + 0.015*\"supplement\" + 0.014*\"pill\" + 0.010*\"great\" + 0.010*\"omega\"\n",
      "INFO : topic #2 (1.384): 0.100*\"product\" + 0.052*\"great\" + 0.043*\"good\" + 0.036*\"use\" + 0.020*\"recommend\" + 0.018*\"love\" + 0.018*\"work\" + 0.014*\"buy\" + 0.013*\"price\" + 0.012*\"order\"\n",
      "INFO : topic #3 (0.679): 0.030*\"day\" + 0.029*\"feel\" + 0.024*\"work\" + 0.019*\"week\" + 0.017*\"start\" + 0.017*\"try\" + 0.016*\"energy\" + 0.015*\"pain\" + 0.013*\"help\" + 0.013*\"notice\"\n",
      "INFO : topic #4 (0.121): 0.067*\"skin\" + 0.057*\"hair\" + 0.025*\"nail\" + 0.023*\"eye\" + 0.020*\"notice\" + 0.020*\"grow\" + 0.017*\"look\" + 0.014*\"naturewise\" + 0.012*\"dry\" + 0.011*\"strong\"\n",
      "INFO : topic diff=0.098647, rho=0.168848\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=13850, num_topics=5, decay=0.5, chunksize=7000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LDA for 5 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated coherence score...:  -2.38490514601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 217530 documents\n",
      "DEBUG : 217530/217530 documents converged within 400 iterations\n",
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "100%|██████████| 1/1 [11:43<00:00, 703.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved html file:  pyLDAvis_5.html\n",
      "CPU times: user 11min 43s, sys: 1.9 s, total: 11min 45s\n",
      "Wall time: 11min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics_list = [5]\n",
    "chunksize = 7000    # number of docs processed at a time\n",
    "passes = 4\n",
    "iterations = 400\n",
    "eval_every = None    # change to 1 if you want to check for convergence\n",
    "eta = eta           # asymmetric prior (seeded)\n",
    "alpha = 'auto'       # asymmetric prior learned from data\n",
    "\n",
    "coherenceList_umass = []\n",
    "\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_seeded = LdaModel(bow_corpus, num_topics=num_topics, id2word=vocab_dictionary, chunksize=chunksize, \n",
    "                          passes=passes, eta=eta, alpha=alpha, eval_every=eval_every, iterations=iterations, \n",
    "                          random_state=42)\n",
    "    save_df_s3(lda_seeded, bucket_name, filepath='amazon_reviews/kk/lda_seeded_{}.pkl'.format(num_topics), \n",
    "               filetype='pickle')\n",
    "    print('Trained LDA for {} topics...'.format(num_topics))\n",
    "    \n",
    "    # performance metric\n",
    "    cm = CoherenceModel(model=lda_seeded, corpus=bow_corpus, dictionary=vocab_dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    print('Calculated coherence score...: ', cm.get_coherence())\n",
    "    \n",
    "    # visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_seeded, bow_corpus, vocab_dictionary)\n",
    "    plot_fname = 'pyLDAvis_{}.html'.format(num_topics)\n",
    "    pyLDAvis.save_html(vis, plot_fname)\n",
    "    print('Saved html file: ', plot_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    save_df_s3(lda_seeded, bucket_name, filepath='amazon_reviews/kk/lda_seeded_{}.pkl'.format(num_topics), \n",
    "               filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:53:47.723284Z",
     "start_time": "2018-05-01T22:53:45.846118Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_temp = load_df_s3(bucket_name, filepath='amazon_reviews/kk/lda_seeded_3.pkl', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
